When	O
using	O
all	O
the	O
three	O
layer	O
types	O
,	O
we	O
can	O
obtain	O
the	O
best	O
81.8	B-MetricValue
score	O
,	O
1.4	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
(	O
80.4	B-MetricValue
)	O
and	O
0.6	B-MetricValue
higher	O
than	O
the	O
model	O
searched	O
with	O
only	O
self	O
-	O
attention	O
and	O
feedforward	O
(	O
81.2	B-MetricValue
)	O
.	O

Given	O
that	O
the	O
ASR	B-TaskName
system	I-TaskName
was	O
trained	O
on	O
an	O
underlying	O
orthography	O
,	O
the	O
final	O
result	O
of	O
<	O
10	O
hours	O
of	O
human	O
effort	O
per	O
hour	O
of	O
audio	O
is	O
a	O
transcribed	O
and	O
partially	O
parsed	O
corpus	O
.	O

Generally	O
,	O
this	O
problem	O
can	O
contain	O
single	O
or	O
multiple	O
documents	O
as	O
context	O
(	O
containing	O
relevant	O
information	O
needed	O
to	O
understand	O
and	O
answer	O
the	O
question	O
)	O
,	O
a	O
question	O
(	O
a	O
sentence	O
with	O
at	O
least	O
one	O
asking	O
parameter	O
)	O
,	O
and	O
an	O
answer	O
(	O
which	O
is	O
the	O
parameter	O
value	O
of	O
the	O
question).In	O
the	O
Task	O
of	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
(	O
ReCAM	B-TaskName
)	O
,	O
we	O
have	O
one	O
passage	O
as	O
a	O
context	O
,	O
one	O
question	O
and	O
five	O
candidate	O
answers	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

Table	O
4	O
shows	O
the	O
BERT	B-MethodName
model	O
performance	O
after	O
including	O
three	O
types	O
of	O
augmented	O
data	O
.	O

QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
dataset	O
converted	O
from	O
The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

Starting	O
with	O
the	O
pre	O
-	O
HIT	B-TaskName
phase	O
,	O
the	O
major	O
steps	O
largely	O
center	O
around	O
multiple	O
rounds	O
of	O
qualifications	O
to	O
filter	O
for	O
the	O
highest	O
quality	O
workers	O
available	O
.	O

GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

This	O
implies	O
a	O
lack	O
of	O
LU	B-TaskName
robustness	I-TaskName
in	O
detecting	O
unseen	O
entities	O
.	O

Hence	O
,	O
we	O
set	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

Then	O
we	O
can	O
update	O
the	O
parameters	O
of	O
the	O
BERT	B-MethodName
model	O
and	O
projection	O
head	O
g	O
to	O
minimize	O
the	O
loss	O
L.	O
Input	O
:	O
encoder	O
f	O
(	O
BERT	B-MethodName
)	O
,	O
project	O
head	O
g	O
,	O
data	O
augmentation	O
module	O
,	O
data	O
batch	O
{	O
s	O
k	O
}	O
N	O
k=1	O
;	O
for	O
k=1	O
,	O
...	O
,	O
N	O
do	O
v	O
,	O
v	O
=	O
data_augment(s	O
k	O
)	O
;	O
z	O
2k−1	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
z	O
2k	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
end	O
L	O
=	O
1	O
2N	O
N	O
k=1	O
[	O
l(z	O
2k−1	O
,	O
z	O
2k	O
)	O
+	O
l(z	O
2k	O
,	O
z	O
2k−1	O
)	O
]	O
Figure	O
2	O
shows	O
the	O
training	O
procedure	O
of	O
our	O
framework	O
.	O

Georgian	B-MethodName
segmentation	I-MethodName
models	I-MethodName
.	O

For	O
instance	O
,	O
a	O
question	O
"	O
What	O
are	O
the	O
differences	O
between	O
global	O
warming	O
and	O
climate	O
change	O
?	O
"	O
becomes	O
"	O
What	O
are	O
the	O
differences	O
between	O
[	O
NP	O
]	O
and	O
[	O
NP]?"Exemplars	B-MethodName
for	I-MethodName
Guidance	I-MethodName
(	I-MethodName
EXPLGEN	I-MethodName
)	I-MethodName
.	O

Adaptor	B-MethodName
Grammars	I-MethodName
have	O
proved	O
successful	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
a	O
variety	O
of	O
typologically	O
diverse	O
languages	O
(	O
Eskander	O
et	O
al	O
.	O
,	O

We	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

ASR	B-TaskName
for	I-TaskName
endangered	I-TaskName
languages	I-TaskName
is	O
made	O
difficult	O
not	O
simply	O
because	O
of	O
limited	O
resources	O
for	O
training	O
a	O
robust	O
system	O
but	O
by	O
a	O
series	O
of	O
factors	O
briefly	O
discussed	O
in	O
this	O
section	O
.	O

A	O
slightly	O
surprising	O
observation	O
was	O
that	O
there	O
is	O
a	O
degradation	O
in	O
accuracy	O
on	O
using	O
RoBERTa	B-MethodName
.	O

Meanwhile	O
,	O
adding	O
question	O
types	O
helps	O
BART	B-MethodName
generate	O
more	O
relevant	O
questions	O
than	O
using	O
question	O
words	O
,	O
indicating	O
the	O
value	O
of	O
our	O
new	O
question	O
type	O
ontology	O
.	O

Distri-	O
butions	O
of	O
question	O
types	O
for	O
the	O
two	O
datasets	O
are	O
listed	O
in	O
Table	O
8	O
in	O
Appendix	O
B.4	O
Type	B-TaskName
-	I-TaskName
aware	I-TaskName
Open	I-TaskName
-	I-TaskName
ended	I-TaskName
Question	I-TaskName
GenerationIn	I-TaskName
this	O
section	O
,	O
we	O
present	O
our	O
type	B-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
.	O

SemEval-2021	B-DatasetName
Task-4	I-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

Three	O
combinations	O
have	O
been	O
evaluated	O
:	O
(	O
1	O
)	O
ROVER	B-MethodName
among	O
only	O
linguistic	O
units	O
(	O
i.e.	O
,	O
morae	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
word	B-MethodName
)	O
,	O
(	O
2	O
)	O
ROVER	B-MethodName
among	O
only	O
sub	O
-	O
word	O
units	O
(	O
in	O
this	O
case	O
BPE	B-MethodName
)	O
;	O
and	O
(	O
3	O
)	O
ROVER	B-MethodName
combination	O
utilizing	O
all	O
seven	O
systems	O
.	O

This	O
is	O
because	O
multiple	O
robustness	O
issues	O
may	O
be	O
included	O
in	O
one	O
real	O
case	O
,	O
while	O
each	O
augmentation	O
method	O
in	O
LAUG	B-MethodName
evaluates	O
them	O
separately	O
.	O

We	O
used	O
β	B-HyperparameterName
=	O
80	B-HyperparameterValue
,	O
i.e.	O
,	O
selecting	O
the	O
top	O
20	O
percentile	O
of	O
attention	O
scores	O
to	O
compare	O
with	O
ground	O
truth	O
rationales	O
.	O

Our	O
question	O
type	O
classifiers	O
and	O
template	O
exemplar	O
classifiers	O
are	O
trained	O
with	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
.	O

For	O
subtask	O
-	O
I	O
,	O
to	O
obtain	O
the	O
linguistic	O
features	O
mentioned	O
in	O
3.2	O
,	O
and	O
to	O
obtain	O
the	O
hypernyms	O
and	O
hyponyms	O
for	O
subtask	O
-	O
II	O
,	O
we	O
use	O
the	O
lexical	O
database	O
,	O
WordNet	B-DatasetName
provided	O
by	O
NLTK	O
(	O
Bird	O
and	O
Loper	O
,	O
2004	O
)	O
,	O
a	O
library	O
in	O
Python	O
.	O

Furthermore	O
,	O
unsupervised	O
rationale	O
generation	O
was	O
able	O
to	O
consistently	O
improve	O
the	O
overall	O
LL	B-TaskName
performance	O
from	O
the	O
baseline	O
without	O
relying	O
on	O
human	O
-	O
annotated	O
rationales	O
.	O

Figure	O
2	O
presents	O
results	O
for	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
with	O
an	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
restricted	O
to	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
500	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

We	O
reformulated	O
the	O
longformer	B-MethodName
model	O
to	O
learn	O
abstract	B-TaskName
meaning	I-TaskName
as	I-TaskName
a	I-TaskName
new	I-TaskName
level	I-TaskName
of	I-TaskName
semantic	I-TaskName
in	I-TaskName
machine	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

Both	O
homogeneous	B-MethodName
(	O
Jin	O
et	O
al	O
.	O
,	O

We	O
study	O
the	O
average	O
performance	O
of	O
5	O
runs	O
on	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
set	O
.	O

BiLSTM	B-MethodName
seems	O
relatively	O
less	O
accurate	O
for	O
samples	O
in	O
the	O
supportive	O
range	O
(	O
−0.75	O
to	O
−0.25	O
)	O
.	O

As	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
based	O
from	O
LAMOL	B-MethodName
(	O
Sun	O
et	O
al	O
.	O
,	O

For	O
imperceptibility	B-TaskName
,	O
we	O
create	O
statistical	O
embeddings	O
using	O
features	O
that	O
have	O
a	O
high	O
correlation	O
with	O
concreteness	O
.	O

End	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
is	O
used	O
to	O
rapidly	O
increase	O
corpus	O
size	O
while	O
offering	O
the	O
opportunity	O
to	O
target	O
certain	O
genres	O
(	O
such	O
as	O
expert	O
conversations	O
on	O
the	O
nomenclature	O
,	O
classification	O
,	O
and	O
use	O
of	O
local	O
flora	O
and	O
fauna	O
;	O
ritual	O
discourse	O
;	O
material	O
cultural	O
production	O
;	O
techniques	O
for	O
fishing	O
and	O
hunting	O
)	O
that	O
are	O
of	O
ethnographic	O
interest	O
but	O
are	O
often	O
insufficiently	O
covered	O
in	O
EL	O
documentation	O
projects	O
that	O
struggle	O
to	O
produce	O
large	O
and	O
varied	O
corpora	O
.	O

We	O
also	O
build	O
baseline	O
systems	O
that	O
are	O
variants	O
of	O
standard	O
dialogue	O
models	O
and	O
report	O
their	O
results	O
on	O
ABCD	B-DatasetName
.	O

Prediction	O
accuracies	B-MetricName
(	O
Table	O
4	O
)	O
are	O
over	O
70	B-MetricValue
%	I-MetricValue
with	O
linguistic	O
features	O
only	O
,	O
indicating	O
that	O
the	O
features	O
that	O
we	O
have	O
identified	O
are	O
relatively	O
strong	O
predictors	O
of	O
engagement	O
.	O

Yahoo	B-DatasetName
dataset	O
is	O
more	O
balanced	O
,	O
with	O
PROCEDURAL	O
questions	O
being	O
the	O
most	O
frequent	O
type	O
(	O
19.9	O
%	O
of	O
all	O
samples	O
)	O
.	O

scores	O
for	O
sentence	O
classification	O
tasks	O
(	O
XNLI	O
,	O
PAWS	O
-	O
X	O
)	O
,	O
and	O
F1	B-MetricName
/	O
Exact	B-MetricName
Match	I-MetricName
(	O
F1	B-MetricName
/	O
EM	B-MetricName
)	O
scores	O
for	O
question	O
answering	O
tasks	O
(	O
XQuAD	O
,	O
MLQA	O
,	O
TyDiQA	O
)	O
.	O

BPR	B-MetricName
is	O
the	O
classical	O
metric	O
for	O
evaluating	O
morphological	B-TaskName
segmentation	I-TaskName
;	O
it	O
compares	O
the	O
boundaries	O
in	O
the	O
proposed	O
segmentation	O
to	O
those	O
in	O
the	O
reference	O
.	O

native	O
script	O
and	O
SLP1	O
)	O
described	O
in	O
Section	O
3.1	O
,	O
we	O
study	O
three	O
different	O
units	O
for	O
the	O
acoustic	O
modeling	O
(	O
AM	O
)	O
in	O
ASR	B-TaskName
,	O
viz	O
.	O
,	O

Instead	O
,	O
we	O
take	O
advantage	O
of	O
the	O
observation	O
that	O
some	O
of	O
the	O
topics	O
inferred	O
by	O
the	B-MethodName
LDA	I-MethodName
model	O
correspond	O
to	O
swear	O
words	O
and	O
filler	O
terms	O
,	O
and	O
measure	O
the	O
proportions	O
of	O
these	O
topics	O
.	O

This	O
would	O
make	O
MERGEDISTILL	B-MethodName
highly	O
beneficial	O
for	O
low	O
-	O
resource	O
languages	O
that	O
do	O
not	O
have	O
a	O
strong	O
teacher	O
or	O
limited	O
gold	O
data	O
.	O

For	O
instance	O
(	O
Figure	O
7	O
)	O
,	O
in	O
VQA-2	B-DatasetName
,	O
we	O
identify	O
clusters	O
of	O
hard	O
-	O
to	O
-	O
learn	O
examples	O
that	O
require	O
optical	O
character	O
recognition	O
(	O
OCR	O
)	O
for	O
reasoning	O
about	O
text	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
first	O
word	O
on	O
the	O
black	O
car	O
?	O
"	O
)	O
;	O
another	O
cluster	O
requires	O
external	O
knowledge	O
to	O
answer	O
(	O
"	O
What	O
is	O
the	O
symbol	O
on	O
the	O
hood	O
often	O
associated	O
with	O
?	O
"	O
)	O
.	O

It	O
contains	O
positive	O
and	O
negative	O
sentiment	O
labels	O
to	O
be	O
predicted	O
by	O
the	O
model.•	O
SciFact	B-DatasetName
(	O
Wadden	O
et	O
al	O
.	O
,	O

This	O
could	O
be	O
because	O
even	O
though	O
it	O
was	O
pretrained	O
more	O
robustly	O
than	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
it	O
was	O
not	O
pretrained	O
on	O
the	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
Task	I-TaskName
,	O
and	O
hence	O
,	O
might	O
perform	O
worse	O
on	O
Textual	B-TaskName
Entailment	I-TaskName
tasks	I-TaskName
.	O

Most	O
of	O
the	O
classification	B-MetricName
errors	I-MetricName
(	O
18.54	B-MetricValue
%	I-MetricValue
)	O
were	O
due	O
to	O
incorrect	O
classification	O
of	O
very	O
easy	O
words	O
as	O
easy	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
Lai	O
et	O
al	O
.	O
(	O

Thus	O
,	O
the	O
first	O
Complex	B-TaskName
Word	I-TaskName
Identification	I-TaskName
(	O
CWI	O
)	O
shared	O
task	O
referred	O
to	O
binary	O
identification	O
of	O
complex	O
words	O
(	O
Zampieri	O
et	O
al	O
.	O
,	O

SemEval-2021	B-DatasetName
(	O
Task	O
1	O
)	O
shared	O
task	O
on	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
(	I-TaskName
LCP	I-TaskName
)	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

Rather	O
than	O
expanding	O
wider	O
,	O
ABCD	B-DatasetName
instead	O
focuses	O
deeper	O
by	O
increasing	O
the	O
count	O
and	O
diversity	O
of	O
actions	O
within	O
a	O
single	O
domain	O
.	O

2018	O
downstream	O
tasks	O
with	O
most	O
hyperparameters	O
the	O
same	O
as	O
those	O
of	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

In	O
order	O
to	O
probe	O
the	O
effect	O
of	O
this	O
on	O
the	O
computational	O
models	O
,	O
we	O
created	O
a	O
variant	O
of	O
Ruddit	B-DatasetName
by	O
replacing	O
all	O
the	O
identity	O
terms	O
(	O
from	O
the	O
list	O
given	O
in	O
Appendix	O
A.4	O
)	O
in	O
the	O
comments	O
with	O
the	O
[	O
group	O
]	O
token	O
and	O
observed	O
the	O
effect	O
on	O
the	O
models	O
'	O
per	O
-	O
formance	O
.	O

Then	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
is	O
finetuned	O
on	O
the	O
processed	O
data	O
so	O
that	O
it	O
can	O
be	O
aware	O
of	O
dialog	O
context	O
when	O
generating	O
paraphrases	O
.	O

We	O
propose	O
the	O
Critical	O
Component	O
Identification	O
(	O
CCI	O
)	O
algorithm	O
,	O
pointing	O
out	O
the	O
most	O
plastic	O
block	O
of	O
our	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
LL	I-MethodName
model	I-MethodName
before	O
moving	O
on	O
to	O
a	O
new	O
task	O
completely	O
.	O
(	O

We	O
introduce	O
two	O
types	O
of	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
language	O
-	O
specific	O
grammars	O
;	O
and	O
2	O
)	O
linguistprovided	B-MethodName
affixes	I-MethodName
,	O
collected	O
by	O
an	O
expert	O
in	O
the	O
language	O
and	O
seeded	O
into	O
the	O
grammars	O
.	O

Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
classify	O
the	O
LU	B-TaskName
robustness	I-TaskName
systematically	O
into	O
three	O
aspects	O
that	O
occur	O
in	O
real	O
-	O
world	O
dialog	O
,	O
including	O
linguistic	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
;	O
(	O
2	O
)	O
We	O
propose	O
a	O
general	O
and	O
model	O
-	O
agnostic	O
toolkit	O
,	O
LAUG	B-MethodName
,	O
which	O
is	O
an	O
integration	O
of	O
four	O
data	O
augmentation	O
methods	O
on	O
LU	O
that	O
covers	O
the	O
three	O
aspects	O
.	O
(	O

Specifically	O
,	O
we	O
leverage	O
BERT	B-MethodName
along	O
with	O
some	O
of	O
its	O
popular	O
and	O
successful	O
variants	O
such	O
as	O
:	B-MethodName
Dis	I-MethodName
-	I-MethodName
tilBERT	I-MethodName
,	I-MethodName
ALBERT	I-MethodName
,	I-MethodName
and	I-MethodName
RoBERTa	I-MethodName
.	O

Generally	O
,	O
any	O
combination	O
of	O
{	O
active	O
learning	O
strategy	O
×	O
model	O
×	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
×	O
analysis	O
/	O
acquisition	O
plot	O
}	O
is	O
present	O
in	O
this	O
paper	O
,	O
and	O
is	O
available	O
in	O
the	O
public	O
code	O
repository	O
.	O

Recording	O
conditions	O
:	O
Noisy	O
environments	O
,	O
including	O
overlapping	O
speech	O
,	O
reverberation	O
in	O
indoor	O
recordings	O
,	O
natural	O
sounds	O
in	O
outdoor	O
recordings	O
,	O
less	O
than	O
optimal	O
microphone	O
placement	O
(	O
e.g.	O
,	O
a	O
boom	O
mic	O
in	O
video	O
recordings	O
)	O
,	O
and	O
failure	O
to	O
separately	O
mike	O
speakers	O
for	O
multichannel	O
recordings	O
all	O
negatively	O
impact	O
the	O
accuracy	B-MetricName
of	O
ASR	B-TaskName
output	O
.	O

Table	O
6	O
illustrates	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
dramatically	O
reduce	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
.	O

2019).We	O
fine	O
-	O
tuned	O
HateBERT	B-MethodName
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
.	O

We	O
describe	O
a	O
joint	O
question	O
focus	O
detection	O
and	O
question	O
generation	O
framework	O
with	O
a	O
novel	O
semantic	B-TaskName
graphaugmented	I-TaskName
representation	I-TaskName
,	O
which	O
is	O
directly	O
built	O
on	O
large	O
pre	O
-	O
trained	O
models	O
.	O

2020	O
)	O
maintain	O
an	O
ever	O
-	O
growing	O
list	O
of	O
BERT	B-MethodName
models	O
here	O
most	O
commonly	O
been	O
used	O
for	O
task	O
-	O
specific	O
model	O
compression	O
of	O
a	O
teacher	O
into	O
a	O
single	O
-	O
task	O
student	O
(	O
Tang	O
et	O
al	O
.	O
,	O

Besides	O
this	O
data	O
,	O
the	O
upgraded	O
version	O
SQuAD	B-DatasetName
2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

To	O
ease	O
reproduction	O
,	O
we	O
train	O
models	O
on	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
that	O
is	O
open	O
-	O
sourced	O
and	O
of	O
similar	O
size	O
with	O
the	O
corpus	O
used	O
by	O
BERT	B-DatasetName
.	O

Pre	O
-	O
training	O
a	O
single	O
language	O
model	O
already	O
needs	O
to	O
consume	O
a	O
large	O
amount	O
of	O
computation	O
,	O
e.g.	O
,	O
2400	O
P100	O
GPU	O
days	O
for	O
pre	O
-	O
training	O
BERT	B-MethodName
.	O

A	O
dataset	O
closer	O
to	O
ours	O
is	O
ELI5	B-DatasetName
,	O
which	O
also	O
obtains	O
open	O
-	O
ended	O
questionanswer	O
pairs	O
from	O
Reddit	B-DatasetName
,	O
while	O
one	O
of	O
our	O
datasets	O
includes	O
more	O
Reddit	O
communities	O
and	O
thus	O
covers	O
a	O
wider	O
range	O
of	O
topics	O
.	O

The	O
attention	O
score	O
a	O
i	O
,	O
j	O
is	O
calculated	O
as	O
in	O
GATs	B-MethodName
.	O

We	O
used	O
Mean	O
Squared	O
Error	O
(	O
MSE	O
)	O
loss	O
as	O
the	O
objective	O
function	O
,	O
Adam	B-HyperparameterValue
with	O
0.001	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
the	O
optimizer	B-HyperparameterName
,	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
,	O
and	O
a	O
dropout	B-HyperparameterName
of	O
0.5	B-HyperparameterValue
.	O

Moreover	O
,	O
using	O
unsupervised	O
rationale	O
generation	O
instead	O
of	O
human	O
rationales	O
also	O
yielded	O
competitive	O
performance	O
,	O
achieving	O
average	O
improvements	O
of	O
2.67	B-MetricValue
%	I-MetricValue
from	O
original	O
LAMOL	B-MethodName
.	O

We	O
propose	O
two	O
additional	O
dialog	O
tasks	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
and	O
establish	O
a	O
series	O
of	O
baselines	O
involving	O
large	O
-	O
scale	O
,	O
pre	O
-	O
trained	O
language	O
models	O
on	O
this	O
dataset	O
.	O

A	O
few	O
examples	O
of	O
these	O
are	O
AraBERT	B-MethodName
(	O
Antoun	O
et	O
al	O
.	O
,	O

The	O
action	O
restrictions	O
within	O
ABCD	B-DatasetName
are	O
made	O
explicit	O
by	O
the	O
Agent	O
Guidelines	O
manual	O
.	O

The	O
key	O
contributions	O
of	O
this	O
paper	O
are	O
(	O
1	O
)	O
we	O
introduce	O
and	O
formalize	O
the	O
problem	O
of	O
inferring	B-TaskName
finegrained	I-TaskName
provenance	I-TaskName
for	O
an	O
article	O
;	O
(	O
2	O
)	O
we	O
propose	O
a	O
general	O
framework	O
to	O
infer	O
the	O
source	O
articles	O
that	O
have	O
provided	O
important	O
information	O
for	O
the	O
given	O
article	O
,	O
including	O
(	O
a	O
)	O
a	O
ranking	O
module	O
that	O
can	O
identify	O
sentences	O
that	O
contain	O
important	O
external	O
information	O
based	O
on	O
the	O
main	O
topic	O
and	O
the	O
main	O
entities	O
in	O
the	O
article	O
;	O
(	O
b	O
)	O
a	O
query	O
generator	O
that	O
can	O
generate	O
possible	O
metadata	O
for	O
the	O
source	O
article	O
,	O
e.g.	O
,	O
the	O
title	O
,	O
the	O
published	O
date	O
,	O
the	O
source	O
website	O
,	O
based	O
on	O
the	O
context	O
of	O
the	O
selected	O
sentences	O
;	O
(	O
c	O
)	O
an	O
integer	O
linear	O
program	O
(	O
ILP	O
)	O
based	O
algorithm	O
to	O
jointly	O
identify	O
the	O
source	O
articles	O
from	O
all	O
of	O
the	O
candidates	O
.	O
(	O

And	O
a	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
has	O
been	O
considered	O
to	O
regularize	O
the	O
model	O
and	O
avoid	O
overfitting	O
.	O

Instead	O
,	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
creates	O
an	O
evaluation	O
example	O
for	O
the	O
remainder	O
of	O
each	O
conversation	O
starting	O
from	O
each	O
turn	O
.	O

Among	O
the	O
vanilla	O
models	O
,	O
BERT	B-MethodName
FitB	I-MethodName
Large	I-MethodName
performs	O
the	O
best	O
.	O

YMC	B-DatasetName
(	O
referring	O
only	O
to	O
the	O
Mixtec	O
of	O
the	O
community	O
of	O
Yoloxóchitl	O
[	O
16.81602	O
,	O
-98.68597	O
]	O
)	O
manifests	O
28	O
distinct	O
tonal	O
patterns	O
on	O
1,451	O
to	O
-	O
date	O
identified	O
bimoraic	O
lexical	O
stems	O
.	O

2018	O
)	O
introduce	O
GPT	B-MethodName
that	O
changes	O
the	O
backbone	O
to	O
transformers	O
where	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
are	O
arrayed	O
interleavedly	O
.	O

The	O
experimental	O
results	O
on	O
three	O
relation	O
extraction	O
benchmark	O
datasets	O
demonstrate	O
that	O
our	O
method	O
can	O
improve	O
the	O
BERT	B-MethodName
model	O
representation	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

2016	O
)	O
,	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
.	O
,	O

The	O
disagreement	O
that	O
arises	O
in	O
tuples	O
having	O
two	O
items	O
that	O
are	O
close	O
together	O
in	O
their	O
degree	O
of	O
offensiveness	O
is	O
a	O
useful	O
signal	O
for	O
BWS	B-MethodName
(	O
helping	O
it	O
give	O
similar	O
scores	O
to	O
the	O
two	O
items	O
)	O
.	O

2020).Despite	O
its	O
advancement	O
,	O
contrastive	O
learning	O
has	O
not	O
been	O
well	O
studied	O
in	O
biomedical	O
natural	O
language	O
processing	O
(	O
BioNLP	O
)	O
,	O
especially	O
for	O
relation	B-TaskName
extraction	I-TaskName
(	O
RE	B-TaskName
)	O
tasks	O
.	O

Cloze	O
-	O
Style	O
QAThe	O
first	O
model	O
we	O
employ	O
follows	O
a	O
cloze	B-MethodName
-	I-MethodName
style	I-MethodName
question	I-MethodName
answering	I-MethodName
approach	O
,	O
in	O
which	O
we	O
use	O
various	O
pretrained	O
transformer	O
models	O
as	O
encoders	O
,	O
followed	O
by	O
a	O
decoder	O
layer	O
,	O
which	O
helps	O
us	O
to	O
select	O
the	O
correct	O
answer	O
.	O

In	O
our	O
work	O
,	O
using	O
the	O
BWS	B-MethodName
comparative	O
annotation	O
setup	O
,	O
we	O
assign	O
fine	O
-	O
grained	O
continuous	O
scores	O
to	O
comments	O
to	O
denote	O
their	O
degree	O
of	O
offensiveness	O
.	O

Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
(	O
henceforth	O
YM	B-DatasetName
)	O
,	O
an	O
endangered	O
Mixtecan	O
language	O
spoken	O
in	O
the	O
municipality	O
of	O
San	O
Luis	O
Acatlán	O
,	O
Guerrero	O
,	O
Mexico	O
,	O
is	O
one	O
of	O
some	O
50	O
languages	O
in	O
the	O
Mixtec	O
language	O
family	O
,	O
which	O
is	O
within	O
a	O
larger	O
unit	O
,	O
Otomanguean	O
,	O
that	O
Suárez	O
(	O
1983	O
)	O
considers	O
a	O
hyper	O
-	O
family	O
or	O
stock	O
.	O

We	O
have	O
shown	O
how	O
different	O
approaches	O
can	O
be	O
leveraged	O
to	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
.	O

We	O
use	O
separate	O
cross	O
attentions	O
to	O
attend	O
the	O
representations	O
of	O
the	O
focused	O
words	O
,	O
similar	O
to	O
how	O
node	O
representations	O
are	O
attended	O
in	O
JOINTGEN.We	B-MethodName
recognize	O
that	O
having	O
separate	O
stages	O
of	O
exemplar	O
selection	O
and	O
template	O
generation	O
introduces	O
extra	O
model	O
training	O
cost	O
and	O
potential	O
errors	O
in	O
the	O
pipeline	O
.	O

As	O
described	O
in	O
the	O
introduction	O
,	O
we	O
use	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
engagement	O
metric	O
,	O
defined	O
as	O
the	O
proportion	O
of	O
the	O
show	O
's	O
first	O
-	O
time	O
listeners	O
who	O
stream	O
at	O
least	O
five	O
minutes	O
of	O
the	O
episode	O
.	O

Table	O
3	O
presents	O
the	O
performance	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
pre	O
-	O
trained	O
models	O
on	O
GLUE	B-DatasetName
testing	O
set	O
.	O

This	O
is	O
shown	O
in	O
Table	O
3	O
,	O
where	O
the	O
F1	B-MetricName
scores	O
of	O
InvRat	B-MethodName
are	O
quite	O
low	O
when	O
compared	O
with	O
human	O
rationales	O
.	O

2011	O
;	O
,	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018).Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
.	O

Finally	O
,	O
Ruddit	B-DatasetName
was	O
created	O
with	O
the	O
intention	O
to	O
look	O
at	O
wide	O
ranging	O
offensive	O
language	O
of	O
various	O
degrees	O
as	O
opposed	O
to	O
detecting	O
offensive	O
language	O
towards	O
specific	O
target	O
groups	O
.	O

γ	O
a	O
obtains	O
macro	B-MetricName
F1	I-MetricName
scores	O
of	O
0.48	B-MetricValue
and	O
0.46	B-MetricValue
on	O
the	O
same	O
reserved	O
test	O
set	O
over	O
all	O
types	O
after	O
training	O
on	O
Yahoo	O
and	O
Reddit	O
,	O
respectively	O
.	O

LXMERT	B-MethodName
.	O

Our	O
proposed	O
model	O
is	O
trained	O
for	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
for	O
each	O
task	O
.	O

The	O
last	O
variant	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
(	I-MethodName
concatenation	I-MethodName
)	I-MethodName
also	O
performs	O
lower	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
,	O
further	O
indicating	O
that	O
directly	O
concatenating	O
true	O
entity	O
knowledge	O
is	O
not	O
a	O
good	O
way	O
for	O
incorporating	O
entity	O
knowledge	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
LL	B-TaskName
framework	O
for	O
language	O
models	O
.	O

2020	O
)	O
,	O
and	O
post	B-MethodName
-	I-MethodName
filtering	I-MethodName
.	O

Figure	O
3	O
shows	O
the	O
performance	O
(	O
micro	B-MetricName
and	I-MetricName
macro	I-MetricName
F1	I-MetricName
)	O
of	O
our	O
model	O
CompareNet	B-MethodName
on	O
LUN	B-DatasetName
validation	I-DatasetName
set	O
with	O
different	O
number	O
of	O
top	B-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
P	I-HyperparameterName
to	O
each	O
sentence	O
.	O

Blue	O
and	O
yellow	O
dots	O
denote	O
the	O
accuracy	B-MetricName
of	O
top	O
10	O
candidates	O
for	O
each	O
method	O
respectively	O
,	O
while	O
the	O
plots	O
mean	O
their	O
averages	O
.	O

2007	O
)	O
,	O
a	O
type	O
of	O
nonparametric	O
Bayesian	O
models	O
that	O
generalize	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammars	I-MethodName
(	O
PCFGs	O
)	O
.	O

Results	O
:	O
Tables	O
3	O
,	O
4	O
and	O
5	O
,	O
present	O
the	O
WERs	B-MetricName
from	O
ASR	B-TaskName
systems	O
built	O
using	O
different	O
choices	O
of	O
AM	O
and	O
LM	O
units	O
using	O
both	O
the	O
graphemic	O
representations	O
(	O
Native	O
and	O
SLP1	O
)	O
for	O
Sanskrit	O
,	O
Gujarati	O
and	O
Telugu	O
,	O
respectively	O
.	O

2014	O
)	O
with	O
256	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
as	O
in	O
Chang	O
et	O
al	O
.	O
(	O

Analysis	O
of	O
the	O
weights	O
of	O
the	O
bag	B-MethodName
of	I-MethodName
n	I-MethodName
-	I-MethodName
grams	I-MethodName
models	O
surface	O
patterns	O
in	O
language	O
usage	O
that	O
corroborate	O
our	O
analysis	O
on	O
linguistic	O
features	O
-swearing	O
and	O
negative	O
sentiment	O
is	O
predictive	O
of	O
low	O
engagement	O
,	O
for	O
example	O
.	O

With	O
the	O
human	O
effortreducing	O
advances	O
in	O
ASR	B-TaskName
for	O
YMC	B-DatasetName
presented	O
in	O
this	O
paper	O
,	O
such	O
extensive	O
targeted	O
recording	O
of	O
endangered	O
cultural	O
knowledge	O
can	O
now	O
easily	O
be	O
included	O
in	O
the	O
documentation	O
effort	O
.	O

Yet	O
,	O
the	O
fact	O
that	O
the	O
models	O
still	O
obtain	O
performance	O
of	O
up	O
to	O
0.8	B-MetricValue
(	O
r	B-MetricName
)	O
demonstrates	O
that	O
they	O
necessitate	O
and	O
are	O
able	O
to	O
learn	O
other	O
types	O
of	O
offensiveness	O
features	O
.	O

For	O
all	O
our	O
experiments	O
,	O
we	O
use	O
Adam	B-HyperparameterName
Optimiser	I-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2017	O
)	O
and	O
Cross	B-HyperparameterName
Entropy	I-HyperparameterName
Loss	I-HyperparameterName
.	O

BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
performs	O
better	O
than	O
vanilla	B-MethodName
BERT	I-MethodName
FitB	I-MethodName
on	O
both	O
subtasks	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
exploiting	O
rationales	O
with	O
LAMOL	B-MethodName
to	O
further	O
improve	O
the	O
LLL	B-TaskName
performance	O
,	O
discussed	O
next	O
.	O

We	O
report	O
Pearson	B-MetricName
correlation	I-MetricName
(	I-MetricName
r	I-MetricName
)	I-MetricName
and	O
MSE	B-MetricName
,	O
averaged	O
over	O
all	O
folds	O
.	O

We	O
consider	O
the	O
benefits	O
of	O
using	O
BPE	O
as	O
a	O
subword	O
unit	O
for	O
Sanskrit	O
ASR.While	B-TaskName
BPE	O
is	O
a	O
purely	O
data	O
-	O
driven	O
segmentation	O
strategy	O
,	O
we	O
next	O
present	O
a	O
linguistically	O
motivated	O
segmentation	O
approach	O
that	O
might	O
be	O
aligned	O
with	O
finding	O
syllable	O
units	O
for	O
ASR	B-TaskName
that	O
are	O
more	O
phonetically	O
compliant	O
.	O

2020	O
)	O
2	O
,	O
an	O
open	O
-	O
source	O
morphologicalsegmentation	B-TaskName
framework	O
that	O
is	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
AGs	O
)	O
(	O
Johnson	O
et	O
al	O
.	O
,	O

We	O
compare	O
with	O
DEEPQG	B-MethodName
(	O
Pan	O
et	O
al	O
.	O
,	O

2020).We	O
introduce	O
two	O
types	O
of	O
linguistic	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
a	O
languagespecific	O
grammar	O
that	O
is	O
tailored	O
for	O
the	O
language	O
of	O
interest	O
by	O
modeling	O
specific	O
morphological	O
phenomena	O
,	O
and	O
2	O
)	O
linguist	B-MethodName
-	I-MethodName
provided	I-MethodName
affixes	I-MethodName
,	O
where	O
an	O
expert	O
in	O
the	O
underlying	O
language	O
compiles	O
a	O
list	O
of	O
carefully	O
selected	O
affixes	O
and	O
seeds	O
it	O
into	O
the	O
grammars	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
.	O

We	O
used	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
(	O
trainsplit	O
)	O
for	O
training	O
and	O
other	O
YMC	B-DatasetName
corpora	O
for	O
evaluation	O
.	O

Averaging	O
across	O
all	O
turns	O
would	O
yield	O
a	O
final	O
cascading	B-MetricName
success	I-MetricName
rate	I-MetricName
of	O
45.8	B-MetricValue
%	I-MetricValue
.	O

This	O
indicates	O
that	O
HateBERT	B-MethodName
is	O
efficient	O
in	O
dealing	O
with	O
offensive	O
language	O
that	O
does	O
not	O
lie	O
in	O
the	O
extreme	O
offensive	O
end	O
.	O

Since	O
the	O
original	O
corpus	O
is	O
an	O
English	O
-	O
language	O
collection	O
,	O
all	O
of	O
our	O
analysis	O
is	O
constrained	O
to	O
English	O
,	O
and	O
we	O
filter	O
out	O
any	O
stray	O
examples	O
in	O
the	O
corpus	O
that	O
are	O
detected	O
as	O
non	O
-	O
English	O
after	O
running	B-TaskName
language	I-TaskName
identification	I-TaskName
(	O
Lui	O
and	O
Baldwin	O
,	O
2011	O
)	O
on	O
the	O
descriptions	O
.	O

Hyperparameter	O
Tuning	O
We	O
tuned	O
hyperparameters	O
for	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
.	O

In	O
the	O
BWS	B-MethodName
annotation	O
setup	O
,	O
the	O
annotators	O
are	O
given	O
an	O
n	O
-	O
tuple	O
(	O
where	O
n	O
>	O
1	O
,	O
and	O
commonly	O
n	O
=	O
4	O
)	O
,	O
and	O
asked	O
which	O
item	O
is	O
the	O
best	O
and	O
which	O
is	O
the	O
worst	O
(	O
best	O
and	O
worst	O
correspond	O
to	O
the	O
highest	O
and	O
the	O
lowest	O
with	O
respect	O
to	O
a	O
property	O
of	O
interest	O
)	O
.	O

Among	O
all	O
the	O
language	O
models	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

The	O
Answer	B-MethodName
-	I-MethodName
Attention	I-MethodName
system	I-MethodName
gave	O
us	O
a	O
dev	O
score	O
of	O
≈61	B-MetricName
%	I-MetricName
on	O
subtask	O
-	O
I	O
,	O
which	O
is	O
much	O
higher	O
than	O
the	O
baseline	O
.	O

This	O
motivates	O
the	O
study	O
of	O
claim	B-TaskName
provenance	I-TaskName
,	O
which	O
seeks	O
to	O
trace	O
and	O
explain	O
the	O
origins	O
of	O
claims	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
listed	O
below:•	O
We	O
demonstrate	O
the	O
importance	O
of	O
freezing	O
plastic	O
components	O
(	O
i.e.	O
,	O
components	O
that	O
are	O
most	O
susceptible	O
to	O
change	O
)	O
in	O
transformerbased	B-MethodName
models	I-MethodName
to	O
strengthen	O
memories	O
of	O
the	O
previously	O
learned	O
tasks	O
in	O
the	O
LLL	B-TaskName
setting.•	O
We	O
propose	O
critical	O
component	O
identification	O
algorithm	O
which	O
analyzes	O
the	O
transformerbased	B-MethodName
LLL	I-MethodName
model	I-MethodName
with	O
rationales	O
so	O
as	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
freeze	O
.	O

Figure	O
2	O
:	O
Overview	O
of	O
MERGEDISTILL	B-MethodName
:	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pretraining	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

The	O
effort	O
reduction	O
of	O
75	O
percent	O
documented	O
here	O
for	O
YMC	B-DatasetName
is	O
,	O
therefore	O
,	O
approaching	O
what	O
may	O
be	O
considered	O
the	O
minimum	O
amount	O
of	O
time	O
to	O
proofread	O
transcription	O
of	O
natural	O
speech	O
in	O
an	O
endangered	O
language	O
.	O

The	O
most	O
recent	O
efforts	O
(	O
2020	O
and	O
2021	O
)	O
have	O
adopted	O
the	O
ESPNet	O
toolkit	O
for	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
E2E	B-TaskName
ASR	I-TaskName
)	O
.	O

00:00:49.142	O
-	O
>	O
00:00:53.458	O
ASR	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
43	O
Exp	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
42	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
tone	O
42	O
,	O
writing	O
43	O
instead	O
.	O

In	O
regards	O
to	O
WER	B-MetricName
,	O
performance	O
improved	O
by	O
17.9	B-MetricValue
,	O
16.3	B-MetricValue
,	O
and	O
22.6	B-MetricValue
percent	O
across	O
the	O
same	O
three	O
corpora	O
.	O

3.3	O
Intrinsic	O
metrics	O
:	O
CER	B-MetricName
,	O
WER	B-MetricName
,	O
and	O
consistency	B-MetricName
in	I-MetricName
transcriptions	I-MetricName
used	I-MetricName
as	I-MetricName
reference	I-MetricName
:	O
Although	O
both	O
CER	B-MetricName
and	O
WER	B-MetricName
reference	O
"	O
error	O
rate	O
"	O
in	O
regards	O
to	O
character	O
and	O
word	O
,	O
respectively	O
,	O
the	O
question	O
of	O
the	O
accuracy	O
of	O
the	O
reference	O
itself	O
is	O
rarely	O
explored	O
(	O
but	O
cf	O
.	O

The	O
appendix	O
reports	O
similar	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

2019	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

The	O
domain	O
pre	O
-	O
training	O
stage	O
follows	O
that	O
of	O
the	O
BERT	B-MethodName
using	O
the	O
masked	O
language	O
model	O
and	O
next	O
sentence	O
prediction	O
technique	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

We	O
sample	O
a	O
diverse	O
set	O
of	O
these	O
methods	O
:	O
Random	B-MethodName
Sampling	I-MethodName
serves	O
as	O
our	O
baseline	O
passive	O
approach	O
for	O
acquiring	O
examples	O
.	O

We	O
use	O
two	O
GSA	B-MethodName
layers	O
on	O
top	O
of	O
BERT	B-MethodName
Embeddings	O
,	O
and	O
use	O
the	O
same	O
decoder	O
and	O
selection	O
method	O
as	O
BERT	B-MethodName
FitB.	I-MethodName
In	O
all	O
our	O
experiments	O
,	O
we	O
use	O
the	O
PyTorch	O
implementations	O
of	O
the	O
transformers	O
-	O
based	O
models	O
provided	O
by	O
the	O
HuggingFace	O
.The	O
metric	O
for	O
all	O
the	O
3	O
subtasks	O
is	O
accuracy	B-MetricName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
systematic	O
robustness	B-TaskName
evaluation	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
(	I-TaskName
LU	I-TaskName
)	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

LAMOL	B-MethodName
is	O
an	O
LLL	B-TaskName
general	O
framework	O
that	O
has	O
garnered	O
recent	O
interest	O
due	O
to	O
its	O
simplicity	O
(	O
Sun	O
et	O
al	O
.	O
,	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
different	O
linguistic	O
features	O
by	O
comparing	O
group	O
means	O
between	O
the	O
top	O
and	O
bottom	O
25	O
%	O
of	O
podcasts	O
by	O
engagement	O
within	O
each	O
popularity	O
quartile	O
(	O
approximately	O
335	O
podcasts	O
per	O
group	O
)	O
with	O
bootstrapped	O
Welch	B-MethodName
's	I-MethodName
ttests	I-MethodName
.	O

We	O
propose	O
segmenting	O
words	O
at	O
vowel	O
boundaries	O
to	O
extract	O
the	O
units	O
for	O
which	O
alignment	O
with	O
speech	O
is	O
learnt	O
within	O
the	O
ASR	B-TaskName
system	O
.	O

For	O
fine	O
-	O
tuning	O
on	O
downstream	O
tasks	O
,	O
most	O
of	O
the	O
hyperparameters	O
are	O
the	O
same	O
as	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

Linguistics	O
features	O
,	O
such	O
as	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tag	O
,	O
dependency	O
parsing	O
relations	O
,	O
and	O
syllable	O
counts	O
,	O
as	O
well	O
as	O
statistical	O
features	O
,	O
such	O
as	O
word	O
length	O
and	O
word	O
frequency	O
,	O
have	O
been	O
widely	O
used	O
for	O
predicting	O
lexical	B-MetricName
complexity	I-MetricName
(	O
Mukherjee	O
et	O
al	O
.	O
,	O

Due	O
to	O
the	O
performance	O
issues	O
,	O
the	O
model	B-HyperparameterName
max	I-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
is	O
set	O
to	O
4096	B-HyperparameterValue
tokens	I-HyperparameterValue
which	O
are	O
sufficient	O
in	O
our	O
case	O
.	O

SHR	B-MetricName
scores	O
of	O
over	O
0.8	B-MetricValue
indicate	O
substantial	O
reliability	O
.	O

2018	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

Improving	O
Long	O
Text	O
Comprehension	O
with	O
Semantic	B-MethodName
Graph	I-MethodName
.	O

This	O
environment	O
may	O
have	O
introduced	O
reverb	O
or	O
other	O
effects	O
that	O
might	O
have	O
negatively	O
affected	O
ASR	B-TaskName
CER	B-MetricName
and	O
WER.Accessibility	B-MetricName
:	O
All	O
three	O
corpora	O
(	O
119.32	O
hours	O
)	O
are	O
available	O
at	O
the	O
OpenSLR	O
data	O
portal	O
(	O
Amith	O
and	O
Castillo	O
García	O
,	O
2020	O
)	O
3	O
Goals	O
and	O
challenges	O
of	O
corpora	O
-	O
based	O
endangered	O
language	O
documentation	O
The	O
oft	O
-	O
cited	O
Boasian	O
trilogy	O
of	O
grammar	O
,	O
dictionaries	O
,	O
and	O
texts	O
is	O
a	O
common	O
foundation	O
for	O
EL	O
documentation	O
.	O

We	O
ran	O
these	O
ML	O
methods	O
by	O
the	O
scikit	B-MethodName
-	I-MethodName
learn	I-MethodName
open	O
-	O
source	O
machine	O
-	O
learning	O
package	O
in	O
python	O
5	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O

2020).The	O
pipeline	O
model	O
uses	O
a	O
BERT	B-MethodName
model	O
trained	O
with	O
the	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
.	O

In	O
order	O
to	O
validate	O
if	O
component	O
freezing	O
truly	O
helps	O
reduce	O
catastrophic	O
forgetting	O
,	O
we	O
performed	O
partial	O
brute	O
force	O
block	O
-	O
level	O
freezing	O
on	O
each	O
task	O
permutation	O
to	O
approximately	O
determine	O
the	O
upper	O
bound	O
of	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
block	O
.	O

Overall	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
bridges	O
the	O
gap	O
between	O
LL	B-TaskName
in	O
NLP	O
with	O
model	O
understanding	O
through	O
rationales	O
,	O
exhibiting	O
potential	O
for	O
a	O
true	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
as	O
well	O
as	O
limiting	O
catastrophic	O
forgetting	O
.	O

The	O
layer	O
type	O
set	O
of	O
current	O
BERTlike	B-MethodName
models	O
consists	O
of	O
self	O
-	O
attention	O
for	O
information	O
communication	O
and	O
feed	O
-	O
forward	O
for	O
nonlinear	O
transformation	O
.	O

00:01:14.768	O
-	O
>	O
00:01:18.281	O
ASR	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
Exp	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
,	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
using	O
BERT	B-MethodName
and	O
its	O
derivative	O
models	O
such	O
as	O
DistilBERT	B-MethodName
,	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

Combined	O
with	O
time	O
required	O
for	O
CCI	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
required	O
approximately	O
2.4	O
times	O
more	O
time	O
than	O
vanilla	O
LAMOL	B-MethodName
to	O
completely	O
train	O
a	O
model	O
as	O
shown	O
in	O
Figure	O
3	O
.	O

We	O
can	O
see	O
that	O
the	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
operation	O
yields	O
the	O
best	O
results	O
on	O
all	O
three	O
tasks	O
.	O

We	O
generate	O
the	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
dataset	O
similarly	O
,	O
compiling	O
a	O
list	O
of	O
the	O
20	O
commonly	O
occurring	O
food	O
categories	O
by	O
GloVe	O
vector	O
similarity	O
to	O
the	O
word	O
"	O
food	O
.	O
"	O

Once	O
this	O
threshold	O
is	O
reached	O
,	O
it	O
is	O
unlikely	O
that	O
further	O
improvement	O
will	O
significantly	O
reduce	O
the	O
human	O
effort	O
needed	O
to	O
check	O
the	O
ASR	B-TaskName
output	O
for	O
accuracy	O
.	O

Three	O
native	O
speaker	O
experts	O
have	O
worked	O
with	O
Amith	O
on	O
transcription	O
for	O
over	O
six	O
years	O
,	O
but	O
the	O
reference	O
for	O
ASR	B-TaskName
development	O
are	O
native	O
-	O
speaker	O
transcriptions	O
carefully	O
proofed	O
by	O
Amith	O
,	O
a	O
process	O
that	O
both	O
corrected	O
simple	O
errors	O
and	O
applied	O
a	O
single	O
standard	O
implemented	O
by	O
one	O
researcher	O
.	O

Then	O
,	O
we	O
extract	O
the	O
target	O
word	O
frequency	O
using	O
Google	B-MethodName
N	I-MethodName
-	I-MethodName
gram	I-MethodName
4	I-MethodName
word	I-MethodName
frequencies	I-MethodName
.	O

2020).Text	O
Paraphrasing	O
The	O
target	O
of	O
text	O
paraphrasing	O
is	O
to	O
generate	O
a	O
new	O
utterance	O
x	O
=	O
x	O
while	O
maintaining	O
its	O
dialog	O
act	O
unchanged	O
,	O
i.e.	O
y	O
=	O
y.	O
We	O
applied	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
,	O
a	O
finetuned	O
language	O
model	O
conditioned	O
on	O
the	O
dialog	O
acts	O
,	O
to	O
paraphrase	O
the	O
sentences	O
as	O
data	O
augmentation	O
.	O

Content	I-MethodName
-	I-MethodName
Based	I-MethodName
Podcast	I-MethodName
Recommendations	I-MethodName
Yang	O
et	O
al	O
.	O
(	O

Early	O
work	O
relies	O
on	O
syntactic	B-MethodName
transformation	I-MethodName
to	O
convert	O
declarative	O
sentences	O
to	O
questions	O
(	O
Heilman	O
and	O
Smith	O
,	O
2010;Chali	O
and	O
Hasan	O
,	O
2015	O
)	O
.	O

Recently	O
,	O
E2E	B-TaskName
ASR	I-TaskName
has	O
reached	O
comparable	O
or	O
better	O
performances	O
than	O
conventional	O
Hidden	B-MethodName
-	I-MethodName
Markov	I-MethodName
-	I-MethodName
Model	I-MethodName
-	O
based	O
ASR	B-TaskName
(	O
Graves	O
and	O
Jaitly	O
,	O
2014;Chiu	O
et	O
al	O
.	O
,	O

We	O
systematically	O
remove	O
examples	O
with	O
a	O
low	O
product	O
value	O
and	O
observe	O
how	O
active	O
learning	O
performance	O
changes	O
(	O
see	O
Figure	O
8).We	O
observe	O
a	O
2	O
-	O
3x	O
improvement	O
in	O
sample	O
efficiency	O
when	O
removing	O
50	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
entire	I-HyperparameterValue
data	I-HyperparameterValue
pool	I-HyperparameterValue
,	O
consisting	O
mainly	O
of	O
collective	O
outliers	O
(	O
Figure	O
8c	O
)	O
.	O

Do	O
listeners	O
seem	O
to	O
prefer	O
descriptions	O
that	O
accurately	O
convey	O
the	O
topics	O
and	O
synopsis	O
of	O
the	O
episode	O
?	O
We	O
measure	O
faithfulness	O
of	O
the	O
episode	O
description	O
to	O
the	O
first	O
ten	O
minutes	O
of	O
the	O
transcript	O
as	O
the	O
cosine	B-MethodName
similarity	I-MethodName
between	O
the	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
bag	I-MethodName
of	I-MethodName
words	I-MethodName
representation	O
of	O
both	O
texts	O
.	O

We	O
make	O
a	O
single	O
dataset	O
for	O
podcasts	O
across	O
all	O
quartiles	O
by	O
aggregating	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
within	O
each	O
quartile	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
grounds	O
of	O
LLL	B-TaskName
are	O
left	O
largely	O
underexplored	O
.	O

Student	O
similar	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
99112	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
162	O
M	O
parameters	O
.	O

The	O
novelty	O
in	O
the	O
task	O
lies	O
in	O
its	O
3	O
subtasks	O
:	O
Imperceptibility	B-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
I	I-TaskName
)	I-TaskName
,	I-TaskName
Non	I-TaskName
-	I-TaskName
Specificity	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
II	I-TaskName
)	I-TaskName
,	I-TaskName
and	I-TaskName
Intersection	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
III	I-TaskName
)	O
.	O

Besides	O
designing	O
pre	O
-	O
training	O
objectives	O
,	O
some	O
other	O
works	O
try	O
to	O
extend	O
BERT	B-MethodName
by	O
incorporating	O
knowledge	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

To	O
control	O
for	O
duration	O
effects	O
in	O
the	O
analysis	O
of	O
transcripts	O
,	O
we	O
truncate	O
transcripts	O
at	O
ten	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

Component	O
Freezing	O
While	O
component	O
freezing	O
is	O
also	O
a	O
common	O
practice	O
in	O
the	O
fine	O
-	O
tuning	O
process	O
,	O
it	O
is	O
done	O
to	O
prevent	O
loss	O
in	O
general	O
knowledge	O
in	O
lower	O
layers	O
of	O
the	O
model	O
(	O
Raganato	O
and	O
Tiedemann	O
,	O
2018).By	O
contrast	O
,	O
many	O
architecture	O
-	O
based	O
LL	B-TaskName
methods	O
,	O
for	O
example	O
Rusu	O
et	O
al	O
.	O
(	O

Note	O
that	O
"	O
snacks	O
"	O
is	O
also	O
an	O
option	O
;	O
however	O
,	O
food	O
is	O
more	O
non	B-TaskName
-	I-TaskName
specific	I-TaskName
than	O
"	O
snacks	O
"	O
and	O
hence	O
,	O
food	O
is	O
the	O
correct	O
option	O
.	O

2016	O
)	O
,	O
an	O
Automatic	B-MethodName
Speech	I-MethodName
Recognition	I-MethodName
(	I-MethodName
ASR	I-MethodName
)	I-MethodName
system	I-MethodName
.	O

Table	O
13	O
investigates	O
which	O
error	O
type	O
the	O
model	O
has	O
made	O
on	O
the	O
real	O
test	O
set	O
by	O
manually	O
checking	O
all	O
the	O
error	O
outputs	O
of	O
BERT	B-MethodName
Ori	O
.	O
"	O

If	O
GAttn	B-MethodName
denotes	O
the	O
global	O
attention	O
function	O
,	O
we	O
have	O
:	O
d	O
i	O
=	O
basemodel(b	O
)	O
(	O
3	O
)	O
g	O
i	O
=	O
GAttn(d	O
i	O
)	O
.1(i	O
∈	O
A	O
)	O
(	O
4)where	O
d	O
i	O
is	O
the	O
raw	O
output	O
vector	O
for	O
each	O
input	O
token	O
.	O

First	O
,	O
the	O
advantage	O
for	O
ASR	B-TaskName
accuracy	O
of	O
targeting	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
in	O
addition	O
to	O
,	O
or	O
in	O
substitution	O
of	O
,	O
linguistic	O
units	O
(	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
morae	B-MethodName
)	O
and	O
then	O
using	O
ROVER	B-MethodName
for	O
system	O
combination	O
.	O

It	O
is	O
further	O
augmented	O
by	O
a	O
semantic	B-MethodName
graph	I-MethodName
that	O
leverages	O
both	O
semantic	O
roles	O
and	O
dependency	O
relations	O
,	O
facilitating	O
long	O
text	O
comprehension	O
to	O
pinpoint	O
salient	O
concepts	O
.	O

Setup	O
We	O
use	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
dataset	O
introduced	O
in	O
Section	O
3	O
.	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
document	O
.	O

We	O
call	O
the	O
method	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching).We	I-MethodName
normalise	O
the	O
computed	O
weights	O
:	O
norm	O
weight	O
ij	O
=	O
weight	O
ij	O
n	O
i	O
j=1	O
weight	O
ij	O
(	O
3)where	O
n	O
i	O
is	O
the	O
number	O
of	O
chunks	O
in	O
the	O
i	O
th	O
sample	O
.	O

As	O
a	O
result	O
,	O
we	O
find	O
that	O
the	O
average	O
token	O
length	O
of	O
generated	O
utterances	O
with	O
/	O
without	O
"	O
*	O
"	O
is	O
15.96/12.67	O
respectively	O
after	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
's	O
finetuning	O
on	O
MultiWOZ.It	B-DatasetName
should	O
be	O
noted	O
that	O
slot	O
values	O
of	O
an	O
utterance	O
can	O
be	O
paraphrased	O
by	O
models	O
,	O
resulting	O
in	O
a	O
different	O
semantic	O
meaning	O
y	O
.	O

The	O
experimental	O
results	O
demonstrate	O
that	O
our	O
method	O
outperforms	O
the	O
original	O
BERT	B-MethodName
model	O
on	O
three	O
relation	O
extraction	O
benchmarks	O
.	O

We	O
followed	O
the	O
procedure	O
described	O
in	O
Kiritchenko	O
and	O
Mohammad	O
(	O
2016	O
)	O
to	O
obtain	O
BWS	B-MethodName
annotations	O
.	O

AlBERT	B-MethodName
model	O
has	O
an	O
inter	O
-	O
sentence	O
coherence	O
task	O
and	O
a	O
lighter	O
memory	O
footprint	O
compared	O
to	O
BERT	B-MethodName
,	O
while	O
RoBERTa	B-MethodName
model	O
has	O
substantially	O
more	O
data	O
and	O
hyper	O
-	O
parameter	O
tuning	O
in	O
pretraining	O
than	O
BERT.In	B-MethodName
the	O
future	O
,	O
we	O
also	O
plan	O
to	O
include	O
GPT	O
-	O
based	O
models	O
,	O
such	O
as	O
DialoGPT	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O

These	O
features	O
were	O
selected	O
using	O
the	O
Linear	B-MethodName
Regression	I-MethodName
algorithm	O
,	O
which	O
was	O
also	O
selected	O
as	O
a	O
baseline	O
algorithm	O
by	O
the	O
task	O
organizers	O
.	O

In	O
contrast	O
,	O
the	O
longformer	B-MethodName
global	O
attention	O
mechanism	O
relaxes	O
this	O
limitation	O
as	O
we	O
only	O
need	O
to	O
pay	O
attention	O
to	O
a	O
small	O
factor	O
of	O
context	O
and	O
more	O
focus	O
on	O
the	O
local	O
window	O
.	O

Japanese	O
:	O
Both	O
the	O
STD	B-MethodName
and	B-MethodName
STD	I-MethodName
-	I-MethodName
LS	I-MethodName
models	O
perform	O
well	O
on	B-MethodName
prefix	I-MethodName
segmentation	I-MethodName
,	O
achieving	B-MetricName
F1	I-MetricName
-	O
scores	O
of	O
more	O
than	O
90	B-MetricValue
%	I-MetricValue
in	O
the	O
detection	O
of	O
several	O
one	O
-	O
character	O
prefixes	O
,	O
such	O
as	O
お	O
and	O
ご	O
.	O

ASR	B-TaskName
sometimes	O
wrongly	O
identifies	O
one	O
word	O
as	O
another	O
with	O
similar	O
pronunciation	O
.	O

The	O
benefits	O
of	O
such	O
systems	O
are	O
reflected	O
in	O
the	O
recent	O
trends	O
of	O
using	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
for	O
EL	O
documentation	O
(	O
Adams	O
et	O
al	O
.	O
,	O

In	O
contrast	O
,	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

The	O
closest	O
prior	O
work	O
to	O
ABCD	B-DatasetName
is	O
the	O
Schema	B-DatasetName
Guided	I-DatasetName
Dialogue	I-DatasetName
(	O
SGD	B-DatasetName
)	O
dataset	O
,	O
which	O
contains	O
dozens	O
of	O
API	O
calls	O
that	O
can	O
be	O
interpreted	O
as	O
individual	O
actions	O
sending	O
commands	O
to	O
a	O
SQL	O
engine	O
(	O
Rastogi	O
et	O
al	O
.	O
,	O

We	O
utilize	O
the	O
AIMed	B-DatasetName
corpus	O
for	O
the	O
PPI	B-TaskName
task	O
,	O
and	O
we	O
will	O
employ	O
10	B-HyperparameterValue
-	O
fold	O
cross	O
-	O
validation	O
on	O
it	O
since	O
there	O
is	O
no	O
standard	O
split	O
of	O
training	O
and	O
test	O
.	O

1992	O
)	O
,	O
to	O
capture	O
deeper	O
levels	O
of	O
cognition	O
,	O
such	O
as	O
causal	B-TaskName
reasoning	I-TaskName
and	I-TaskName
judgments	I-TaskName
.	O

2017	O
)	O
,	O
the	O
drug	B-TaskName
-	I-TaskName
drug	I-TaskName
interactions	I-TaskName
(	O
DDI	B-TaskName
)	O
(	O
Herrero	O
-	O
Zazo	O
et	O
al	O
.	O
,	O

Therefore	O
,	O
we	O
propose	O
an	O
automatic	O
method	O
LAUG	B-MethodName
for	O
Language	B-MethodName
understanding	I-MethodName
AUGmentation	I-MethodName
in	O
this	O
paper	O
to	O
approximate	O
the	O
natural	O
perturbations	O
to	O
existing	O
data	O
.	O

2011	O
)	O
for	O
all	O
our	O
ASR	B-TaskName
experiments	O
.	O

Four	O
data	O
augmentation	O
approaches	O
covering	O
the	O
three	O
aspects	O
are	O
assembled	O
in	O
LAUG	B-MethodName
,	O
which	O
reveals	O
critical	O
robustness	O
issues	O
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

We	O
found	O
that	O
pre	O
-	O
trained	O
models	O
perform	O
decent	O
on	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
but	O
there	O
is	O
a	O
large	O
gap	O
between	O
humans	O
agents	O
and	O
the	O
top	O
systems	O
for	B-TaskName
Cascading	I-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

The	O
topics	O
enrich	O
the	O
news	O
representation	O
,	O
and	O
the	O
external	O
KB	O
offers	O
evidences	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

The	O
implication	O
of	O
the	O
preceding	O
is	O
that	O
ASR	O
using	O
linguistic	O
units	O
yields	O
significantly	O
lower	O
accuracy	O
than	O
ASR	B-TaskName
that	O
uses	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

In	O
our	O
case	O
,	O
we	O
hypothesize	O
that	O
training	O
on	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
exposes	O
the	O
model	O
to	O
sufficient	O
data	O
for	O
it	O
to	O
generalize	O
well	O
enough	O
and	O
mask	O
the	O
benefits	O
of	O
teacher	O
LM	O
predictions	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
scores	O
for	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
were	O
only	O
produced	O
after	O
Castillo	O
used	O
the	O
ELAN	O
interface	O
to	O
correct	O
the	O
ASR	B-TaskName
hypotheses	O
for	O
this	O
corpus	O
(	O
see	O
Appendix	O
A	O
for	O
an	O
example	O
ASR	B-TaskName
output	O
)	O
.	O

Furthermore	O
,	O
we	O
use	O
a	O
Threshold	B-MethodName
Method	I-MethodName
towards	O
improving	O
the	O
model	O
performance	O
.	O

2003	O
)	O
(	O
the	O
total	B-HyperparameterName
topic	I-HyperparameterName
number	I-HyperparameterName
K	B-HyperparameterName
is	O
set	O
as	O
100	B-HyperparameterValue
)	O
to	O
mine	O
the	O
latent	O
topics	O
T	O
from	O
all	O
the	O
sentences	O
of	O
all	O
the	O
documents	O
in	O
our	O
dataset	O
.	O

We	O
believe	O
our	O
modifications	O
have	O
only	O
scratched	O
the	O
surface	O
and	O
that	O
improving	O
the	O
user	O
experience	O
for	O
data	O
collection	O
offers	O
an	O
interesting	O
line	O
of	O
HCI	B-TaskName
research	O
to	O
explore	O
.	O

In	O
the	O
contrastive	O
pre	O
-	O
training	O
step	O
of	O
the	O
BERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
with	O
the	O
fine	O
-	O
tuning	O
,	O
and	O
the	O
training	B-HyperparameterName
epoch	I-HyperparameterName
is	O
selected	O
from	O
[	O
2,4,6,8,10	B-HyperparameterValue
]	O
based	O
on	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O

In	O
practice	O
,	O
E2E	B-TaskName
ASR	I-TaskName
systems	I-TaskName
are	O
less	O
affected	O
by	O
linguistic	O
constraints	O
and	O
are	O
generally	O
easier	O
to	O
train	O
.	O

We	O
annotate	O
two	O
datasets	O
with	O
morphological	B-TaskName
segmentation	I-TaskName
that	O
we	O
use	O
as	O
the	O
gold	O
standard	O
to	O
evaluate	O
our	O
segmentation	B-TaskName
models	O
for	O
Japanese	O
and	O
Georgian	O
.	O

For	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Similarity	I-MethodName
)	I-MethodName
,	O
the	O
weights	O
are	O
calculated	O
as	O
:	O
weight	O
ij	O
=	O
u	O
i	O
.v	O
j	O
||u	O
i	O
||||v	O
j	O
||	O
(	O
1)where	O
u	O
i	O
is	O
the	O
embedding	O
of	O
the	O
question	O
in	O
the	O
i	O
th	O
sample	O
,	O
and	O
v	O
j	O
is	O
the	O
embedding	O
of	O
the	O
j	O
th	O
chunk	O
of	O
the	O
sample	O
's	O
article	O
.	O

Removing	O
both	O
topics	O
and	O
external	O
knowledge	O
(	O
i.e.	O
,	O
w/o	O
Both	O
)	O
will	O
lead	O
to	O
substantial	O
performance	O
drop	O
(	O
4.0	B-MetricValue
-	I-MetricValue
5.0	I-MetricValue
%	I-MetricValue
)	O
.	O

We	O
reckon	O
that	O
with	O
more	O
careful	O
tuning	O
of	O
parameters	O
such	O
as	O
the	O
threshold	O
in	O
the	O
Difference	B-MethodName
Method	I-MethodName
,	O
we	O
will	O
be	O
able	O
to	O
achieve	O
these	O
gains	O
on	O
the	O
test	O
set	O
.	O

This	O
model	O
reached	O
the	O
peak	O
observed	O
performance	O
of	O
32.7	B-TaskName
%	I-TaskName
,	O
highlighting	O
the	O
importance	O
of	O
both	O
components	O
.	O

Secondly	O
,	O
our	O
obtained	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
superiority	O
over	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

For	O
LDA	B-MethodName
features	O
,	O
we	O
note	O
significance	O
after	O
a	O
Bonferroni	B-MetricName
correction	I-MetricName
of	I-MetricName
α	I-MetricName
=	O
0.05/100	B-MetricValue
,	O
and	O
for	O
the	O
other	O
linguistic	O
features	O
,	O
a	O
Bonferroni	B-HyperparameterName
correction	I-HyperparameterName
of	I-HyperparameterName
α	I-HyperparameterName
=	O
0.05/30.In	B-HyperparameterValue
the	O
results	O
,	O
'	O
description	O
'	O
refers	O
to	O
the	O
concatenation	O
of	O
the	O
show	O
description	O
and	O
the	O
representative	O
episode	O
's	O
description	O
.	O

The	O
decision	O
to	O
use	O
underlying	O
representations	O
in	O
ASR	B-TaskName
training	O
has	O
,	O
however	O
,	O
several	O
more	O
important	O
advantages	O
.	O

The	O
logit	O
(	O
score	O
)	O
of	O
each	O
ent	O
token	O
will	O
be	O
calculated	O
using	O
a	O
linear	O
transformation	O
function	O
,	O
then	O
the	O
prediction	O
distribution	O
over	O
the	O
answer	O
candidates	O
(	O
ent	O
tokens	O
)	O
will	O
be	O
outputted	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
250k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

Finally	O
,	O
the	O
models	O
obtained	O
the	O
performance	O
of	O
up	O
to	O
0.78	B-MetricValue
(	O
r	B-MetricName
)	O
on	O
the	O
reduced	O
-	O
range	O
dataset	O
,	O
which	O
shows	O
that	O
even	O
if	O
the	O
comments	O
from	O
the	O
extreme	O
ends	O
of	O
the	O
offensiveness	O
scale	O
are	O
removed	O
,	O
Ruddit	O
still	O
presents	O
an	O
interesting	O
and	O
feasible	O
offensiveness	O
scoring	O
task	O
.	O

MILU	B-MethodName
and	O
BERT	B-MethodName
failed	O
in	O
most	O
of	O
these	O
cases	O
but	O
fixed	O
some	O
error	O
after	O
augmented	O
training	O
.	O

Language	B-MethodName
Modeling	I-MethodName
for	I-MethodName
Lifelong	I-MethodName
Language	I-MethodName
Learning	I-MethodName
(	O
LAMOL	B-MethodName
)	O
(	O
Sun	O
et	O
al	O
.	O
,	O

We	O
report	O
the	O
mean	B-MetricName
precision	I-MetricName
and	O
recall	B-MetricName
of	I-MetricName
the	I-MetricName
top	I-MetricName
-	I-MetricName
k	I-MetricName
results	I-MetricName
respectively	O
.	O

In	O
order	O
to	O
make	O
the	O
BERT	B-MethodName
model	O
identify	O
the	O
positions	O
of	O
the	O
entities	O
,	O
we	O
replace	O
the	O
relevant	O
entity	O
names	O
with	O
predefined	O
tags	O
by	O
following	O
the	O
standard	O
pre	O
-	O
processing	O
step	O
for	O
relation	O
extraction	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

For	O
subtask	O
-	O
I	O
,	O
in	O
table	O
2	O
,	O
we	O
also	O
demonstrate	O
the	O
results	O
of	O
BERT	B-MethodName
Ensemble	I-MethodName
,	O
in	O
which	O
we	O
ensemble	O
(	O
i.e.	O
,	O
averaging	O
over	O
the	O
predictions	O
)	O
two	O
checkpoints	O
saved	O
during	O
the	O
training	O
process	O
.	O

In	O
-	O
depth	O
experiments	O
and	O
analysis	O
are	O
conducted	O
on	O
MultiWOZ	B-DatasetName
and	O
Frames	B-DatasetName
,	O
with	O
both	O
classification	O
-	O
and	O
generation	O
-	O
based	O
LU	O
models	O
.	O

2	O
The	O
transcription	O
strategy	O
for	O
YMC	B-DatasetName
was	O
unusual	O
in	O
that	O
the	O
practical	O
orthography	O
was	O
a	O
deep	O
,	O
underlying	O
system	O
that	O
represented	O
segmental	O
morpheme	O
boundaries	O
and	O
showed	O
elided	O
tones	O
in	O
parentheses	O
.	O

We	O
used	O
a	O
comparative	O
annotation	O
technique	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
which	O
addresses	O
the	O
limitations	O
of	O
traditional	O
rating	O
scales	O
.	O

YM	B-DatasetName
is	O
spoken	O
in	O
four	O
communities	O
:	O
Yoloxóchitl	O
,	O
Cuanacaxtitlan	O
,	O
Arroyo	O
Cumiapa	O
,	O
and	O
Buena	O
Vista	O
.	O

Note	O
:	O
A	O
total	O
16	O
out	O
of	O
33	O
segments	O
/	O
utterances	O
are	O
without	O
ASR	B-TaskName
error	O
.	O

As	O
we	O
can	O
see	O
clearly	O
,	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
first	O
consistently	O
rises	O
with	O
the	O
increase	O
of	O
P	B-HyperparameterName
and	O
then	O
drops	O
when	O
P	B-HyperparameterName
is	O
larger	O
than	O
2	O
.	O

However	O
,	O
recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
contextualized	O
word	O
representations	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Pre	O
-	O
training	O
:	O
In	O
this	O
experiment	O
,	O
we	O
make	O
use	O
of	O
pre	O
-	O
existing	O
multilingual	O
models	O
:	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

Automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
is	O
interesting	O
,	O
owing	O
to	O
the	O
various	O
linguistic	O
peculiarities	O
present	O
in	O
the	O
language	O
.	O

On	O
the	O
prediction	O
task	O
,	O
we	O
achieve	O
up	O
to	O
71.15	B-MetricValue
%	I-MetricValue
(	O
Table	O
8)	O
accuracy	B-MetricName
using	O
only	O
linguistic	O
features	O
,	O
similar	O
to	O
the	O
performance	O
on	O
podcasts	O
.	O

The	O
experimental	O
results	O
show	O
that	O
our	O
method	O
boosts	O
the	O
BERT	B-MethodName
model	O
performance	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
three	O
tasks	O
.	O

The	O
performance	O
of	O
BERT	B-MethodName
on	O
augmented	O
test	O
sets	O
is	O
improved	O
when	O
augmentation	O
ratio	O
is	O
less	O
than	O
0.5	O
but	O
becomes	O
almost	O
unchanged	O
after	O
0.5	O
while	O
GPT-2	B-MethodName
keeps	O
increasing	O
stably	O
.	O

Morphological	B-TaskName
segmentation	I-TaskName
is	O
an	O
essential	O
subtask	O
in	O
many	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
applications	O
,	O
especially	O
in	O
the	O
case	O
of	O
morphologically	O
complex	O
languages	O
.	O

2020	O
)	O
,	O
we	O
make	O
two	O
measurements	O
:	O
the	O
Flesch	B-MetricName
-	I-MetricName
Kincaid	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Flesch	O
,	O
1948	O
)	O
that	O
measures	O
the	O
number	O
of	O
syllables	O
per	O
word	O
and	O
the	O
number	O
of	O
words	O
per	O
sentence	O
,	O
and	O
the	O
Dale	B-MetricName
-	I-MetricName
Chall	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Chall	O
and	O
Dale	O
,	O
1948	O
)	O
which	O
measures	O
word	O
'	O
difficulty	O
'	O
using	O
a	O
lookup	O
table	O
.	O

For	O
the	O
FitB	B-HyperparameterName
models	O
,	O
we	O
keep	O
all	O
the	O
layers	O
unfrozen	O
.	O

We	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pretrained	O
models	O
(	O
Radford	O
et	O
al	O
.	O
,	O

We	O
use	O
40dimensional	O
MFCCs	O
as	O
our	O
input	O
features	O
along	O
with	O
100	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
i	B-HyperparameterName
-	I-HyperparameterName
vector	I-HyperparameterName
based	I-HyperparameterName
speaker	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Saon	O
et	O
al	O
.	O
,	O

Originally	O
,	O
InvRat	B-MethodName
was	O
designed	O
for	O
single	O
-	O
input	O
tasks	O
such	O
as	O
sentiment	O
analysis	O
.	O

2019	O
)	O
by	O
trying	O
out	O
their	O
various	O
combinations	O
with	O
BERT.In	B-MethodName
Section	O
2	O
,	O
we	O
perform	O
a	O
succinct	O
literature	O
survey	O
.	O

Compared	O
to	O
the	O
best	O
baseline	O
model	O
,	O
CompareNet	B-HyperparameterName
improves	O
both	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
by	O
nearly	O
3	B-MetricValue
%	I-MetricValue
.	O

These	O
words	O
are	O
related	O
to	O
"	O
legislative	O
"	O
which	O
exhibits	O
the	O
fact	O
that	O
BERT	B-MethodName
FitB	I-MethodName
is	O
not	O
only	O
able	O
to	O
learn	O
the	O
concept	O
of	O
imperceptibility	B-TaskName
,	O
but	O
is	O
also	O
able	O
to	O
predict	O
a	O
suitable	O
word	O
.	O

In	O
this	O
section	O
,	O
we	O
study	O
the	O
influence	O
of	O
training	O
/	O
prediction	O
schemes	O
on	O
LU	B-TaskName
robustness	I-TaskName
.	O

2021	O
)	O
and	O
in	O
yielding	O
ASR	B-TaskName
hypotheses	O
of	O
an	O
accuracy	O
capable	O
of	O
significantly	O
reducing	O
the	O
extent	O
of	O
human	O
effort	O
needed	O
to	O
finalize	O
accurate	O
transcribed	O
audio	O
for	O
permanent	O
archiving	O
as	O
here	O
demonstrated	O
.	O

Despite	O
the	O
difference	O
,	O
model	O
performance	O
on	O
the	O
real	O
data	O
is	O
remarkably	O
improved	O
after	O
every	O
model	O
is	O
finetuned	O
on	O
the	O
augmented	O
data	O
,	O
verifying	O
that	O
LAUG	B-MethodName
effectively	O
enhances	O
the	O
model	O
's	O
real	O
-	O
world	O
robustness	O
.	O

2019	O
)	O
and	O
building	O
open	B-MethodName
-	I-MethodName
domain	I-MethodName
dialogue	I-MethodName
systems	I-MethodName
(	O
Shum	O
et	O
al	O
.	O
,	O

The	O
Spotify	B-DatasetName
Podcast	I-DatasetName
Dataset	O
is	O
a	O
recently	O
released	O
corpus	O
of	O
over	O
100	O
,	O
000	O
podcast	O
episodes	O
,	O
mostly	O
in	O
English	O
,	O
that	O
are	O
transcribed	O
with	O
Google	B-MethodName
's	I-MethodName
Speech	I-MethodName
to	I-MethodName
Text	I-MethodName
commercial	I-MethodName
speech	I-MethodName
recognition	I-MethodName
,	O
reported	O
in	O
the	O
paper	O
to	O
have	O
an	O
18	B-MetricValue
%	I-MetricValue
word	B-MetricName
error	I-MetricName
on	O
podcasts	O
.	O

Answer	O
-	O
Attention	O
Since	O
GA	B-MethodName
