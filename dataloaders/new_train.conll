Lifelong	B-TaskName
learning	I-TaskName
(	I-TaskName
LL	I-TaskName
)	I-TaskName
aims	O
to	O
train	O
a	O
neural	O
network	O
on	O
a	O
stream	O
of	O
tasks	O
while	O
retaining	O
knowledge	O
from	O
previous	O
tasks	O
.	O

However	O
,	O
many	O
prior	O
attempts	O
in	O
NLP	O
still	O
suffer	O
from	O
the	O
catastrophic	O
forgetting	O
issue	O
,	O
where	O
the	O
model	O
completely	O
forgets	O
what	O
it	O
just	O
learned	O
in	O
the	O
previous	O
tasks	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
LL	B-TaskName
framework	O
for	O
language	O
models	O
.	O

In	O
order	O
to	O
alleviate	O
catastrophic	O
forgetting	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
enhances	O
LAMOL	B-MethodName
,	O
a	O
recent	O
LL	B-TaskName
model	O
,	O
by	O
applying	O
critical	O
freezing	O
guided	O
by	O
human	O
rationales	O
.	O

When	O
the	O
human	O
rationales	O
are	O
not	O
available	O
,	O
we	O
propose	O
exploiting	O
unsupervised	O
generated	O
rationales	O
as	O
substitutions	O
.	O

In	O
the	O
experiment	O
,	O
we	O
tested	O
Rational	B-MethodName
LAMOL	I-MethodName
on	O
permutations	O
of	O
three	O
datasets	O
from	O
the	O
ERASER	O
benchmark	O
.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
framework	O
outperformed	O
vanilla	O
LAMOL	B-MethodName
on	O
most	O
permutations	O
.	O

Furthermore	O
,	O
unsupervised	O
rationale	O
generation	O
was	O
able	O
to	O
consistently	O
improve	O
the	O
overall	O
LL	B-TaskName
performance	O
from	O
the	O
baseline	O
without	O
relying	O
on	O
human	O
-	O
annotated	O
rationales	O
.	O

We	O
made	O
our	O
code	O
publicly	O
available	O
at	O
https://github	O
.	O

com	O
/	O
kanwatchara	O
-	O
k	O
/	O
r_lamol	O
.	O

The	O
grounds	O
of	O
lifelong	B-TaskName
learning	I-TaskName
(	O
LL	B-TaskName
)	O
stem	O
from	O
the	O
ability	O
of	O
humans	O
to	O
continually	O
acquire	O
,	O
consolidate	O
,	O
and	O
transfer	O
knowledge	O
and	O
skills	O
throughout	O
their	O
lifespan	O
.	O

This	O
ability	O
is	O
also	O
important	O
for	O
real	O
-	O
world	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
applications	O
,	O
where	O
autonomous	O
agents	O
are	O
required	O
to	O
interact	O
with	O
users	O
from	O
various	O
domains	O
through	O
continuous	O
streams	O
of	O
information	O
and	O
language	O
semantic	O
drifts	O
occur	O
over	O
time	O
.	O

The	O
existing	O
dominant	O
paradigm	O
for	O
machine	O
learning	O
,	O
however	O
,	O
is	O
isolated	O
learning	O
(	O
Chen	O
and	O
Liu	O
,	O
2016	O
)	O
.	O

While	O
isolated	O
learning	O
has	O
shown	O
some	O
successes	O
in	O
a	O
variety	O
of	O
domains	O
,	O
their	O
applicability	O
remains	O
limited	O
to	O
the	O
assumption	O
that	O
all	O
samples	O
are	O
available	O
during	O
the	O
learning	O
phase	O
.	O

When	O
a	O
stream	O
of	O
tasks	O
are	O
trained	O
sequentially	O
,	O
machine	O
learning	O
and	O
neural	O
network	O
models	O
face	O
catastrophic	O
forgetting	O
or	O
interference	O
(	O
McCloskey	O
and	O
Cohen	O
,	O
1989	O
)	O
.	O

This	O
occurs	O
due	O
to	O
the	O
non	O
-	O
stationary	O
data	O
distribution	O
that	O
biases	O
the	O
model	O
.	O

We	O
focus	O
on	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
(	O
LLL	B-TaskName
)	O
,	O
which	O
is	O
lifelong	B-TaskName
learning	I-TaskName
on	O
a	O
stream	O
of	O
NLP	O
tasks	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
grounds	O
of	O
LLL	B-TaskName
are	O
left	O
largely	O
underexplored	O
.	O

LAMOL	B-MethodName
is	O
an	O
LLL	B-TaskName
general	O
framework	O
that	O
has	O
garnered	O
recent	O
interest	O
due	O
to	O
its	O
simplicity	O
(	O
Sun	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

In	O
particular	O
,	O
LAMOL	B-MethodName
transforms	O
all	O
NLP	O
tasks	O
into	O
the	O
question	O
answering	O
(	O
QA	O
)	O
format	O
according	O
to	O
McCann	O
et	O
al	O
.	O
(	O

2018	O
)	O
and	O
generates	O
pseudo	O
-	O
samples	O
of	O
old	O
tasks	O
using	O
its	O
language	O
modeling	O
(	O
LM	O
)	O
capability	O
to	O
refresh	O
the	O
learned	O
knowledge	O
.	O

However	O
,	O
there	O
is	O
still	O
a	O
gap	O
between	O
the	O
performance	O
of	O
LAMOL	B-MethodName
and	O
the	O
result	O
of	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
which	O
is	O
generally	O
considered	O
as	O
the	O
upper	O
bound	O
of	O
LLL	B-TaskName
performance	O
.	O

This	O
indicates	O
that	O
only	O
pseudo	O
-	O
samples	O
generation	O
may	O
not	O
be	O
sufficient	O
to	O
prevent	O
catastrophic	O
forgetting	O
.	O

In	O
this	O
paper	O
,	O
we	O
improve	O
existing	O
LLL	B-TaskName
strategies	O
by	O
proposing	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
rationale	O
-	O
based	O
lifelong	O
learning	O
framework	O
which	O
equips	O
the	O
original	O
LAMOL	B-MethodName
with	O
critical	O
freezing	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O

2020	O
)	O
to	O
further	O
prevent	O
catastrophic	O
forgetting	O
.	O

Particularly	O
,	O
we	O
devise	O
an	O
algorithm	O
to	O
identify	O
critical	O
components	O
in	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
language	I-MethodName
models	I-MethodName
using	O
rationales	O
,	O
and	O
the	O
selected	O
compo	O
-	O
nents	O
will	O
be	O
frozen	O
to	O
maintain	O
learned	O
knowledge	O
while	O
being	O
trained	O
on	O
a	O
new	O
task	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
listed	O
below:•	O
We	O
demonstrate	O
the	O
importance	O
of	O
freezing	O
plastic	O
components	O
(	O
i.e.	O
,	O
components	O
that	O
are	O
most	O
susceptible	O
to	O
change	O
)	O
in	O
transformerbased	B-MethodName
models	I-MethodName
to	O
strengthen	O
memories	O
of	O
the	O
previously	O
learned	O
tasks	O
in	O
the	O
LLL	B-TaskName
setting.•	O
We	O
propose	O
critical	O
component	O
identification	O
algorithm	O
which	O
analyzes	O
the	O
transformerbased	B-MethodName
LLL	I-MethodName
model	I-MethodName
with	O
rationales	O
so	O
as	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
freeze	O
.	O

This	O
step	O
is	O
so	O
called	O
critical	O
freezing	O
,	O
firstly	O
devised	O
in	O
computer	O
vision	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O

2020	O
)	O
but	O
we	O
adapted	O
it	O
to	O
NLP.•	O
We	O
propose	O
that	O
unsupervised	O
generated	O
rationales	O
by	O
InvRat	B-MethodName
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
can	O
be	O
effectively	O
used	O
as	O
substitutions	O
of	O
human	O
rationales	O
,	O
allowing	O
our	O
framework	O
to	O
be	O
applied	O
to	O
generic	O
NLP	O
datasets	O
.	O

We	O
evaluated	O
Rational	B-MethodName
LAMOL	I-MethodName
on	O
six	O
task	O
order	O
permutations	O
of	O
three	O
datasets	O
from	O
the	O
ERASER	O
benchmark	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
framework	O
outperformed	O
the	O
original	O
LAMOL	B-MethodName
on	O
five	O
out	O
of	O
the	O
six	O
permutations	O
,	O
achieving	O
average	O
improvements	O
of	O
1.83	B-MetricValue
%	I-MetricValue
with	O
a	O
lower	O
standard	O
deviation	O
of	O
4.57	B-MetricValue
%	I-MetricValue
.	O

Moreover	O
,	O
using	O
unsupervised	O
rationale	O
generation	O
instead	O
of	O
human	O
rationales	O
also	O
yielded	O
competitive	O
performance	O
,	O
achieving	O
average	O
improvements	O
of	O
2.67	B-MetricValue
%	I-MetricValue
from	O
original	O
LAMOL	B-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
introduce	O
the	O
concept	O
of	O
lifelong	B-TaskName
learning	I-TaskName
,	O
catastrophic	O
forgetting	O
,	O
and	O
component	O
freezing	O
which	O
are	O
relevant	O
to	O
the	O
core	O
idea	O
of	O
Rational	B-MethodName
LAMOL	I-MethodName
.	O

We	O
also	O
briefly	O
summarize	O
prominent	O
researches	O
related	O
to	O
rationales	O
.	O

While	O
people	O
fine	O
tune	O
a	O
pre	O
-	O
trained	O
model	O
to	O
perform	O
a	O
single	O
task	O
,	O
lifelong	B-TaskName
learning	I-TaskName
(	O
LL	B-TaskName
)	O
is	O
a	O
setting	O
in	O
which	O
a	O
learner	O
performs	O
sequential	O
learning	O
of	O
infinitely	O
incoming	O
tasks	O
τ	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
...	O
,	O
τ	O
i	O
,	O
...	O
,	O
}	O
,	O
where	O
τ	O
i	O
is	O
the	O
i	O
-	O
th	O
task	O
to	O
learn	O
at	O
a	O
particular	O
point	O
in	O
time	O
.	O

The	O
objective	O
of	O
the	O
LL	B-TaskName
learner	O
is	O
to	O
ideally	O
both	O
optimize	O
the	O
performance	O
on	O
the	O
new	O
task	O
and	O
maintain	O
optimal	O
performance	O
on	O
previous	O
tasks	O
τ	O
t	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
...	O
,	O
i.	O
Moreover	O
,	O
the	O
ability	O
to	O
transfer	O
knowledge	O
across	O
different	O
tasks	O
is	O
also	O
desired	O
.	O

However	O
,	O
naively	O
training	O
on	O
a	O
sequence	O
of	O
tasks	O
without	O
accounting	O
for	O
the	O
difference	O
in	O
data	O
distributions	O
would	O
result	O
in	O
an	O
abrupt	O
decrease	O
in	O
old	O
tasks	O
performance	O
.	O

This	O
phenomenon	O
is	O
known	O
as	O
Catastrophic	O
Forgetting	O
(	O
McCloskey	O
and	O
Cohen	O
,	O
1989	O
)	O
.	O

There	O
are	O
multiple	O
existing	O
works	O
that	O
aim	O
to	O
mitigate	O
catastrophic	O
forgetting	O
in	O
LL	B-TaskName
.	O

They	O
can	O
be	O
categorized	O
into	O
three	O
major	O
approaches	O
.	O

First	O
,	O
regularization	O
methods	O
use	O
a	O
regularization	O
term	O
to	O
constrain	O
changes	O
when	O
updating	O
weights	O
in	O
a	O
new	O
task	O
(	O
Kirkpatrick	O
et	O
al	O
.	O
,	O

2017;Aljundi	O
et	O
al	O
.	O
,	O

2017	O
;	O
.	O

Second	O
,	O
data	O
-	O
based	O
methods	O
disallow	O
significant	O
deviation	O
of	O
weights	O
from	O
previous	O
tasks	O
by	O
keeping	O
a	O
small	O
subset	O
of	O
data	O
from	O
the	O
previous	O
tasks	O
or	O
generating	O
pseudo	O
-	O
data	O
to	O
refresh	O
the	O
learned	O
knowledge	O
(	O
Lopez	O
-	O
Paz	O
and	O
Ranzato	O
,	O
2017;Chaudhry	O
et	O
al	O
.	O
,	O

2019;de	O
Masson	O
d'Autume	O
et	O
al	O
.	O
,	O

2019;Li	O
and	O
Hoiem	O
,	O
2018	O
)	O
.	O

Third	O
,	O
architecture	O
-	O
based	O
methods	O
dynamically	O
transform	O
the	O
neural	O
network	O
architectures	O
in	O
order	O
to	O
accommodate	O
new	O
knowledge	O
(	O
Rusu	O
et	O
al	O
.	O
,	O

2016;.Lifelong	B-TaskName
Language	I-TaskName
Learning	I-TaskName
or	O
LLL	B-TaskName
is	O
a	O
scenario	O
where	O
a	O
model	O
sequentially	O
learns	O
from	O
a	O
stream	O
of	O
NLP	O
tasks	O
in	O
an	O
LL	B-TaskName
manner	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
LLL	B-TaskName
has	O
rarely	O
been	O
studied	O
and	O
previous	O
works	O
usually	O
target	O
a	O
single	O
type	O
of	O
NLP	O
tasks	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2015;Liu	O
et	O
al	O
.	O
,	O

2019;de	O
Masson	O
d'Autume	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

To	O
go	O
beyond	O
this	O
limitation	O
,	O
Sun	O
et	O
al	O
.	O
(	O

2020	O
)	O
proposed	O
LAMOL	B-MethodName
,	O
a	O
learning	O
framework	O
that	O
utilizes	O
a	O
language	O
model	O
to	O
simultaneously	O
predict	O
outputs	O
and	O
learn	O
to	O
generate	O
pseudo	O
-	O
training	O
examples	O
,	O
which	O
are	O
exploited	O
to	O
alleviate	O
catastrophic	O
forgetting	O
.	O

Hence	O
,	O
LAMOL	B-MethodName
,	O
as	O
well	O
as	O
our	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
naturally	O
falls	O
into	O
the	O
data	O
-	O
based	O
LL	B-TaskName
approach	O
since	O
data	O
from	O
previous	O
tasks	O
,	O
albeit	O
generated	O
,	O
is	O
utilized	O
to	O
constrain	O
a	O
model	O
.	O

Component	O
Freezing	O
While	O
component	O
freezing	O
is	O
also	O
a	O
common	O
practice	O
in	O
the	O
fine	O
-	O
tuning	O
process	O
,	O
it	O
is	O
done	O
to	O
prevent	O
loss	O
in	O
general	O
knowledge	O
in	O
lower	O
layers	O
of	O
the	O
model	O
(	O
Raganato	O
and	O
Tiedemann	O
,	O
2018).By	O
contrast	O
,	O
many	O
architecture	O
-	O
based	O
LL	B-TaskName
methods	O
,	O
for	O
example	O
Rusu	O
et	O
al	O
.	O
(	O

2016	O
)	O
,	O
utilize	O
component	O
freezing	O
to	O
prevent	O
changes	O
to	O
learned	O
knowledge	O
from	O
previous	O
tasks	O
and	O
enlarge	O
the	O
model	O
to	O
accommodate	O
new	O
tasks	O
,	O
thereby	O
making	O
the	O
model	O
immune	O
to	O
forgetting	O
.	O

Our	O
Rational	B-MethodName
LAMOL	I-MethodName
also	O
uses	O
component	O
freezing	O
,	O
but	O
unlike	O
architecturebased	O
methods	O
,	O
only	O
a	O
small	O
part	O
of	O
the	O
model	O
is	O
frozen	O
and	O
its	O
size	O
is	O
constant	O
throughout	O
the	O
learning	O
process	O
.	O

Rationales	O
Rationales	O
are	O
reasons	O
for	O
labels	O
or	O
predictions	O
.	O

In	O
NLP	O
,	O
they	O
are	O
usually	O
parts	O
of	O
the	O
input	O
texts	O
which	O
support	O
or	O
contribute	O
to	O
the	O
class	O
labels	O
.	O

Rationales	O
could	O
be	O
either	O
annotated	O
by	O
humans	O
or	O
generated	O
by	O
machine	O
learning	O
models	O
.	O

Human	O
rationales	O
have	O
been	O
used	O
to	O
enhance	O
machine	O
learning	O
in	O
multiple	O
studies	O
.	O

For	O
instance	O
,	O
Rajani	O
et	O
al	O
.	O
(	O

2019	O
)	O
used	O
the	O
rationales	O
to	O
guide	O
a	O
neural	O
network	O
toward	O
better	O
reasoning	O
.	O

Bao	O
et	O
al	O
.	O
(	O

2018	O
)	O
utilized	O
rationales	O
as	O
auxiliary	O
information	O
to	O
train	O
a	O
neural	O
network	O
model	O
,	O
reducing	O
training	O
examples	O
required	O
to	O
achieve	O
good	O
results	O
.	O

Recently	O
,	O
DeYoung	O
et	O
al	O
.	O
(	O

2020	O
)	O
introduced	O
the	O
ERASER	O
benchmark	O
consisting	O
of	O
multiple	O
datasets	O
,	O
all	O
of	O
which	O
are	O
annotated	O
with	O
human	O
rationales	O
.	O

This	O
facilitates	O
the	O
advancement	O
of	O
research	O
on	O
interpretable	O
NLP	O
.	O

In	O
the	O
experiment	O
,	O
we	O
used	O
human	O
rationales	O
from	O
ERASER	O
in	O
the	O
critical	O
component	O
identification	O
step	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
be	O
frozen	O
.	O

Meanwhile	O
,	O
some	O
researchers	O
attempt	O
to	O
design	O
architectures	O
to	O
predict	O
rationales	O
from	O
labelled	O
data	O
.	O

Existing	O
rationalization	O
techniques	O
commonly	O
use	O
the	O
maximum	O
mutual	O
information	O
(	O
MMI	O
)	O
criterion	O
to	O
select	O
rationales	O
,	O
which	O
is	O
prone	O
to	O
choosing	O
spurious	O
correlation	O
between	O
input	O
features	O
and	O
outputs	O
as	O
rationales	O
(	O
Lei	O
et	O
al	O
.	O
,	O

2016;Yu	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

To	O
fix	O
this	O
issue	O
,	O
Invariant	O
Rationalization	O
(	O
InvRat	O
)	O
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
follows	O
the	O
invariant	O
risk	O
minimization	O
(	O
IRM	O
)	O
paradigm	O
,	O
as	O
introduced	O
by	O
Arjovsky	O
et	O
al	O
.	O
(	O

2019	O
)	O
.	O

It	O
utilizes	O
the	O
environment	O
variable	O
to	O
isolate	O
and	O
select	O
the	O
causal	O
features	O
that	O
faithfully	O
explain	O
the	O
output	O
.	O

In	O
order	O
to	O
allow	O
Rational	B-MethodName
LAMOL	I-MethodName
to	O
be	O
applied	O
to	O
any	O
NLP	O
dataset	O
,	O
we	O
choose	O
to	O
leverage	O
InvRat	O
to	O
automatically	O
produce	O
rationales	O
due	O
to	O
its	O
superior	O
performance	O
and	O
straightforward	O
application	O
,	O
removing	O
the	O
need	O
for	O
human	O
rationales	O
.	O

We	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section	O
.	O

As	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
based	O
from	O
LAMOL	B-MethodName
(	O
Sun	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
briefly	O
explain	O
LAMOL	B-MethodName
in	O
Section	O
3.1	O
.	O

Then	O
we	O
introduce	O
the	O
core	O
lifelong	B-TaskName
learning	I-TaskName
framework	O
of	O
Rational	B-MethodName
LAMOL	I-MethodName
in	O
Section	O
3.2	O
.	O

This	O
is	O
followed	O
by	O
two	O
proposed	O
enhancements	O
including	O
critical	O
component	O
identification	O
and	O
unsupervised	O
rationale	O
generation	O
,	O
detailed	O
in	O
Section	O
3.3	O
and	O
3.4	O
,	O
respectively	O
.	O

Language	B-MethodName
Modeling	I-MethodName
for	I-MethodName
Lifelong	I-MethodName
Language	I-MethodName
Learning	I-MethodName
(	O
LAMOL	B-MethodName
)	O
(	O
Sun	O
et	O
al	O
.	O
,	O

2020	O
)	O
utilizes	O
a	O
single	O
language	O
model	O
(	O
LM	O
)	O
as	O
a	O
multipurpose	O
model	O
.	O

Framing	O
all	O
tasks	O
as	O
question	O
answering	O
(	O
QA	O
)	O
,	O
the	O
LM	O
now	O
poses	O
as	O
a	O
generic	O
task	O
-	O
agnostic	O
model	O
.	O

In	O
addition	O
,	O
LAMOL	B-MethodName
trains	O
the	O
LM	O
as	O
a	O
generative	O
model	O
upon	O
receiving	O
a	O
special	O
generation	O
token	O
.	O

Using	O
a	O
single	O
model	O
for	O
both	O
providing	O
answers	O
and	O
generating	O
pseudo	O
-	O
samples	O
,	O
LAMOL	B-MethodName
truly	O
exhibits	O
a	O
model	O
of	O
LM	O
and	O
QA	O
duality	O
.	O

The	O
benefit	O
that	O
comes	O
with	O
the	O
generative	O
part	O
of	O
the	O
model	O
tackles	O
the	O
long	O
-	O
standing	O
issue	O
of	O
LLcatastrophic	B-TaskName
forgetting	O
.	O

While	O
other	O
methods	O
make	O
use	O
of	O
extra	O
memory	O
or	O
model	O
capacity	O
to	O
preserve	O
a	O
subset	O
of	O
real	O
samples	O
(	O
Lopez	O
-	O
Paz	O
and	O
Ranzato	O
,	O
2017;Chaudhry	O
et	O
al	O
.	O
,	O

2019	O
)	O
or	O
to	O
accomodate	O
a	O
separate	O
generator	O
(	O
Shin	O
et	O
al	O
.	O
,	O

2017;Kemker	O
and	O
Kanan	O
,	O
2017	O
)	O
,	O
LAMOL	B-MethodName
transfers	O
all	O
the	O
responsibilities	O
into	O
a	O
single	O
model	O
.	O

It	O
learns	O
the	O
ability	O
to	O
select	O
potentially	O
prominent	O
features	O
befitting	O
learning	O
by	O
modeling	O
the	O
input	O
.	O

This	O
allows	O
the	O
model	O
to	O
replay	O
meaningful	O
pseudo	O
-	O
samples	O
from	O
previous	O
tasks	O
while	O
forcing	O
the	O
model	O
to	O
memorize	O
knowledge	O
acquired	O
from	O
previous	O
tasks	O
tied	O
to	O
the	O
generation	O
token	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
exploiting	O
rationales	O
with	O
LAMOL	B-MethodName
to	O
further	O
improve	O
the	O
LLL	B-TaskName
performance	O
,	O
discussed	O
next	O
.	O

Rational	B-MethodName
LAMOL	I-MethodName
,	O
illustrated	O
in	O
Figure	O
1	O
(	O
right	O
)	O
,	O
is	O
a	O
learning	O
framework	O
revolving	O
around	O
the	O
original	O
methodologies	O
of	O
LAMOL	B-MethodName
.	O

We	O
consider	O
an	O
LL	B-TaskName
setting	O
where	O
τ	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
...	O
,	O
τ	O
i	O
,	O
...	O
}	O
is	O
a	O
stream	O
of	O
learning	O
tasks	O
and	O
τ	O
i	O
is	O
the	O
i	O
-	O
th	O
task	O
to	O
train	O
at	O
a	O
particular	O
point	O
in	O
time	O
.	O

Let	O
M	O
i	O
denote	O
the	O
model	O
M	O
after	O
being	O
trained	O
for	O
task	O
i	O
,	O
where	O
M	O
0	O
is	O
the	O
initialized	O
pre	O
-	O
trained	O
model	O
.	O

Using	O
these	O
notations	O
and	O
starting	O
from	O
M	O
0	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
works	O
iteratively	O
in	O
four	O
steps	O
as	O
follows	O
.	O

First	O
,	O
given	O
a	O
model	O
M	O
i	O
,	O
it	O
trains	O
M	O
i	O
with	O
the	O
task	O
τ	O
i+1	O
using	O
LAMOL	B-MethodName
's	O
training	O
procedure	O
to	O
obtainM	O
i+1	O
.	O

Second	O
,	O
for	O
i	O
>	O
0	O
,	O
it	O
applies	O
critical	O
component	O
identification	O
,	O
which	O
is	O
described	O
in	O
Section	O
3.3	O
,	O
on	O
M	O
i	O
andM	O
i+1	O
with	O
the	O
rationales	O
of	O
task	O
τ	O
i	O
to	O
dissect	O
the	O
most	O
plastic	O
layers	O
or	O
blocks	O
.	O

Third	O
,	O
we	O
take	O
a	O
step	O
back	O
to	O
work	O
at	O
M	O
i	O
and	O
apply	O
critical	O
freezing	O
,	O
i.e.	O
,	O
freezing	O
the	O
most	O
plastic	O
components	O
,	O
The	O
input	O
is	O
fed	O
through	O
each	O
attention	O
block	O
AT	O
j	O
,	O
where	O
each	O
block	O
j	O
has	O
multiple	O
heads	O
.	O

B	O
:	O
A	O
single	O
attention	O
head	O
AT	O
j	O
,	O
a	O
consists	O
of	O
the	O
attention	O
of	O
the	O
sequence	O
in	O
relation	O
to	O
all	O
other	O
tokens	O
,	O
as	O
shown	O
in	O
C.	O
Finally	O
,	O
the	O
IoU	O
calculation	O
F	O
is	O
applied	O
on	O
the	O
hard	O
selection	O
of	O
attention	O
token	O
with	O
percentiles	O
D	O
and	O
the	O
rationale	O
ground	O
truth	O
in	O
E.to	O
obtain	O
M	O
CF	O
i	O
.	O

Lastly	O
,	O
we	O
train	O
M	O
CF	O
i	O
through	O
the	O
task	O
τ	O
i+1	O
again	O
to	O
get	O
a	O
new	O
model	O
M	O
i+1	O
that	O
retains	O
the	O
most	O
plastic	O
memories	O
.	O

Note	O
that	O
despite	O
the	O
unique	O
nature	O
of	O
LAMOL	B-MethodName
,	O
our	O
Rational	B-MethodName
LAMOL	I-MethodName
does	O
not	O
limit	O
its	O
usage	O
to	O
a	O
single	O
model	O
architecture	O
.	O

It	O
has	O
potential	O
applications	O
to	O
general	O
attention	O
-	O
based	O
models	O
suffering	O
from	O
catastrophic	O
forgetting	O
through	O
domain	O
shifts	O
across	O
tasks	O
.	O

We	O
propose	O
the	O
Critical	O
Component	O
Identification	O
(	O
CCI	O
)	O
algorithm	O
,	O
pointing	O
out	O
the	O
most	O
plastic	O
block	O
of	O
our	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
LL	I-MethodName
model	I-MethodName
before	O
moving	O
on	O
to	O
a	O
new	O
task	O
completely	O
.	O
(	O

This	O
shares	O
the	O
same	O
spirit	O
as	O
Nguyen	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
proposing	O
Auto	O
DeepVis	O
to	O
find	O
the	O
most	O
plastic	O
blocks	O
of	O
CNN	O
models	O
for	O
image	O
classification	O
.	O
)	O

The	O
chosen	O
block	O
is	O
the	O
one	O
that	O
forgets	O
what	O
it	O
has	O
learned	O
from	O
the	O
recent	O
task	O
the	O
most	O
when	O
being	O
introduced	O
a	O
new	O
task	O
,	O
so	O
we	O
will	O
freeze	O
the	O
block	O
to	O
prevent	O
catastrophic	O
forgetting	O
in	O
Rational	B-MethodName
LAMOL.As	I-MethodName
shown	O
in	O
Algorithm	O
1	O
,	O
for	O
each	O
validation	O
sample	O
x	O
∈	O
X	O
of	O
task	O
i	O
,	O
the	O
CCI	O
compares	O
the	O
attention	O
maps	O
AT	O
produced	O
by	O
the	O
model	O
M	O
i	O
(	O
i.e.	O
,	O
the	O
old	O
model	O
M	O
O	O
in	O
Algorithm	O
1	O
)	O
andM	O
i+1	O
(	O
i.e.	O
,	O
the	O
new	O
model	O
M	O
N	O
in	O
Algorithm	O
1	O
)	O
to	O
find	O
the	O
most	O
plastic	O
block	O
b	O
with	O
respect	O
to	O
this	O
sample	O
.	O

Then	O
it	O
returns	O
the	O
block	O
F	O
which	O
is	O
the	O
mode	O
of	O
all	O
b	O
,	O
voted	O
by	O
most	O
of	O
the	O
samples	O
in	O
X.	O
Note	O
that	O
most	O
of	O
the	O
variable	O
names	O
are	O
preserved	O
similar	O
to	O
Nguyen	O
et	O
al	O
.	O
(	O

2020	O
)	O
for	O
ease	O
of	O
reference	O
,	O
and	O
some	O
sections	O
are	O
refactored	O
for	O
readability	O
.	O

In	O
particular	O
,	O
to	O
find	O
b	O
for	O
the	O
sample	O
x	O
,	O
we	O
iterate	O
over	O
all	O
blocks	O
j	O
=	O
1	O
,	O
...	O
,	O
K	O
and	O
perform	O
two	O
steps	O
.	O

First	O
,	O
we	O
find	O
the	O
representative	O
map	O
of	O
the	O
block	O
j	O
in	O
M	O
O	O
with	O
respect	O
to	O
the	O
ground	O
truth	O
GT	O
(	O
i.e.	O
,	O
RM	O
M	O
O	O
,	O
GT	O
(	O
j	O
)	O
)	O
by	O
selecting	O
the	O
attention	O
map	O
of	O
the	O
attention	O
head	O
a	O
*	O
and	O
the	O
token	O
s	O
*	O
in	O
x	O
from	O
the	O
block	O
j	O
that	O
is	O
most	O
similar	O
to	O
the	O
human	O
rationale	O
for	O
the	O
sample	O
x	O
(	O
i.e.	O
,	O
ground	O
truth	O
GT	O
in	O
Algorithm	O
1	O
)	O
.	O

Although	O
interpretable	O
NLP	O
stands	O
to	O
be	O
a	O
nascent	O
subfield	O
for	O
exploration	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
elementary	O
visualization	O
of	O
attentions	O
are	O
possible	O
in	O
Transformers	B-MethodName
(	O
Vig	O
,	O
2019;Hoover	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

These	O
self	O
-	O
attention	O
mechanisms	O
associate	O
distant	O
positions	O
of	O
a	O
single	O
sequence	O
and	O
many	O
appear	O
to	O
exhibit	O
behavior	O
related	O
to	O
the	O
sentences	O
'	O
syntactic	O
and	O
semantic	O
structure	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

We	O
hypothesize	O
that	O
the	O
semantic	O
nature	O
of	O
the	O
self	O
-	O
attention	O
mechanisms	O
would	O
opt	O
for	O
tokens	O
most	O
relating	O
to	O
positive	O
evidence	O
vital	O
for	O
predictions	O
,	O
being	O
analogous	O
to	O
rationales	O
-	O
snippets	O
thatAlgorithm	O
1	O
Critical	O
Component	O
Identification	O
Input	O
:	O
Validation	O
set	O
X	O
,	O
ground	O
truth	O
rationale	O
GT	O
,	O
old	O
model	O
M	O
O	O
,	O
new	O
model	O
M	O
N	O
,	O
number	O
of	O
blocks	O
K	O
Output	O
:	O
Critical	O
block	O
F	O
Ł←	O
∅	O
for	O
all	O
validation	O
sample	O
x	O
∈	O
X	O
do	O
:	O
IoUs	O
←	O
∅	O
AT	O
O	O
,	O
AT	O
N	O
←	O
[	O
M	O
O	O
(	O
x	O
)	O
,	O
M	O
N	O
(	O
x	O
)	O
]	O
for	O
j	O
=	O
1	O
,	O
K	O
do	O
:	O
RM	O
M	O
O	O
,	O
GT	O
←	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
with	O
highest	O
IoU	O
M	O
O	O
,	O
GT	O
RM	O
M	O
N	O
,	O
M	O
O	O
←	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
with	O
highest	O
IoU	O
M	O
N	O
,	O
M	O
O	O
APPEND(IoUs	O
,	O
max(IoU	O
M	O
N	O
,	O
M	O
O	O
)	O
)	O
end	O
for	O
b	O
←	O
arg	O
min	O
j	O
IoUs	O
APPEND(Ł	O
,	O
b	O
)	O
end	O
for	O
F	O
=	O
MODE(Ł	O
)	O
return	O
Fsupport	O
outputs	O
.	O

To	O
compute	O
the	O
similarity	O
between	O
attention	O
maps	O
and	O
human	O
rationales	O
,	O
we	O
use	O
Intersection	O
over	O
Union	O
(	O
IoU	O
)	O
.	O

Formally	O
,	O
the	O
following	O
equations	O
explain	O
this	O
step	O
.	O

RM	O
M	O
,	O
GT	O
(	O
j	O
)	O
=	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
(	O
1)where	O
(	O
a	O
*	O
,	O
s	O
*	O
)	O
=	O
arg	O
max	O
a∈A	O
,	O
s∈S	O
(	O
IoU	O
M	O
,	O
GT	O
(	O
j	O
,	O
a	O
,	O
s))andIoU	O
M	O
,	O
GT	O
(	O
j	O
,	O
a	O
,	O
s	O
)	O
=	O
P	O
β	O
(	O
AT	O
j	O
,	O
a	O
,	O
s	O
)	O
∩	O
GT	O
P	O
β	O
(	O
AT	O
j	O
,	O
a	O
,	O
s	O
)	O
∪	O
GT(3)A	O
is	O
the	O
set	O
of	O
all	O
attention	O
heads	O
in	O
the	O
block	O
,	O
and	O
S	O
is	O
the	O
set	O
of	O
all	O
tokens	O
in	O
x.	O
IoU	O
M	O
,	O
GT	O
(	O
j	O
,	O
a	O
,	O
s	O
)	O
reflects	O
the	O
similarity	O
between	O
the	O
ground	O
truth	O
and	O
the	O
attention	O
map	O
of	O
the	O
block	O
j	O
,	O
head	O
a	O
,	O
and	O
token	O
s	O
in	O
x.	O
Since	O
the	O
ground	O
truth	O
contains	O
binary	O
labels	O
indicating	O
whether	O
a	O
token	O
is	O
a	O
part	O
of	O
the	O
rationale	O
or	O
not	O
,	O
we	O
need	O
to	O
convert	O
the	O
attention	O
map	O
AT	O
j	O
,	O
a	O
,	O
s	O
into	O
binary	O
labels	O
using	O
P	O
β	O
-a	O
simple	O
binary	O
thresholding	O
which	O
returns	O
1	O
for	O
the	O
value	O
greater	O
than	O
the	O
β	O
-	O
th	O
percentile	O
on	O
the	O
entire	O
sequence	O
(	O
otherwise	O
,	O
0	O
)	O
.	O

This	O
is	O
required	O
as	O
IoU	O
works	O
for	O
comparing	O
two	O
binary	O
masks	O
.	O

Actually	O
,	O
transformer	O
blocks	O
are	O
not	O
the	O
finest	O
granularity	O
that	O
we	O
could	O
freeze	O
.	O

Since	O
each	O
block	O
contains	O
several	O
attention	O
heads	O
,	O
it	O
is	O
possible	O
to	O
freeze	O
some	O
attention	O
heads	O
individually	O
.	O

Hence	O
,	O
we	O
propose	O
another	O
algorithm	O
,	O
applying	O
to	O
heads	O
.	O

This	O
is	O
similar	O
to	O
Algorithm	O
1	O
,	O
but	O
instead	O
of	O
searching	O
for	O
blocks	O
with	O
lowest	O
maximum	O
IoU	O
,	O
the	O
algorithm	O
searches	O
using	O
both	O
the	O
attention	O
blocks	O
and	O
attention	O
heads	O
together	O
as	O
keys	O
.	O

Although	O
the	O
definition	O
of	O
IoU	O
stays	O
the	O
same	O
,	O
the	O
definition	O
of	O
the	O
representative	O
map	O
will	O
be	O
at	O
a	O
higher	O
granularity	O
.	O

Formally	O
,	O
for	O
a	O
block	O
index	O
j	O
and	O
attention	O
head	O
a	O
,	O
RM	O
M	O
,	O
GT	O
will	O
be	O
computed	O
as	O
:	O
RM	O
M	O
,	O
GT	O
(	O
j	O
,	O
a	O
)	O
=	O
AT	O
j	O
,	O
a	O
,	O
s	O
*	O
(	O
4)where(s	O
*	O
)	O
=	O
arg	O
max	O
s∈S	O
(	O
IoU	O
M	O
,	O
GT	O
(	O
j	O
,	O
a	O
,	O
s))(5)and	O
we	O
can	O
freeze	O
top	O
n	O
heads	O
that	O
receives	O
most	O
votes	O
from	O
the	O
samples	O
in	O
the	O
validation	O
set	O
X.	O
As	O
described	O
in	O
Section	O
3.2	O
,	O
our	O
framework	O
requires	O
rationales	O
as	O
an	O
input	O
.	O

However	O
,	O
most	O
existing	O
NLP	O
datasets	O
are	O
not	O
annotated	O
with	O
rationales	O
.	O

To	O
overcome	O
the	O
limitation	O
,	O
we	O
leverage	O
a	O
recent	O
unsupervised	O
rationale	O
generation	O
framework	O
,	O
In	B-MethodName
-	I-MethodName
vRat	I-MethodName
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
to	O
generate	O
rationales	O
as	O
substitutions	O
.	O

Originally	O
,	O
InvRat	B-MethodName
was	O
designed	O
for	O
single	O
-	O
input	O
tasks	O
such	O
as	O
sentiment	O
analysis	O
.	O

However	O
,	O
since	O
some	O
of	O
the	O
datasets	O
we	O
experimented	O
with	O
are	O
text	O
-	O
pair	O
classification	O
,	O
we	O
append	O
the	O
query	O
(	O
or	O
question	O
)	O
at	O
the	O
end	O
of	O
each	O
sample	O
to	O
accommodate	O
these	O
tasks	O
.	O

4	O
Experimental	O
Setup	O
To	O
evaluate	O
our	O
proposed	O
framework	O
,	O
we	O
conducted	O
an	O
experiment	O
on	O
three	O
English	O
text	O
classification	O
datasets	O
,	O
curated	O
and	O
made	O
publicly	O
available	O
by	O
ERASER	O
1	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

All	O
of	O
the	O
three	O
datasets	O
,	O
as	O
listed	O
below	O
,	O
are	O
provided	O
with	O
rationales	O
marked	O
by	O
humans	O
.	O

Table	O
1	O
contains	O
a	O
summary	O
of	O
the	O
datasets	O
,	O
dataset	O
sizes	O
,	O
and	O
metrics.•	O
BoolQ	B-DatasetName
(	O
Clark	O
et	O
al	O
.	O
,	O

2019	O
):	O
a	O
dataset	O
comprises	O
selected	O
passages	O
from	O
Wikipedia	O
and	O
naturally	O
occurring	O
yes	O
/	O
no	O
questions	O
to	O
be	O
answered	O
by	O
the	O
model.•	O
Movie	B-DatasetName
Reviews	I-DatasetName
(	O
Zaidan	O
and	O
Eisner	O
,	O
2008	O
):	O
a	O
dataset	O
composed	O
of	O
movie	O
reviews	O
.	O

It	O
contains	O
positive	O
and	O
negative	O
sentiment	O
labels	O
to	O
be	O
predicted	O
by	O
the	O
model.•	O
SciFact	B-DatasetName
(	O
Wadden	O
et	O
al	O
.	O
,	O

2020	O
):	O
a	O
dataset	O
containing	O
expert	O
-	O
written	O
scientific	O
claims	O
coupled	O
with	O
evidence	O
-	O
containing	O
abstracts	O
.	O

Given	O
a	O
claim	O
,	O
the	O
model	O
has	O
to	O
identify	O
if	O
the	O
abstract	O
supports	O
or	O
refutes	O
the	O
claim	O
.	O

We	O
ran	O
our	O
proposed	O
framework	O
on	O
all	O
six	O
permutations	O
of	O
task	O
order	O
for	O
three	O
times	O
with	O
different	O
random	O
seeds	O
.	O

The	O
average	O
results	O
are	O
then	O
reported	O
in	O
Section	O
5	O
.	O

We	O
followed	O
the	O
best	O
LAMOL	B-MethodName
configuration	O
from	O
Sun	O
et	O
al	O
.	O
(	O

2020	O
)	O
.	O

All	O
parameters	O
were	O
kept	O
at	O
the	O
default	O
values	O
.	O

For	O
all	O
methods	O
,	O
we	O
use	O
the	O
small	O
GPT-2	O
model	O
(	O
Radford	O
et	O
al	O
.	O
,	O

2019	O
)	O
as	O
the	O
language	O
model	O
.	O

Each	O
task	O
was	O
trained	O
for	O
five	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

We	O
applied	O
greedy	O
decoding	O
during	O
inference	O
.	O

Due	O
to	O
fine	O
-	O
tuning	O
instability	O
of	O
neural	O
network	O
,	O
in	O
each	O
task	O
order	O
,	O
we	O
used	O
the	O
same	O
first	O
task	O
model	O
M	O
1	O
for	O
all	O
methods	O
in	O
each	O
run	O
for	O
fair	O
comparison	O
.	O

Critical	O
freezing	O
was	O
applied	O
to	O
a	O
model	O
with	O
two	O
different	O
levels	O
of	O
granularity	O
:	O
block	O
level	O
and	O
head	O
level	O
.	O

The	O
validation	O
set	O
of	O
each	O
task	O
was	O
used	O
as	O
input	O
to	O
Algorithm	O
1	O
.	O

For	O
block	O
level	O
granularity	O
,	O
we	O
chose	O
to	O
freeze	O
the	O
most	O
frequent	O
block	O
obtained	O
from	O
the	O
algorithm	O
,	O
while	O
for	O
head	O
level	O
granularity	O
,	O
12	O
heads	O
chosen	O
returned	O
by	O
the	O
algorithm	O
were	O
kept	O
frozen	O
during	O
training	O
.	O

We	O
used	O
β	B-HyperparameterName
=	O
80	B-HyperparameterValue
,	O
i.e.	O
,	O
selecting	O
the	O
top	O
20	O
percentile	O
of	O
attention	O
scores	O
to	O
compare	O
with	O
ground	O
truth	O
rationales	O
.	O

As	O
the	O
ERASER	O
benchmark	O
has	O
an	O
average	O
ratio	O
of	O
rationale	O
tokens	O
to	O
document	O
tokens	O
of	O
around	O
9.4	O
%	O
,	O
we	O
allowed	O
rationale	B-HyperparameterName
selection	I-HyperparameterName
to	O
be	O
two	O
times	O
the	O
average	O
ratio	O
(	O
i.e.	O
,	O
20%).For	B-HyperparameterValue
InvRat	B-MethodName
,	O
we	O
opted	O
for	O
300	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
GloVe	B-HyperparameterName
embeddings	I-HyperparameterName
(	O
Pennington	O
et	O
al	O
.	O
,	O

2014	O
)	O
.	O

The	O
generator	O
and	O
the	O
predictor	O
modules	O
of	O
InvRat	B-MethodName
were	O
based	O
on	O
1	B-HyperparameterValue
-	I-HyperparameterValue
layer	I-HyperparameterValue
bidirectional	B-HyperparameterName
gated	I-HyperparameterName
recurrent	I-HyperparameterName
units	I-HyperparameterName
(	O
Chung	O
et	O
al	O
.	O
,	O

2014	O
)	O
with	O
256	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
as	O
in	O
Chang	O
et	O
al	O
.	O
(	O

2020	O
)	O
.	O

Maximum	B-HyperparameterName
model	I-HyperparameterName
input	I-HyperparameterName
was	O
set	O
to	O
1,024	B-HyperparameterValue
tokens	I-HyperparameterValue
.	O

All	O
hyperparameters	O
for	O
each	O
task	O
were	O
tuned	O
on	O
the	O
validation	O
set	O
.	O

This	O
section	O
reports	O
the	O
performance	O
of	O
Rationale	B-MethodName
LAMOL	I-MethodName
and	O
compares	O
it	O
with	O
LAMOL	B-MethodName
as	O
the	O
baseline	O
as	O
well	O
as	O
multitask	B-TaskName
learning	I-TaskName
,	O
which	O
is	O
considered	O
as	O
the	O
upper	O
bound	O
of	O
LL	B-TaskName
.	O

We	O
also	O
analyze	O
the	O
effect	O
of	O
each	O
component	O
in	O
the	O
proposed	O
framework	O
.	O

In	O
order	O
to	O
validate	O
if	O
component	O
freezing	O
truly	O
helps	O
reduce	O
catastrophic	O
forgetting	O
,	O
we	O
performed	O
partial	O
brute	O
force	O
block	O
-	O
level	O
freezing	O
on	O
each	O
task	O
permutation	O
to	O
approximately	O
determine	O
the	O
upper	O
bound	O
of	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
block	O
.	O

Due	O
to	O
limited	O
computing	O
resources	O
,	O
we	O
compromised	O
with	O
searching	O
for	O
all	O
even	O
-	O
numbered	O
block	O
indices	O
,	O
and	O
choosing	O
the	O
model	O
with	O
maximum	O
average	O
score	O
of	O
the	O
first	O
two	O
tasks	O
to	O
do	O
the	O
brute	O
force	O
on	O
the	O
latter	O
two	O
tasks	O
.	O

Since	O
brute	O
force	O
was	O
performed	O
on	O
a	O
per	O
-	O
task	O
basis	O
,	O
our	O
search	O
space	O
would	O
be	O
6	O
+	O
6	O
,	O
the	O
first	O
six	O
being	O
the	O
six	O
blocks	O
on	O
the	O
first	O
two	O
tasks	O
,	O
and	O
the	O
latter	O
six	O
being	O
the	O
six	O
blocks	O
on	O
the	O
last	O
two	O
tasks	O
.	O

Do	O
note	O
that	O
true	O
brute	O
force	O
would	O
be	O
12×12	O
.	O

Although	O
it	O
is	O
possible	O
that	O
our	O
partial	O
brute	O
force	O
is	O
sub	O
-	O
optimal	O
,	O
we	O
find	O
that	O
it	O
is	O
a	O
good	O
compromise	O
due	O
to	O
limited	O
computing	O
resources	O
.	O

The	O
results	O
are	O
presented	O
in	O
Table	O
2	O
.	O

Brute	O
force	O
was	O
able	O
to	O
outperform	O
vanilla	O
LAMOL	B-MethodName
by	O
a	O
substantial	O
margin	O
of	O
3.68	B-MetricValue
%	I-MetricValue
,	O
only	O
1.36	B-MetricValue
%	I-MetricValue
from	O
the	O
multitask	O
upper	O
bound	O
.	O

This	O
suggests	O
that	O
component	O
freezing	O
is	O
able	O
to	O
further	O
nullify	O
the	O
effect	O
of	O
catastrophic	O
forgetting	O
from	O
LAMOL	B-MethodName
.	O

It	O
also	O
achieved	O
a	O
standard	O
deviation	O
of	O
only	O
2.3	B-MetricValue
%	I-MetricValue
compared	O
with	O
LAMOL	B-MethodName
's	O
5.28	B-MetricValue
%	I-MetricValue
.	O

This	O
suggests	O
that	O
freezing	O
the	O
right	O
component	O
helps	O
with	O
task	O
order	O
resilience	O
.	O

A	O
sample	O
of	O
accuracy	B-MetricName
graphs	O
(	O
as	O
the	O
learning	O
progressed	O
)	O
of	O
the	O
compared	O
methods	O
,	O
with	O
the	O
BoolQ	B-DatasetName
→	O
SciFact	B-DatasetName
→	O
Movies	B-DatasetName
(	O
BSM	O
)	O
task	O
order	O
is	O
shown	O
in	O
Figure	O
4	O
from	O
top	O
to	O
bottom	O
,	O
respectively	O
.	O

As	O
the	O
first	O
task	O
,	O
BoolQ	B-DatasetName
was	O
not	O
really	O
affected	O
by	O
SciFact	B-DatasetName
,	O
but	O
encountered	O
a	O
heavy	O
drop	O
during	O
the	O
third	O
task	O
of	O
Movies	B-DatasetName
.	O

In	O
the	O
baseline	O
,	O
BoolQ	B-DatasetName
dropped	O
from	O
61	B-MetricValue
%	I-MetricValue
to	O
a	O
mere	O
6	B-MetricValue
%	I-MetricValue
,	O
while	O
only	O
rebounding	O
up	O
to	O
26	B-MetricValue
%	I-MetricValue
at	O
the	O
end	O
.	O

However	O
,	O
after	O
freezing	O
the	O
most	O
plastic	O
block	O
identified	O
by	O
partial	O
brute	O
forcing	O
,	O
BoolQ	B-DatasetName
dropped	O
from	O
62	B-MetricValue
%	I-MetricValue
to	O
15	B-MetricValue
%	I-MetricValue
,	O
and	O
rebounding	O
up	O
to	O
47	B-MetricValue
%	I-MetricValue
.	O

Comparatively	O
,	O
in	O
the	O
second	O
task	O
,	O
SciFact	B-DatasetName
encountered	O
a	O
smaller	O
drop	O
during	O
the	O
third	O
task	O
from	O
63	B-MetricValue
%	I-MetricValue
to	O
55	B-MetricValue
%	I-MetricValue
,	O
and	O
then	O
Green	O
background	O
refers	O
to	O
the	O
epochs	O
on	O
which	O
the	O
model	O
is	O
first	O
introduced	O
with	O
a	O
particular	O
task	O
.	O

In	O
this	O
figure	O
,	O
for	O
example	O
,	O
the	O
model	O
is	O
trained	O
on	O
Bool	B-DatasetName
-	I-DatasetName
Q	I-DatasetName
and	O
evaluated	O
on	O
all	O
the	O
three	O
tasks	O
during	O
epoch	O
1	O
-	O
5	O
.	O

rebounded	O
back	O
to	O
65	B-MetricValue
%	I-MetricValue
.	O

As	O
the	O
last	O
task	O
,	O
movies	B-DatasetName
was	O
not	O
affected	O
by	O
catastrophic	O
forgetting	O
.	O

Accuracy	B-MetricName
graphs	O
for	O
all	O
permutation	O
of	O
tasks	O
is	O
available	O
in	O
Appendix	O
6	O
from	O
which	O
we	O
make	O
several	O
observations	O
concerning	O
the	O
effect	O
of	O
task	O
orders	O
on	O
the	O
overall	O
performance:•	O
There	O
is	O
evidence	O
that	O
Movies	B-DatasetName
accelerate	O
the	O
forgetting	O
process	O
of	O
first	O
task	O
due	O
to	O
the	O
abrupt	O
change	O
in	O
data	O
distribution.•	O
However	O
,	O
the	O
performance	O
on	O
the	O
task	O
Movies	B-DatasetName
itself	O
is	O
barely	O
affected	O
by	O
the	O
task	O
order	O
.	O

We	O
attribute	O
it	O
to	O
the	O
low	O
difficulty	O
of	O
the	O
task.•	O
There	O
is	O
usually	O
no	O
interference	O
between	O
the	O
tasks	O
Bool	B-DatasetName
-	I-DatasetName
Q	I-DatasetName
and	O
SciFact	B-DatasetName
when	O
these	O
tasks	O
are	O
trained	O
in	O
adjacency	O
since	O
they	O
are	O
similar	O
.	O

It	O
is	O
unrealistic	O
to	O
perform	O
brute	O
force	O
in	O
every	O
single	O
setting	O
.	O

So	O
,	O
it	O
is	O
crucial	O
that	O
our	O
algorithm	O
uses	O
reasonable	O
amount	O
of	O
time	O
while	O
still	O
maintaining	O
improvements	O
from	O
the	O
baseline	O
.	O

The	O
CCI	O
algorithm	O
requires	O
each	O
task	O
except	O
task	O
1	O
to	O
be	O
repeated	O
twice	O
.	O

This	O
doubles	O
the	O
time	O
needed	O
to	O
train	O
a	O
single	O
task	O
.	O

Combined	O
with	O
time	O
required	O
for	O
CCI	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
required	O
approximately	O
2.4	O
times	O
more	O
time	O
than	O
vanilla	O
LAMOL	B-MethodName
to	O
completely	O
train	O
a	O
model	O
as	O
shown	O
in	O
Figure	O
3	O
.	O

On	O
the	O
other	O
hand	O
,	O
our	O
algorithm	O
used	O
only	O
approximately	O
half	O
of	O
the	O
time	O
it	O
took	O
to	O
train	O
in	O
the	O
partial	O
brute	O
force	O
fashion	O
.	O

Currently	O
,	O
CCI	O
only	O
measures	O
plasticity	O
in	O
between	O
two	O
models	O
(	O
M	O
i	O
andM	O
i+1	O
)	O
.	O

Single	O
model	O
analysis	O
for	O
layer	O
plasticity	O
evaluation	O
is	O
left	O
for	O
future	O
work	O
.	O

From	O
Table	O
2	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
block	O
outperformed	O
LAMOL	B-MethodName
by	O
1.83	B-MetricValue
%	I-MetricValue
average	O
accuracy	B-MetricName
(	O
0.97	B-MetricValue
%	I-MetricValue
average	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
over	O
all	O
permutations	O
while	O
having	O
smaller	O
standard	O
deviation	O
,	O
indicating	O
that	O
it	O
is	O
also	O
more	O
robust	O
to	O
task	O
orders	O
.	O

Rational	B-MethodName
LAMOL	I-MethodName
head	O
was	O
able	O
to	O
match	O
or	O
outperform	O
LAMOL	B-MethodName
in	O
five	O
out	O
of	O
six	O
task	O
orders	O
,	O
but	O
the	O
significant	O
decrease	O
in	O
the	O
SBM	O
order	O
lowered	O
the	O
average	O
to	O
a	O
0.43	B-MetricValue
%	I-MetricValue
gain	O
(	O
and	O
a	O
slight	O
decrease	O
in	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
from	O
the	O
baseline	O
.	O

Upon	O
further	O
inspection	O
,	O
we	O
found	O
that	O
the	O
pseudo	O
-	O
samples	O
of	O
SciFact	B-DatasetName
contained	O
high	O
variance	O
in	O
quality	O
during	O
pseudodata	O
replay	O
.	O

In	O
addition	O
to	O
generation	O
token	O
mismatch	O
,	O
i.e.	O
,	O
a	O
situation	O
where	O
a	O
pseudo	O
-	O
sample	O
has	O
an	O
answer	O
token	O
from	O
a	O
wrong	O
task	O
,	O
the	O
low	O
volume	O
of	O
SciFact	B-DatasetName
training	O
data	O
affected	O
the	O
quality	O
of	O
the	O
pseudo	O
-	O
samples	O
generated	O
.	O

So	O
,	O
this	O
accelerated	O
catastrophic	O
forgetting	O
rather	O
than	O
alleviating	O
.	O

Without	O
the	O
SBM	O
drop	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
head	O
performed	O
comparatively	O
well	O
or	O
slightly	O
higher	O
with	O
the	O
block	O
-	O
level	O
.	O

Performing	O
a	O
one	O
-	O
tailed	O
paired	O
ttest	O
on	O
all	O
data	O
points	O
of	O
the	O
total	O
3	O
random	O
seeds	O
,	O
we	O
observed	O
that	O
block	O
-	O
level	O
freezing	O
is	O
able	O
to	O
win	O
against	O
the	O
original	O
LAMOL	B-MethodName
with	O
statistical	O
significance	O
(	O
p	O
-	O
value	O
of	O
0.023	O
and	O
0.042	O
for	O
block	O
-	O
level	O
and	O
generated	O
block	O
-	O
level	O
respectively	O
)	O
.	O

With	O
the	O
SBM	O
result	O
neglected	O
as	O
an	O
outlier	O
,	O
both	O
block	O
-	O
level	O
and	O
head	O
-	O
level	O
significantly	O
improved	O
the	O
results	O
compared	O
with	O
the	O
original	O
LAMOL	B-MethodName
(	O
p	O
-	O
value	O
of	O
0.015	O
,	O
0.014	O
,	O
0.010	O
,	O
0.049	O
for	O
block	O
-	O
level	O
,	O
generated	O
block	O
-	O
level	O
,	O
head	O
-	O
level	O
,	O
and	O
generated	O
headlevel	O
respectively	O
)	O
.	O

However	O
,	O
there	O
is	O
no	O
conclusive	O
evidence	O
of	O
which	O
method	O
(	O
head	O
-	O
level	O
or	O
blocklevel	O
freezing	O
)	O
being	O
significantly	O
better	O
(	O
p	O
-	O
value	O
of	O
0.133	O
)	O
.	O

Even	O
though	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
outperformed	O
the	O
baseline	O
,	O
there	O
was	O
still	O
a	O
gap	O
from	O
the	O
brute	O
force	O
upper	O
bound	O
.	O

This	O
could	O
be	O
due	O
to	O
many	O
incompatibilities	O
between	O
human	O
rationales	O
and	O
machine	O
attention	O
scores	O
,	O
as	O
mentioned	O
in	O
Bao	O
et	O
al	O
.	O
(	O

2018	O
)	O
,	O
which	O
made	O
our	O
algorithm	O
choose	O
sub	O
-	O
optimal	O
layers	O
/	O
heads	O
.	O

Due	O
to	O
the	O
difference	O
in	O
focus	O
between	O
human	O
and	O
machines	O
,	O
it	O
is	O
conceivable	O
that	O
the	O
rationales	O
generated	O
by	O
InvRat	B-MethodName
would	O
be	O
mostly	O
misaligned	O
with	O
human	O
rationales	O
.	O

This	O
is	O
shown	O
in	O
Table	O
3	O
,	O
where	O
the	O
F1	B-MetricName
scores	O
of	O
InvRat	B-MethodName
are	O
quite	O
low	O
when	O
compared	O
with	O
human	O
rationales	O
.	O

Figure	O
5	O
shows	O
an	O
example	O
of	O
generated	O
rationales	O
output	O
by	B-MethodName
InvRat	I-MethodName
compared	O
with	O
human	O
rationales	O
.	O

Despite	O
that	O
,	O
Generated	O
Rational	B-MethodName
LAMOL	I-MethodName
block	O
outperformed	O
both	O
Rational	B-MethodName
LAMOL	I-MethodName
and	O
LAMOL	B-MethodName
baseline	O
by	O
0.84	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
(	O
0.31	B-MetricValue
%	I-MetricValue
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
and	O
2.67	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
(	O
1.27	B-MetricValue
%	I-MetricValue
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
respectively	O
,	O
further	O
reducing	O
the	O
gap	O
to	O
Brute	O
Force	O
,	O
the	O
approximate	O
upper	O
bound	O
of	O
the	O
proposed	O
CCI	O
.	O

This	O
suggests	O
that	O
rationales	O
chosen	O
by	O
InvRat	B-MethodName
,	O
regardless	O
of	O
how	O
nonsensical	O
they	O
appear	O
,	O
still	O
carry	O
information	O
that	O
eliminates	O
the	O
need	O
for	O
human	O
rationales	O
.	O

The	O
results	O
are	O
consistent	O
with	O
Bao	O
et	O
al	O
.	O
(	O

2018	O
)	O
who	O
showed	O
that	O
significant	O
gains	O
are	O
achieved	O
when	O
using	O
machines	O
attention	O
scores	O
as	O
an	O
additional	O
supervision	O
signal	O
instead	O
of	O
using	O
human	O
rationales	O
.	O

Last	O
but	O
not	O
least	O
,	O
Figure	O
3	O
shows	O
that	O
the	O
process	O
of	O
generating	O
rationales	O
using	O
InvRat	B-MethodName
,	O
including	O
training	O
and	O
inference	O
,	O
contributed	O
only	O
marginally	O
,	O
about	O
15	O
minutes	O
,	O
to	O
the	O
total	O
time	O
used	O
in	O
the	O
training	O
process	O
.	O

To	O
effectively	O
retain	O
learned	O
knowledge	O
in	O
LL	B-TaskName
for	O
NLP	O
tasks	O
,	O
we	O
proposed	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
learning	O
framework	O
that	O
uses	O
rationales	O
to	O
identify	O
and	O
freeze	O
the	O
most	O
critical	O
components	O
of	O
the	O
model	O
while	O
being	O
trained	O
on	O
a	O
new	O
task	O
.	O

We	O
showed	O
that	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
able	O
to	O
outperform	O
LAMOL	B-MethodName
by	O
a	O
significant	O
margin	O
.	O

Furthermore	O
,	O
our	O
framework	O
can	O
be	O
applied	O
to	O
any	O
NLP	O
datasets	O
by	O
leveraging	O
unsupervised	O
rationale	O
generation	O
,	O
eliminating	O
the	O
need	O
for	O
human	O
rationales	O
while	O
maintaining	O
comparable	O
improvements	O
.	O

Overall	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
bridges	O
the	O
gap	O
between	O
LL	B-TaskName
in	O
NLP	O
with	O
model	O
understanding	O
through	O
rationales	O
,	O
exhibiting	O
potential	O
for	O
a	O
true	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
as	O
well	O
as	O
limiting	O
catastrophic	O
forgetting	O
.	O

Empirical	O
Methods	O
in	O
Natural	O
Language	O
Processing	O
and	O
the	O
9th	O
International	O
Joint	O
Conference	O
on	O
Natural	O
Language	O
Processing	O
(	O
EMNLP	O
-	O
IJCNLP	O
)	O
,	O
pages	O
4094	O
-	O
4103	O
,	O
Hong	O
Kong	O
,	O
China	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

A	O
Learning	O
Curves	O
of	O
All	O
Task	O
Permutations	O

Active	O
learning	O
promises	O
to	O
alleviate	O
the	O
massive	O
data	O
needs	O
of	O
supervised	O
machine	O
learning	O
:	O
it	O
has	O
successfully	O
improved	O
sample	O
efficiency	O
by	O
an	O
order	O
of	O
magnitude	O
on	O
traditional	O
tasks	O
like	O
topic	O
classification	O
and	O
object	O
recognition	O
.	O

However	O
,	O
we	O
uncover	O
a	O
striking	O
contrast	O
to	O
this	O
promise	O
:	O
across	O
5	O
models	O
and	O
4	O
datasets	O
on	O
the	O
task	O
of	O
visual	B-TaskName
question	I-TaskName
answering	I-TaskName
,	O
a	O
wide	O
variety	O
of	O
active	O
learning	O
approaches	O
fail	O
to	O
outperform	O
random	O
selection	O
.	O

To	O
understand	O
this	O
discrepancy	O
,	O
we	O
profile	O
8	O
active	O
learning	O
methods	O
on	O
a	O
per	O
-	O
example	O
basis	O
,	O
and	O
identify	O
the	O
problem	O
as	O
collective	O
outliers	O
-groups	O
of	O
examples	O
that	O
active	O
learning	O
methods	O
prefer	O
to	O
acquire	O
but	O
models	O
fail	O
to	O
learn	O
(	O
e.g.	O
,	O
questions	O
that	O
ask	O
about	O
text	O
in	O
images	O
or	O
require	O
external	O
knowledge	O
)	O
.	O

Through	O
systematic	O
ablation	O
experiments	O
and	O
qualitative	O
visualizations	O
,	O
we	O
verify	O
that	O
collective	O
outliers	O
are	O
a	O
general	O
phenomenon	O
responsible	O
for	O
degrading	O
pool	O
-	O
based	O
active	O
learning	O
.	O

Notably	O
,	O
we	O
show	O
that	O
active	O
learning	O
sample	O
efficiency	O
increases	O
significantly	O
as	O
the	O
number	O
of	O
collective	O
outliers	O
in	O
the	O
active	O
learning	O
pool	O
decreases	O
.	O

We	O
conclude	O
with	O
a	O
discussion	O
and	O
prescriptive	O
recommendations	O
for	O
mitigating	O
the	O
effects	O
of	O
these	O
outliers	O
in	O
future	O
work	O
.	O

Today	O
,	O
language	O
-	O
equipped	O
vision	O
systems	O
such	O
as	O
VizWiz	O
,	O
TapTapSee	O
,	O
BeMyEyes	O
,	O
and	O
CamFind	O
are	O
actively	O
being	O
deployed	O
across	O
a	O
broad	O
spectrum	O
of	O
users	O
.	O

1	O
As	O
underlying	O
methods	O
improve	O
,	O
these	O
systems	O
will	O
be	O
expected	O
to	O
operate	O
over	O
diverse	O
visual	O
environments	O
and	O
understand	O
myriad	O
language	O
inputs	O
(	O
Bigham	O
et	O
al	O
.	O
,	O

2010;Tellex	O
et	O
al	O
.	O
,	O

2011;Mei	O
et	O
al	O
.	O
,	O

2016;Anderson	O
et	O
al	O
.	O
,	O

2018b;Park	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
VQA	B-TaskName
)	O
,	O
the	O
task	O
of	O
answering	O
questions	O
about	O
Figure	O
1	O
:	O
We	O
systematically	O
evaluate	O
active	O
learning	O
on	O
VQA	B-TaskName
datasets	O
and	O
isolate	O
their	O
inability	O
to	O
perform	O
better	O
than	O
random	O
sampling	O
due	O
to	O
the	O
presence	O
of	O
collective	O
outliers	O
.	O

Active	O
learning	O
methods	O
prefer	O
to	O
acquire	O
these	O
outliers	O
,	O
which	O
are	O
hard	O
and	O
often	O
impossible	O
for	O
models	O
to	O
learn	O
.	O

We	O
show	O
that	O
Dataset	O
Maps	O
,	O
like	O
the	O
one	O
shown	O
here	O
,	O
can	O
heuristically	O
identify	O
these	O
collective	O
outliers	O
as	O
examples	O
assigned	O
low	O
model	O
confidence	O
and	O
prediction	O
variability	O
during	O
training	O
.	O

visual	O
inputs	O
,	O
is	O
a	O
popular	O
benchmark	O
used	O
to	O
evaluate	O
progress	O
towards	O
such	O
open	O
-	O
ended	O
systems	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2015;Krishna	O
et	O
al	O
.	O
,	O

2017;Gordon	O
et	O
al	O
.	O
,	O

2018;Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
.	O

Unfortunately	O
,	O
today	O
's	O
VQA	B-TaskName
models	O
are	O
data	O
hungry	O
:	O
Their	O
performance	O
scales	O
monotonically	O
with	O
more	O
train	O
-	O
ing	O
data	O
(	O
Lu	O
et	O
al	O
.	O
,	O

2016;Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
motivating	O
the	O
need	O
for	O
data	O
acquisition	O
mechanisms	O
such	O
as	O
active	O
learning	O
,	O
which	O
maximize	O
performance	O
while	O
minimizing	O
expensive	O
data	O
labeling	O
.	O

While	O
active	O
learning	O
is	O
often	O
key	O
to	O
effective	O
data	O
acquisition	O
when	O
such	O
labeled	O
data	O
is	O
difficult	O
to	O
obtain	O
(	O
Lewis	O
and	O
Catlett	O
,	O
1994;Tong	O
and	O
Koller	O
,	O
2001;Culotta	O
and	O
McCallum	O
,	O
2005;Settles	O
,	O
2009	O
)	O
,	O
we	O
find	O
that	O
8	O
modern	O
active	O
learning	O
methods	O
Siddhant	O
and	O
Lipton	O
,	O
2018;Lowell	O
et	O
al	O
.	O
,	O

2019	O
)	O
show	O
little	O
to	O
no	O
improvement	O
in	O
sample	O
efficiency	O
across	O
5	O
models	O
on	O
4	O
VQA	B-TaskName
datasets	O
-indeed	O
,	O
in	O
some	O
cases	O
performing	O
worse	O
than	O
randomly	O
selecting	O
data	O
to	O
label	O
.	O

This	O
finding	O
is	O
in	O
stark	O
contrast	O
to	O
the	O
successful	O
application	O
of	O
active	O
learning	O
methods	O
on	O
a	O
variety	O
of	O
traditional	O
tasks	O
,	O
such	O
as	O
topic	O
classification	O
(	O
Siddhant	O
and	O
Lipton	O
,	O
2018;Lowell	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
object	O
recognition	O
(	O
Deng	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
digit	O
classification	O
,	O
and	O
named	O
entity	O
recognition	O
(	O
Shen	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

Our	O
negative	O
results	O
hold	O
even	O
when	O
accounting	O
for	O
common	O
active	O
learning	O
ailments	O
:	O
cold	O
starts	O
,	O
correlated	O
sampling	O
,	O
and	O
uncalibrated	O
uncertainty	O
.	O

We	O
mitigate	O
the	O
cold	O
start	O
challenge	O
of	O
needing	O
a	O
representative	O
initial	O
dataset	O
by	O
varying	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
seed	I-HyperparameterName
set	I-HyperparameterName
in	O
our	O
experiments	O
.	O

We	O
account	O
for	O
sampling	O
correlated	O
data	O
within	O
a	O
given	O
batch	O
by	O
including	O
Core	O
-	O
Set	O
selection	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018	O
)	O
in	O
the	O
set	O
of	O
active	O
learning	O
methods	O
we	O
evaluate	O
.	O

Finally	O
,	O
we	O
use	O
deep	B-MethodName
Bayesian	I-MethodName
active	I-MethodName
learning	I-MethodName
to	O
calibrate	O
model	O
uncertainty	O
to	O
high	O
-	O
dimensional	O
data	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011;Gal	O
and	O
Ghahramani	O
,	O
2016;.After	O
concluding	O
that	O
negative	O
results	O
are	O
consistent	O
across	O
all	O
experimental	O
conditions	O
,	O
we	O
investigate	O
active	O
learning	O
's	O
ineffectiveness	O
on	O
VQA	B-TaskName
as	O
a	O
data	O
problem	O
and	O
identify	O
the	O
existence	O
of	O
collective	O
outliers	O
(	O
Han	O
and	O
Kamber	O
,	O
2000	O
)	O
as	O
the	O
source	O
of	O
the	O
problem	O
.	O

Leveraging	O
recent	O
advances	O
in	O
model	O
interpretability	O
,	O
we	O
build	O
Dataset	O
Maps	O
,	O
which	O
distinguish	O
between	O
collective	O
outliers	O
and	O
useful	O
data	O
that	O
improve	O
validation	O
set	O
performance	O
(	O
see	O
Figure	O
1	O
)	O
.	O

While	O
global	O
outliers	O
deviate	O
from	O
the	O
rest	O
of	O
the	O
data	O
and	O
are	O
often	O
a	O
consequence	O
of	O
labeling	O
error	O
,	O
collective	O
outliers	O
cluster	O
together	O
;	O
they	O
may	O
not	O
individually	O
be	O
identifiable	O
as	O
outliers	O
but	O
collectively	O
deviate	O
from	O
other	O
examples	O
in	O
the	O
dataset	O
.	O

For	O
instance	O
,	O
VQA-2	B-DatasetName
(	O
Goyal	O
et	O
al	O
.	O
,	O

2017	O
)	O
is	O
riddled	O
with	O
collections	O
of	O
hard	O
questions	O
that	O
require	O
external	O
knowledge	O
to	O
answer	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
symbol	O
on	O
the	O
hood	O
often	O
associated	O
with	O
?	O
"	O
)	O
or	O
that	O
ask	O
the	O
model	O
to	O
read	O
text	O
in	O
the	O
images	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
word	O
on	O
the	O
wall	O
?	O
"	O
)	O
.	O

Similarly	O
,	O
GQA	B-DatasetName
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
asks	O
underspecified	O
questions	O
(	O
e.g.	O
,	O
"	O
what	O
is	O
the	O
person	O
wearing	O
?	O
"	O
which	O
can	O
have	O
multiple	O
correct	O
answers	O
)	O
.	O

Collective	O
outliers	O
are	O
not	O
specific	O
to	O
VQA	B-TaskName
,	O
but	O
can	O
similarly	O
be	O
found	O
in	O
many	O
open	O
-	O
ended	O
tasks	O
,	O
including	O
visual	O
navigation	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

2018b	O
)	O
(	O
e.g.	O
,	O
"	O
Go	O
to	O
the	O
grandfather	O
clock	O
"	O
requires	O
identifying	O
rare	O
grandfather	O
clocks	O
)	O
,	O
and	O
open	O
-	O
domain	O
question	O
answering	O
(	O
Kwiatkowski	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
amongst	O
others	O
.	O

Using	O
Dataset	O
Maps	O
,	O
we	O
profile	O
active	O
learning	O
methods	O
and	O
show	O
that	O
they	O
prefer	O
acquiring	O
collective	O
outliers	O
that	O
models	O
are	O
unable	O
to	O
learn	O
,	O
explaining	O
their	O
poor	O
improvements	O
in	O
sample	O
efficiency	O
relative	O
to	O
random	O
sampling	O
.	O

Building	O
on	O
this	O
,	O
we	O
use	O
these	O
maps	O
to	O
perform	O
ablations	O
where	O
we	O
identify	O
and	O
remove	O
outliers	O
iteratively	O
from	O
the	O
active	O
learning	O
pool	O
,	O
observing	O
correlated	O
improvements	O
in	O
sample	O
efficiency	O
.	O

This	O
allows	O
us	O
to	O
conclude	O
that	O
collective	O
outliers	O
are	O
,	O
indeed	O
,	O
responsible	O
for	O
the	O
ineffectiveness	O
of	O
active	O
learning	O
for	O
VQA	B-TaskName
.	O

We	O
end	O
with	O
prescriptive	O
suggestions	O
for	O
future	O
work	O
in	O
building	O
active	O
learning	O
methods	O
robust	O
to	O
these	O
types	O
of	O
outliers	O
.	O

Our	O
work	O
tests	O
the	O
utility	O
of	O
multiple	O
recent	O
active	O
learning	O
methods	O
on	O
the	O
open	O
-	O
ended	O
understanding	O
task	O
of	O
VQA	B-TaskName
.	O

We	O
draw	O
on	O
the	O
dataset	O
analysis	O
literature	O
to	O
identify	O
collective	O
outliers	O
as	O
the	O
bottleneck	O
hindering	O
active	O
learning	O
methods	O
in	O
this	O
setting	O
.	O

Active	O
Learning	O
.	O

Active	O
learning	O
strategies	O
have	O
been	O
successfully	O
applied	O
to	O
image	O
recognition	O
(	O
Joshi	O
et	O
al	O
.	O
,	O

2009;Sener	O
and	O
Savarese	O
,	O
2018	O
)	O
,	O
information	O
extraction	O
(	O
Scheffer	O
et	O
al	O
.	O
,	O

2001;Finn	O
and	O
Kushmerick	O
,	O
2003;Jones	O
et	O
al	O
.	O
,	O

2003;Culotta	O
and	O
McCallum	O
,	O
2005	O
)	O
,	O
named	O
entity	O
recognition	O
(	O
Hachey	O
et	O
al	O
.	O
,	O

2005;Shen	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
semantic	O
parsing	O
(	O
Dong	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
and	O
text	O
categorization	O
(	O
Lewis	O
and	O
Gale	O
,	O
1994;Hoi	O
et	O
al	O
.	O
,	O

2006	O
)	O
.	O

However	O
,	O
these	O
same	O
methods	O
struggle	O
to	O
outperform	O
a	O
random	O
baseline	O
when	O
applied	O
to	O
the	O
task	O
of	O
VQA	B-TaskName
(	O
Lin	O
and	O
Parikh	O
,	O
2017;Jedoui	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

To	O
study	O
this	O
discrepancy	O
,	O
we	O
systematically	O
apply	O
8	O
diverse	O
active	O
learning	O
methods	O
to	O
VQA	B-TaskName
,	O
including	O
methods	O
that	O
use	O
model	B-MethodName
uncertainty	I-MethodName
(	O
Abramson	O
and	O
Freund	O
,	O
2004;Collins	O
et	O
al	O
.	O
,	O

2008;Joshi	O
et	O
al	O
.	O
,	O

2009	O
)	O
,	O
Bayesian	B-MethodName
uncertainty	I-MethodName
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016;Kendall	O
and	O
Gal	O
,	O
2017	O
)	O
,	O
disagreement	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011	O
;	O
,	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018).Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
.	O

Progress	O
on	O
VQA	B-TaskName
has	O
been	O
heralded	O
as	O
a	O
marker	O
for	O
progress	O
on	O
general	O
open	O
-	O
ended	O
understanding	O
tasks	O
,	O
resulting	O
in	O
several	O
benchmarks	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2015;Malinowski	O
et	O
al	O
.	O
,	O

2015;Ren	O
et	O
al	O
.	O
,	O

2015a;Goyal	O
et	O
al	O
.	O
,	O

2017;Krishna	O
et	O
al	O
.	O
,	O

2017;Suhr	O
et	O
al	O
.	O
,	O

2019;Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
and	O
models	O
(	O
Zhou	O
et	O
al	O
.	O
,	O

2015;Fukui	O
et	O
al	O
.	O
,	O

2016;Lu	O
et	O
al	O
.	O
,	O

2016;Zhu	O
et	O
al	O
.	O
,	O

2016;Wu	O
et	O
al	O
.	O
,	O

2016;Anderson	O
et	O
al	O
.	O
,	O

2018a;Tan	O
and	O
Bansal	O
,	O
2019;Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

To	O
ensure	O
that	O
our	O
negative	O
results	O
are	O
not	O
dataset	O
or	O
model	O
-	O
specific	O
,	O
we	O
sample	O
4	O
datasets	O
and	O
5	O
representative	O
models	O
,	O
each	O
utilizing	O
unique	O
visual	O
and	O
linguistic	O
features	O
and	O
employing	O
different	O
inductive	O
biases	O
.	O

Interpreting	O
and	O
Analyzing	O
Datasets	O
.	O

Given	O
the	O
prevalence	O
of	O
large	O
datasets	O
in	O
modern	O
machine	O
learning	O
,	O
it	O
is	O
critical	O
to	O
assess	O
dataset	O
properties	O
to	O
remove	O
redundancies	O
(	O
Gururangan	O
et	O
al	O
.	O
,	O

2018;Li	O
and	O
Vasconcelos	O
,	O
2019	O
)	O
or	O
biases	O
(	O
Torralba	O
and	O
Efros	O
,	O
2011;Khosla	O
et	O
al	O
.	O
,	O

2012;Bolukbasi	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
both	O
of	O
which	O
negatively	O
impact	O
sample	O
efficiency	O
.	O

Prior	O
work	O
has	O
used	O
training	O
dynamics	O
to	O
find	O
examples	O
which	O
are	O
frequently	O
forgotten	O
(	O
Krymolowski	O
,	O
2002;Toneva	O
et	O
al	O
.	O
,	O

2019	O
)	O
versus	O
those	O
that	O
are	O
easy	O
to	O
learn	O
(	O
Bras	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

This	O
work	O
suggests	O
using	O
two	O
model	O
-	O
specific	O
measures	O
confidence	O
and	O
prediction	O
variance	O
-as	O
indicators	O
of	O
a	O
training	O
example	O
's	O
"	O
learnability	O
"	O
(	O
Chang	O
et	O
al	O
.	O
,	O

2017	O
;	O
.	O

Dataset	O
Maps	O
,	O
a	O
recently	O
introduced	O
framework	O
uses	O
these	O
two	O
measures	O
to	O
profile	O
datasets	O
to	O
find	O
learnable	O
examples	O
.	O

Unlike	O
prior	O
datasets	O
analyzed	O
by	O
Dataset	O
Maps	O
that	O
have	O
a	O
small	O
number	O
of	O
global	O
outliers	O
as	O
hard	O
examples	O
,	O
we	O
discover	O
that	O
VQA	B-TaskName
datasets	O
contain	O
copious	O
amounts	O
of	O
collective	O
outliers	O
,	O
which	O
are	O
difficult	O
or	O
even	O
impossible	O
for	O
models	O
to	O
learn	O
.	O

We	O
adopt	O
the	O
standard	O
pool	O
-	O
based	O
active	O
learning	O
setup	O
from	O
prior	O
work	O
(	O
Lewis	O
and	O
Gale	O
,	O
1994;Settles	O
,	O
2009;Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
consisting	O
of	O
a	O
model	O
M	O
,	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
of	O
labeled	O
examples	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
∈	O
D	O
seed	O
used	O
to	O
initialize	O
M	O
,	O
an	O
unlabeled	O
pool	O
of	O
data	O
D	O
pool	O
,	O
and	O
an	O
acquisition	O
function	O
A(x	O
,	O
M	O
)	O
.	O

We	O
run	O
active	O
learning	O
over	O
a	O
series	O
of	O
acquisition	O
iterations	O
T	O
where	O
at	O
each	O
iteration	O
we	O
acquire	O
a	O
batch	O
of	O
B	O
new	O
examples	O
per	O
:x	O
∈	O
D	O
pool	O
to	O
label	O
per	O
x	O
=	O
arg	O
max	O
x∈D	O
pool	O
A(x	O
,	O
M).Acquiring	O
an	O
example	O
often	O
refers	O
to	O
using	O
an	O
oracle	O
or	O
human	O
expert	O
to	O
annotate	O
a	O
new	O
example	O
with	O
a	O
correct	O
label	O
.	O

We	O
follow	O
prior	O
work	O
to	O
simulate	O
an	O
oracle	O
using	O
existing	O
datasets	O
,	O
forming	O
D	O
seed	O
from	O
a	O
fixed	O
percentage	O
of	O
the	O
full	O
dataset	O
,	O
and	O
using	O
the	O
remainder	O
as	O
D	O
pool	O
Lin	O
and	O
Parikh	O
,	O
2017;Siddhant	O
and	O
Lipton	O
,	O
2018	O
)	O
.	O

We	O
re	O
-	O
train	O
M	O
after	O
each	O
acquisition	O
iteration	O
.	O

Prior	O
work	O
has	O
noted	O
the	O
impact	O
of	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
on	O
active	O
learning	O
performance	O
(	O
Lin	O
and	O
Parikh	O
,	O
2017;Misra	O
et	O
al	O
.	O
,	O

2018;Jedoui	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

We	O
run	O
multiple	O
active	O
learning	O
evaluations	O
with	O
varying	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
(	O
ranging	O
from	O
5	B-HyperparameterValue
%	I-HyperparameterValue
to	I-HyperparameterValue
50	I-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
size	I-HyperparameterValue
)	O
.	O

We	O
keep	O
the	O
size	O
of	O
each	O
acquisition	O
batch	B-HyperparameterName
B	I-HyperparameterName
to	O
a	O
constant	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
overall	I-HyperparameterValue
pool	I-HyperparameterValue
size	I-HyperparameterValue
.	O

Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
VQA	B-TaskName
)	O
requires	O
reasoning	O
over	O
two	O
modalities	O
:	O
images	O
and	O
text	O
.	O

Most	O
models	O
use	O
feature	O
"	O
backbones	O
"	O
(	O
e.g.	O
,	O
features	O
from	O
object	O
recognition	O
models	O
pretrained	O
on	O
I	O
m	O
a	O
-	O
geNet	O
,	O
and	O
pretrained	O
word	O
vectors	O
for	O
text	O
)	O
.	O

For	O
image	O
features	O
we	O
use	O
grid	O
-	O
based	O
features	O
from	O
ResNet-101	O
,	O
or	O
object	O
-	O
based	O
features	O
from	O
Faster	O
R	O
-	O
CNN	O
(	O
Ren	O
et	O
al	O
.	O
,	O

2015b	O
)	O
finetuned	O
on	O
Visual	O
Genome	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

2018a	O
)	O
.	O

We	O
evaluate	O
with	O
a	O
representative	O
sample	O
of	O
existing	O
VQA	B-TaskName
models	O
,	O
including	O
the	O
following	O
:	O
2LogReg	B-MethodName
is	O
a	O
logistic	O
regression	O
model	O
that	O
uses	O
either	O
ResNet-101	O
or	O
Faster	O
R	O
-	O
CNN	O
image	O
features	O
with	O
mean	O
-	O
pooled	O
GloVe	O
question	O
embeddings	O
(	O
Pennington	O
et	O
al	O
.	O
,	O

2014	O
)	O
.	O

Although	O
these	O
models	O
are	O
not	O
as	O
performant	O
as	O
the	O
subsequent	O
models	O
,	O
logistic	O
regression	O
has	O
been	O
effective	O
on	O
VQA	B-TaskName
(	O
Suhr	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
and	O
is	O
pervasive	O
in	O
the	O
active	O
learning	O
literature	O
(	O
Schein	O
and	O
Ungar	O
,	O
2007;Yang	O
and	O
Loog	O
,	O
2018;Mussmann	O
and	O
Liang	O
,	O
2018).LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
is	O
a	O
standard	O
model	O
introduced	O
with	O
VQA-1	B-TaskName
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2015	O
)	O
.	O

We	O
use	O
more	O
performant	O
ResNet-101	O
features	O
instead	O
of	O
the	O
original	O
VGGNet	O
features	O
as	O
our	O
visual	O
backbone	O
.	O

BUTD	B-MethodName
(	B-MethodName
Bottom	I-MethodName
-	I-MethodName
Up	I-MethodName
Top	I-MethodName
-	I-MethodName
Down	I-MethodName
Attention	I-MethodName
)	O
uses	O
object	O
-	O
based	O
features	O
in	O
tandem	O
with	O
attention	O
over	O
objects	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

2018a	O
)	O
.	O

BUTD	B-MethodName
won	O
the	O
2017	O
VQA	B-TaskName
Challenge	O
(	O
Teney	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
and	O
has	O
been	O
a	O
consistent	O
baseline	O
for	O
recent	O
work	O
in	O
VQA.LXMERT	B-TaskName
is	O
a	O
large	O
multi	O
-	O
modal	O
transformer	O
model	O
that	O
uses	O
BUTD	B-MethodName
's	O
object	O
features	O
and	O
contextualized	O
BERT	O
language	O
features	O
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
.	O

LXMERT	B-MethodName
is	O
pretrained	O
on	O
a	O
corpus	O
of	O
aligned	O
image	O
-	O
and	O
-	O
textual	O
data	O
spanning	O
MS	O
COCO	O
,	O
Visual	O
Genome	O
,	O
VQA-2	B-DatasetName
,	O
NLVR-2	O
,	O
and	O
GQA	B-DatasetName
(	O
Lin	O
et	O
al	O
.	O
,	O

2014;Krishna	O
et	O
al	O
.	O
,	O

2017;Goyal	O
et	O
al	O
.	O
,	O

2017;Suhr	O
et	O
al	O
.	O
,	O

2019;Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
,	O
initializing	O
a	O
cross	O
-	O
modal	O
representation	O
space	O
conducive	O
to	O
fine	O
-	O
tuning	O
.	O

3	O
Several	O
active	B-MethodName
learning	I-MethodName
methods	O
have	O
been	O
developed	O
to	O
account	O
for	O
different	O
aspects	O
of	O
the	O
machine	O
learning	O
training	O
pipeline	O
:	O
while	O
some	O
acquire	O
examples	O
with	O
high	O
aleotoric	O
uncertainty	O
(	O
Settles	O
,	O
2009	O
)	O
(	O
having	O
to	O
do	O
with	O
the	O
natural	O
uncertainty	O
in	O
the	O
data	O
)	O
or	O
epistemic	O
uncertainty	O
(	O
having	O
to	O
do	O
with	O
the	O
uncertainty	O
in	O
the	O
modeling	O
/	O
learning	O
process	O
)	O
,	O
others	O
attempt	O
to	O
acquire	O
examples	O
that	O
reflect	O
the	O
distribution	O
of	O
data	O
in	O
the	O
pool	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018	O
)	O
.	O

We	O
sample	O
a	O
diverse	O
set	O
of	O
these	O
methods	O
:	O
Random	B-MethodName
Sampling	I-MethodName
serves	O
as	O
our	O
baseline	O
passive	O
approach	O
for	O
acquiring	O
examples	O
.	O

Least	B-MethodName
Confidence	I-MethodName
acquires	O
examples	O
with	O
lowest	O
model	O
prediction	O
probability	O
(	O
Settles	O
,	O
2009).Entropy	B-MethodName
acquires	O
examples	O
with	O
the	O
highest	O
entropy	O
in	O
the	O
model	O
's	O
output	O
(	O
Settles	O
,	O
2009	O
)	O
.	O

MC	B-MethodName
-	I-MethodName
Dropout	I-MethodName
Entropy	I-MethodName
(	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
with	I-MethodName
Entropy	I-MethodName
acquisition	I-MethodName
)	O
acquires	O
examples	O
with	O
high	O
entropy	O
in	O
the	O
model	O
's	O
output	O
averaged	O
over	O
multiple	O
passes	O
through	O
a	O
neural	O
network	O
with	O
different	O
dropout	O
masks	O
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016	O
)	O
.	O

This	O
process	O
is	O
a	O
consequence	O
of	O
a	O
theoretical	O
casting	O
of	O
dropout	O
as	O
approximate	O
Bayesian	O
inference	O
in	O
deep	O
Gaussian	O
processes	O
.	O

BALD	B-MethodName
(	O
Bayesian	B-MethodName
Active	I-MethodName
Learning	I-MethodName
by	I-MethodName
Disagreement	I-MethodName
)	O
builds	O
upon	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
by	O
proposing	O
a	O
decision	O
theoretic	O
objective	O
;	O
it	O
acquires	O
examples	O
that	O
maximise	O
the	O
decrease	O
in	O
expected	O
posterior	O
entropy	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011;Siddhant	O
and	O
Lipton	O
,	O
2018	O
)	O
-capturing	O
"	O
disagreement	O
"	O
across	O
different	O
dropout	O
masks	O
.	O

Selection	O
samples	O
examples	O
that	O
capture	O
the	O
diversity	O
of	O
the	O
data	O
pool	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018;Coleman	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

It	O
acquires	O
examples	O
to	O
minimize	O
the	O
distance	O
between	O
an	O
example	O
in	O
the	O
unlabeled	O
pool	O
to	O
its	O
closest	O
labeled	O
example	O
.	O

Since	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
operates	O
over	O
a	O
representation	O
space	O
(	O
and	O
not	O
an	O
output	O
distribution	O
,	O
like	O
prior	O
strategies	O
)	O
and	O
VQA	B-TaskName
models	O
operate	O
over	O
two	O
modalities	O
,	O
we	O
employ	O
three	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
variants	O
:	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Language	O
)	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Vision	O
)	O
operate	O
over	O
their	O
respective	O
representation	O
spaces	O
while	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Fused	O
)	O
operates	O
over	O
the	O
"	O
fused	O
"	O
vision	O
and	O
language	O
representation	O
space	O
.	O

We	O
evaluate	O
the	O
8	O
active	O
learning	O
strategies	O
across	O
the	O
5	O
models	O
described	O
in	O
the	O
previous	O
section	O
.	O

Figures	O
2	O
-	O
5	O
show	O
a	O
representative	O
sample	O
of	O
active	O
learning	O
results	O
across	O
datasets	O
.	O

Due	O
to	O
space	O
constraints	O
,	O
we	O
only	O
visualize	O
4	O
active	O
learning	O
strategies	O
-Least	B-MethodName
-	I-MethodName
Confidence	I-MethodName
,	O
BALD	B-MethodName
,	O
CoreSet	B-MethodName
-	O
Fused	O
,	O
and	O
the	O
Random	B-MethodName
Baseline	I-MethodName
-using	O
3	O
models	O
(	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
BUTD	B-MethodName
,	O
LXMERT	B-MethodName
)	O
.	O

4	O
Results	O
and	O
trends	O
are	O
consistent	O
across	O
the	O
different	O
acquisition	O
functions	O
,	O
models	O
and	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
(	O
see	O
the	O
appendix	O
for	O
results	O
with	O
other	O
models	O
,	O
acquisition	O
functions	O
,	O
and	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
)	O
.	O

We	O
now	O
go	O
on	O
to	O
provide	O
descriptions	O
of	O
the	O
datasets	O
we	O
evaluate	O
against	O
,	O
and	O
the	O
corresponding	O
results	O
.	O

Strategies	O
perform	O
on	O
par	O
with	O
or	O
worse	O
than	O
the	O
random	O
baseline	O
,	O
when	O
using	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
dataset	I-HyperparameterValue
as	O
the	O
seed	B-HyperparameterName
set.4	I-HyperparameterName
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-VQA-2	B-DatasetName
Random	O
Baseline	O
Least	O
-	O
Confidence	O
BALD	O
Core	O
-	O
Set	O
(	O
Fused	O
)	O
4	O
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
BUTD	B-MethodName
-VQA-2	B-DatasetName
4	O
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
LXMERT	B-MethodName
-VQA-2Figure	B-DatasetName
3	O
:	O
Results	O
for	O
the	O
full	O
VQA-2	B-DatasetName
dataset	O
,	O
also	O
using	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
dataset	I-HyperparameterValue
as	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
.	O

Similar	O
to	O
the	O
plot	O
above	O
,	O
all	O
active	O
learning	O
methods	O
perform	O
similar	O
to	O
a	O
random	O
baseline	O
.	O

One	O
complexity	O
of	O
VQA	B-TaskName
is	O
the	O
size	O
of	O
the	O
output	O
space	O
and	O
the	O
number	O
of	O
examples	O
present	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2015;Goyal	O
et	O
al	O
.	O
,	O

2017	O
)	O
;	O
VQA-2	B-DatasetName
has	O
400k	O
training	O
examples	O
,	O
and	O
in	O
excess	O
of	O
3k	O
possible	O
answers	O
(	O
see	O
Table	O
1	O
)	O
.	O

However	O
,	O
prior	O
work	O
in	O
active	O
learning	O
focuses	O
on	O
smaller	O
datasets	O
like	O
the	O
10	O
-	O
class	O
MNIST	O
dataset	O
,	O
binary	O
classification	O
(	O
Siddhant	O
and	O
Lipton	O
,	O
2018	O
)	O
,	O
or	O
small	O
-	O
cardinality	O
(	O
≤	O
20	O
classes	O
)	O
text	O
categorization	O
(	O
Lowell	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

To	O
ensure	O
our	O
results	O
and	O
conclusions	O
are	O
not	O
due	O
to	O
the	O
size	O
of	O
the	O
output	O
space	O
,	O
we	O
build	O
two	O
meaningful	O
,	O
but	O
narrow	O
-	O
domain	O
VQA	B-TaskName
datasets	O
from	O
subsets	O
of	O
VQA-2	B-DatasetName
.	O

These	O
simplified	O
datasets	O
reduce	O
the	O
complexity	O
of	O
the	O
underlying	O
learning	O
problem	O
and	O
provide	O
a	O
fair	O
comparison	O
to	O
existing	O
active	O
learning	O
literature	O
.	O

VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
.	O

We	O
generate	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
by	O
compiling	O
a	O
list	O
of	O
20	O
popular	O
sports	O
(	O
e.g.	O
,	O
soccer	O
,	O
football	O
,	O
tennis	O
,	O
etc	O
.	O
)	O

in	O
VQA-2	B-DatasetName
,	O
and	O
restricting	O
the	O
set	O
of	O
questions	O
to	O
those	O
with	O
answers	O
in	O
this	O
list	O
.	O

We	O
picked	O
the	O
sports	O
categories	O
by	O
ranking	O
the	O
GloVe	O
vector	O
similarity	O
between	O
the	O
word	O
"	O
sports	O
"	O
to	O
answers	O
in	O
VQA-2	B-DatasetName
,	O
and	O
selected	O
the	O
20	O
most	O
commonly	O
occurring	O
answers	O
.	O

VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

We	O
generate	O
the	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
dataset	O
similarly	O
,	O
compiling	O
a	O
list	O
of	O
the	O
20	O
commonly	O
occurring	O
food	O
categories	O
by	O
GloVe	O
vector	O
similarity	O
to	O
the	O
word	O
"	O
food	O
.	O
"	O

Results	O
.	O

Figure	O
2	O
presents	O
results	O
for	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
with	O
an	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
restricted	O
to	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
500	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

The	O
appendix	O
reports	O
similar	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

For	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
Least	O
-	O
Confidence	O
appears	O
to	O
be	O
slightly	O
more	O
sample	O
efficient	O
,	O
while	O
all	O
other	O
strategies	O
perform	O
on	O
par	O
with	O
or	O
worse	O
than	O
random	O
.	O

For	O
BUTD	B-MethodName
,	O
all	O
methods	O
are	O
on	O
par	O
with	O
random	O
;	O
for	O
LXMERT	B-MethodName
,	O
they	O
perform	O
worse	O
than	O
random	O
.	O

Generally	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
active	O
learning	O
performance	O
varies	O
,	O
but	O
fails	O
to	O
outperform	O
random	O
acquisition	O
.	O

VQA-2	B-DatasetName
is	O
the	O
canonical	O
dataset	O
for	O
evaluating	O
VQA	O
models	O
(	O
Goyal	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

In	O
keeping	O
with	O
prior	O
work	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

2018a;Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
,	O
we	O
filter	O
the	O
training	O
set	O
to	O
only	O
include	O
answers	O
that	O
appear	O
at	O
least	O
9	O
times	O
,	O
resulting	O
in	O
3130	O
unique	O
answers	O
.	O

Unlike	O
traditional	O
VQA-2	B-DatasetName
evaluation	O
,	O
which	O
treats	O
the	O
task	O
as	O
a	O
multi	O
-	O
label	O
binary	O
classification	O
problem	O
,	O
we	O
follow	O
prior	O
active	O
learning	O
work	O
on	O
VQA	O
(	O
Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
which	O
formulates	O
it	O
as	O
a	O
multi	O
-	O
class	O
classification	O
problem	O
,	O
enabling	O
the	O
use	O
of	O
acquisition	O
functions	O
such	O
as	O
uncertainty	O
sampling	O
and	O
BALD	O
.	O

the	O
right	O
of	O
?	O
"	O
.	O

We	O
use	O
the	O
standard	O
GQA	B-DatasetName
training	O
set	O
of	O
943k	O
questions	O
,	O
900k	O
of	O
which	O
we	O
use	O
for	O
the	O
active	O
learning	O
pool	O
.	O

Results	O
.	O

Figure	O
5	O
shows	O
results	O
on	O
GQA	B-DatasetName
using	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
of	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
90k	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

Despite	O
its	O
notable	O
differences	O
in	O
question	O
structure	O
to	O
VQA-2	B-DatasetName
,	O
active	O
learning	O
still	O
performs	O
on	O
par	O
with	O
or	O
slightly	O
worse	O
than	O
random	O
.	O

The	O
previous	O
section	O
shows	O
that	O
active	O
learning	O
fails	O
to	O
improve	O
over	O
random	O
acquisition	O
on	O
VQA	O
across	O
models	O
and	O
datasets	O
.	O

A	O
simple	O
question	O
remains	O
-why	O
?	O
One	O
hypothesis	O
is	O
that	O
sample	O
inefficiency	O
stems	O
from	O
the	O
data	O
itself	O
:	O
there	O
is	O
only	O
a	O
2	B-MetricValue
%	I-MetricValue
gain	O
in	O
validation	O
accuracy	B-MetricName
when	O
training	O
on	O
half	O
versus	O
the	O
whole	O
dataset	O
.	O

Working	O
from	O
this	O
,	O
we	O
characterize	O
the	O
underlying	O
datasets	O
using	O
Dataset	O
Maps	O
and	O
discover	O
that	O
active	O
learning	O
methods	O
prefer	O
sampling	O
"	O
hard	O
-	O
tolearn	O
"	O
examples	O
,	O
leading	O
to	O
poor	O
performance	O
.	O

(	O
Han	O
and	O
Kamber	O
,	O
2000	O
)	O
-they	O
often	O
present	O
as	O
fundamental	O
subproblems	O
of	O
a	O
broader	O
task	O
.	O

For	O
instance	O
(	O
Figure	O
7	O
)	O
,	O
in	O
VQA-2	B-DatasetName
,	O
we	O
identify	O
clusters	O
of	O
hard	O
-	O
to	O
-	O
learn	O
examples	O
that	O
require	O
optical	O
character	O
recognition	O
(	O
OCR	O
)	O
for	O
reasoning	O
about	O
text	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
first	O
word	O
on	O
the	O
black	O
car	O
?	O
"	O
)	O
;	O
another	O
cluster	O
requires	O
external	O
knowledge	O
to	O
answer	O
(	O
"	O
What	O
is	O
the	O
symbol	O
on	O
the	O
hood	O
often	O
associated	O
with	O
?	O
"	O
)	O
.	O

In	O
GQA	B-DatasetName
,	O
we	O
identify	O
different	O
clusters	O
of	O
collective	O
outliers	O
;	O
one	O
cluster	O
stems	O
from	O
innate	O
underspecification	O
(	O
e.g.	O
,	O
"	O
what	O
is	O
on	O
the	O
shelf	O
?	O
"	O
with	O
multiple	O
objects	O
present	O
on	O
the	O
shelf	O
)	O
;	O
another	O
cluster	O
requires	O
multiple	O
reasoning	O
hops	O
difficult	O
for	O
current	O
models	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
vehicle	O
that	O
is	O
driving	O
down	O
the	O
road	O
the	O
box	O
is	O
on	O
the	O
side	O
of	O
?	O
"	O
)	O
.	O

We	O
sample	O
100	O
random	O
"	O
hard	O
-	O
to	O
-	O
learn	O
"	O
examples	O
from	O
both	O
VQA-2	B-DatasetName
and	O
GQA	B-DatasetName
and	O
find	O
that	O
100	O
%	O
Ablating	O
Outliers	O
.	O

To	O
verify	O
that	O
collective	O
outliers	O
are	O
responsible	O
for	O
the	O
degradation	O
of	O
active	O
learning	O
performance	O
,	O
we	O
re	O
-	O
run	O
our	O
experiments	O
using	O
active	O
learning	O
pools	O
with	O
varying	O
numbers	B-HyperparameterName
of	I-HyperparameterName
outliers	I-HyperparameterName
removed	I-HyperparameterName
.	O

To	O
remove	O
these	O
outliers	O
,	O
we	O
sort	O
and	O
remove	O
all	O
examples	O
in	O
the	O
data	O
pool	O
using	O
the	O
product	O
of	O
their	O
model	O
confidence	O
and	O
prediction	O
variability	O
(	O
x	O
and	O
y	O
-	O
axis	O
values	O
of	O
the	O
Dataset	O
Maps	O
)	O
.	O

We	O
systematically	O
remove	O
examples	O
with	O
a	O
low	O
product	O
value	O
and	O
observe	O
how	O
active	O
learning	O
performance	O
changes	O
(	O
see	O
Figure	O
8).We	O
observe	O
a	O
2	O
-	O
3x	O
improvement	O
in	O
sample	O
efficiency	O
when	O
removing	O
50	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
entire	I-HyperparameterValue
data	I-HyperparameterValue
pool	I-HyperparameterValue
,	O
consisting	O
mainly	O
of	O
collective	O
outliers	O
(	O
Figure	O
8c	O
)	O
.	O

This	O
improvement	O
decreases	O
if	O
we	O
only	O
remove	O
25	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
Figure	O
8b	O
)	O
,	O
and	O
further	O
degrades	O
if	O
we	O
remove	O
only	O
10	B-HyperparameterValue
%	I-HyperparameterValue
(	O
Figure	O
8a	O
)	O
.	O

This	O
ablation	O
demonstrates	O
that	O
active	O
learning	O
methods	O
are	O
more	O
sample	O
efficient	O
than	O
the	O
random	O
baseline	O
when	O
collective	O
outliers	O
are	O
absent	O
from	O
the	O
unlabelled	O
pool	O
.	O

This	O
paper	O
asks	O
a	O
simple	O
question	O
-why	O
does	O
the	O
modern	O
neural	O
active	O
learning	O
toolkit	O
fail	O
when	O
applied	O
to	O
complex	O
,	O
open	O
ended	O
tasks	O
?	O
While	O
we	O
focus	O
on	O
VQA	B-TaskName
,	O
collective	O
outliers	O
are	O
abundant	O
in	O
tasks	O
such	O
as	O
natural	O
language	O
inference	O
(	O
Bowman	O
et	O
al	O
.	O
,	O

2015;Williams	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
opendomain	O
question	O
answering	O
(	O
Kwiatkowski	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
amongst	O
others	O
.	O

More	O
insidious	O
is	O
their	O
nature	O
;	O
collective	O
outliers	O
can	O
take	O
multiple	O
forms	O
,	O
requiring	O
external	O
domain	O
knowledge	O
or	O
"	O
commonsense	O
"	O
reasoning	O
,	O
containing	O
underspecification	O
,	O
or	O
requiring	O
capabilities	O
beyond	O
the	O
scope	O
of	O
a	O
given	O
model	O
(	O
e.g.	O
,	O
requiring	O
OCR	O
ability	O
)	O
.	O

While	O
we	O
perform	O
ablations	O
in	O
this	O
work	O
removing	O
collective	O
outliers	O
,	O
demonstrating	O
that	O
active	O
learning	O
fails	O
as	O
collective	O
outliers	O
take	O
up	O
larger	O
portions	O
of	O
the	O
dataset	O
,	O
this	O
is	O
only	O
an	O
analytical	O
tool	O
;	O
these	O
outliers	O
are	O
,	O
and	O
will	O
continue	O
to	O
be	O
,	O
pervasive	O
in	O
open	O
-	O
ended	O
datasets	O
-and	O
as	O
such	O
,	O
we	O
will	O
need	O
to	O
develop	O
better	O
tools	O
for	O
learning	O
(	O
and	O
performing	O
active	O
learning	O
)	O
in	O
their	O
presence	O
.	O

Selective	O
Classification	O
.	O

One	O
potential	O
direction	O
for	O
future	O
work	O
is	O
to	O
develop	O
systems	O
that	O
abstain	O
when	O
they	O
encounter	O
collective	O
outliers	O
.	O

Historical	O
artificial	O
intelligence	O
systems	O
,	O
such	O
as	O
SHRDLU	O
(	O
Winograd	O
,	O
1972	O
)	O
and	O
QUALM	O
(	O
Lehnert	O
,	O
1977	O
)	O
,	O
were	O
designed	O
to	O
flag	O
input	O
sequences	O
that	O
they	O
were	O
not	O
designed	O
to	O
parse	O
.	O

Ideas	O
from	O
those	O
methods	O
can	O
and	O
should	O
be	O
resurrected	O
using	O
modern	O
techniques	O
;	O
for	O
example	O
,	O
recent	O
work	O
suggests	O
that	O
a	O
simple	O
classifier	O
can	O
be	O
trained	O
to	O
identify	O
out	O
-	O
ofdomain	O
data	O
inputs	O
,	O
provided	O
a	O
seed	O
out	O
-	O
of	O
-	O
domain	O
dataset	O
(	O
Kamath	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Active	O
learning	O
methods	O
can	O
be	O
augmented	O
with	O
a	O
similar	O
classifier	O
,	O
which	O
re	O
-	O
calibrates	O
active	O
learning	O
uncertainty	O
scores	O
with	O
this	O
classifier	O
's	O
predictions	O
.	O

Other	O
work	O
learns	O
to	O
identify	O
novel	O
utterances	O
by	O
learning	O
to	O
intelligently	O
set	O
thresholds	O
in	O
representation	O
space	O
(	O
Karamcheti	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
a	O
powerful	O
idea	O
especially	O
if	O
combined	O
with	O
other	O
representation	O
-	O
centric	O
active	O
learning	O
methods	O
like	O
Core	O
-	O
Set	O
Sampling	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018).Active	O
Learning	O
with	O
Global	O
Reasoning	O
.	O

Another	O
direction	O
for	O
future	O
work	O
to	O
explore	O
is	O
to	O
leverage	O
Dataset	O
Maps	O
to	O
perform	O
more	O
global	O
,	O
holistic	O
reasoning	O
over	O
datasets	O
,	O
to	O
intelligently	O
identify	O
promising	O
examples	O
-in	O
a	O
sense	O
,	O
baking	O
part	O
of	O
the	O
analysis	O
done	O
in	O
this	O
work	O
directly	O
into	O
the	O
active	O
learning	O
algorithms	O
.	O

A	O
possible	O
instantiation	O
of	O
this	O
idea	O
would	O
be	O
in	O
training	O
a	O
discriminator	O
to	O
differentiate	O
between	O
"	O
learnable	O
"	O
examples	O
(	O
upper	O
half	O
of	O
each	O
Dataset	O
Map	O
)	O
from	O
the	O
"	O
unlearnable	O
"	O
,	O
collective	O
outliers	O
with	O
low	O
confidence	O
and	O
low	O
variability	O
.	O

Between	O
each	O
active	O
learning	O
acquisition	O
iteration	O
,	O
one	O
can	O
generate	O
an	O
updated	O
Dataset	O
Map	O
,	O
thereby	O
reflecting	O
what	O
models	O
are	O
learning	O
as	O
they	O
obtain	O
new	O
labeled	O
examples	O
.	O

Machine	O
learning	O
systems	O
deployed	O
in	O
realworld	O
settings	O
will	O
inevitably	O
encounter	O
open	O
-	O
world	O
datasets	O
,	O
ones	O
that	O
contain	O
a	O
mixture	O
of	O
learnable	O
and	O
unlearnable	O
inputs	O
.	O

Our	O
work	O
provides	O
a	O
framework	O
to	O
study	O
when	O
models	O
encounter	O
such	O
inputs	O
.	O

Overall	O
,	O
we	O
hope	O
that	O
our	O
experiments	O
serve	O
as	O
a	O
catalyst	O
for	O
future	O
work	O
on	O
evaluating	O
active	O
learning	O
methods	O
with	O
inputs	O
drawn	O
from	O
open	O
-	O
world	O
datasets	O
.	O

All	O
code	O
for	O
data	O
preprocessing	O
,	O
model	O
implementation	O
,	O
and	O
active	O
learning	O
algorithms	O
is	O
made	O
available	O
at	O
https://github.com/siddk/vqa-outliers	O
.	O

Additionally	O
,	O
this	O
repository	O
also	O
contains	O
the	O
full	O
set	O
of	O
results	O
and	O
dataset	O
maps	O
as	O
well	O
.	O

Due	O
to	O
the	O
broad	O
scope	O
of	O
our	O
experiments	O
and	O
analysis	O
,	O
we	O
were	O
unable	O
to	O
fit	O
all	O
our	O
results	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
.	O

Furthermore	O
,	O
given	O
the	O
limited	O
length	O
provided	O
by	O
the	O
appendix	O
,	O
we	O
provide	O
only	O
salient	O
implementation	O
details	O
and	O
other	O
representative	O
results	O
here	O
;	O
however	O
,	O
we	O
make	O
all	O
code	O
,	O
models	O
,	O
data	O
,	O
results	O
,	O
active	O
learning	O
implementations	O
available	O
at	O
this	O
link	O
:	O
https	O
:	O
//github.com	O
/	O
siddk	O
/	O
vqa	O
-	O
outliers	O
.	O

Generally	O
,	O
any	O
combination	O
of	O
{	O
active	O
learning	O
strategy	O
×	O
model	O
×	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
×	O
analysis	O
/	O
acquisition	O
plot	O
}	O
is	O
present	O
in	O
this	O
paper	O
,	O
and	O
is	O
available	O
in	O
the	O
public	O
code	O
repository	O
.	O

Where	O
applicable	O
,	O
we	O
implement	O
our	O
models	O
based	O
on	O
publicly	O
available	O
PyTorch	O
implementations	O
.	O

For	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
,	O
we	O
base	O
our	O
implementation	O
off	O
of	O
this	O
repository	O
:	O
https://github.com/	O
Shivanshu	O
-	O
Gupta	O
/	O
Visual	B-TaskName
-	I-TaskName
Question	I-TaskName
-	I-TaskName
Answering	I-TaskName
,	O
while	O
for	O
the	O
Bottom	O
-	O
Up	O
Top	O
-	O
Down	O
Attention	O
Model	O
,	O
we	O
use	O
this	O
repository	O
:	O
https://github.com/	O
hengyuan	O
-	O
hu	O
/	O
bottom	O
-	O
up	O
-	O
attention	O
-	O
vqa	O
,	O
keeping	O
default	O
hyperparameters	O
the	O
same	O
.	O

Logistic	O
Regression	O
.	O

When	O
implementing	O
Logistic	O
Regression	O
,	O
we	O
base	O
our	O
PyTorch	O
implementation	O
on	O
the	O
broadly	O
used	O
Scikit	O
-	O
Learn	O
(	O
https	O
:	O
//scikit	O
-	O
learn.org	O
)	O
implementation	O
,	O
using	O
the	O
default	O
parameters	O
(	O
including	O
L2	O
weight	O
decay	O
)	O
.	O

We	O
optimize	O
our	O
models	O
via	O
stochastic	O
gradient	O
descent	O
.	O

LXMERT	B-MethodName
.	O

As	O
mentioned	O
in	O
Section	O
3	O
,	O
the	O
default	O
LXMERT	B-MethodName
checkpoint	O
and	O
fine	O
-	O
tuning	O
code	O
made	O
publicly	O
available	O
in	O
Tan	O
and	O
Bansal	O
(	O
2019	O
)	O
(	O
associated	O
code	O
repository	O
:	O
https://github.com/	O
airsplay	O
/	O
lxmert	O
)	O
is	O
pretrained	O
on	O
data	O
from	O
VQA-2	B-DatasetName
and	O
GQA	B-DatasetName
,	O
leaking	O
information	O
that	O
could	O
substantially	O
affect	O
our	O
active	O
learning	O
results	O
.	O

To	O
mitigate	O
this	O
,	O
we	O
contacted	O
the	O
authors	O
,	O
who	O
kindly	O
provided	O
us	O
with	O
a	O
checkpoint	O
of	O
the	O
model	O
without	O
VQA	B-TaskName
pretraining	O
.	O

However	O
,	O
in	O
addition	O
to	O
this	O
model	O
obtaining	O
different	O
results	O
from	O
those	O
reported	O
in	O
the	O
original	O
work	O
,	O
the	O
provided	O
pretrained	O
checkpoint	O
behaves	O
slightly	O
differently	O
during	O
fine	O
-	O
tuning	O
,	O
requiring	O
different	O
hyperparameters	O
than	O
provided	O
in	O
the	O
original	O
repository	O
.	O

We	O
perform	O
a	O
coarse	O
grid	O
search	O
over	O
hyperparameters	O
,	O
using	O
the	O
LXMERT	B-MethodName
implementation	O
provided	O
by	O
HuggingFace	O
Transformers	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
and	O
find	O
that	O
using	O
an	O
AdamW	O
optimizer	O
rather	O
than	O
the	O
BERT	O
-	O
Adam	O
Optimizer	O
used	O
in	O
the	O
original	O
work	O
without	O
any	O
special	O
learning	O
rate	O
scheduling	O
results	O
in	O
the	O
best	O
fine	O
-	O
tuning	O
performance	O
.	O

We	O
use	O
standard	O
implementations	O
of	O
the	O
8	O
active	O
learning	O
strategies	O
described	O
,	O
borrowing	O
from	O
prior	O
implementations	O
(	O
Mussmann	O
and	O
Liang	O
,	O
2018	O
)	O
and	O
existing	O
code	O
repositories	O
(	O
https://github.com/	O
google	O
/	O
active	O
-	O
learning	O
)	O
.	O

We	O
provide	O
additional	O
details	O
below	O
.	O

Monte	O
-	O
Carlo	O
Dropout	O
.	O

For	O
our	O
implementations	O
of	O
the	O
deep	O
Bayesian	O
active	O
learning	O
methods	O
(	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
w/	I-MethodName
Entropy	I-MethodName
,	O
BALD	B-MethodName
)	O
,	O
we	O
follow	O
Gal	O
and	O
Ghahramani	O
(	O
2016	O
)	O
and	O
estimate	O
a	O
Dropout	O
distribution	O
via	O
test	O
-	O
time	O
dropout	O
,	O
running	O
multiple	O
forward	O
passes	O
through	O
our	O
neural	O
networks	O
,	O
with	O
different	O
,	O
randomly	O
sampled	O
Dropout	O
masks	O
.	O

We	O
use	O
a	O
value	O
of	O
k	O
=	O
10	O
forward	O
passes	O
to	O
form	O
our	O
Dropout	O
distribution	O
.	O

Amortized	O
Core	O
-	O
Set	O
Selection	O
.	O

In	O
the	O
original	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
active	O
learning	O
work	O
introduced	O
by	O
Sener	O
and	O
Savarese	O
(	O
2018	O
)	O
,	O
it	O
is	O
shown	O
that	O
Core	O
-	O
Set	O
selection	O
for	O
active	O
learning	O
can	O
be	O
reduced	O
to	O
a	O
version	O
of	O
the	O
k	O
-	O
centers	O
problem	O
,	O
which	O
can	O
be	O
solved	O
approximately	O
(	O
2	O
-	O
OPT	O
)	O
with	O
a	O
greedy	O
algorithm	O
.	O

However	O
,	O
running	O
this	O
algorithm	O
on	O
highdimensional	O
representations	O
,	O
across	O
large	O
pools	O
can	O
be	O
prohibitive	O
;	O
Core	O
-	O
Set	O
selection	O
is	O
batch	O
-	O
aware	O
,	O
requiring	O
recomputing	O
distances	O
from	O
each	O
"	O
clustercenter	O
"	O
(	O
points	O
in	O
the	O
set	O
of	O
acquired	O
examples	O
)	O
to	O
all	O
points	O
in	O
the	O
active	O
learning	O
pool	O
after	O
each	O
acquisition	O
in	O
a	O
batch	O
.	O

While	O
we	O
can	O
run	O
this	O
out	O
completely	O
for	O
smaller	O
datasets	O
(	O
and	O
indeed	O
,	O
this	O
is	O
what	O
we	O
do	O
for	O
our	O
small	O
datasets	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
and	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
)	O
,	O
a	O
single	O
acquisition	O
iteration	O
for	O
a	O
large	O
dataset	O
for	O
the	O
full	O
VQA-2	B-DatasetName
dataset	O
takes	O
approximately	O
20	O
GPU	O
-	O
hours	O
on	O
the	O
resources	O
we	O
have	O
available	O
,	O
or	O
up	O
to	O
9	O
days	O
for	O
a	O
single	O
Core	O
-	O
Set	O
selection	O
run	O
.	O

For	O
GQA	B-DatasetName
,	O
performing	O
exact	O
Core	O
-	O
Set	O
selection	O
takes	O
at	O
least	O
twice	O
as	O
long	O
.	O

To	O
still	O
capture	O
the	O
spirit	O
of	O
Core	O
-	O
Set	O
diversitybased	O
selection	O
in	O
our	O
evaluation	O
,	O
we	O
instead	O
introduce	O
an	O
amortized	O
implementation	O
of	O
Core	O
-	O
Set	O
selection	O
,	O
which	O
is	O
comprised	O
of	O
two	O
steps	O
.	O

We	O
first	O
downsample	O
the	O
high	O
-	O
dimensional	O
representations	O
(	O
of	O
either	O
the	O
fused	O
language	O
and	O
text	O
,	O
or	O
either	O
unimodal	O
representations	O
)	O
via	O
Principal	O
Component	O
Analysis	O
(	O
PCA	O
)	O
to	O
make	O
the	O
distance	O
computation	O
faster	O
by	O
an	O
order	O
of	O
magnitude	O
.	O

Then	O
,	O
rather	O
than	O
updating	O
distances	O
from	O
examples	O
in	O
our	O
acquired	O
set	O
to	O
points	O
in	O
our	O
pool	O
after	O
each	O
acquisitionx	O
,	O
we	O
delay	O
updates	O
,	O
instead	O
only	O
refreshing	O
the	O
distance	O
computation	O
every	O
2000	O
acquisitions	O
(	O
roughly	O
5	O
%	O
of	O
an	O
acquisition	O
batch	O
for	O
VQA-2	B-DatasetName
)	O
.	O

This	O
allows	O
us	O
to	O
report	O
results	O
for	O
Core	O
-	O
Set	O
selection	O
with	O
the	O
three	O
different	O
proposed	O
representations	O
(	O
Fused	O
,	O
Language	O
-	O
Only	O
,	O
Vision	O
-	O
Only	O
)	O
for	O
VQA-2	B-DatasetName
;	O
unfortunately	O
,	O
for	O
GQA	B-DatasetName
and	O
LXMERT	B-MethodName
(	O
due	O
to	O
the	O
high	O
cost	O
of	O
training	O
)	O
,	O
even	O
running	O
this	O
amortized	O
version	O
of	O
Core	O
-	O
Set	O
selection	O
is	O
prohibitive	O
,	O
so	O
we	O
report	O
a	O
subset	O
of	O
results	O
,	O
and	O
omit	O
the	O
rest	O
.	O

We	O
include	O
further	O
results	O
from	O
our	O
study	O
of	O
active	O
learning	O
applied	O
to	O
VQA	B-TaskName
,	O
including	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
(	O
not	O
included	O
in	O
the	O
main	O
body	O
)	O
,	O
active	O
learning	O
results	O
for	O
the	O
two	O
logistic	O
regression	O
models	O
-Log	O
-	O
Reg	O
(	O
ResNet-101	O
)	O
and	O
Log	O
-	O
Reg	O
(	O
Faster	O
R	O
-	O
CNN	O
)	O
,	O
as	O
well	O
as	O
with	O
the	O
4	O
acquisition	O
strategies	O
not	O
included	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
-Entropy	O
,	O
Monte	O
-	O
Carlo	O
Dropout	O
w/	O
Entropy	O
,	O
Core	O
-	O
Set	O
(	O
Language	O
)	O
,	O
and	O
Core	O
-	O
Set	O
(	O
Vision	O
)	O
.	O

Figure	O
9	O
shows	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
with	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
BUTD	B-MethodName
,	O
and	O
LXMERT	B-MethodName
models	O
,	O
with	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
comprised	O
of	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
.	O

The	O
results	O
are	O
mostly	O
similar	O
to	O
those	O
reported	O
in	O
the	O
paper	O
;	O
strategies	O
track	O
or	O
underperform	O
random	O
sampling	O
,	O
with	O
the	O
exception	O
of	O
Least	O
-	O
Confidence	O
for	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
-however	O
,	O
this	O
is	O
the	O
sole	O
exception	O
,	O
and	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
has	O
the	O
highest	O
training	O
variance	O
of	O
all	O
the	O
models	O
we	O
try	O
.	O

Figure	O
10	O
shows	O
active	O
learning	O
results	O
for	O
the	O
Lo	O
-	O
gReg	O
(	O
ResNet-101	O
)	O
model	O
on	O
VQA	O
-	O
Sports	O
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
,	O
and	O
VQA-2	B-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
50	B-HyperparameterValue
%	I-HyperparameterValue
)	O
.	O

Results	O
are	O
similar	O
to	O
those	O
reported	O
in	O
the	O
paper	O
,	O
with	O
active	O
learning	O
failing	O
to	O
outperform	O
random	O
acqusition	O
.	O

Figure	O
11	O
presents	O
the	O
same	O
set	O
of	O
experiments	O
as	O
the	O
prior	O
section	O
,	O
except	O
with	O
the	O
LogReg	B-MethodName
(	O
Faster	O
R	O
-	O
CNN	O
)	O
model	O
.	O

While	O
the	O
object	O
-	O
based	O
Faster	O
R	O
-	O
CNN	O
representation	O
enables	O
much	O
higher	O
performance	O
than	O
the	O
ResNet-101	O
representation	O
,	O
active	O
learning	O
results	O
are	O
consistent	O
with	O
those	O
reported	O
in	O
the	O
paper	O
.	O

Figure	O
12	O
presents	O
results	O
for	O
the	O
four	O
other	O
active	O
learning	O
strategies	O
we	O
implement	O
-Entropy	O
,	O
Monte	O
Carlo	O
Dropout	O
w/	O
Entropy	O
,	O
Core	O
-	O
Set	O
(	O
Language	O
)	O
,	O
and	O
Core	O
-	O
Set	O
(	O
Vision	O
)	O
-for	O
the	O
BUTD	B-MethodName
model	O
.	O

Results	O
are	O
across	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
,	O
and	O
VQA-2	B-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
50	B-HyperparameterValue
%	I-HyperparameterValue
)	O
-despite	O
the	O
unique	O
features	O
of	O
each	O
strategy	O
,	O
the	O
trends	O
remain	O
consistent	O
with	O
those	O
in	O
the	O
paper	O
.	O

Figure	O
12	O
:	O
Results	O
with	O
the	O
BUTD	B-MethodName
on	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
VQA-2	B-DatasetName
and	O
GQA	O
using	O
the	O
alternative	O
4	O
acquisition	O
strategies	O
not	O
included	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
.	O

Unsurprisingly	O
,	O
results	O
are	O
consistent	O
with	O
those	O
reported	O
in	O
the	O
paper	O
.	O

Given	O
that	O
the	O
map	O
for	O
GQA	B-DatasetName
is	O
similar	O
to	O
the	O
map	O
for	O
VQA-2	B-DatasetName
,	O
it	O
is	O
not	O
surprising	O
that	O
the	O
active	O
learning	O
acquisitions	O
follow	O
a	O
similar	O
trend	O
,	O
preferring	O
to	O
select	O
"	O
hard	O
-	O
to	O
-	O
learn	O
"	O
examples	O
.	O

The	O
authors	O
are	O
fully	O
committed	O
to	O
maintaining	O
this	O
repository	O
,	O
in	O
terms	O
of	O
both	O
functionality	O
and	O
ease	O
of	O
use	O
,	O
and	O
will	O
actively	O
monitor	O
both	O
email	O
and	O
Github	O
Issues	O
should	O
there	O
be	O
problems	O
.	O

We	O
thank	O
Kaylee	O
Burns	O
,	O
Eric	O
Mitchell	O
,	O
Stephen	O
Mussman	O
,	O
Dorsa	O
Sadigh	O
,	O
and	O
our	O
anonymous	O
ACL	O
reviewers	O
for	O
their	O
useful	O
feedback	O
on	O
earlier	O
versions	O
of	O
this	O
paper	O
.	O

We	O
are	O
also	O
grateful	O
to	O
Hao	O
Tan	O
for	O
providing	O
us	O
with	O
the	O
LXMERT	B-MethodName
checkpoint	O
trained	O
without	O
access	O
to	O
VQA	B-TaskName
datasets	O
,	O
as	O
well	O
as	O
for	O
general	O
LXMERT	B-MethodName
fine	O
-	O
tuning	O
pointers	O
.	O

Siddharth	O
Karamcheti	O
is	O
graciously	O
supported	O
by	O
the	O
Open	O
Philanthropy	O
Project	O
AI	O
Fellowship	O
.	O

Christopher	O
D.	O
Manning	O
is	O
a	O
CIFAR	O
Fellow	O
.	O

Contrastive	O
learning	O
has	O
been	O
used	O
to	O
learn	O
a	O
high	O
-	O
quality	O
representation	O
of	O
the	O
image	O
in	O
computer	O
vision	O
.	O

However	O
,	O
contrastive	O
learning	O
is	O
not	O
widely	O
utilized	O
in	O
natural	O
language	O
processing	O
due	O
to	O
the	O
lack	O
of	O
a	O
general	O
method	O
of	O
data	O
augmentation	O
for	O
text	O
data	O
.	O

In	O
this	O
work	O
,	O
we	O
explore	O
the	O
method	O
of	O
employing	O
contrastive	O
learning	O
to	O
improve	O
the	O
text	O
representation	O
from	O
the	O
BERT	B-MethodName
model	O
for	O
relation	B-TaskName
extraction	I-TaskName
.	O

The	O
key	O
knob	O
of	O
our	O
framework	O
is	O
a	O
unique	O
contrastive	O
pre	O
-	O
training	O
step	O
tailored	O
for	O
the	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
by	O
seamlessly	O
integrating	O
linguistic	O
knowledge	O
into	O
the	O
data	O
augmentation	O
.	O

Furthermore	O
,	O
we	O
investigate	O
how	O
large	O
-	O
scale	O
data	O
constructed	O
from	O
the	O
external	O
knowledge	O
bases	O
can	O
enhance	O
the	O
generality	O
of	O
contrastive	O
pre	O
-	O
training	O
of	O
BERT	B-MethodName
.	O

The	O
experimental	O
results	O
on	O
three	O
relation	O
extraction	O
benchmark	O
datasets	O
demonstrate	O
that	O
our	O
method	O
can	O
improve	O
the	O
BERT	B-MethodName
model	O
representation	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
addition	O
,	O
we	O
explore	O
the	O
interpretability	O
of	O
models	O
by	O
showing	O
that	O
BERT	B-MethodName
with	O
contrastive	O
pre	O
-	O
training	O
relies	O
more	O
on	O
rationales	O
for	O
prediction	O
.	O

Our	O
code	O
and	O
data	O
are	O
publicly	O
available	O
at	O
:	O
https://github	O
.	O

com	O
/	O
udel	O
-	O
biotm	O
-	O
lab	O
/	O
BERT	O
-	O
CLRE	O
.	O

Contrastive	O
learning	O
is	O
a	O
family	O
of	O
methods	O
to	O
learn	O
a	O
discriminative	O
model	O
by	O
comparing	O
input	O
pairs	O
(	O
Le	O
-	O
Khac	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
comparison	O
is	O
performed	O
between	O
positive	O
pairs	O
of	O
"	O
similar	O
"	O
inputs	O
and	O
negative	O
pairs	O
of	O
"	O
dissimilar	O
"	O
inputs	O
.	O

The	O
positive	O
pairs	O
can	O
be	O
generated	O
in	O
an	O
automatic	O
way	O
by	O
transforming	O
the	O
original	O
data	O
to	O
variants	O
without	O
changing	O
the	O
key	O
information	O
(	O
e.g.	O
,	O
rotate	O
an	O
image	O
)	O
.	O

Contrastive	O
learning	O
can	O
encode	O
general	O
properties	O
(	O
e.g.	O
invariance	O
)	O
in	O
the	O
learned	O
representation	O
while	O
it	O
is	O
relatively	O
hard	O
for	O
other	O
representation	O
learning	O
methods	O
to	O
achieve	O
(	O
Bengio	O
et	O
al	O
.	O
,	O

1	O
These	O
authors	O
contributed	O
equally	O
.	O

2013	O
;	O
Le	O
-	O
Khac	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Therefore	O
,	O
contrastive	O
learning	O
provides	O
a	O
powerful	O
approach	O
to	O
learn	O
representations	O
in	O
a	O
self	O
-	O
supervised	O
manner	O
and	O
has	O
shown	O
great	O
promise	O
and	O
achieved	O
the	O
state	O
of	O
the	O
art	O
results	O
in	O
recent	O
years	O
(	O
He	O
et	O
al	O
.	O
,	O

2020;Chen	O
et	O
al	O
.	O
,	O

2020).Despite	O
its	O
advancement	O
,	O
contrastive	O
learning	O
has	O
not	O
been	O
well	O
studied	O
in	O
biomedical	O
natural	O
language	O
processing	O
(	O
BioNLP	O
)	O
,	O
especially	O
for	O
relation	B-TaskName
extraction	I-TaskName
(	O
RE	B-TaskName
)	O
tasks	O
.	O

One	O
obstacle	O
lies	O
in	O
the	O
discrete	O
characteristics	O
of	O
text	O
data	O
.	O

Compared	O
to	O
computer	O
vision	O
,	O
it	O
is	O
more	O
challenging	O
to	O
design	O
a	O
general	O
and	O
efficient	O
data	O
augmentation	O
method	O
to	O
construct	O
positive	O
pairs	O
.	O

Instead	O
,	O
there	O
have	O
been	O
significant	O
advances	O
in	O
the	O
development	O
of	O
pretrained	O
language	O
models	O
to	O
facilitate	O
downstream	O
BioNLP	O
tasks	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019;Radford	O
et	O
al	O
.	O
,	O

2019;Peng	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Therefore	O
,	O
leveraging	O
contrastive	O
learning	O
in	O
the	O
large	O
pre	O
-	O
trained	O
language	O
models	O
to	O
learn	O
more	O
general	O
representation	O
for	O
RE	B-TaskName
tasks	O
remains	O
unexplored	O
.	O

To	O
bridge	O
this	O
gap	O
,	O
this	O
paper	O
presents	O
an	O
innovative	O
method	O
of	O
contrastive	O
pre	O
-	O
training	O
to	O
improve	O
the	O
language	O
model	O
representation	O
for	O
biomedical	B-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

As	O
the	O
main	O
difference	O
from	O
the	O
existing	O
contrastive	O
learning	O
framework	O
,	O
we	O
augment	O
the	O
datasets	O
for	O
RE	B-TaskName
tasks	O
by	O
randomly	O
changing	O
the	O
words	O
that	O
do	O
not	O
affect	O
the	O
relation	O
expression	O
.	O

Here	O
,	O
we	O
hypothesize	O
that	O
the	O
shortest	O
dependency	O
path	O
(	O
SDP	O
)	O
between	O
two	O
entities	O
captures	O
the	O
required	O
knowledge	O
for	O
the	O
relation	O
expression	O
.	O

We	O
hence	O
keep	O
words	O
on	O
SDP	O
fixed	O
during	O
the	O
data	O
augmentation	O
.	O

In	O
addition	O
,	O
we	O
utilize	O
external	O
knowledge	O
bases	O
to	O
construct	O
more	O
data	O
to	O
make	O
the	O
learned	O
representation	O
generalize	O
better	O
,	O
which	O
is	O
a	O
method	O
that	O
is	O
frequently	O
used	O
in	O
distant	O
supervision	O
(	O
Mintz	O
et	O
al	O
.	O
,	O

2009;Peng	O
et	O
al	O
.	O
,	O

2016).To	O
verify	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
,	O
we	O
use	O
the	O
transformer	O
-	O
based	O
BERT	B-MethodName
model	O
as	O
a	O
backbone	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
evaluate	O
our	O
method	O
on	O
three	O
widely	O
studied	O
RE	B-TaskName
tasks	O
in	O
the	O
biomedical	O
domain	O
:	O
the	O
chemical	B-TaskName
-	I-TaskName
protein	I-TaskName
interactions	I-TaskName
(	O
ChemProt	B-TaskName
)	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
the	O
drug	B-TaskName
-	I-TaskName
drug	I-TaskName
interactions	I-TaskName
(	O
DDI	B-TaskName
)	O
(	O
Herrero	O
-	O
Zazo	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
and	O
the	O
protein	B-TaskName
-	I-TaskName
protein	I-TaskName
interactions	I-TaskName
(	O
PPI	B-TaskName
)	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2008	O
)	O
.	O

The	O
experimental	O
results	O
show	O
that	O
our	O
method	O
boosts	O
the	O
BERT	B-MethodName
model	O
performance	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
three	O
tasks	O
.	O

Interest	O
has	O
also	O
grown	O
in	O
designing	O
interpretable	O
BioNLP	O
models	O
that	O
are	O
both	O
plausible	O
(	O
accurate	O
)	O
and	O
rely	O
on	O
a	O
specific	O
part	O
of	O
the	O
input	O
(	O
faithful	O
rationales	O
)	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020;Lei	O
et	O
al	O
.	O
,	O

2016).Here	O
rationale	O
is	O
defined	O
as	O
the	O
supporting	O
evidence	O
in	O
the	O
inputs	O
for	O
the	O
model	O
to	O
make	O
correct	O
predictions	O
.	O

In	O
this	O
direction	O
,	O
we	O
propose	O
a	O
new	O
metric	O
,	O
"	O
prediction	O
shift	O
"	O
,	O
to	O
measure	O
the	O
sensitivity	O
degree	O
to	O
which	O
the	O
small	O
changes	O
(	O
out	O
of	O
the	O
SDP	O
)	O
of	O
the	O
inputs	O
will	O
make	O
model	O
change	O
its	O
predictions	O
.	O

We	O
show	O
that	O
the	O
contrastively	O
pre	O
-	O
trained	O
model	O
is	O
more	O
robust	O
than	O
the	O
original	O
model	O
,	O
suggesting	O
that	O
our	O
model	O
is	O
more	O
likely	O
to	O
make	O
predictions	O
based	O
on	O
the	O
rationales	O
of	O
the	O
inputs	O
.	O

In	O
sum	O
,	O
the	O
contribution	O
of	O
this	O
work	O
is	O
fourfold	O
.	O
(	O

1	O
)	O
We	O
propose	O
a	O
new	O
method	O
that	O
utilizes	O
contrastive	O
learning	O
to	O
improve	O
the	O
BERT	B-MethodName
model	O
on	O
biomedical	B-TaskName
relation	I-TaskName
extraction	I-TaskName
tasks	O
.	O
(	O

2	O
)	O
We	O
utilize	O
external	O
knowledge	O
to	O
generate	O
more	O
data	O
for	O
learning	O
more	O
generalized	O
text	O
representation.(3	O
)	O
We	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
three	O
benchmark	O
datasets	O
of	O
relation	O
extraction	O
tasks	O
.	O
(	O

4	O
)	O
We	O
propose	O
a	O
new	O
metric	O
that	O
aims	O
to	O
reveal	O
the	O
rationales	O
that	O
the	O
model	O
uses	O
for	O
predicting	O
relations	O
.	O

The	O
code	O
and	O
the	O
new	O
rationale	O
test	O
datasets	O
are	O
available	O
at	O
https://github	O
.	O

com	O
/	O
udel	O
-	O
biotm	O
-	O
lab	O
/	O
BERT	O
-	O
CLRE	O
.	O

The	O
history	O
of	O
contrastive	O
representation	O
learning	O
can	O
be	O
traced	O
back	O
to	O
(	O
Hadsell	O
et	O
al	O
.	O
,	O

2006	O
)	O
,	O
in	O
which	O
the	O
authors	O
explore	O
the	O
method	O
of	O
representation	O
learning	O
that	O
similar	O
inputs	O
are	O
mapped	O
to	O
nearby	O
points	O
in	O
the	O
representation	O
space	O
.	O

Recently	O
,	O
with	O
the	O
development	O
of	O
data	O
augmentation	O
techniques	O
,	O
deep	O
neural	O
network	O
architectures	O
,	O
contrastive	O
learning	O
regains	O
attention	O
and	O
achieves	O
superior	O
performance	O
on	O
visual	O
representation	O
learning	O
(	O
He	O
et	O
al	O
.	O
,	O

2020;Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

In	O
(	O
He	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
the	O
Momentum	O
Contrast	O
(	O
MoCo	O
)	O
framework	O
is	O
designed	O
to	O
learn	O
representation	O
using	O
the	O
mechanism	O
of	O
dictionary	O
look	O
-	O
up	O
:	O
an	O
encoded	O
example	O
(	O
the	O
query	O
)	O
should	O
be	O
similar	O
to	O
its	O
matching	O
key	O
(	O
augmented	O
sample	O
from	O
the	O
same	O
data	O
example	O
)	O
and	O
dissimilar	O
to	O
others	O
.	O

In	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
the	O
authors	O
propose	O
the	O
SimCLR	O
frame	O
to	O
learn	O
the	O
representations	O
by	O
maximizing	O
the	O
agreement	O
between	O
augmented	O
views	O
of	O
the	O
same	O
data	O
point	O
.	O

The	O
contrastive	O
representation	O
has	O
all	O
the	O
properties	O
that	O
a	O
good	O
representation	O
should	O
have	O
:	O
1	O
)	O
Distributed	O
property	O
;	O
2	O
)	O
Abstraction	O
and	O
invariant	O
property	O
;	O
3	O
)	O
Disentangled	O
representation	O
(	O
Bengio	O
et	O
al	O
.	O
,	O

2013;Le	O
-	O
Khac	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
distributed	O
property	O
emphasizes	O
the	O
expressive	O
aspect	O
of	O
the	O
representation	O
(	O
different	O
data	O
points	O
should	O
have	O
distinguishable	O
representations	O
)	O
.	O

The	O
capture	O
of	O
abstract	O
concepts	O
and	O
the	O
invariance	O
to	O
small	O
and	O
local	O
changes	O
are	O
concerned	O
in	O
the	O
abstraction	O
and	O
invariant	O
property	O
.	O

From	O
the	O
disentangled	O
representation	O
's	O
perspective	O
,	O
it	O
should	O
encode	O
as	O
much	O
information	O
as	O
possible	O
.	O

In	O
this	O
work	O
,	O
we	O
will	O
show	O
contrastive	O
learning	O
can	O
improve	O
the	O
invariant	O
aspect	O
of	O
the	O
representation	O
.	O

In	O
the	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
field	O
,	O
several	O
works	O
have	O
utilized	O
the	O
contrastive	O
learning	O
technique	O
.	O

Fang	O
et	O
al	O
.	O
(	O

2020	O
)	O
propose	O
a	O
pre	O
-	O
trained	O
language	O
representation	O
model	O
(	O
CERT	B-MethodName
)	O
using	O
contrastive	O
learning	O
at	O
the	O
sentence	O
level	O
to	O
benefit	O
the	O
language	O
understanding	O
tasks	O
.	O

Klein	O
and	O
Nabi	O
(	O
2020	O
)	O
employ	O
contrastive	O
self	O
-	O
supervised	O
learning	O
to	O
solve	O
the	O
commonsense	O
reasoning	O
problem	O
.	O

Peng	O
et	O
al	O
.	O
(	O

2020	O
)	O
propose	O
a	O
self	O
-	O
supervised	O
pretraining	O
framework	O
for	O
relation	O
extraction	O
to	O
explore	O
the	O
encoded	O
information	O
for	O
the	O
textual	O
context	O
and	O
entity	O
type	O
.	O

Compared	O
with	O
the	O
previous	O
works	O
,	O
we	O
employ	O
different	O
data	O
augmentation	O
techniques	O
and	O
utilize	O
data	O
from	O
external	O
knowledge	O
bases	O
in	O
contrastive	O
learning	O
to	O
improve	O
the	O
model	O
for	O
relation	O
extraction	O
tasks	O
.	O

Relation	O
extraction	O
is	O
usually	O
seen	O
as	O
a	O
classification	O
problem	O
when	O
the	O
entity	O
mentions	O
are	O
given	O
in	O
the	O
text	O
.	O

Many	O
different	O
methods	O
have	O
been	O
proposed	O
to	O
solve	O
the	O
relation	B-TaskName
extraction	I-TaskName
problem	O
(	O
Culotta	O
and	O
Sorensen	O
,	O
2004;Sierra	O
et	O
al	O
.	O
,	O

2008;Sahu	O
and	O
Anand	O
,	O
2018;Zhang	O
et	O
al	O
.	O
,	O

2019;Su	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

However	O
,	O
the	O
language	O
model	O
methods	O
redefine	O
this	O
field	O
with	O
their	O
superior	O
performance	O
(	O
Dai	O
and	O
Le	O
,	O
2015;Peters	O
et	O
al	O
.	O
,	O

2018;Devlin	O
et	O
al	O
.	O
,	O

2019;Radford	O
et	O
al	O
.	O
,	O

2019;Su	O
and	O
Vijay	O
-	O
Shanker	O
,	O
2020	O
)	O
.	O

Among	O
all	O
the	O
language	O
models	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
-a	O
language	O
representation	O
model	O
based	O
on	O
bidirectional	O
Transformer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
attracts	O
lots	O
of	O
attention	O
in	O
Figure	O
1	O
:	O
The	O
framework	O
of	O
contrastive	O
learning	O
.	O

For	O
the	O
data	O
augmentation	O
of	O
relation	O
extraction	O
,	O
we	O
randomly	O
replace	O
some	O
words	O
that	O
are	O
not	O
affecting	O
the	O
relation	O
expression	O
(	O
w	O
i	O
→	O
w	O
i	O
in	O
the	O
left	O
sample	O
,	O
w	O
j	O
→	O
w	O
j	O
in	O
the	O
right	O
sample	O
)	O
.	O

different	O
fields	O
.	O

Several	O
BERT	B-MethodName
models	O
have	O
been	O
adapted	O
for	O
biomedical	O
domain	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
SciBERT	B-MethodName
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
Blue	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Peng	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

BioBERT	B-MethodName
,	O
SciBERT	B-MethodName
and	O
BlueBERT	B-MethodName
are	O
pre	O
-	O
trained	O
based	O
on	O
the	O
general	O
-	O
domain	O
BERT	B-MethodName
using	O
different	O
pre	O
-	O
training	O
data	O
.	O

In	O
contrast	O
,	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

2021	O
)	O
is	O
pre	O
-	O
trained	O
from	O
scratch	O
using	O
PubMed	O
abstracts	O
.	O

In	O
recent	O
years	O
,	O
there	O
is	O
increasing	O
interest	O
in	O
designing	O
more	O
interpretable	O
NLP	O
models	O
that	O
reveal	O
the	O
logic	O
behind	O
model	O
predictions	O
.	O

In	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
multiple	O
datasets	O
of	O
rationales	O
(	O
from	O
human	O
experts	O
)	O
are	O
collected	O
to	O
facilitate	O
the	O
research	O
on	O
interpretable	O
models	O
in	O
NLP	O
.	O

In	O
(	O
Lei	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
the	O
authors	O
propose	O
an	O
encoder	O
-	O
generator	O
framework	O
to	O
automatically	O
generate	O
candidate	O
rationales	O
to	O
justify	O
the	O
predictions	O
of	O
neural	O
network	O
models	O
.	O

Our	O
goal	O
is	O
to	O
learn	O
a	O
text	O
representation	O
by	O
maximizing	O
agreement	O
between	O
inputs	O
from	O
positive	O
pairs	O
via	O
a	O
contrastive	O
loss	O
in	O
the	O
latent	O
space	O
and	O
the	O
learned	O
representation	O
can	O
then	O
be	O
used	O
for	O
relation	O
extraction	O
.	O

Figure	O
1	O
shows	O
our	O
framework	O
of	O
contrastive	O
learning	O
.	O

Given	O
a	O
sentence	O
s	O
=	O
w	O
1	O
,	O
...	O
w	O
n	O
,	O
we	O
first	O
produce	O
two	O
augmented	O
views	O
(	O
a	O
positive	O
pair	O
)	O
v	O
=	O
w	O
1	O
,	O
...	O
,	O
w	O
i	O
,	O
...	O
w	O
n	O
and	O
v	O
=	O
w	O
1	O
...	O
,	O
w	O
j	O
,	O
...	O
w	O
n	O
(	O
i	O
=	O
j	O
)	O
from	O
s	O
by	O
applying	O
text	O
augmentation	O
technique	O
(	O
Section	O
3.1.1).Our	O
framework	O
then	O
uses	O
one	O
neural	O
network	O
to	O
encode	O
the	O
two	O
inputs	O
,	O
which	O
consists	O
of	O
a	O
neural	O
network	O
encoder	O
f	O
(	O
Section	O
3.1.2	O
)	O
and	O
a	O
projection	O
head	O
g	O
(	O
Section	O
3.1.3	O
)	O
.	O

From	O
the	O
first	O
augmented	O
view	O
v	O
,	O
we	O
output	O
a	O
representation	O
h	O
f	O
(	O
v	O
)	O
and	O
a	O
projection	O
z	O
g(h	O
)	O
.	O

From	O
the	O
second	O
augmented	O
view	O
v	O
,	O
we	O
output	O
h	O
f	O
(	O
v	O
)	O
and	O
another	O
projection	O
z	O
g(h	O
)	O
.	O

The	O
contrastive	O
learning	O
method	O
learns	O
the	O
representation	O
by	O
comparing	O
different	O
samples	O
in	O
the	O
training	O
data	O
(	O
Section	O
3.1.4	O
)	O
.	O

The	O
comparison	O
is	O
performed	O
between	O
both	O
similar	O
inputs	O
and	O
dissimilar	O
inputs	O
,	O
and	O
the	O
similar	O
inputs	O
are	O
positive	O
pairs	O
and	O
the	O
dissimilar	O
inputs	O
are	O
negative	O
pairs	O
.	O

During	O
the	O
training	O
,	O
the	O
representations	O
are	O
learned	O
by	O
leading	O
the	O
positive	O
pairs	O
to	O
have	O
similar	O
representations	O
and	O
making	O
negative	O
pairs	O
have	O
dissimilar	O
representations	O
.	O

In	O
applications	O
,	O
the	O
positive	O
pairs	O
are	O
usually	O
from	O
the	O
augmented	O
data	O
of	O
the	O
same	O
sample	O
,	O
and	O
the	O
negative	O
pairs	O
are	O
generated	O
by	O
selecting	O
augmented	O
data	O
from	O
different	O
samples	O
.	O

At	O
the	O
end	O
of	O
training	O
,	O
we	O
only	O
keep	O
the	O
encoder	O
f	O
as	O
in	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

For	O
any	O
text	O
input	O
x	O
,	O
h	O
=	O
f	O
(	O
x	O
)	O
will	O
be	O
the	O
representation	O
of	O
x	O
from	O
contrastive	O
learning	O
.	O

The	O
data	O
augmentation	O
module	O
is	O
a	O
key	O
component	O
of	O
contrastive	O
learning	O
,	O
which	O
needs	O
to	O
randomly	O
generate	O
two	O
correlated	O
views	O
for	O
the	O
original	O
data	O
point	O
.	O

At	O
the	O
same	O
time	O
,	O
the	O
generated	O
data	O
should	O
be	O
different	O
from	O
each	O
other	O
to	O
make	O
them	O
distinguishable	O
(	O
from	O
the	O
model	O
's	O
perspective	O
)	O
,	O
but	O
should	O
not	O
be	O
significantly	O
different	O
to	O
change	O
the	O
structure	O
and	O
semantics	O
of	O
the	O
original	O
data	O
.	O

It	O
is	O
especially	O
difficult	O
to	O
augment	O
the	O
text	O
data	O
of	O
relation	O
extraction	O
.	O

In	O
this	O
work	O
,	O
we	O
only	O
focus	O
on	O
binary	O
relations	O
.	O

Given	O
<	O
s	O
,	O
e	O
1	O
,	O
e	O
2	O
,	O
r	O
>	O
,	O
where	O
e	O
1	O
and	O
e	O
2	O
are	O
two	O
entity	O
mentions	O
in	O
the	O
sentence	O
s	O
with	O
the	O
relation	O
type	O
r	O
,	O
we	O
keep	O
e	O
1	O
and	O
e	O
2	O
in	O
the	O
sentence	O
and	O
retain	O
the	O
relation	O
expression	O
between	O
e	O
1	O
and	O
e	O
2	O
in	O
the	O
augmented	O
views	O
.	O

Specifically	O
,	O
we	O
propose	O
a	O
data	O
augmentation	O
method	O
utilizing	O
the	O
shortest	O
dependency	O
path	O
(	O
SDP	O
)	O
between	O
the	O
two	O
entities	O
in	O
the	O
text	O
.	O

We	O
hypothesize	O
that	O
the	O
shortest	O
dependency	O
path	O
captures	O
the	O
required	O
information	O
to	O
assert	O
the	O
relationship	O
of	O
the	O
two	O
entities	O
.	O

Therefore	O
we	O
fix	O
the	O
shortest	O
dependency	O
path	O
,	O
and	O
randomly	O
change	O
the	O
other	O
tokens	O
in	O
the	O
text	O
to	O
generate	O
the	O
augmented	O
data	O
.	O

This	O
idea	O
is	O
inspired	O
by	O
(	O
Wei	O
and	O
Zou	O
,	O
2019	O
)	O
,	O
which	O
We	O
further	O
show	O
that	O
@PROTEIN$	O
directly	O
interacts	O
with	O
@PROTEIN$	O
and	O
Rpn4	O
.	O

After	O
SR	O
We	O
further	O
show	O
that	O
@PROTEIN$	O
straight	O
interacts	O
with	O
@PROTEIN$	O
and	O
Rpn4	O
.	O

After	O
RS	O
Further	O
we	O
show	O
that	O
@PROTEIN$	O
directly	O
interacts	O
with	O
@PROTEIN$	O
and	O
Rpn4	O
.	O

After	O
RD	O
We	O
further	O
show	O
that	O
@PROTEIN$	O
interacts	O
with	O
@PROTEIN$	O
and	O
Rpn4.Table	O
1	O
:	O
Examples	O
after	O
the	O
three	O
operations	O
for	O
data	O
augmentation	O
.	O

The	O
shortest	O
dependency	O
path	O
between	O
two	O
proteins	O
is	O
"	O
@PROTEIN$	O
interacts	O
@PROTEIN$	O
"	O
,	O
which	O
is	O
marked	O
with	O
underline	O
in	O
the	O
examples	O
.	O

The	O
changed	O
words	O
are	O
also	O
marked	O
with	O
bold	O
font	O
.	O

employed	O
easy	O
data	O
augmentation	O
techniques	O
to	O
improve	O
model	O
performance	O
on	O
text	O
classification	O
tasks	O
.	O

As	O
the	O
preliminary	O
study	O
,	O
we	O
experiment	O
with	O
three	O
techniques	O
to	O
randomly	O
replace	O
the	O
tokens	O
to	O
generate	O
the	O
augmented	O
data	O
and	O
choose	O
the	O
best	O
one	O
for	O
our	O
contrastive	O
learning	O
method	O
:	O
1	O
)	O
Synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
,	O
2	O
)	O
Random	B-MethodName
swap	I-MethodName
(	O
RS	B-MethodName
)	O
,	O
and	O
3	O
)	O
Random	B-MethodName
deletion	I-MethodName
(	O
RD).Table	B-MethodName
1	O
gives	O
some	O
samples	O
after	O
applying	O
the	O
three	O
operations	O
on	O
a	O
sentence	O
from	O
the	O
PPI	B-TaskName
task	O
.	O

For	O
the	O
synonym	B-MethodName
replacement	I-MethodName
,	O
we	O
randomly	O
replace	O
n	O
words	O
with	O
their	O
synonyms	O
.	O

To	O
acquire	O
the	O
synonym	O
of	O
a	O
word	O
,	O
we	O
utilize	O
the	O
WordNet	O
database	O
(	O
Miller	O
,	O
1995	O
)	O
to	O
extract	O
a	O
list	O
of	O
synonyms	O
and	O
randomly	O
choose	O
one	O
from	O
the	O
list	O
.	O

For	O
the	O
random	O
swap	O
,	O
we	O
swap	O
the	O
positions	O
of	O
two	O
words	O
and	O
repeat	O
this	O
operation	O
n	O
times	O
.	O

For	O
the	O
random	O
deletion	O
,	O
we	O
delete	O
some	O
words	O
with	O
the	O
probability	B-HyperparameterName
p.	I-HyperparameterName
The	O
probability	B-HyperparameterName
p	I-HyperparameterName
is	O
set	O
to	O
0.1	B-HyperparameterValue
in	O
our	O
experiments	O
and	O
the	O
parameter	O
n	O
for	O
SR	B-MethodName
and	O
RS	B-MethodName
is	O
calculated	O
by	O
p	O
×	O
l	O
,	O
where	O
l	O
is	O
the	O
length	O
of	O
the	O
sentence	O
.	O

To	O
examine	O
which	O
operation	O
performs	O
better	O
for	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
,	O
we	O
train	O
three	O
BERT	B-MethodName
models	O
using	O
the	O
three	O
types	O
of	O
augmented	O
data	O
(	O
combined	O
with	O
the	O
original	O
training	O
data	O
)	O
.	O

Table	O
4	O
shows	O
that	O
the	O
synonym	B-TaskName
replacement	I-TaskName
(	O
SR	B-TaskName
)	O
operation	O
achieves	O
the	O
best	O
performance	O
on	O
all	O
three	O
tasks	O
and	O
we	O
will	O
employ	O
this	O
operation	O
in	O
our	O
data	O
augmentation	O
module	O
in	O
our	O
contrastive	O
learning	O
experiments	O
(	O
We	O
will	O
further	O
discuss	O
it	O
in	O
Section	O
5.2	O
)	O
.	O

In	O
this	O
work	O
,	O
we	O
employ	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
as	O
our	O
encoder	O
for	O
the	O
text	O
data	O
and	O
the	O
classification	O
token	O
(	O
[	O
CLS	O
]	O
)	O
output	O
in	O
the	O
last	O
layer	O
will	O
be	O
the	O
representation	O
of	O
the	O
input	O
.	O

As	O
demonstrated	O
in	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
adding	O
a	O
nonlinear	O
projection	O
head	O
on	O
the	O
model	O
output	O
will	O
improve	O
the	O
representation	O
quality	O
during	O
training	O
.	O

Following	O
the	O
same	O
idea	O
,	O
a	O
multi	O
-	O
layer	O
perceptron	O
(	O
MLP	O
)	O
will	O
be	O
applied	O
to	O
the	O
model	O
output	O
h.	O
Formally	O
,	O
z	O
=	O
g(h	O
)	O
=	O
W	O
2	O
φ(W	O
1	O
h)and	O
φ	O
is	O
the	O
ReLU	O
activation	O
function	O
,	O
W	O
1	O
and	O
W	O
2	O
are	O
the	O
weights	O
of	O
the	O
perceptron	O
in	O
the	O
hidden	O
layers	O
.	O

Contrastive	O
learning	O
is	O
designed	O
to	O
make	O
similar	O
representations	O
be	O
learned	O
for	O
the	O
augmented	O
samples	O
(	O
positive	O
pairs	O
)	O
from	O
the	O
same	O
data	O
point	O
.	O

We	O
follow	O
the	O
work	O
of	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
to	O
design	O
the	O
loss	O
function	O
(	O
Algorithm	O
1	O
)	O
.	O

During	O
contrastive	O
learning	O
,	O
the	O
contrastive	O
loss	O
is	O
calculated	O
based	O
on	O
the	O
augmented	O
batch	O
derived	O
from	O
the	O
original	O
batch	O
.	O

Given	O
N	O
sentences	O
in	O
a	O
batch	O
,	O
we	O
first	O
employ	O
the	O
data	O
augmentation	O
technique	O
to	O
acquire	O
two	O
views	O
for	O
each	O
sentence	O
in	O
the	O
batch	O
.	O

Therefore	O
,	O
we	O
have	O
2N	O
views	O
from	O
the	O
batch	O
.	O

Given	O
one	O
positive	O
pair	O
(	O
two	O
views	O
from	O
the	O
same	O
sentence	O
)	O
,	O
we	O
treat	O
the	O
other	O
2(N	O
−	O
1	O
)	O
within	O
the	O
batch	O
as	O
negative	O
examples	O
.	O

Similar	O
to	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
the	O
loss	O
for	O
a	O
positive	O
pair	O
is	O
defined	O
as	O
:	O
l(z	O
,	O
z	O
)	O
=	O
−log	O
exp(sim(z	O
,	O
z	O
)	O
/τ	O
)	O
2N	O
k=1	O
1	O
[	O
z	O
k	O
=	O
z	O
]	O
exp(sim(z	O
,	O
z	O
k	O
)	O
/τ	O
)	O
where	O
sim(•	O
,	O
•	O
)	O
is	O
the	O
cosine	O
similarity	O
function,1	O
[	O
z	O
k	O
=	O
z	O
]	O
is	O
the	O
indicator	O
function	O
and	O
τ	O
is	O
the	O
temperature	O
parameter	O
.	O

The	O
final	O
loss	O
L	O
is	O
computed	O
across	O
all	O
positive	O
pairs	O
,	O
both	O
(	O
z	O
,	O
z	O
)	O
and	O
(	O
z	O
,	O
z	O
)	O
,	O
in	O
a	O
batch	O
.	O

For	O
computation	O
convenience	O
,	O
we	O
arrange	O
the	O
(	O
2k	O
−	O
1)-th	O
example	O
and	O
the	O
2k	O
-	O
th	O
example	O
in	O
the	O
batch	O
are	O
generated	O
from	O
the	O
same	O
sentence	O
,	O
a.k.a	O
.	O
,	O
(	O

2k	O
−	O
1	O
,	O
2k	O
)	O
is	O
a	O
positive	O
pair	O
.	O

Please	O
see	O
Algorithm	O
1	O
for	O
calculating	O
the	O
contrastive	O
loss	O
in	O
one	O
batch	O
.	O

Then	O
we	O
can	O
update	O
the	O
parameters	O
of	O
the	O
BERT	B-MethodName
model	O
and	O
projection	O
head	O
g	O
to	O
minimize	O
the	O
loss	O
L.	O
Input	O
:	O
encoder	O
f	O
(	O
BERT	B-MethodName
)	O
,	O
project	O
head	O
g	O
,	O
data	O
augmentation	O
module	O
,	O
data	O
batch	O
{	O
s	O
k	O
}	O
N	O
k=1	O
;	O
for	O
k=1	O
,	O
...	O
,	O
N	O
do	O
v	O
,	O
v	O
=	O
data_augment(s	O
k	O
)	O
;	O
z	O
2k−1	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
z	O
2k	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
end	O
L	O
=	O
1	O
2N	O
N	O
k=1	O
[	O
l(z	O
2k−1	O
,	O
z	O
2k	O
)	O
+	O
l(z	O
2k	O
,	O
z	O
2k−1	O
)	O
]	O
Figure	O
2	O
shows	O
the	O
training	O
procedure	O
of	O
our	O
framework	O
.	O

It	O
consists	O
of	O
three	O
stages	O
.	O

First	O
,	O
we	O
pretrain	O
the	O
BERT	B-MethodName
model	O
on	O
a	O
large	O
amount	O
of	O
unlabeled	O
data	O
from	O
a	O
specific	O
domain(e.g	O
.	O
,	O

biomedical	O
domain	O
)	O
.	O

Second	O
,	O
we	O
conduct	O
contrastive	O
pretraining	O
on	O
task	O
-	O
specific	O
data	O
as	O
a	O
continual	O
pretraining	O
step	O
after	O
the	O
domain	O
pre	O
-	O
training	O
of	O
BERT	B-MethodName
model	O
.	O

In	O
this	O
way	O
,	O
we	O
retain	O
the	O
learned	O
knowledge	O
from	O
general	O
pre	O
-	O
training	O
,	O
and	O
add	O
the	O
new	O
features	O
from	O
contrastive	O
learning	O
.	O

Finally	O
,	O
we	O
finetune	O
the	O
model	O
on	O
the	O
RE	B-TaskName
tasks	O
to	O
further	O
gain	O
taskspecific	O
knowledge	O
through	O
supervised	O
training	O
on	O
the	O
labeled	O
datasets	O
.	O

The	O
domain	O
pre	O
-	O
training	O
stage	O
follows	O
that	O
of	O
the	O
BERT	B-MethodName
using	O
the	O
masked	O
language	O
model	O
and	O
next	O
sentence	O
prediction	O
technique	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019).In	O
our	O
experiments	O
,	O
we	O
use	O
two	O
pre	O
-	O
trained	O
versions	O
for	O
the	O
biomedical	O
domain	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

Contrastive	O
pre	O
-	O
training	O
requires	O
a	O
large	O
-	O
scale	O
dataset	O
to	O
generalize	O
the	O
representation	O
.	O

Also	O
,	O
our	O
data	O
augmentation	O
for	O
contrastive	O
learning	O
needs	O
SDP	O
between	O
two	O
given	O
entities	O
,	O
so	O
we	O
need	O
to	O
construct	O
the	O
augmented	O
dataset	O
with	O
the	O
entities	O
mentioned	O
in	O
the	O
text	O
.	O

For	O
these	O
purposes	O
,	O
we	O
utilize	O
external	O
databases	O
for	O
the	O
relations	O
to	O
acquire	O
extra	O
instances	O
for	O
contrastive	O
learning	O
.	O

Formally	O
,	O
assuming	O
a	O
curated	O
database	O
for	O
relation	O
r	O
contains	O
all	O
the	O
relevant	O
entities	O
and	O
text	O
,	O
we	O
consider	O
every	O
combination	O
of	O
the	O
entity	O
pairs	O
in	O
one	O
sentence	O
and	O
use	O
them	O
as	O
examples	O
for	O
this	O
relation	O
.	O

For	O
instance	O
,	O
there	O
are	O
three	O
proteins	O
in	O
the	O
sentence	O
s	O
:	O
"	O
Thus	O
NIPP1	O
works	O
as	O
a	O
molecular	O
sensor	O
for	O
PP1	O
to	O
recognize	O
phosphorylated	O
Sap155	O
.	O
"	O

We	O
will	O
generate	O
three	O
examples	O
for	O
PPI	O
task	O
from	O
this	O
sentence	O
:	O
<	O
s	O
,	O
NIPP1,PP1,PPI	O
>	O
,	O
<	O
s	O
,	O
NIPP1,Sap155,PPI	O
>	O
and	O
<	O
s	O
,	O
PP1,Sap155,PPI>.We	O
use	O
the	O
IntAct	O
database	O
(	O
Orchard	O
et	O
al	O
.	O
,	O

2014	O
)	O
as	O
the	O
interacting	O
protein	O
pairs	O
database	O
for	O
the	O
PPI	O
task	O
.	O

Similarly	O
,	O
DrugBank	O
(	O
Wishart	O
et	O
al	O
.	O
,	O

2008	O
)	O
and	O
BioGRID	O
(	O
Stark	O
et	O
al	O
.	O
,	O

2006	O
)	O
are	O
utilized	O
for	O
DDI	B-TaskName
and	O
ChemProt	B-TaskName
,	O
respectively	O
.	O

In	O
the	O
column	O
"	O
EK	O
"	O
of	O
Table	O
2	O
,	O
we	O
show	O
the	O
statistics	O
of	O
datasets	O
for	O
each	O
task	O
generated	O
by	O
external	O
knowledge	O
bases	O
.	O

We	O
can	O
see	O
that	O
the	O
datasets	O
from	O
the	O
external	O
database	O
are	O
much	O
larger	O
than	O
that	O
of	O
the	O
human	O
-	O
labeled	O
datasets	O
.	O

As	O
discussed	O
before	O
,	O
we	O
will	O
utilize	O
the	O
BERT	B-MethodName
model	O
as	O
the	O
encoder	O
for	O
the	O
inputs	O
.	O

In	O
particular	O
,	O
we	O
will	O
employ	O
two	O
BERT	B-MethodName
models	O
pre	O
-	O
trained	O
for	O
the	O
biomedical	O
domain	O
in	O
our	O
experiments	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

We	O
will	O
evaluate	O
our	O
method	O
on	O
three	O
benchmark	O
datasets	O
.	O

The	O
statistics	O
of	O
these	O
datasets	O
is	O
shown	O
in	O
Table	O
2	O
.	O

For	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
tasks	O
,	O
we	O
employ	O
the	O
corpora	O
in	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
(	O
Herrero	O
-	O
Zazo	O
et	O
al	O
.	O
,	O

2013	O
)	O
PubMedBERT	B-MethodName
model	O
(	O
Gu	O
et	O
al	O
.	O
,	O

2021	O
)	O
during	O
the	O
model	O
evaluation	O
.	O

We	O
utilize	O
the	O
AIMed	B-DatasetName
corpus	O
for	O
the	O
PPI	B-TaskName
task	O
,	O
and	O
we	O
will	O
employ	O
10	B-HyperparameterValue
-	O
fold	O
cross	O
-	O
validation	O
on	O
it	O
since	O
there	O
is	O
no	O
standard	O
split	O
of	O
training	O
and	O
test	O
.	O

PPI	B-TaskName
is	O
a	O
binary	O
classification	O
problem	O
,	O
and	O
we	O
will	O
use	O
the	O
standard	O
precision	B-MetricName
(	O
P	B-MetricName
)	O
,	O
recall	B-MetricName
(	O
R	B-MetricName
)	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
F	B-MetricName
)	O
to	O
measure	O
the	O
model	O
performance	O
.	O

However	O
,	O
the	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
tasks	O
are	O
multiclass	O
classification	O
problems	O
.	O

The	O
ChemProt	B-DatasetName
corpus	O
is	O
labeled	O
with	O
five	O
positive	O
classes	O
and	O
the	O
negative	O
class	O
:	O
CPR:3	O
,	O
CPR:4	O
,	O
CPR:5	O
,	O
CPR:6	O
,	O
CPR:9	O
and	O
negative	O
.	O

Similar	O
to	O
the	O
DDI	B-DatasetName
corpus	O
,	O
there	O
are	O
four	O
positive	O
labels	O
and	O
one	O
negative	O
label	O
:	O
AD	O
-	O
VICE	O
,	O
EFFECT	O
,	O
INT	O
,	O
MECHANISM	O
and	O
negative	O
.	O

The	O
models	O
for	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
will	O
be	O
evaluated	O
utilizing	O
micro	B-MetricName
precision	I-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
non	O
-	O
negative	O
classes	O
.	O

One	O
instance	O
of	O
relation	B-TaskName
extraction	I-TaskName
task	O
contains	O
two	O
parts	O
:	O
the	O
text	O
and	O
the	O
entity	O
mentions	O
.	O

In	O
order	O
to	O
make	O
the	O
BERT	B-MethodName
model	O
identify	O
the	O
positions	O
of	O
the	O
entities	O
,	O
we	O
replace	O
the	O
relevant	O
entity	O
names	O
with	O
predefined	O
tags	O
by	O
following	O
the	O
standard	O
pre	O
-	O
processing	O
step	O
for	O
relation	O
extraction	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Specifically	O
,	O
all	O
the	O
protein	O
names	O
are	O
replaced	O
with	O
@PROTEIN$	O
,	O
drug	O
names	O
with	O
@DRUG$	O
,	O
and	O
chemical	O
names	O
with	O
@CHEMI	O
-	O
CAL$.	O
In	O
Table	O
1	O
,	O
we	O
show	O
a	O
pre	O
-	O
processed	O
example	O
of	O
the	O
PPI	O
task	O
.	O

For	O
the	O
fine	O
-	O
tuning	O
of	O
the	O
BioBERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	B-HyperparameterValue
,	O
training	B-HyperparameterName
epoch	I-HyperparameterName
of	O
10	B-HyperparameterValue
,	O
and	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
128.During	B-HyperparameterValue
the	O
fine	O
-	O
tuning	O
of	O
PubMedBERT	B-MethodName
models	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	B-HyperparameterValue
,	O
training	B-HyperparameterName
epoch	I-HyperparameterName
of	O
10	B-HyperparameterValue
and	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
256	B-HyperparameterValue
are	O
utilized	O
.	O

In	O
the	O
contrastive	O
pre	O
-	O
training	O
step	O
of	O
the	O
BERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
with	O
the	O
fine	O
-	O
tuning	O
,	O
and	O
the	O
training	B-HyperparameterName
epoch	I-HyperparameterName
is	O
selected	O
from	O
[	O
2,4,6,8,10	B-HyperparameterValue
]	O
based	O
on	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O

If	O
there	O
is	O
no	O
development	O
set	O
(	O
e.g.	O
,	O
PPI	B-TaskName
task	O
)	O
,	O
we	O
will	O
use	O
6	B-HyperparameterValue
as	O
the	O
default	O
training	O
epoch	B-HyperparameterName
.	O

Since	O
contrastive	O
learning	O
benefits	O
more	O
from	O
larger	O
batch	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
utilize	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
and	O
128	B-HyperparameterValue
for	O
BioBERT	B-MethodName
and	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
respectively	O
.	O

In	O
addition	O
,	O
the	O
temperature	B-HyperparameterName
parameter	I-HyperparameterName
τ	I-HyperparameterName
is	O
set	O
to	O
0.1	B-HyperparameterValue
during	O
the	O
training	O
.	O

5.1	O
BERT	B-MethodName
model	O
performance	O
with	O
contrastive	O
pre	O
-	O
training	O
ever	O
,	O
contrastive	O
pre	O
-	O
training	O
on	O
human	O
-	O
labeled	O
dataset	O
only	O
improves	O
the	O
model	O
with	O
a	O
small	O
margin	O
.	O

We	O
hypothesize	O
that	O
the	O
limited	O
improvement	O
might	O
be	O
due	O
to	O
the	O
poor	O
generalization	O
on	O
small	O
training	O
set	O
.	O

Therefore	O
,	O
we	O
include	O
more	O
data	O
(	O
EK	O
data	O
)	O
in	O
contrastive	O
learning	O
to	O
enhance	O
the	O
model	O
generalizability	O
.	O

The	O
data	O
generated	O
from	O
the	O
external	O
knowledge	O
base	O
are	O
much	O
more	O
than	O
the	O
training	O
data	O
of	O
the	O
human	O
-	O
labeled	O
dataset	O
(	O
column	O
"	O
EK	O
"	O
and	O
"	O
train	O
"	O
in	O
Table	O
2	O
)	O
.	O

As	O
shown	O
in	O
the	O
third	O
and	O
sixth	O
row	O
in	O
Table	O
3	O
,	O
contrastive	O
learning	O
with	O
more	O
external	O
data	O
can	O
further	O
boost	O
the	O
model	O
performance	O
.	O

Compared	O
with	O
the	O
BERT	B-MethodName
models	O
without	O
contrastive	O
pre	O
-	O
training	O
,	O
we	O
observe	O
an	O
averaged	O
F1	B-MetricName
score	I-MetricName
improvement	O
(	O
on	O
the	O
two	O
BERT	B-MethodName
models	O
)	O
of	O
1.2	B-MetricValue
%	I-MetricValue
,	O
1.2	B-MetricValue
%	I-MetricValue
,	O
and	O
0.85	B-MetricValue
%	I-MetricValue
on	O
ChemProt	B-DatasetName
,	O
DDI	B-DatasetName
,	O
and	O
PPI	B-DatasetName
datasets	O
,	O
respectively	O
.	O

Since	O
PubMedBERT	B-MethodName
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
model	O
on	O
these	O
three	O
tasks	O
,	O
we	O
further	O
improve	O
its	O
performance	O
by	O
adding	O
contrastive	O
pretraining	O
.	O

Thus	O
,	O
we	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
all	O
three	O
datasets	O
.	O

Table	O
4	O
shows	O
the	O
BERT	B-MethodName
model	O
performance	O
after	O
including	O
three	O
types	O
of	O
augmented	O
data	O
.	O

We	O
can	O
see	O
that	O
the	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
operation	O
yields	O
the	O
best	O
results	O
on	O
all	O
three	O
tasks	O
.	O

Therefore	O
we	O
use	O
it	O
as	O
our	O
default	O
operation	O
to	O
generate	O
augmented	O
data	O
in	O
all	O
our	O
contrastive	O
learning	O
experiments	O
.	O

We	O
also	O
notice	O
that	O
the	O
augmented	O
data	O
from	O
the	O
random	B-MethodName
swap	I-MethodName
(	O
RS	B-MethodName
)	O
operation	O
hurt	O
the	O
model	O
performance	O
on	O
the	O
DDI	B-TaskName
and	O
PPI	B-TaskName
tasks	O
,	O
which	O
indicates	O
that	O
this	O
operation	O
might	O
change	O
the	O
relation	O
expression	O
in	O
the	O
sentence	O
.	O

Thus	O
it	O
is	O
necessary	O
to	O
verify	O
the	O
effectiveness	O
of	O
the	O
operations	O
before	O
applying	O
them	O
on	O
contrastive	O
learning	O
.	O
(	O

1	O
)	O
Instead	O
,	O
radiolabeled	O
@CHEMICAL$	O
resulting	O
from	O
@PROTEIN$	O
hydrolysis	O
were	O
observed	O
.	O

CPR:9(2	O
)	O
Or	O
else	O
,	O
radiolabeled	O
@CHEMICAL$	O
resulting	O
from	O
@PROTEIN$	O
hydrolysis	O
were	O
observed	O
.	O
(	O

1	O
)	O
These	O
results	O
indicate	O
that	O
membrane	O
@PRO	O
-	O
TEIN$	O
levels	O
in	O
N-38	O
neurons	O
are	O
dynamically	O
autoregulated	O
by	O
@CHEMICAL$.CPR:3(2	O
)	O
These	O
results	O
indicate	O
that	O
membrane	O
@PRO	O
-	O
TEIN$	O
levels	O
in	O
N-38	O
nerve	O
cell	O
are	O
dynamically	O
autoregulated	O
by	O
@CHEMICAL$.	O
As	O
discussed	O
previously	O
,	O
we	O
hypothesize	O
the	O
words	O
on	O
the	O
shortest	O
dependency	O
path	O
(	O
SDP	O
)	O
as	O
the	O
rationales	O
in	O
the	O
input	O
.	O

Therefore	O
,	O
the	O
model	O
should	O
make	O
its	O
predictions	O
based	O
on	O
them	O
.	O

If	O
the	O
model	O
predictions	O
are	O
all	O
made	O
based	O
on	O
a	O
specific	O
part	O
of	O
the	O
input	O
,	O
we	O
can	O
define	O
this	O
specific	O
part	O
of	O
the	O
input	O
to	O
be	O
the	O
completely	O
faithful	O
rationales	O
.	O

In	O
practice	O
,	O
the	O
rationales	O
are	O
more	O
faithful	O
means	O
they	O
are	O
more	O
influential	O
on	O
the	O
model	O
predictions	O
.	O

In	O
this	O
work	O
,	O
we	O
define	O
a	O
new	O
metric	O
to	O
measure	O
the	O
faithfulness	O
of	O
the	O
rationales	O
:	O
"	O
prediction	B-MetricName
shift	I-MetricName
"	O
.	O

If	O
the	O
model	O
predicts	O
one	O
test	O
example	O
(	O
nonnegative	O
)	O
with	O
label	O
L	O
t	O
,	O
but	O
changes	O
its	O
prediction	O
on	O
its	O
neighbor	O
(	O
the	O
augmented	O
data	O
point	O
)	O
with	O
another	O
label	O
L	O
t	O
,	O
we	O
will	O
say	O
a	O
"	O
prediction	O
shift	O
"	O
happens	O
(	O
In	O
Table	O
5	O
,	O
we	O
give	O
two	O
examples	O
of	O
pre	O
-	O
diction	O
shift	O
on	O
PubMedBERT	B-MethodName
model	O
)	O
.	O

Fewer	O
"	O
prediction	O
shift	O
"	O
indicates	O
the	O
information	O
outside	O
of	O
SDP	O
influences	O
the	O
prediction	O
less	O
,	O
which	O
means	O
the	O
rationales	O
are	O
more	O
faithful	O
.	O

To	O
generate	O
a	O
similar	O
set	O
(	O
with	O
test	O
set	O
)	O
for	O
the	O
measurement	O
of	O
"	O
prediction	O
shift	O
"	O
,	O
we	O
apply	O
the	O
same	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
technique	O
on	O
the	O
original	O
test	O
data	O
.	O

Since	O
we	O
retain	O
the	O
words	O
that	O
are	O
on	O
the	O
shortest	O
dependency	O
path	O
between	O
the	O
two	O
entities	O
,	O
the	O
generated	O
data	O
should	O
express	O
the	O
same	O
relation	O
with	O
the	O
original	O
ones	O
.	O

The	O
trained	O
model	O
should	O
predict	O
them	O
with	O
the	O
same	O
labels	O
if	O
the	O
rationales	O
of	O
input	O
are	O
utilized	O
during	O
inference	O
,	O
and	O
in	O
that	O
case	O
,	O
we	O
say	O
the	O
rationales	O
are	O
faithful	O
.	O

We	O
compare	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
on	O
two	O
types	O
of	O
BERT	B-MethodName
model	O
:	O
the	O
original	O
BERT	B-MethodName
and	O
the	O
BERT	B-MethodName
model	O
with	O
contrastive	O
pre	O
-	O
training	O
.	O

Table	O
6	O
illustrates	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
dramatically	O
reduce	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
.	O

Those	O
results	O
indicate	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
rely	O
more	O
on	O
the	O
information	O
of	O
shortest	O
dependency	O
path	O
for	O
prediction	O
,	O
a.k.a	O
.	O
,	O

the	O
rationales	O
are	O
more	O
faithful	O
.	O

From	O
another	O
perspective	O
,	O
the	O
results	O
in	O
Table	O
6	O
also	O
demonstrate	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
are	O
resilient	O
to	O
small	O
changes	O
of	O
the	O
inputs	O
,	O
which	O
means	O
the	O
models	O
are	O
more	O
robust	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
contrastive	O
pre	O
-	O
training	O
method	O
to	O
improve	O
the	O
text	O
representation	O
of	O
the	O
BERT	B-MethodName
model	O
.	O

Our	O
approach	O
differs	O
from	O
previous	O
studies	O
in	O
the	O
choice	O
of	O
text	O
data	O
augmentation	O
with	O
linguistic	O
knowledge	O
and	O
the	O
use	O
of	O
the	O
external	O
knowledge	O
bases	O
to	O
construct	O
large	O
-	O
scale	O
data	O
to	O
facilitate	O
contrastive	O
learning	O
.	O

The	O
experimental	O
results	O
demonstrate	O
that	O
our	O
method	O
outperforms	O
the	O
original	O
BERT	B-MethodName
model	O
on	O
three	O
relation	O
extraction	O
benchmarks	O
.	O

Additionally	O
,	O
our	O
method	O
shows	O
robustness	O
to	O
slightly	O
changed	O
inputs	O
over	O
the	O
BERT	B-MethodName
models	O
.	O

In	O
the	O
future	O
,	O
we	O
will	O
investigate	O
different	O
settings	O
of	O
data	O
augmentation	O
and	O
contrastive	O
pre	O
-	O
training	O
to	O
exploit	O
their	O
capability	O
on	O
language	O
models	O
.	O

We	O
also	O
hope	O
that	O
our	O
work	O
can	O
inspire	O
researchers	O
to	O
design	O
better	O
metrics	O
and	O
create	O
highquality	O
datasets	O
for	O
the	O
exploration	O
of	O
model	O
interpretability	O
.	O

Existing	O
goal	O
-	O
oriented	O
dialogue	O
datasets	O
focus	O
mainly	O
on	O
identifying	O
slots	O
and	O
values	O
.	O

However	O
,	O
customer	O
support	O
interactions	O
in	O
reality	O
often	O
involve	O
agents	O
following	O
multi	O
-	O
step	O
procedures	O
derived	O
from	O
explicitly	O
-	O
defined	O
company	O
policies	O
as	O
well	O
.	O

To	O
study	B-DatasetName
customer	I-DatasetName
service	I-DatasetName
dialogue	I-DatasetName
systems	I-DatasetName
in	I-DatasetName
more	I-DatasetName
realistic	I-DatasetName
settings	I-DatasetName
,	O
we	O
introduce	O
the	B-DatasetName
Action	I-DatasetName
-	I-DatasetName
Based	I-DatasetName
Conversations	I-DatasetName
Dataset	O
(	O
ABCD	B-DatasetName
)	O
,	O
a	O
fully	O
-	O
labeled	O
dataset	O
with	O
over	O
10	O
K	O
human	O
-	O
to	O
-	O
human	O
dialogues	O
containing	O
55	O
distinct	O
user	O
intents	O
requiring	O
unique	O
sequences	O
of	O
actions	O
constrained	O
by	O
policies	O
to	O
achieve	O
task	O
success	O
.	O

We	O
propose	O
two	O
additional	O
dialog	O
tasks	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
and	O
establish	O
a	O
series	O
of	O
baselines	O
involving	O
large	O
-	O
scale	O
,	O
pre	O
-	O
trained	O
language	O
models	O
on	O
this	O
dataset	O
.	O

Empirical	O
results	O
demonstrate	O
that	O
while	O
more	O
sophisticated	O
networks	O
outperform	O
simpler	O
models	O
,	O
a	O
considerable	O
gap	O
(	O
50.8	B-MetricValue
%	I-MetricValue
absolute	B-MetricName
accuracy	I-MetricName
)	O
still	O
exists	O
to	O
reach	O
human	O
-	O
level	O
performance	O
on	O
ABCD	O
.	O

1	O
The	O
broad	O
adoption	O
of	O
virtual	O
assistants	O
and	O
customer	O
service	O
chatbots	O
in	O
recent	O
years	O
has	O
been	O
driven	O
in	O
no	O
small	O
part	O
by	O
the	O
usefulness	O
of	O
these	O
tools	O
,	O
whereby	O
actions	O
are	O
taken	O
on	O
behalf	O
of	O
the	O
user	O
to	O
accomplish	O
their	O
desired	O
targets	O
(	O
Amazon	O
,	O
2019	O
;	O
Google	O
,	O
2019	O
)	O
.	O

Research	O
into	O
taskoriented	O
dialogue	O
has	O
concurrently	O
made	O
tremendous	O
progress	O
on	O
natural	O
language	O
understanding	O
of	O
user	O
needs	O
(	O
Wu	O
et	O
al	O
.	O
,	O

2019;Rastogi	O
et	O
al	O
.	O
,	O

2020b;Liang	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

However	O
,	O
selecting	O
actions	O
in	O
real	O
life	O
requires	O
not	O
only	O
obeying	O
user	O
requests	O
,	O
but	O
also	O
following	O
practical	O
policy	O
limitations	O
which	O
may	O
be	O
at	O
odds	O
with	O
those	O
requests	O
.	O

For	O
example	O
,	O
while	O
a	O
user	O
may	O
ask	O
for	O
a	O
refund	O
on	O
their	O
purchase	O
,	O
an	O
agent	O
should	O
only	O
honor	O
such	O
a	O
request	O
if	O
it	O
is	O
valid	O
with	O
regards	O
to	O
the	O
store	O
's	O
return	O
policy	O
.	O

Described	O
in	O
actions	O
,	O
before	O
an	O
agent	O
1	O
All	O
code	O
and	O
data	O
will	O
be	O
available	O
at	O
this	O
location	O
.	O

Figure	O
1	O
:	O
An	O
interaction	O
from	O
ABCD	B-DatasetName
(	O
left	O
)	O
starts	O
with	O
the	O
customer	O
receiving	O
a	O
prompt	O
(	O
top	O
right	O
)	O
to	O
ground	O
the	O
dialogue	O
.	O

The	O
agent	O
follows	O
the	O
guidelines	O
(	O
bottom	O
right	O
)	O
to	O
identify	O
the	O
customer	O
intent	O
and	O
to	O
assist	O
them	O
in	O
resolving	O
the	O
issue	O
through	O
a	O
series	O
of	O
actions	O
.	O

can	O
[	O
Offer	O
Refund	O
]	O
,	O
they	O
must	O
first	O
[	O
Validate	O
Purchase	O
]	O
.	O

Furthermore	O
,	O
resolving	O
customer	O
issues	O
often	O
concerns	O
multiple	O
actions	O
completed	O
in	O
succession	O
with	O
a	O
specific	O
order	O
since	O
prior	O
steps	O
may	O
influence	O
future	O
decision	O
states	O
.	O
(	O

See	O
Figure	O
1)To	O
more	O
closely	O
model	O
real	O
customer	O
service	O
agents	O
,	O
we	O
present	O
the	O
Action	B-DatasetName
-	I-DatasetName
Based	I-DatasetName
Conversations	I-DatasetName
Dataset	O
(	O
ABCD	B-DatasetName
)	O
consisting	O
of	O
10,042	O
conversations	O
containing	O
numerous	O
actions	O
with	O
precise	O
procedural	O
requirements	O
.	O

These	O
actions	O
differ	O
from	O
typical	O
dialogue	O
acts	O
because	O
tracking	O
them	O
necessitates	O
striking	O
a	O
balance	O
between	O
external	O
user	O
requests	O
and	O
internally	O
-	O
imposed	O
guidelines	O
.	O

Thus	O
,	O
the	O
major	O
difference	O
between	O
ABCD	O
and	O
other	O
dialogue	O
datasets	O
,	O
such	O
as	O
Mul	B-DatasetName
-	I-DatasetName
tiWOZ	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
is	O
that	O
it	O
asks	O
the	O
agent	O
to	O
adhere	O
to	O
a	O
set	O
of	O
policies	O
while	O
simultaneously	O
dealing	O
with	O
customer	O
requests	O
.	O

While	O
the	O
prevalent	O
data	O
collection	O
paradigm	O
involves	O
Wizard	O
-	O
of	O
-	O
Oz	O
techniques	O
,	O
our	O
situation	O
containing	O
asymmetric	O
speakers	O
compelled	O
the	O
design	B-TaskName
of	I-TaskName
a	I-TaskName
novel	I-TaskName
Expert	I-TaskName
Live	I-TaskName
Chat	I-TaskName
system	I-TaskName
.	O

Our	O
dataset	O
includes	O
asymmetric	O
speakers	O
because	O
,	O
unlike	O
customers	O
,	O
agents	O
must	O
undergo	O
extensive	O
training	O
to	O
be	O
able	O
to	O
navigate	O
the	O
Agent	O
Guidelines	O
during	O
real	O
-	O
time	O
conversations	O
.	O

This	O
makes	O
a	O
naive	O
pairing	O
process	O
untenable	O
since	O
arbitrary	O
matching	O
might	O
lead	O
to	O
chats	O
containing	O
two	O
users	O
who	O
share	O
the	O
same	O
role	O
.	O

Based	O
on	O
the	O
unique	O
aspects	O
of	O
ABCD	B-DatasetName
,	O
we	O
propose	O
two	O
new	O
tasks	O
.	O

To	O
start	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
(	O
AST	B-MetricName
)	O
closely	O
mirrors	O
the	O
format	O
of	O
Dialogue	O
State	O
Tracking	O
where	O
the	O
user	O
intent	O
is	O
inferred	O
from	O
the	O
dialogue	O
history	O
.	O

AST	B-MetricName
then	O
differs	O
since	O
the	O
correct	O
state	O
must	O
also	O
be	O
reconciled	O
with	O
the	O
requirements	O
outlined	O
in	O
the	O
Agent	O
Guidelines	O
.	O

As	O
a	O
second	O
task	O
,	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
(	O
CDS	B-MetricName
)	O
extends	O
this	O
notion	O
across	O
the	O
entire	O
conversation	O
.	O

At	O
each	O
turn	O
,	O
the	O
agent	O
decides	O
to	O
take	O
an	O
action	O
,	O
respond	O
with	O
an	O
utterance	O
or	O
end	O
the	O
chat	O
.	O

As	O
needed	O
,	O
the	O
agent	O
should	O
also	O
predict	O
the	O
right	O
action	O
or	O
select	O
the	O
best	O
utterance	O
.	O

For	O
each	O
task	O
,	O
we	O
build	O
various	O
models	O
to	O
establish	O
baseline	O
performance	O
and	O
to	O
highlight	O
the	O
importance	O
of	O
each	O
constraint	O
.	O

Experiments	O
show	O
that	O
in	O
addition	O
to	O
conversation	O
history	O
,	O
conditioning	O
on	O
the	O
Agent	O
Guidelines	O
further	O
boosts	O
performance	O
,	O
with	O
top	O
models	O
relying	O
on	O
both	O
aspects	O
to	O
reach	O
31.9	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
.	O

Additional	O
results	O
show	O
removing	O
action	O
context	O
hurts	O
performance	O
,	O
implying	O
the	O
importance	O
of	O
taking	O
into	O
account	O
the	O
sequential	O
nature	O
of	O
actions	O
.	O

Lastly	O
,	O
human	O
evaluation	O
reaches	O
82.7	B-MetricValue
%	I-MetricValue
,	O
demonstrating	O
ample	O
room	O
for	O
future	O
improvement	O
.	O

The	O
contribution	O
of	O
this	O
work	O
is	O
three	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
provide	O
a	O
novel	O
,	O
large	O
-	O
scale	O
dataset	O
containing	O
context	O
-	O
dependent	O
,	O
procedural	O
actions	O
along	O
with	O
corresponding	O
Agent	O
Guidelines	O
.	O
(	O

2	O
)	O
We	O
establish	O
a	O
new	O
technique	O
called	O
Expert	B-MethodName
Live	I-MethodName
Chat	I-MethodName
for	O
capturing	O
natural	O
dialogue	O
between	O
two	O
unequal	O
interlocutors	O
.	O
(	O

3	O
)	O
We	O
propose	O
two	O
metrics	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
for	O
measuring	O
dialogue	O
comprehension	O
with	O
policy	O
constraints	O
.	O

Finally	O
,	O
we	O
build	O
on	O
pretrained	O
neural	O
models	O
to	O
serve	O
as	O
baselines	O
for	O
these	O
tasks	O
.	O

Traditional	O
Dialogue	O
Datasets	O
In	O
recent	O
years	O
,	O
dialogue	O
datasets	O
have	O
grown	O
in	O
size	O
from	O
hundreds	O
of	O
conversations	O
to	O
the	O
tens	O
of	O
thousands	O
(	O
Henderson	O
et	O
al	O
.	O
,	O

2014;Budzianowski	O
et	O
al	O
.	O
,	O

2018;Peskov	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Unlike	O
opendomain	O
chatbots	O
often	O
built	O
for	O
entertainment	O
,	O
task	O
-	O
oriented	O
dialogue	O
systems	O
trained	O
on	O
such	O
datasets	O
are	O
intended	O
for	O
solving	O
user	O
issues	O
.	O

The	O
resolution	O
of	O
these	O
issues	O
implicitly	O
requires	O
taking	O
actions	O
,	O
where	O
an	O
action	O
is	O
a	O
non	O
-	O
utterance	O
decision	O
that	O
depends	O
on	O
both	O
user	O
and	O
system	O
inputs	O
.	O

Despite	O
the	O
tremendous	O
number	O
of	O
dialogues	O
,	O
examples	O
in	O
previous	O
benchmarks	O
fixate	O
on	O
the	O
single	O
knowledge	O
base	O
(	O
KB	O
)	O
lookup	O
action	O
where	O
the	O
agent	O
searches	O
for	O
an	O
item	O
that	O
matches	O
the	O
user	O
's	O
desires	O
and	O
is	O
available	O
in	O
the	O
KB	O
.	O

By	O
sticking	O
to	O
this	O
sole	O
interaction	O
,	O
conversations	O
can	O
be	O
generated	O
through	O
rules	O
(	O
Weston	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
paraphrased	O
from	O
templates	O
(	O
Byrne	O
et	O
al	O
.	O
,	O

2019	O
)	O
or	O
taken	O
from	O
static	O
text	O
scenarios	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
leading	O
to	O
dialogues	O
that	O
are	O
predominantly	O
homogeneous	O
in	O
nature	O
.	O

Many	O
datasets	O
have	O
scaled	O
to	O
more	O
domains	O
as	O
well	O
Budzianowski	O
et	O
al	O
.	O
,	O

2018;Peskov	O
et	O
al	O
.	O
,	O

2019	O
)	O
Since	O
each	O
new	O
domain	O
introduces	O
a	O
KB	O
lookup	O
requiring	O
different	O
slotvalues	O
,	O
the	O
number	O
of	O
unique	O
actions	O
grows	O
as	O
a	O
linear	O
function	O
of	O
the	O
number	O
of	O
domains	O
covered	O
.	O

Rather	O
than	O
expanding	O
wider	O
,	O
ABCD	B-DatasetName
instead	O
focuses	O
deeper	O
by	O
increasing	O
the	O
count	O
and	O
diversity	O
of	O
actions	O
within	O
a	O
single	O
domain	O
.	O

Exploring	O
Other	O
Avenues	O
Multiple	O
aspects	O
are	O
explored	O
by	O
conversational	O
datasets	O
attempting	O
to	O
mimic	O
reality	O
.	O

Rashkin	O
et	O
al	O
.	O
(	O

2019	O
)	O
studies	O
the	O
ability	O
of	O
a	O
dialogue	O
model	O
to	O
handle	O
empathy	O
,	O
while	O
Zhou	O
et	O
al	O
.	O
(	O

2018	O
)	O
focuses	O
on	O
commonsense	O
reasoning	O
.	O

Another	O
approach	O
is	O
to	O
augment	O
dialogues	O
with	O
multi	O
-	O
modality	O
including	O
audio	O
(	O
Castro	O
et	O
al	O
.	O
,	O

2019	O
)	O
or	O
visual	O
(	O
Das	O
et	O
al	O
.	O
,	O

2017a	O
)	O
components	O
.	O

Other	O
researchers	O
have	O
explored	O
grounding	O
conversations	O
with	O
external	O
data	O
sources	O
such	O
as	O
personas	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
online	O
reviews	O
(	O
Ghazvininejad	O
et	O
al	O
.	O
,	O

2018	O
)	O
or	O
large	O
knowledge	O
bases	O
(	O
Dinan	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Intricate	O
dialogues	O
can	O
also	O
appear	O
when	O
studying	O
collaboration	O
(	O
He	O
et	O
al	O
.	O
,	O

2017	O
;	O
or	O
negotiation	O
(	O
Lewis	O
et	O
al	O
.	O
,	O

2017;He	O
et	O
al	O
.	O
,	O

2018	O
)	O
which	O
strongly	O
encourage	O
interaction	O
with	O
the	O
other	O
participant	O
.	O

In	O
comparison	O
,	O
ABCD	O
aims	O
to	O
make	O
dialogue	O
more	O
realistic	O
by	O
considering	O
distinct	O
constraints	O
from	O
policies	O
.	O

Dialogues	O
with	O
Policies	O
Procedural	O
actions	O
following	O
strict	O
guidelines	O
naturally	O
emerge	O
in	O
dialogue	O
research	O
geared	O
towards	O
real	O
-	O
world	O
appli	O
-	O
Subflows	O
recover	O
-	O
username	O
,	O
1	O
recover	O
-	O
password	O
,	O
1	O
reset-2fa	O
,	O
1	O
status	O
-	O
service	O
-	O
added	O
,	O
2	O
status	O
-	O
service	O
-	O
removed	O
,	O
2	O
statusshipping	O
-	O
question	O
,	O
2	O
status	O
-	O
credit	O
-	O
missing	O
,	O
2	O
manage	O
-	O
change	O
-	O
address	O
,	O
2	O
manage	O
-	O
change	O
-	O
name	O
,	O
2	O
manage	O
-	O
changephone	O
,	O
2	O
manage	O
-	O
payment	O
-	O
method	O
,	O
2	O
status	O
-	O
mystery	O
-	O
fee	O
,	O
3	O
status	O
-	O
delivery	O
-	O
time	O
,	O
3	O
status	O
-	O
payment	O
-	O
method	O
,	O
3	O
statusquantity	O
,	O
3	O
manage	O
-	O
upgrade	O
,	O
3	O
manage	O
-	O
downgrade	O
,	O
3	O
manage	O
-	O
create	O
,	O
3	O
manage	O
-	O
cancel	O
,	O
3	O
refund	O
-	O
initiate	O
,	O
4	O
refundupdate	O
,	O
4	O
refund	O
-	O
status	O
,	O
4	O
return	O
-	O
stain	O
,	O
4	O
return	O
-	O
color	O
,	O
4	O
return	O
-	O
size	O
,	O
4	O
bad	O
-	O
price	O
-	O
competitor	O
,	O
5	O
bad	O
-	O
price	O
-	O
yesterday	O
,	O
5	O
out	O
-	O
of	O
-	O
stock	O
-	O
general	O
,	O
5	O
out	O
-	O
of	O
-	O
stock	O
-	O
one	O
-	O
item	O
,	O
5	O
promo	O
-	O
code	O
-	O
invalid	O
,	O
5	O
promo	O
-	O
code	O
-	O
out	O
-	O
of	O
-	O
date	O
,	O
5	O
mistimed	O
-	O
billingalready	O
-	O
returned	O
,	O
5	O
mistimed	O
-	O
billing	O
-	O
never	O
-	O
bought	O
,	O
5	O
status	O
,	O
6	O
manage	O
,	O
6	O
missing	O
,	O
6	O
cost	O
,	O
6	O
boots	O
,	O
7	O
shirt	O
,	O
7	O
jeans	O
,	O
7	O
jacket	O
,	O
7	O
pricing	O
,	O
8	O
membership	O
,	O
8	O
timing	O
,	O
8	O
policy	O
,	O
8	O
status	O
-	O
active	O
,	O
9	O
status	O
-	O
due	O
-	O
amount	O
,	O
9	O
status	O
-	O
due	O
-	O
date	O
,	O
9	O
manage	O
-	O
pay	O
-	O
bill	O
,	O
9	O
manage	O
-	O
extension	O
,	O
9	O
manage	O
-	O
dispute	O
-	O
bill	O
,	O
9	O
credit	O
-	O
card	O
,	O
10	O
shopping	O
-	O
cart	O
,	O
10	O
search	O
-	O
results	O
,	O
10	O
slow	O
-	O
speed	O
10	O
Actions	O
verify	O
-	O
identity	O
,	O
ask	O
-	O
the	O
-	O
oracle	O
,	O
validate	O
-	O
purchase	O
,	O
make	O
-	O
password	O
,	O
promo	O
-	O
code	O
,	O
subscription	O
-	O
status	O
,	O
offer	O
-	O
refund	O
,	O
make	O
-	O
purchase	O
,	O
record	O
-	O
reason	O
,	O
enter	O
-	O
details	O
,	O
shipping	O
-	O
status	O
,	O
update	O
-	O
order	O
,	O
pull	O
-	O
up	O
-	O
account	O
,	O
update	O
-	O
account	O
,	O
sendlink	O
,	O
notify	O
-	O
team	O
,	O
membership	O
,	O
search	O
-	O
faq	O
,	O
try	O
-	O
again	O
,	O
log	O
-	O
out	O
-	O
in	O
,	O
instructions	O
,	O
search	O
-	O
jeans	O
,	O
search	O
-	O
shirt	O
,	O
searchboots	O
,	O
search	O
-	O
jacket	O
,	O
search	O
-	O
pricing	O
,	O
search	O
-	O
membership	O
,	O
search	O
-	O
timing	O
,	O
search	O
-	O
policy	O
,	O
select	O
-	O
faq	O
Table	O
1	O
:	O
Full	O
ontology	O
of	O
Agent	O
Guidelines	O
decomposable	O
into	O
high	O
-	O
level	O
flows	O
describing	O
the	O
overall	O
category	O
and	O
subflows	O
defining	O
a	O
unique	O
set	O
of	O
intents	O
.	O

All	O
actions	O
are	O
also	O
shown	O
.	O

Upper	O
script	O
numeral	O
indicates	O
the	O
flow	O
that	O
the	O
subflow	O
belongs	O
to	O
.	O

1	O
:	O
account	O
access	O
,	O
2	O
:	O
manage	O
account	O
,	O
3	O
:	O
order	O
issue	O
,	O
4	O
:	O
product	O
defect	O
,	O
5	O
:	O
purchase	O
dispute	O
,	O
6	O
:	O
shipping	O
issue	O
,	O
7	O
:	O
single	O
item	O
query	O
,	O
8	O
:	O
storewide	O
query	O
,	O
9	O
:	O
subscription	O
inquiry	O
,	O
10	O
:	O
troubleshoot	O
site	O
cations	O
.	O

Hybrid	O
Code	O
Networks	O
encode	O
business	O
logic	O
through	O
masking	O
templates	O
since	O
various	O
behaviors	O
become	O
nonsensical	O
in	O
certain	O
situations	O
(	O
Williams	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

Research	O
from	O
Moiseeva	O
et	O
al	O
.	O
(	O

2020	O
)	O
studies	O
multi	O
-	O
purpose	O
virtual	O
assistants	O
that	O
attempt	O
to	O
distinguish	O
among	O
thirteen	O
explicit	O
actions	O
.	O

The	O
closest	O
prior	O
work	O
to	O
ABCD	B-DatasetName
is	O
the	O
Schema	B-DatasetName
Guided	I-DatasetName
Dialogue	I-DatasetName
(	O
SGD	B-DatasetName
)	O
dataset	O
,	O
which	O
contains	O
dozens	O
of	O
API	O
calls	O
that	O
can	O
be	O
interpreted	O
as	O
individual	O
actions	O
sending	O
commands	O
to	O
a	O
SQL	O
engine	O
(	O
Rastogi	O
et	O
al	O
.	O
,	O

2020b	O
)	O
.	O

The	O
functionality	O
of	O
these	O
actions	O
is	O
occasionally	O
restricted	O
to	O
reflect	O
constraints	O
of	O
real	O
-	O
life	O
services	O
.	O

The	O
action	O
restrictions	O
within	O
ABCD	B-DatasetName
are	O
made	O
explicit	O
by	O
the	O
Agent	O
Guidelines	O
manual	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
task	O
setting	O
of	O
ABCD	B-DatasetName
by	O
following	O
along	O
with	O
the	O
example	O
dialog	O
shown	O
in	O
Figure	O
1	O
.	O

During	O
data	O
collection	O
,	O
customers	O
are	O
given	O
a	O
simple	O
prompt	O
(	O
such	O
as	O
"	O
You	O
want	O
to	O
keep	O
your	O
subscription	O
another	O
year	O
.	O
"	O
)	O

instead	O
of	O
step	O
-	O
by	O
-	O
step	O
instructions	O
,	O
which	O
reflects	O
how	O
real	O
-	O
world	O
customers	O
innately	O
understand	O
their	O
own	O
issue	O
,	O
but	O
only	O
have	O
a	O
rough	O
idea	O
of	O
how	O
to	O
resolve	O
said	O
issue	O
.	O

Accordingly	O
,	O
customers	O
within	O
ABCD	B-DatasetName
remain	O
oblivious	O
towards	O
what	O
values	O
apply	O
to	O
which	O
actions	O
,	O
nor	O
are	O
they	O
aware	O
that	O
actions	O
exist	O
in	O
first	O
place	O
.	O

This	O
ambiguity	O
forces	O
the	O
agent	O
and	O
customer	O
to	O
collaboratively	O
uncover	O
the	O
correct	O
latent	O
intent	O
through	O
back	O
and	O
forth	O
communication	O
,	O
naturally	O
leading	O
to	O
longer	O
dialogues	O
.	O

Following	O
the	O
standard	O
dialog	O
setup	O
,	O
the	O
agent	O
starts	O
by	O
parsing	O
the	O
dialogue	O
history	O
to	O
capture	O
the	O
customer	O
intent	O
,	O
which	O
in	O
Figure	O
1	O
is	O
a	O
subscription	O
extension	O
.	O

ABCD	B-DatasetName
then	O
diverges	O
as	O
the	O
next	O
step	O
involves	O
interpreting	O
the	O
Agent	O
Guidelines	O
,	O
a	O
document	O
representing	O
the	O
internal	O
policies	O
of	O
a	O
company	O
in	O
the	O
online	O
retail	O
domain	O
(	O
See	O
Table	O
1	O
)	O
.	O

Using	O
the	O
guidelines	O
,	O
the	O
trained	O
agent	O
should	O
find	O
the	O
one	O
unique	O
subflow	O
corresponding	O
to	O
the	O
customer	O
intent	O
.	O

Each	O
subflow	O
in	O
turn	O
is	O
defined	O
by	O
exactly	O
one	O
unique	O
sequence	O
of	O
actions	O
.	O

While	O
identifying	O
a	O
subflow	O
may	O
seem	O
straightforward	O
,	O
information	O
asymmetry	O
prevents	O
the	O
customers	O
from	O
directly	O
revealing	O
the	O
name	O
of	O
their	O
intent	O
.	O

For	O
example	O
,	O
a	O
customer	O
might	O
inquire	O
about	O
the	O
status	O
of	O
their	O
recent	O
purchase	O
,	O
but	O
an	O
agent	O
has	O
over	O
a	O
dozen	O
different	O
subflows	O
related	O
to	O
order	O
statuses	O
,	O
so	O
selecting	O
the	O
right	O
one	O
suddenly	O
becomes	O
highly	O
non	O
-	O
trivial	O
.	O

In	O
our	O
case	O
,	O
the	O
agent	O
eventually	O
figures	O
out	O
the	O
correct	O
subflow	O
and	O
begins	O
to	O
execute	O
actions	O
,	O
which	O
consists	O
of	O
recording	O
values	O
given	O
by	O
the	O
customer	O
,	O
namely	O
the	O
customer	O
's	O
full	O
name	O
or	O
account	O
ID	O
in	O
order	O
to	O
[	O
Pull	O
up	O
Account	O
]	O
.	O

As	O
the	O
third	O
action	O
,	O
the	O
guidelines	O
instruct	O
the	O
agent	O
to	O
ask	O
for	O
the	O
customer	O
's	O
membership	O
level	O
.	O

After	O
the	O
customer	O
supplies	O
this	O
information	O
,	O
the	O
agent	O
enters	O
the	O
"	O
guest	O
"	O
value	O
into	O
the	O
agent	O
dashboard	O
by	O
clicking	O
the	O
[	O
Membership	O
]	O
button	O
.	O

Buttons	O
have	O
variable	O
slots	O
that	O
may	O
or	O
may	O
not	O
need	O
to	O
be	O
filled	O
,	O
depending	O
on	O
the	O
context	O
(	O
See	O
Table	O
1	O
for	O
a	O
full	O
list	O
)	O
.	O

Dialogue	O
success	O
demands	O
that	O
agents	O
execute	O
a	O
chain	O
of	O
such	O
actions	O
in	O
the	O
right	O
order	O
with	O
the	O
right	O
values	O
,	O
while	O
simultaneously	O
engaging	O
the	O
customer	O
in	O
natural	O
language	O
conversation	O
.	O

There	O
are	O
three	O
reasons	O
that	O
make	O
carrying	O
out	O
a	O
series	O
of	O
actions	O
more	O
difficult	O
than	O
the	O
task	O
lets	O
on	O
.	O

To	O
start	O
,	O
the	O
permitted	O
actions	O
in	O
a	O
given	O
state	O
are	O
determined	O
not	O
only	O
by	O
Agent	O
Guidelines	O
,	O
but	O
also	O
by	O
the	O
user	O
's	O
desire	O
,	O
which	O
may	O
be	O
in	O
conflict	O
.	O

For	O
example	O
,	O
the	O
customer	O
in	O
Figure	O
1	O
wanted	O
to	O
extend	O
their	O
subscription	O
,	O
but	O
the	O
guidelines	O
prevented	O
the	O
agent	O
from	O
doing	O
so	O
.	O

Secondly	O
,	O
actions	O
must	O
be	O
completed	O
in	O
order	O
.	O

This	O
procedural	O
requirement	O
comes	O
from	O
the	O
realization	O
that	O
completing	O
actions	O
out	O
of	O
order	O
(	O
or	O
with	O
missing	O
steps	O
)	O
do	O
not	O
make	O
sense	O
in	O
many	O
real	O
-	O
world	O
scenarios	O
.	O

For	O
example	O
,	O
it	O
is	O
critical	O
to	O
[	O
Verify	O
Identity	O
]	O
before	O
resetting	O
someone	O
's	O
password	O
,	O
not	O
after	O
.	O

Finally	O
,	O
actions	O
themselves	O
induce	O
stochastic	O
outcomes	O
,	O
preventing	O
agents	O
from	O
memorizing	O
patterns	O
of	O
subflow	O
resolution	O
.	O

As	O
an	O
example	O
,	O
[	O
Ask	O
the	O
Oracle	O
]	O
often	O
determines	O
if	O
a	O
customer	O
complaint	O
was	O
valid	O
.	O

In	O
the	O
case	O
of	O
a	O
company	O
error	O
,	O
the	O
agent	O
is	O
compelled	O
to	O
immediately	O
resolve	O
the	O
issue	O
,	O
whereas	O
a	O
misunderstanding	O
made	O
by	O
the	O
customer	O
warrants	O
a	O
different	O
set	O
of	O
responses	O
.	O

This	O
section	O
outlines	O
how	O
we	O
collect	O
and	O
annotate	O
our	O
dataset	O
with	O
context	O
-	O
dependent	O
actions	O
.	O

Managing	O
complex	O
guidelines	O
requires	O
filtering	O
for	O
top	O
agents	O
,	O
which	O
we	O
do	O
by	O
certifying	O
Mechanical	O
Turk	O
(	O
MTurk	O
)	O
workers	O
through	O
an	O
extensive	O
20	O
-	O
question	O
quiz	O
touching	O
on	O
all	O
aspects	O
of	O
task	O
completion	O
.	O

Keeping	O
the	O
bar	O
high	O
,	O
we	O
set	O
a	O
minimum	O
threshold	O
of	O
80	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
of	O
the	O
quiz	O
which	O
resulted	O
in	O
a	O
low	O
20	O
%	O
pass	O
rate	O
.	O

After	O
passing	O
the	O
exam	O
,	O
we	O
offered	O
the	O
answer	O
key	O
to	O
agents	O
which	O
further	O
improved	O
understanding	O
.	O

We	O
also	O
created	O
short	O
,	O
10	O
-	O
minute	O
tutorial	O
videos	O
to	O
showcase	O
how	O
to	O
handle	O
the	O
most	O
difficult	O
aspects	O
of	O
the	O
task	O
.	O

A	O
group	O
chat	O
app	O
was	O
also	O
deployed	O
to	O
offer	O
live	O
feedback	O
for	O
agents	O
,	O
simulating	O
how	O
supervisors	O
coach	O
customer	O
service	O
representatives	O
in	O
real	O
life	O
.	O

Finally	O
,	O
we	O
carefully	O
designed	O
an	O
incentive	O
structure	O
that	O
rewards	O
agents	O
for	O
correctly	O
identifying	O
the	O
user	O
intent	O
to	O
encourage	O
clarification	O
behavior	O
.	O
(	O

Appendix	O
A	O
covers	O
more	O
details	O
.	O
)	O

Rather	O
than	O
utilizing	O
Wizard	O
-	O
of	O
-	O
Oz	O
techniques	O
(	O
such	O
as	O
in	O
MultiWOZ	B-DatasetName
)	O
,	O
we	O
developed	O
Expert	O
Live	O
Chat	O
which	O
contains	O
three	O
unique	O
aspects:(1	O
)	O
Conversations	O
are	O
conducted	O
continuously	O
in	O
real	O
-	O
time	O
.	O
(	O

2	O
)	O
Users	O
involved	O
are	O
not	O
interchangeable	O
.	O
(	O

3	O
)	O
Players	O
are	O
informed	O
that	O
all	O
participants	O
are	O
human	O
-no	O
wizard	O
behind	O
the	O
scenes	O
.	O

Normal	O
human	O
conversations	O
occur	O
in	O
real	O
-	O
time	O
,	O
but	O
coordinating	O
multiple	O
users	O
in	O
this	O
manner	O
is	O
resource	O
-	O
intensive	O
,	O
so	O
other	O
datasets	O
often	O
employed	O
workarounds	O
to	O
avoid	O
this	O
difficulty	O
.	O

For	O
example	O
,	O
other	O
works	O
have	O
applied	O
rules	O
(	O
Bordes	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
templates	O
(	O
Byrne	O
et	O
al	O
.	O
,	O

2019	O
)	O
or	O
paraphrasing	O
(	O
Shah	O
et	O
al	O
.	O
,	O

2018	O
)	O
to	O
produce	O
conversations	O
.	O

Wizard	O
-	O
of	O
-	O
Oz	O
(	O
WoZ	O
)	O
techniques	O
incorporate	O
humans	O
into	O
the	O
mix	O
by	O
allowing	O
one	O
of	O
them	O
to	O
play	O
the	O
system	O
role	O
as	O
a	O
wizard	O
behind	O
the	O
scenes	O
(	O
Kelley	O
,	O
1984	O
)	O
.	O

In	O
particular	O
,	O
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

2018	O
)	O
decomposed	O
dialogues	O
into	O
individual	O
turns	O
,	O
where	O
for	O
each	O
turn	O
a	O
new	O
author	O
is	O
responsible	O
for	O
reading	O
the	O
context	O
and	O
generating	O
the	O
next	O
plausible	O
response	O
.	O

Despite	O
the	O
time	O
-	O
consuming	O
nature	O
,	O
some	O
datasets	O
have	O
produced	O
synchronous	O
dialogues	O
between	O
two	O
humans	O
(	O
Lewis	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

However	O
,	O
the	O
skill	O
sets	O
of	O
ABCD	B-DatasetName
workers	O
are	O
notably	O
unequal	O
,	O
exacerbating	O
the	O
matching	O
problem	O
.	O

Expert	O
Live	O
Chat	O
matches	O
a	O
highly	O
trained	O
agent	O
with	O
a	O
knowledgeable	O
,	O
yet	O
otherwise	O
average	O
customer	O
in	O
real	O
-	O
time	O
.	O

Since	O
the	O
backgrounds	O
are	O
uneven	O
,	O
unlike	O
other	O
datasets	O
with	O
concurrent	O
users	O
(	O
Lewis	O
et	O
al	O
.	O
,	O

2017;Zhang	O
et	O
al	O
.	O
,	O

2018;Das	O
et	O
al	O
.	O
,	O

2017b	O
)	O
,	O
incoming	O
Turkers	O
can	O
not	O
simply	O
be	O
randomly	O
assigned	O
a	O
role	O
.	O

In	O
other	O
words	O
,	O
having	O
twenty	O
participants	O
does	O
not	O
necessarily	O
equate	O
to	O
ten	O
conversations	O
since	O
it	O
's	O
possible	O
that	O
only	O
a	O
quarter	O
of	O
them	O
are	O
qualified	O
as	O
agents	O
.	O

When	O
such	O
an	O
imbalance	O
inevitably	O
arises	O
,	O
one	O
group	O
must	O
wait	O
until	O
someone	O
from	O
the	O
other	O
side	O
becomes	O
available	O
.	O

However	O
,	O
leaving	O
either	O
side	O
waiting	O
for	O
too	O
long	O
leads	O
to	O
serious	O
consequences	O
since	O
idle	O
time	O
directly	O
affects	O
their	O
pay	O
rate	O
.	O

To	O
minimize	O
the	O
likelihood	O
of	O
such	O
an	O
outcome	O
,	O
we	O
first	O
ensure	O
that	O
a	O
reasonable	O
pool	O
of	O
agents	O
are	O
always	O
available	O
.	O

Then	O
,	O
we	O
increase	O
the	O
number	O
of	O
active	O
customers	O
by	O
methodically	O
inviting	O
a	O
subset	O
of	O
customers	O
one	O
batch	O
at	O
a	O
time	O
.	O

To	O
do	O
so	O
,	O
we	O
established	O
a	O
qualification	O
exam	O
for	O
customers	O
to	O
ensure	O
their	O
availability	O
during	O
a	O
specified	O
time	O
period	O
.	O

Finally	O
,	O
we	O
also	O
redesigned	O
the	O
chat	O
application	O
to	O
make	O
the	O
waiting	O
room	O
experience	O
more	O
palatable	O
.	O
(	O

See	O
Appendix	O
B	O
for	O
full	O
breakdown	O
.	O
)	O

With	O
these	O
changes	O
,	O
we	O
successfully	O
increased	O
the	O
pairing	O
rate	O
from	O
18	O
out	O
of	O
80	O
active	O
users	O
up	O
to	O
72	O
out	O
of	O
83	O
,	O
an	O
increase	O
of	O
nearly	O
400	O
%	O
,	O
while	O
maintaining	O
wait	B-HyperparameterName
times	I-HyperparameterName
under	O
10	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

Besides	O
pairing	O
,	O
we	O
increased	O
the	O
likelihood	O
of	O
collecting	O
rich	O
dialogues	O
without	O
the	O
need	O
for	O
extensive	O
instructions	O
by	O
optimizing	O
the	O
chat	O
experience	O
itself	O
.	O

In	O
particular	O
,	O
we	O
observed	O
the	O
greatest	O
gains	O
by	O
grounding	O
the	O
conversation	O
to	O
the	O
relatable	O
scenario	O
of	O
online	O
shopping	O
,	O
which	O
provided	O
immediate	O
context	O
to	O
participants	O
without	O
requiring	O
any	O
extra	O
training	O
.	O

For	O
example	O
,	O
the	O
Agent	O
Dashboard	O
was	O
arranged	O
to	O
closely	O
reflect	O
actual	O
agent	O
workspaces	O
(	O
Figure	O
2	O
)	O
.	O

On	O
the	O
customer	O
side	O
,	O
scenarios	O
in	O
the	O
Customer	O
Panel	O
included	O
an	O
image	O
of	O
the	O
product	O
being	O
discussed	O
,	O
along	O
with	O
other	O
meta	O
-	O
data	O
such	O
as	O
the	O
brand	O
or	O
price	O
to	O
match	O
a	O
true	O
shopping	O
experience	O
as	O
much	O
as	O
possible	O
(	O
Appendix	O
H	O
)	O
.	O

We	O
also	O
explicitly	O
told	O
customers	O
the	O
other	O
speaker	O
was	O
human	O
to	O
encourage	O
natural	O
responses	O
over	O
confined	O
commands	O
meant	O
for	O
machines	O
.	O

Most	O
importantly	O
,	O
customers	O
were	O
given	O
dynamically	O
generated	O
,	O
natural	O
-	O
language	O
prompts	O
that	O
did	O
not	O
include	O
information	O
about	O
the	O
values	O
needed	O
to	O
resolve	O
their	O
issue	O
.	O

As	O
a	O
general	O
framework	O
,	O
Ex	O
-	O
pert	O
Live	O
Chat	O
can	O
be	O
applied	O
in	O
any	O
real	O
-	O
world	O
scenario	O
involving	O
an	O
expert	O
and	O
novice	O
.	O

Indeed	O
,	O
increasing	O
the	O
verisimilitude	O
of	O
the	O
experience	O
is	O
precisely	O
what	O
allowed	O
higher	O
quality	O
dialogues	O
to	O
be	O
generated	O
by	O
the	O
workers	O
.	O

The	O
flows	O
and	O
subflows	O
are	O
automatically	O
annotated	O
since	O
we	O
have	O
the	O
provenance	O
of	O
each	O
intent	O
when	O
generating	O
the	O
customer	O
prompt	O
.	O

Additionally	O
,	O
given	O
the	O
ground	O
truth	O
subflow	O
of	O
each	O
conversation	O
,	O
we	O
can	O
deterministically	O
map	O
them	O
to	O
the	O
correct	O
section	O
within	O
the	O
Agent	O
Guidelines	O
outlining	O
the	O
correct	O
actions	O
.	O

Calculating	O
accuracy	O
then	O
becomes	O
a	O
simple	O
exercise	O
to	O
align	O
the	O
predicted	O
actions	O
with	O
the	O
ones	O
required	O
by	O
the	O
manual	O
.	O

In	O
this	O
way	O
,	O
we	O
capture	O
a	O
key	O
benefit	O
of	O
machine	O
-	O
generated	O
text	O
(	O
Shah	O
et	O
al	O
.	O
,	O

2018	O
)	O
without	O
sacrificing	O
the	O
benefit	O
of	O
engaging	O
real	O
users	O
.	O

We	O
validate	O
all	O
dialogues	O
to	O
pass	O
quality	O
thresholds	O
such	O
as	O
including	O
a	O
minimum	O
number	O
of	O
actions	O
and	O
avoiding	O
copy	O
/	O
paste	O
behavior	O
.	O

After	O
filtering	O
,	O
we	O
end	O
up	O
with	O
10,042	O
total	O
conversations	O
with	O
an	O
average	O
of	O
22.1	O
turns	O
-the	O
highest	O
turn	O
count	O
among	O
all	O
compared	O
datasets	O
.	O

Unsurprisingly	O
,	O
ABCD	B-DatasetName
includes	O
more	O
actions	O
per	O
dialogue	O
than	O
other	O
datasets	O
,	O
by	O
at	O
least	O
a	O
factor	O
of	O
two	O
.	O

ABCD	B-DatasetName
also	O
contains	O
a	O
lower	O
absolute	O
number	O
of	O
tokens	O
,	O
but	O
also	O
has	O
the	O
highest	O
variance	O
in	O
the	O
number	O
of	O
tokens	O
per	O
turn	O
.	O
(	O

See	O
Table	O
2.)Since	O
each	O
subflow	O
represents	O
a	O
unique	O
customer	O
intent	O
,	O
ABCD	B-DatasetName
contains	O
55	O
user	O
intents	O
evenly	O
distributed	O
through	O
the	O
dataset	O
.	O

By	O
interpreting	O
buttons	O
as	O
domains	O
,	O
the	O
dataset	O
contains	O
30	O
domains	O
and	O
231	O
associated	O
slots	O
,	O
compared	O
to	O
7	O
domains	O
and	O
24	O
slots	O
within	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

2018).By	O
grounding	O
to	O
the	O
relatable	O
scenario	O
of	O
chatting	O
with	O
customer	O
support	O
of	O
an	O
online	O
retail	O
company	O
,	O
speakers	O
often	O
showcase	O
various	O
forms	O
of	O
natural	O
dialogue	O
,	O
such	O
as	O
offering	O
diverse	O
reasons	O
for	O
shopping	O
or	O
asking	O
detailed	O
follow	O
-	O
up	O
questions	O
.	O

Furthermore	O
,	O
the	O
unconstrained	O
nature	O
of	O
Expert	O
Live	O
Chat	O
allows	O
users	O
to	O
chat	O
with	O
each	O
other	O
in	O
a	O
free	O
-	O
form	O
style	O
.	O

Dialogues	O
exhibited	O
normal	O
texting	O
behavior	O
such	O
as	O
users	O
speaking	O
for	O
many	O
turns	O
in	O
a	O
row	O
or	O
fixing	O
typos	O
with	O
a	O
star	O
in	O
the	O
subsequent	O
line	O
.	O

Other	O
examples	O
of	O
linguistic	O
phenomenon	O
can	O
be	O
observed	O
in	O
Table	O
5	O
.	O

The	O
novel	O
features	O
in	O
ABCD	B-DatasetName
brings	O
two	O
new	O
dialog	O
tasks	O
,	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
and	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

We	O
also	O
build	O
baseline	O
systems	O
that	O
are	O
variants	O
of	O
standard	O
dialogue	O
models	O
and	O
report	O
their	O
results	O
on	O
ABCD	B-DatasetName
.	O

Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
(	O
AST	B-TaskName
)	O
aims	O
at	O
detecting	O
the	O
pertinent	O
intent	O
by	O
interpreting	O
customer	O
utterances	O
while	O
taking	O
into	O
account	O
constraints	O
from	O
the	O
Agent	O
Guidelines	O
,	O
an	O
aspect	O
not	O
considered	O
in	O
traditional	O
dialog	B-TaskName
state	I-TaskName
tracking	I-TaskName
(	O
DST	B-TaskName
)	O
.	O

For	O
example	O
,	O
a	O
conceivable	O
dialogue	O
task	O
might	O
entail	O
helping	O
a	O
customer	O
[	O
Reset	O
Password	O
]	O
once	O
this	O
intent	O
has	O
been	O
identified	O
.	O

In	O
contrast	O
,	O
the	O
appropriate	O
next	O
step	O
within	O
AST	B-TaskName
is	O
governed	O
by	O
the	O
Agent	O
Guidelines	O
,	O
which	O
might	O
require	O
[	O
Verify	O
Identity	O
]	O
of	O
the	O
customer	O
first	O
,	O
or	O
any	O
number	O
of	O
other	O
actions	O
,	O
before	O
executing	O
the	O
password	O
reset	O
.	O

Each	O
series	O
of	O
actions	O
is	O
considered	O
a	O
unique	O
subflow	O
that	O
belongs	O
to	O
a	O
number	O
of	O
high	O
-	O
level	O
conversational	O
flows	O
.	O

Each	O
individual	O
action	O
includes	O
the	O
active	O
button	O
b	O
to	O
click	O
and	O
its	O
corresponding	O
slots	O
s	O
and	O
values	O
v.	O
The	O
task	O
consists	O
of	O
executing	O
an	O
action	O
,	O
which	O
constitutes	O
a	O
single	O
agent	O
turn	O
.	O

More	O
specifically	O
,	O
given	O
a	O
context	O
C	O
t	O
=	O
[	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O

x	O
t	O
]	O
where	O
x	O
t	O
can	O
be	O
a	O
customer	O
utterance	O
x	O
c	O
t	O
,	O
an	O
agent	O
utterance	O
x	O
a	O
t	O
,	O
or	O
a	O
prior	O
action	O
x	O
b	O
t	O
,	O
a	O
model	O
should	O
predict	O
the	O
button	O
of	O
the	O
current	O
action	O
as	O
well	O
as	O
the	O
relevant	O
slots	O
and	O
values	O
,	O
if	O
any	O
exist	O
{	O
x	O
b	O
t+1	O
=	O
(	O
b	O
,	O
s	O
,	O
v	O
)	O
∈	O
B	O
×	O
S	O
×	O
V	O
}	O
.	O

This	O
structure	O
is	O
designed	O
to	O
mimic	O
DST	O
where	O
each	O
user	O
intent	O
is	O
broken	O
down	O
into	O
domains	O
,	O
slots	O
and	O
values	O
(	O
d	O
,	O
s	O
,	O
v	O
)	O
.	O

For	O
both	O
AST	O
and	O
DST	O
,	O
the	O
higher	O
level	O
domain	O
or	O
button	O
can	O
have	O
vary	O
-	O
ing	O
slots	O
.	O

The	O
reverse	O
is	O
also	O
true	O
-a	O
given	O
slot	O
can	O
be	O
associated	O
with	O
multiple	O
domains	O
or	O
buttons	O
.	O

Lastly	O
,	O
both	O
contain	O
values	O
that	O
can	O
be	O
enumerable	O
(	O
i.e.	O
payment	O
types	O
or	O
shipping	O
statuses	O
)	O
or	O
non	O
-	O
enumerable	O
(	O
phone	O
numbers	O
or	O
email	O
addresses	O
)	O
.	O

Following	O
the	O
pattern	O
set	O
by	O
Rastogi	O
et	O
al	O
.	O
(	O

2020b	O
)	O
,	O
enumerable	O
values	O
are	O
given	O
in	O
the	O
ontology	O
to	O
be	O
accessible	O
by	O
a	O
model	O
,	O
whereas	O
the	O
non	O
-	O
enumerable	O
items	O
are	O
not	O
.	O

Despite	O
the	O
similar	O
structure	O
,	O
AST	B-TaskName
deviates	O
from	O
DST	B-TaskName
since	O
predicting	O
the	O
right	O
action	O
requires	O
not	O
only	O
parsing	O
the	O
customer	O
utterance	O
,	O
but	O
also	O
adhering	O
to	O
Agent	O
Guidelines	O
.	O

Suppose	O
a	O
customer	O
is	O
entitled	O
to	O
a	O
discount	O
which	O
will	O
be	O
offered	O
by	O
issuing	O
a	O
[	O
Promo	O
Code	O
]	O
.	O

The	O
customer	O
might	O
request	O
30	O
%	O
off	O
,	O
but	O
the	O
guidelines	O
stipulate	O
only	O
15	O
%	O
is	O
permitted	O
,	O
which	O
would	O
make	O
"	O
30	O
"	O
a	O
reasonable	O
,	O
but	O
ultimately	O
flawed	O
slot	O
-	O
value	O
.	O

To	O
measure	O
a	O
model	O
's	O
ability	O
to	O
comprehend	O
such	O
nuanced	O
situations	O
,	O
we	O
adopt	O
overall	O
accuracy	O
as	O
the	O
evaluation	O
metric	O
for	O
AST	B-TaskName
.	O

Since	O
the	O
appropriate	O
action	O
often	O
depends	O
on	O
the	O
situation	O
,	O
we	O
propose	O
the	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
(	O
CDS	B-TaskName
)	O
task	O
to	O
measure	O
a	O
model	O
's	O
ability	O
to	O
understand	O
actions	O
in	O
context	O
.	O

Whereas	O
AST	O
assumes	O
an	O
action	O
occurs	O
in	O
the	O
current	O
turn	O
,	O
CDS	B-TaskName
gives	O
an	O
agent	O
the	O
additional	O
options	O
of	O
responding	O
with	O
an	O
utterance	O
or	O
ending	O
the	O
conversation	O
.	O

Moreover	O
,	O
proficiency	O
is	O
no	O
longer	O
measured	O
as	O
success	O
over	O
isolated	O
turns	O
but	O
rather	O
as	O
success	O
over	O
sequences	O
of	O
consecutive	O
turns	O
.	O

Formally	O
,	O
given	O
C	O
t	O
=	O
[	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O

x	O
t	O
]	O
as	O
a	O
context	O
composed	O
of	O
utterances	O
x	O
c	O
,	O
x	O
a	O
∈	O
U	O
and	O
actions	O
x	O
b	O
∈	O
A	O
,	O
a	O
model	O
should	O
predict	O
all	O
remaining	O
steps	O
x	O
>	O
t	O
along	O
with	O
their	O
realized	O
forms	O
.	O

Pos	O
-	O
sible	O
next	O
steps	O
are	O
to	O
take	O
an	O
action	O
,	O
respond	O
with	O
text	O
or	O
end	O
the	O
task	O
.	O

When	O
the	O
next	O
step	O
is	O
an	O
action	O
x	O
b	O
t+1	O
,	O
the	O
model	O
should	O
predict	O
the	O
button	O
with	O
its	O
slots	O
and	O
values	O
as	O
in	O
AST	O
.	O

If	O
the	O
agent	O
speaks	O
in	O
the	O
next	O
step	O
x	O
a	O
t+1	O
,	O
the	O
model	O
should	O
rank	O
the	O
true	O
utterance	O
highest	O
,	O
as	O
measured	O
by	O
recall	O
metrics	O
.	O

1	O
Finally	O
,	O
the	O
model	O
should	O
recognize	O
when	O
to	O
end	O
the	O
conversation	O
.	O

Rewarding	O
the	O
model	O
only	O
when	O
it	O
predicts	O
every	O
step	O
correctly	O
is	O
counter	O
-	O
productive	O
because	O
minor	O
variations	O
in	O
sentence	O
order	O
do	O
not	O
alter	O
overall	O
customer	O
satisfaction	O
.	O

Therefore	O
,	O
CDS	B-TaskName
is	O
scored	O
using	O
a	O
variation	O
on	O
Cascading	O
Evaluation	O
(	O
Suhr	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Rather	O
than	O
receiving	O
a	O
single	O
score	O
for	O
each	O
conversation	O
,	O
cascaded	O
evaluation	O
allows	O
the	O
model	O
to	O
receive	O
"	O
partial	O
credit	O
"	O
whenever	O
it	O
successfully	O
predicts	O
each	O
successive	O
step	O
in	O
the	O
chat	O
.	O

This	O
score	O
is	O
calculated	O
on	O
every	O
turn	O
,	O
and	O
the	O
model	O
is	O
evaluated	O
based	O
on	O
the	O
percent	O
of	O
remaining	O
steps	O
correctly	O
predicted	O
,	O
averaged	O
across	O
all	O
available	O
turns	O
.	O
(	O

See	O
Appendix	O
C	O
for	O
more	O
details	O
.	O
)	O

We	O
also	O
run	O
several	O
baselines	O
on	O
these	O
new	O
tasks	O
.	O

The	O
backbone	O
of	O
all	O
our	O
baseline	O
systems	O
is	O
a	O
pre	O
-	O
trained	O
Transformer	O
-	O
based	O
model	O
acting	O
as	O
a	O
context	O
encoder	O
.	O

More	O
specifically	O
,	O
given	O
the	O
dialogue	O
history	O
as	O
a	O
series	O
of	O
utterances	O
,	O
we	O
first	O
join	O
the	O
utterances	O
together	O
with	O
a	O
[	O
SEP	O
]	O
token	O
and	O
then	O
tokenize	O
the	O
entire	O
input	O
using	O
Word	O
-	O
Piece	O
(	O
Schuster	O
and	O
Nakajima	O
,	O
2012	O
)	O
.	O

Next	O
,	O
we	O
feed	O
the	O
entire	O
input	O
into	O
a	O
BERT	O
model	O
and	O
perform	O
a	O
learned	O
pooling	O
on	O
the	O
hidden	O
states	O
in	O
the	O
final	O
layer	O
,	O
which	O
results	O
in	O
a	O
fixed	O
-	O
length	O
latent	O
vector	O
h	O
enc	O
∈	O
R	O
128	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Afterwards	O
,	O
we	O
attach	O
a	O
variety	O
of	O
prediction	O
heads	O
conditioned	O
on	O
the	O
h	O
enc	O
vector	O
to	O
generate	O
the	O
final	O
output	O
.	O

Details	O
of	O
the	O
prediction	O
heads	O
for	O
the	O
two	O
proposed	O
tasks	O
are	O
described	O
next	O
.	O

We	O
break	O
down	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
(	O
AST	B-TaskName
)	O
into	O
two	O
sub	O
-	O
problems	O
,	O
button	O
-	O
slot	O
prediction	O
and	O
value	O
-	O
filling	O
.	O

Given	O
the	O
ontology	O
,	O
button	O
prediction	O
is	O
a	O
straightforward	O
classification	O
task	O
over	O
231	O
known	O
options	O
,	O
so	O
the	O
prediction	O
head	O
is	O
just	O
a	O
linear	O
classifier	O
with	O
a	O
softmax	O
activation	O
for	O
normalization	O
:P	O
b•slot	O
=	O
Softmax(W	O
a	O
h	O
enc	O
+	O
b	O
a	O
)	O
.To	O
handle	O
value	O
-	O
filling	O
,	O
we	O
further	O
decompose	O
1	O
Sentences	O
with	O
similar	O
semantics	O
may	O
be	O
formulated	O
in	O
several	O
ways	O
,	O
so	O
we	O
opt	O
for	O
response	O
retrieval	O
over	O
text	O
generation	O
since	O
common	O
metrics	O
(	O
i.e.	O
BLEU	O
score	O
)	O
tend	O
to	O
become	O
unreliable	O
in	O
these	O
situations	O
(	O
Liu	O
et	O
al	O
.	O
,	O

2016	O
)	O
.	O

the	O
task	O
into	O
predicting	O
enumerable	O
and	O
nonenumerable	O
values	O
.	O

The	O
ontology	O
lists	O
out	O
all	O
|E|	O
enumerable	O
values	O
,	O
so	O
the	O
prediction	O
head	O
p	O
enum	O
simply	O
maps	O
the	O
hidden	O
state	O
h	O
enc	O
into	O
the	O
appropriate	O
dimensions	O
.	O

To	O
handle	O
non	O
-	O
enumerable	O
values	O
,	O
we	O
follow	O
the	O
insight	O
from	O
(	O
Ma	O
et	O
al	O
.	O
,	O

2019	O
)	O
which	O
notes	O
that	O
practically	O
all	O
such	O
values	O
are	O
stated	O
by	O
the	O
customer	O
in	O
conversation	O
,	O
so	O
a	O
model	O
can	O
copy	O
these	O
values	O
from	O
the	O
tokenized	O
context	O
.	O

During	O
pre	O
-	O
processing	O
,	O
we	O
extract	O
up	O
to	O
|N	O
|	O
unique	O
tokens	O
from	O
the	O
natural	O
language	O
customer	O
utterances	O
,	O
where	O
p	O
copy	O
then	O
represents	O
the	O
distribution	O
over	O
these	O
possible	O
options	O
.	O

2	O
We	O
imitate	O
the	O
TRADE	O
architecture	O
from	O
(	O
Wu	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
where	O
conditioned	O
on	O
the	O
action	O
,	O
the	O
model	O
chooses	O
to	O
either	O
copy	O
from	O
the	O
context	O
p	O
copy	O
or	O
select	O
from	O
the	O
enumerable	O
entities	O
p	O
enum	O
based	O
on	O
a	O
gating	O
mechanism	O
.	O

The	O
gate	O
is	O
conditioned	O
on	O
the	O
hidden	O
state	O
h	O
enc	O
as	O
well	O
as	O
a	O
learned	O
context	O
vector	O
c	O
i	O
.	O

Concretely	O
,	O
p	O
enum	O
=	O
Softmax(W	O
e	O
h	O
enc	O
+	O
b	O
e	O
)	O
∈	O
R	O
|E|	O
p	O
copy	O
=	O
Softmax(W	O
c	O
h	O
enc	O
+	O
b	O
c	O
)	O
∈	O
R	O
|N	O
|	O
c	O
i	O
=	O
W	O
c	O
•	O
p	O
copy	O
∈	O
R	O
hid	O
p	O
gate	O
=	O
σ(W	O
g	O
•	O
[	O
h	O
enc	O
;	O
c	O
i	O
]	O
)	O
∈	O
R	O
1	O
P	O
val	O
=	O
[	O
p	O
gate	O
×	O
p	O
copy	O
;	O
(	O
1	O
−	O
p	O
gate	O
)	O
×	O
p	O
enum	O
]	O
∈	O
R	O
|E+N	O
|where	O
σ	O
represents	O
the	O
Sigmoid	O
function	O
and	O
[	O
•	O
;	O
•	O
]	O
is	O
the	O
concatenation	O
operation	O
.	O

The	O
final	O
value	O
predictions	O
are	O
the	O
argmax	O
of	O
P	O
val	O
which	O
merge	O
the	O
probabilities	O
of	O
p	O
enum	O
and	O
p	O
copy	O
together	O
.	O

For	O
Cascading	O
Dialogue	O
Success	O
(	O
CDS	O
)	O
,	O
we	O
also	O
tackle	O
next	O
step	O
selection	O
,	O
utterance	O
ranking	O
,	O
and	O
intent	O
classification	O
.	O

Next	O
step	O
selection	O
is	O
a	O
choice	O
between	O
retrieve	O
utterance	O
,	O
take	O
action	O
and	O
end	O
conversation	O
.	O

Intent	O
classification	O
consists	O
of	O
choosing	O
from	O
the	O
55	O
available	O
subflows	O
.	O

Given	O
this	O
basic	O
setting	O
,	O
both	O
tasks	O
use	O
the	O
same	O
setup	O
of	O
a	O
linear	O
layer	O
followed	O
by	O
a	O
softmax	O
,	O
albeit	O
with	O
their	O
own	O
respective	O
weights	O
W	O
N	O
S	O
∈	O
R	O
3×hid	O
and	O
W	O
IC	O
∈	O
R	O
55×hid	O
.	O

When	O
the	O
next	O
step	O
is	O
to	O
take	O
action	O
,	O
the	O
AST	B-TaskName
model	O
is	O
reused	O
to	O
determine	O
the	O
button	O
-	O
slot	O
and	O
value	O
.	O

When	O
end	O
conversation	O
is	O
selected	O
,	O
all	O
future	O
predictions	O
are	O
ignored	O
,	O
much	O
like	O
an	O
<	O
EOS	O
>	O
symbol	O
signifies	O
stopping	O
.	O

This	O
leaves	O
us	O
with	O
utterance	O
ranking	O
,	O
which	O
is	O
only	O
evaluated	O
when	O
retrieve	O
utterance	O
is	O
chosen	O
as	O
the	O
next	O
step	O
.	O

Our	O
ranker	O
reproduces	O
the	O
design	O
from	O
(	O
Guu	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
where	O
the	O
encoded	O
context	O
h	O
ctx	O
is	O
compared	O
against	O
each	O
encoded	O
candidate	O
response	O
h	O
cand	O
to	O
produce	O
a	O
ranking	O
score	O
.	O

To	O
embed	O
each	O
j	O
th	O
candidate	O
d	O
j	O
we	O
first	O
create	O
its	O
input	O
d	O
input	O
j	O
.	O

Following	O
standard	O
practice	O
,	O
we	O
prepend	O
the	O
candidate	O
text	O
d	O
j	O
with	O
[	O
CLS	O
]	O
,	O
separate	O
the	O
individual	O
utterances	O
u	O
i	O
within	O
the	O
candidate	O
response	O
using	O
a	O
[	O
SEP	O
]	O
token	O
,	O
and	O
append	O
a	O
final	O
[	O
SEP	O
]	O
token	O
afterwards	O
.	O
(	O

Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

This	O
input	O
d	O
input	O
j	O
is	O
then	O
fed	O
into	O
a	O
static	O
pretrained	O
BERT	O
model	O
to	O
get	O
an	O
initial	O
hidden	O
state	O
,	O
which	O
is	O
finally	O
projected	O
using	O
a	O
learned	O
weight	O
W	O
d	O
j	O
∈	O
R	O
128×hid	O
to	O
produce	O
h	O
cand	O
.	O

To	O
obtain	O
h	O
ctx	O
we	O
start	O
with	O
the	O
hidden	O
state	O
h	O
enc	O
from	O
before	O
and	O
apply	O
a	O
projection	O
matrix	O
W	O
U	O
R	O
∈	O
R	O
128×hid	O
to	O
reach	O
the	O
desired	O
dimensionality.d	O
input	O
j	O
=	O
[	O
CLS]u	O
1	O
[	O
SEP]u	O
2	O
[	O
SEP	O
]	O
...	O
[	O
SEP]u	O
n	O
[	O
SEP	O
]	O
h	O
cand	O
=	O
W	O
d	O
j	O
BERT	O
base	O
(	O
d	O
input	O
j	O
)	O
∈	O
R	O
128	O
h	O
ctx	O
=	O
W	O
U	O
R	O
h	O
enc	O
∈	O
R	O
128	O
f	O
(	O
x	O
i	O
,	O
d	O
j	O
)	O
=	O
h	O
ctx	O
h	O
cand	O
P	O
rank	O
j	O
=	O
exp(f	O
(	O
x	O
i	O
,	O
d	O
j	O
)	O
)	O
Σ	O
d	O
j	O
exp	O
f	O
(	O
x	O
i	O
,	O
d	O
j	O
)	O
The	O
final	O
rank	O
is	O
given	O
by	O
normalizing	O
each	O
j	O
th	O
score	O
against	O
all	O
other	O
candidate	O
scores	O
.	O

We	O
use	O
the	O
training	O
objective	O
from	O
(	O
Henderson	O
et	O
al	O
.	O
,	O

2019	O
)	O
to	O
calculate	O
the	O
loss	O
:	O
J	O
=	O
M	O
=	O
100	O
j=1	O
P	O
(	O
x	O
i	O
,	O
d	O
j	O
)	O
−	O
M	O
i=1	O
log	O
M	O
j=1	O
exp	O
f	O
(	O
x	O
i	O
,	O
d	O
j	O
)	O
where	O
M	O
is	O
the	O
size	O
of	O
the	O
total	O
candidate	O
set	O
.	O

We	O
performed	O
experiments	O
on	O
the	O
two	O
newly	O
proposed	O
tasks	O
,	O
AST	B-TaskName
and	O
CDS	B-TaskName
.	O

AST	B-TaskName
consists	O
of	O
two	O
subtasks	O
,	O
button	O
-	O
slot	O
prediction	O
and	O
value	O
-	O
filling	O
,	O
while	O
CDS	O
builds	O
on	O
this	O
with	O
three	O
additional	O
subtasks	O
of	O
next	O
step	O
selection	O
,	O
utterance	O
ranking	O
,	O
and	O
intent	O
classification	O
.	O

For	O
both	O
tasks	O
,	O
we	O
experimented	O
with	O
two	O
types	O
of	O
frameworks	O
,	O
a	O
pipeline	O
version	O
and	O
an	O
end	O
-	O
to	O
-	O
end	O
version	O
.	O

The	O
pipeline	O
version	O
trains	O
each	O
subtask	O
separately	O
while	O
the	O
end	O
-	O
to	O
-	O
end	O
optimizes	O
all	O
tasks	O
jointly	O
(	O
Liang	O
et	O
al	O
.	O
,	O

2020;Rastogi	O
et	O
al	O
.	O
,	O

2020a;Ham	O
et	O
al	O
.	O
,	O

2020).The	O
pipeline	O
model	O
uses	O
a	O
BERT	B-MethodName
model	O
trained	O
with	O
the	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
.	O

To	O
test	O
the	O
performance	O
of	O
different	O
pretrained	O
models	O
under	O
the	O
end	O
-	O
to	O
-	O
end	O
framework	O
,	O
we	O
experiment	O
with	O
three	O
additional	O
encoders	O
,	O
Al	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
RoBERTa	B-MethodName
-	I-MethodName
Large	I-MethodName
.	O

AlBERT	B-MethodName
model	O
has	O
an	O
inter	O
-	O
sentence	O
coherence	O
task	O
and	O
a	O
lighter	O
memory	O
footprint	O
compared	O
to	O
BERT	B-MethodName
,	O
while	O
RoBERTa	B-MethodName
model	O
has	O
substantially	O
more	O
data	O
and	O
hyper	O
-	O
parameter	O
tuning	O
in	O
pretraining	O
than	O
BERT.In	B-MethodName
the	O
future	O
,	O
we	O
also	O
plan	O
to	O
include	O
GPT	O
-	O
based	O
models	O
,	O
such	O
as	O
DialoGPT	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
in	O
our	O
comparison	O
.	O

For	O
both	O
tasks	O
,	O
moving	O
from	O
the	O
pipeline	O
architecture	O
to	O
a	O
jointly	O
trained	O
method	O
displayed	O
noticeable	O
improvement	O
in	O
accuracy	O
.	O

As	O
hinted	O
at	O
in	O
prior	O
works	O
(	O
Liang	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
suspect	O
the	O
group	O
effort	O
gives	O
each	O
subtask	O
extra	O
supervision	O
from	O
other	O
subtasks	O
for	O
more	O
data	O
efficient	O
training	O
.	O

In	O
the	O
AST	B-TaskName
task	O
,	O
we	O
found	O
steady	O
improvements	O
as	O
we	O
move	O
from	O
the	O
older	O
to	O
the	O
newer	O
models	O
with	O
vanilla	O
BERT	B-MethodName
at	O
59.5	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
and	O
RoBERTa	B-MethodName
doing	O
the	O
best	O
at	O
65.8	B-MetricValue
%	I-MetricValue
.	O

For	O
the	O
CDS	B-TaskName
task	O
,	O
we	O
found	O
a	O
similar	O
trend	O
where	O
RoBERTa	B-MethodName
-	I-MethodName
Large	I-MethodName
outperforms	O
BERT	B-MethodName
,	O
but	O
only	O
by	O
a	O
mere	O
0.6	B-MetricValue
%	I-MetricValue
.	O

We	O
hypothesize	O
this	O
small	O
gap	O
between	O
models	O
is	O
due	O
to	O
the	O
fact	O
that	O
none	O
were	O
particularly	O
trained	O
on	O
dialogue	O
data	O
which	O
impacts	O
their	O
ability	O
to	O
produce	O
a	O
useful	O
encoding	O
(	O
Wu	O
and	O
Xiong	O
,	O
2020	O
)	O
.	O

Separately	O
,	O
we	O
evaluate	O
CDS	B-TaskName
subtask	O
difficulty	O
by	O
asking	O
human	O
volunteers	O
to	O
select	O
the	O
correct	O
label	O
from	O
a	O
list	O
of	O
possible	O
options	O
.	O

As	O
an	O
example	O
,	O
workers	O
would	O
be	O
presented	O
with	O
55	O
different	O
classes	O
for	O
Intent	B-TaskName
Classification	I-TaskName
and	O
asked	O
to	O
choose	O
the	O
right	O
one	O
.	O

Since	O
humans	O
typically	O
struggle	O
when	O
choosing	O
from	O
large	O
collections	O
of	O
items	O
,	O
fine	O
-	O
tuned	O
models	O
performed	O
roughly	O
on	O
par	O
or	O
better	O
compared	O
to	O
humans	O
in	O
this	O
unnatural	O
setting	O
.	O

On	O
the	O
other	O
hand	O
,	O
human	O
evaluation	O
for	O
the	O
overall	O
CDS	B-TaskName
task	O
was	O
judged	O
by	O
measuring	O
the	O
success	O
rate	O
in	O
a	O
standard	O
conversational	O
scenarios	O
where	O
behavioral	O
instincts	O
are	O
activated	O
,	O
so	O
humans	O
were	O
able	O
to	O
excel	O
on	O
this	O
environment	O
.	O

We	O
perform	O
an	O
ablation	O
study	O
to	O
test	O
the	O
significance	O
of	O
the	O
key	O
features	O
in	O
ABCD	B-DatasetName
.	O

Recall	O
,	O
actions	O
are	O
characterized	O
by	O
their	O
dual	O
nature	O
of	O
requiring	O
signals	O
from	O
both	O
the	O
customer	O
and	O
the	O
company	O
guidelines	O
.	O

To	O
that	O
end	O
,	O
we	O
provided	O
the	O
ground	O
truth	O
intent	O
to	O
measure	O
the	O
impact	O
of	O
the	O
customer	O
side	O
.	O

Conversely	O
,	O
we	O
also	O
test	O
the	O
company	O
side	O
by	O
masking	O
out	O
invalid	O
buttons	O
based	O
on	O
the	O
insight	O
that	O
the	O
Agent	O
Guidelines	O
are	O
useful	O
for	O
narrowing	O
down	O
the	O
range	O
of	O
possible	O
actions	O
.	O

In	O
both	O
situations	O
,	O
we	O
would	O
expect	O
that	O
providing	O
such	O
oracle	O
guidance	O
would	O
boost	O
performance	O
.	O

Lastly	O
,	O
note	O
that	O
the	O
appropriate	O
action	O
depends	O
on	O
the	O
outcomes	O
of	O
prior	O
actions	O
,	O
so	O
for	O
a	O
final	O
experiment	O
we	O
removed	O
prior	O
actions	O
and	O
their	O
explanations	O
from	O
the	O
context	O
to	O
test	O
their	O
impact	O
on	O
task	O
success	O
.	O
(	O

See	O
Appendix	O
E	O
for	O
details.)We	O
observe	O
that	O
supplying	O
the	O
intent	O
information	O
to	O
the	O
BERT	B-MethodName
model	O
causes	O
a	O
noticeable	O
boost	O
in	O
dialog	O
success	O
,	O
bringing	O
the	O
score	O
to	O
32.3	B-MetricValue
%	I-MetricValue
.	O

However	O
,	O
augmenting	O
the	O
model	O
with	O
knowledge	O
of	O
the	O
guidelines	O
unexpectedly	O
dropped	O
performance	O
down	O
to	O
30.6	B-MetricValue
%	I-MetricValue
.	O

Further	O
analysis	O
revealed	O
the	O
imperfect	O
intent	O
classifier	O
would	O
occasionally	O
mask	O
out	O
valid	O
buttons	O
,	O
leaving	O
only	O
incorrect	O
ones	O
to	O
choose	O
from	O
.	O

As	O
a	O
result	O
,	O
the	O
downstream	O
action	O
predictor	O
would	O
be	O
prevented	O
from	O
doing	O
its	O
job	O
,	O
causing	O
errors	O
to	O
accumulate	O
.	O

To	O
test	O
this	O
hypothesis	O
,	O
we	O
ran	O
another	O
model	O
(	O
Intent+Guide	O
)	O
which	O
had	O
access	O
to	O
guidelines	O
along	O
with	O
an	O
oracle	O
intent	O
classifier	O
.	O

This	O
model	O
reached	O
the	O
peak	O
observed	O
performance	O
of	O
32.7	B-TaskName
%	I-TaskName
,	O
highlighting	O
the	O
importance	O
of	O
both	O
components	O
.	O

As	O
a	O
final	O
result	O
,	O
removing	O
action	O
information	O
away	O
from	O
actionbased	O
conversations	O
unsurprisingly	O
causes	O
a	O
major	O
performance	O
drop	O
(	O
Table	O
4	O
)	O
.	O

In	O
conclusion	O
,	O
we	O
have	O
presented	O
ABCD	B-DatasetName
which	O
includes	O
over	O
10	O
K	O
dialogues	O
that	O
incorporate	O
procedural	O
,	O
dual	O
-	O
constrained	O
actions	O
.	O

Additionally	O
,	O
we	O
established	O
a	O
scalable	O
method	O
for	O
collecting	O
live	O
human	O
conversations	O
with	O
unequal	O
partners	O
.	O

We	O
found	O
that	O
pre	O
-	O
trained	O
models	O
perform	O
decent	O
on	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
but	O
there	O
is	O
a	O
large	O
gap	O
between	O
humans	O
agents	O
and	O
the	O
top	O
systems	O
for	B-TaskName
Cascading	I-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

We	O
plan	O
to	O
incorporate	O
GPT	O
-	O
related	O
models	O
(	O
Hosseini	O
-	O
Asl	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
as	O
alternate	O
forms	O
of	O
preprocessing	O
have	O
shown	O
promise	O
in	O
other	O
NLP	O
tasks	O
.	O

Other	O
techniques	O
could	O
also	O
be	O
used	O
to	O
incorporate	O
speaker	O
info	O
,	O
action	O
semantics	O
and	O
other	O
meta	O
-	O
data	O
.	O

Wholly	O
new	O
systems	O
that	O
attend	O
to	O
the	O
Agent	O
Guidelines	O
in	O
a	O
fully	O
differentiable	O
manner	O
are	O
also	O
worth	O
exploring	O
.	O

By	O
grounding	O
dialogues	O
to	O
in	O
-	O
depth	O
scenarios	O
with	O
explicit	O
policies	O
,	O
we	O
hope	O
to	O
have	O
pushed	O
towards	O
a	O
better	O
understanding	O
of	O
dialogue	O
success	O
.	O

Optimizing	O
agents	O
performance	O
can	O
be	O
split	O
into	O
preparation	O
before	O
the	O
HIT	B-TaskName
(	O
Human	B-TaskName
Intelligence	I-TaskName
Task	I-TaskName
)	O
,	O
improving	O
HIT	B-TaskName
itself	O
,	O
and	O
ongoing	O
training	O
afterwards	O
.	O

Starting	O
with	O
the	O
pre	O
-	O
HIT	B-TaskName
phase	O
,	O
the	O
major	O
steps	O
largely	O
center	O
around	O
multiple	O
rounds	O
of	O
qualifications	O
to	O
filter	O
for	O
the	O
highest	O
quality	O
workers	O
available	O
.	O

During	O
the	O
post	O
-	O
HIT	B-TaskName
phase	O
,	O
effort	O
shifts	O
to	O
ensuring	O
that	O
each	O
worker	O
becomes	O
increasingly	O
comfortable	O
with	O
the	O
task	O
.	O

Pre	O
-	O
HIT	O
Phase	O
Qualifications	O
take	O
the	O
form	O
of	O
online	O
quizzes	O
which	O
serve	O
the	O
purpose	O
of	O
training	O
motivated	O
workers	O
in	O
addition	O
to	O
simply	O
removing	O
unqualified	O
ones	O
.	O

When	O
designing	O
the	O
qualification	O
,	O
the	O
number	O
and	O
style	O
of	O
questions	O
were	O
iterated	O
on	O
to	O
limit	O
the	O
feeling	O
of	O
a	O
tight	O
time	O
constraint	O
,	O
while	O
still	O
remaining	O
quite	O
difficult	O
.	O

In	O
fact	O
,	O
some	O
agents	O
who	O
had	O
previously	O
had	O
actual	O
customer	O
service	O
jobs	O
mentioned	O
they	O
felt	O
like	O
they	O
were	O
right	O
back	O
at	O
the	O
office	O
.	O

This	O
difficulty	O
resulted	O
in	O
a	O
high	O
rejection	O
rate	O
,	O
which	O
was	O
costly	O
because	O
we	O
paid	O
Turkers	O
$	O
2	O
regardless	O
of	O
passing	O
the	O
exam	O
(	O
with	O
a	O
larger	O
$	O
8	O
bonus	O
for	O
passing	O
)	O
.	O

Although	O
,	O
the	O
cost	O
was	O
well	O
worth	O
the	O
trade	O
-	O
off	O
since	O
having	O
high	O
quality	O
agents	O
would	O
pay	O
dividends	O
down	O
the	O
road	O
.	O

To	O
move	O
efficiently	O
,	O
we	O
leaned	O
heavily	O
on	O
multiple	O
choice	O
questions	O
and	O
MTurk	O
APIs	O
to	O
help	O
automate	O
grading	O
and	O
assignment	O
of	O
qualifications	O
.	O

Finally	O
,	O
we	O
learned	O
that	O
including	O
screenshots	O
of	O
the	O
Agent	O
Dashboard	O
in	O
the	O
quizzes	O
was	O
a	O
great	O
way	O
to	O
familiarize	O
the	O
agents	O
with	O
the	O
platform	O
before	O
performing	O
the	O
actual	O
task	O
.	O

During	O
-	O
HIT	B-TaskName
Phase	O
The	O
HIT	B-TaskName
itself	O
was	O
priced	O
at	O
$	O
1.50	O
for	O
completing	O
the	O
conversation	O
with	O
an	O
extra	O
$	O
1.00	O
bonus	O
for	O
identifying	O
the	O
correct	O
customer	O
intent	O
at	O
the	O
end	O
-	O
of	O
-	O
chat	O
survey	O
.	O

Since	O
agents	O
are	O
naturally	O
focused	O
on	O
getting	O
done	O
as	O
quickly	O
as	O
possible	O
,	O
they	O
would	O
often	O
only	O
take	O
the	O
customer	O
's	O
requests	O
into	O
account	O
,	O
bypassing	O
a	O
key	O
characteristic	O
of	O
what	O
makes	O
ABCD	O
unique	O
.	O

However	O
,	O
by	O
encouraging	O
agents	O
to	O
focus	O
on	O
the	O
customer	O
intent	O
,	O
they	O
were	O
forced	O
to	O
peruse	O
the	O
Agent	O
Guidelines	O
for	O
the	O
associated	O
subflow	O
.	O

Thus	O
,	O
we	O
found	O
this	O
incentive	O
critical	O
for	O
aligning	O
agent	O
behaviors	O
with	O
optimal	O
outcomes	O
.	O

Post	O
-	O
HIT	B-TaskName
Phase	O
For	O
ongoing	O
training	O
,	O
we	O
began	O
producing	O
small	O
lists	O
of	O
bulletpoints	O
to	O
the	O
agents	O
on	O
areas	O
they	O
could	O
improve	O
on	O
.	O

Fur-	O
thermore	O
,	O
we	O
would	O
highlight	O
examples	O
of	O
good	O
and	O
bad	O
decision	O
-	O
making	O
and	O
appropriate	O
behavior	O
when	O
representing	O
the	O
fictitious	O
"	O
AcmeBrands	O
"	O
retail	O
company	O
.	O

Finally	O
,	O
we	O
also	O
recorded	O
videos	O
which	O
gave	O
agents	O
a	O
view	O
of	O
how	O
an	O
"	O
ideal	O
"	O
agent	O
would	O
behave	O
at	O
every	O
step	O
of	O
the	O
chat	O
.	O

We	O
found	O
that	O
by	O
engaging	O
with	O
the	O
Turkers	O
through	O
the	O
group	O
chat	O
and	O
respecting	O
their	O
feedback	O
,	O
they	O
were	O
very	O
willing	O
to	O
work	O
on	O
improving	O
despite	O
not	O
having	O
extra	O
monetary	O
incentive	O
to	O
do	O
so	O
.	O

In	O
total	O
,	O
the	O
agents	O
were	O
quite	O
wonderful	O
to	O
work	O
with	O
and	O
their	O
end	O
-	O
of	O
-	O
task	O
feedback	O
strongly	O
suggests	O
they	O
enjoyed	O
the	O
process	O
as	O
well	O
.	O
(	O

See	O
Figure	O
3	O
)	O
We	O
credit	O
this	O
to	O
the	O
training	O
details	O
mentioned	O
in	O
this	O
section	O
and	O
the	O
development	O
of	O
the	O
Expert	O
Live	O
Chat	O
procedure	O
.	O

In	O
a	O
regular	O
Mechanical	O
Turk	O
(	O
MTurk	O
)	O
setup	O
,	O
HITs	O
are	O
made	O
available	O
to	O
a	O
large	O
audience	O
who	O
can	O
pick	O
up	O
as	O
many	O
or	O
as	O
few	O
as	O
they	O
want	O
.	O

Expert	O
Live	O
Chat	O
dictates	O
a	O
dialogue	O
between	O
two	O
speakers	O
,	O
so	O
we	O
need	O
two	O
types	O
of	O
workers	O
:	O
agents	O
and	O
customers	O
.	O

Let	O
us	O
consider	O
the	O
number	O
of	O
agents	O
available	O
as	O
A	O
and	O
the	O
number	O
of	O
customers	O
available	O
as	O
C.	O
Given	O
budget	O
constraints	O
,	O
we	O
can	O
only	O
pay	O
some	O
maximum	O
number	O
of	O
workers	O
M	O
.	O

Simultaneously	O
,	O
given	O
time	O
constraints	O
,	O
we	O
need	O
a	O
minimum	O
number	O
of	O
conversations	O
collected	O
per	O
week	O
,	O
which	O
is	O
a	O
function	O
of	O
the	O
number	O
of	O
available	O
workers	O
N	O
=	O
f	O
(	O
A	O
,	O
C	O
)	O
.	O

This	O
leads	O
to	O
three	O
issues	O
that	O
must	O
be	O
considered	O
in	O
conjunction	O
:	O
N	O
<	O
A+C	O
<	O
M	O
Operating	O
the	O
Agent	O
Dashboard	O
requires	O
a	O
highly	O
skilled	O
worker	O
,	O
so	O
efficient	O
data	O
collection	O
is	O
limited	O
by	O
the	O
number	O
of	O
available	O
agents	O
.	O

Although	O
the	O
customer	O
side	O
of	O
ABCD	B-DatasetName
is	O
a	O
simpler	O
task	O
,	O
there	O
is	O
still	O
a	O
minimum	O
bar	O
to	O
be	O
met	O
to	O
prevent	O
(	O
a	O
)	O
customers	O
who	O
spam	O
with	O
random	O
text	O
(	O
b	O
)	O
customers	O
who	O
fake	O
scenarios	O
or	O
(	O
c	O
)	O
customers	O
who	O
hoard	O
HITs	O
and	O
never	O
show	O
up	O
to	O
the	O
chat	O
.	O

Thus	O
,	O
there	O
needs	O
to	O
be	O
a	O
sufficient	O
amount	O
of	O
both	O
agent	O
A	O
and	O
customers	O
C	O
qualified	O
and	O
available	O
in	O
order	O
to	O
surpass	O
the	O
minimum	O
threshold	O
set	O
by	O
N	O
.	O

However	O
,	O
simply	O
paying	O
more	O
per	O
HIT	O
bumps	O
up	O
against	O
the	O
limits	O
set	O
by	O
M	O
.C	O
>	O
>	O
A	O
Since	O
training	O
agents	O
is	O
more	O
resource	O
intensive	O
than	O
training	O
customers	O
,	O
it	O
makes	O
sense	O
to	O
simply	O
have	O
more	O
of	O
the	O
latter	O
.	O

Yet	O
by	O
doing	O
so	O
leads	O
to	O
an	O
issue	O
where	O
customers	O
wait	O
around	O
for	O
agents	O
when	O
they	O
arrive	O
in	O
the	O
waitroom	O
.	O

In	O
a	O
typical	O
scenario	O
,	O
a	O
customer	O
might	O
leave	O
the	O
tab	O
open	O
to	O
work	O
on	O
other	O
tasks	O
,	O
but	O
when	O
they	O
are	O
eventually	O
paired	O
,	O
the	O
customer	O
is	O
often	O
busy	O
doing	O
something	O
else	O
,	O
leaving	O
the	O
chat	O
to	O
flounder	O
.	O

In	O
the	O
worst	O
case	O
,	O
the	O
customer	O
starts	O
to	O
verbally	O
abuse	O
the	O
agent	O
about	O
the	O
long	O
wait	O
time	O
when	O
they	O
are	O
finally	O
paired	O
.	O

A	O
>	O
>	O
C	O
Finding	O
as	O
many	O
agents	O
as	O
possible	O
is	O
not	O
the	O
solution	O
either	O
because	O
now	O
the	O
agents	O
will	O
end	O
up	O
waiting	O
around	O
for	O
customers	O
.	O

If	O
the	O
waiting	O
periods	O
are	O
too	O
long	O
,	O
agents	O
will	O
abandon	O
the	O
task	O
and	O
disparage	O
your	O
reputation	O
on	O
various	O
forms	O
of	O
social	O
media	O
.	O

Since	O
the	O
task	O
is	O
difficult	O
,	O
the	O
pool	O
of	O
workers	O
who	O
may	O
eventually	O
qualify	O
as	O
agents	O
is	O
finite	O
,	O
so	O
too	O
many	O
poor	O
interactions	O
can	O
halt	O
the	O
data	O
collection	O
process	O
completely	O
.	O

To	O
resolve	O
this	O
situation	O
,	O
we	O
begin	O
with	O
the	O
maximum	O
number	O
of	O
workers	O
M	O
as	O
the	O
starting	O
constraint	O
given	O
a	O
fixed	O
budget	O
.	O

If	O
we	O
qualify	O
too	O
many	O
workers	O
,	O
then	O
we	O
will	O
not	O
have	O
enough	O
budget	O
left	O
for	O
the	O
actual	O
conversations	O
,	O
so	O
instead	O
we	O
qualify	O
workers	O
in	O
mini	O
-	O
batches	O
.	O

Since	O
the	O
pool	O
of	O
potential	O
workers	O
who	O
may	O
meet	O
the	O
strict	O
requirements	O
for	O
agents	O
A	O
is	O
more	O
limited	O
than	O
customer	O
candidates	O
C	O
,	O
we	O
start	O
on	O
the	O
agent	O
side	O
.	O

Given	O
some	O
amount	O
of	O
qualified	O
agents	O
,	O
only	O
a	O
percentage	O
of	O
them	O
will	O
show	O
up	O
at	O
the	O
desired	O
time	O
slot	O
to	O
perform	O
the	O
task	O
.	O

Thus	O
,	O
we	O
increment	O
the	O
batch	O
size	O
until	O
the	O
number	O
of	O
available	O
workers	O
passes	O
the	O
minimum	O
A	O
>	O
N/2.To	O
limit	O
the	O
number	O
of	O
customers	O
who	O
show	O
up	O
,	O
we	O
filter	O
for	O
users	O
by	O
location	O
,	O
number	O
of	O
completed	O
HITS	O
,	O
and	O
sufficient	O
rating	O
.	O

We	O
also	O
establish	O
an	O
exam	O
that	O
is	O
purposely	O
very	O
easy	O
(	O
to	O
minimize	O
costs	O
)	O
,	O
but	O
just	O
hard	O
enough	O
to	O
deter	O
bots	O
and	O
spammers	O
.	O

To	O
raise	O
the	O
likelihood	O
that	O
the	O
customer	O
will	O
show	O
up	O
,	O
we	O
include	O
a	O
question	O
in	O
the	O
quiz	O
which	O
simply	O
asks	O
when	O
the	O
customer	O
is	O
available	O
to	O
perform	O
the	O
HIT	O
.	O

We	O
really	O
emphasize	O
this	O
question	O
and	O
make	O
it	O
required	O
,	O
so	O
workers	O
are	O
aware	O
of	O
its	O
significance	O
.	O

This	O
allows	O
us	O
to	O
tune	O
the	O
customer	O
count	O
such	O
that	O
C	O
≈	O
A.Note	O
that	O
due	O
to	O
the	O
higher	O
pay	O
rate	O
,	O
agents	O
are	O
more	O
likely	O
to	O
show	O
up	O
than	O
customers	O
.	O

Therefore	O
,	O
there	O
needs	O
to	O
be	O
a	O
higher	O
ratio	O
of	O
customers	O
to	O
account	O
for	O
this	O
imbalance	O
.	O

For	O
some	O
intuition	O
on	O
where	O
to	O
start	O
,	O
we	O
found	O
that	O
a	O
good	O
rule	O
of	O
thumb	O
was	O
to	O
consider	O
the	O
appearance	O
ratio	O
as	O
inversely	O
proportional	O
to	O
the	O
ratio	O
of	O
pay	O
.	O

One	O
final	O
insight	O
is	O
to	O
make	O
the	O
HITs	O
heavily	O
dependent	O
on	O
bonus	O
pay	O
,	O
with	O
base	O
pay	O
very	O
low	O
.	O

This	O
will	O
keep	O
spammers	O
away	O
since	O
they	O
will	O
end	O
up	O
with	O
a	O
pittance	O
when	O
attempting	O
to	O
game	O
the	O
system	O
.	O

To	O
improve	O
the	O
waitroom	O
experience	O
,	O
we	O
added	O
a	O
feature	O
where	O
a	O
user	O
's	O
place	O
in	O
the	O
queue	O
would	O
be	O
updated	O
live	O
,	O
along	O
with	O
a	O
timer	O
indicating	O
the	O
expected	O
wait	O
.	O

For	O
Turkers	O
willing	O
to	O
wait	O
around	O
,	O
helpful	O
and	O
encouraging	O
messages	O
would	O
also	O
be	O
displayed	O
to	O
keep	O
them	O
occupied	O
.	O

Alternatively	O
,	O
for	O
Turkers	O
who	O
were	O
multi	O
-	O
tasking	O
,	O
visual	O
and	O
audio	O
notifications	O
were	O
added	O
to	O
signify	O
the	O
start	O
of	O
a	O
chat	O
,	O
allowing	O
them	O
to	O
attend	O
to	O
other	O
tasks	O
in	O
the	O
meantime	O
.	O

We	O
believe	O
our	O
modifications	O
have	O
only	O
scratched	O
the	O
surface	O
and	O
that	O
improving	O
the	O
user	O
experience	O
for	O
data	O
collection	O
offers	O
an	O
interesting	O
line	O
of	O
HCI	B-TaskName
research	O
to	O
explore	O
.	O

To	O
motivate	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
(	O
CDS	B-TaskName
)	O
over	O
typical	O
other	O
accuracy	O
metrics	O
,	O
consider	O
the	O
scenario	O
where	O
a	O
model	O
gets	O
80	B-MetricValue
%	I-MetricValue
of	O
turns	O
correct	O
,	O
while	O
still	O
achieving	O
0	O
%	O
accuracy	O
on	O
the	O
conversation	O
level	O
because	O
it	O
always	O
messes	O
up	O
somewhere	O
right	O
at	O
the	O
end	O
of	O
the	O
dialogue	O
.	O

A	O
turnbased	O
metric	O
would	O
over	O
-	O
estimate	O
performance	O
since	O
such	O
a	O
metric	O
fails	O
to	O
capture	O
the	O
model	O
's	O
consistent	O
shortcomings	O
in	O
closing	O
conversations	O
.	O

On	O
the	O
other	O
hand	O
,	O
conversation	O
-	O
based	O
metrics	O
under	O
-	O
estimate	O
the	O
model	O
's	O
performance	O
because	O
such	O
measures	O
fail	O
to	O
account	O
for	O
the	O
fact	O
that	O
the	O
system	O
is	O
mostly	O
successful	O
.	O

Moreover	O
,	O
each	O
evaluation	O
would	O
be	O
limited	O
to	O
occurring	O
only	O
once	O
per	O
conversation	O
,	O
which	O
makes	O
inefficient	O
use	O
of	O
scarce	O
data	O
as	O
a	O
resource	O
.	O

Instead	O
,	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
creates	O
an	O
evaluation	O
example	O
for	O
the	O
remainder	O
of	O
each	O
conversation	O
starting	O
from	O
each	O
turn	O
.	O

For	O
example	O
,	O
suppose	O
a	O
chat	O
contained	O
4	O
turns	O
:	O
[	O
A	O
,	O
B	O
,	O
C	O
,	O
and	O
D	O
]	O
,	O
training	O
instances	O
can	O
be	O
created	O
with	O
this	O
data	O
that	O
include	O
:	O
[	O
A	O
,	O
B	O
,	O
C	O
,	O
D	O
]	O
,	O
[	O
B	O
,	O
C	O
,	O
D	O
]	O
,	O
[	O
C	O
,	O
D	O
]	O
and	O
[	O
D	O
]	O
by	O
itself	O
.	O

Now	O
imagine	O
the	O
model	O
consistently	O
predicted	O
turn	O
C	O
incorrectly	O
,	O
and	O
everything	O
else	O
correct	O
.	O

Then	O
its	O
scores	O
would	O
be	O
2/4	O
,	O
1/3	O
,	O
0	O
and	O
1	O
,	O
respectively	O
.	O

Averaging	O
across	O
all	O
turns	O
would	O
yield	O
a	O
final	O
cascading	B-MetricName
success	I-MetricName
rate	I-MetricName
of	O
45.8	B-MetricValue
%	I-MetricValue
.	O

A	O
turn	O
-	O
based	O
metric	O
would	O
yield	O
75	B-MetricValue
%	I-MetricValue
while	O
a	O
conversation	O
-	O
based	O
metric	O
would	O
yield	O
0	B-MetricValue
%	I-MetricValue
.	O

Thus	O
,	O
CDS	B-TaskName
allows	O
a	O
model	O
to	O
earn	O
partial	O
credit	O
on	O
what	O
it	O
has	O
learned	O
without	O
severe	O
penalties	O
in	O
either	O
direction	O
.	O

When	O
training	O
the	O
best	O
model	O
for	B-TaskName
Action	I-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
we	O
ended	O
up	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-5	B-HyperparameterValue
,	B-HyperparameterName
hidden	I-HyperparameterName
dimension	I-HyperparameterName
of	O
1024	B-HyperparameterValue
,	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.05	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
10	B-HyperparameterValue
examples	O
.	O

Training	O
lasted	O
for	O
14	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
where	O
we	O
early	O
stopped	O
if	O
overall	O
accuracy	B-MetricName
failed	O
to	O
improve	O
for	O
three	O
epochs	O
in	O
a	O
row	O
.	O

The	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
had	O
a	O
linear	O
warm	O
-	O
up	O
for	O
three	O
epochs	O
,	O
with	O
hyperparameters	O
kept	O
at	O
their	O
defaults	O
of	O
0.9	B-HyperparameterValue
and	O
0.999	B-HyperparameterValue
.	O

We	O
also	O
add	O
the	O
delexicalized	O
slots	O
into	O
the	O
vocabulary	O
of	O
the	O
tokenizer	O
.	O

For	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
,	O
our	O
best	O
model	O
had	O
a	O
1e-5	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
1024	B-HyperparameterValue
hidden	B-HyperparameterName
dimension	I-HyperparameterName
and	O
no	O
weight	O
decay	O
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
shrunk	O
to	O
3	B-HyperparameterValue
examples	O
,	O
but	O
this	O
was	O
due	O
purely	O
to	O
memory	O
rather	O
than	O
performance	O
reasons	O
.	O

Train	O
-	O
ing	O
was	O
set	O
to	O
21	O
epochs	O
,	O
and	O
again	O
we	O
early	O
stopped	O
if	O
overall	O
accuracy	O
failed	O
to	O
improve	O
for	O
three	O
epochs	O
in	O
a	O
row	O
.	O

Finally	O
,	O
the	O
optimizer	O
again	O
had	O
a	O
linear	O
warm	O
-	O
up	O
for	O
three	O
epochs	O
with	O
hyperparameters	O
kept	O
at	O
their	O
defaults	O
.	O

We	O
augment	O
the	O
model	O
with	O
access	O
to	O
intent	O
information	O
in	O
two	O
ways	O
.	O

First	O
,	O
the	O
subflow	O
is	O
translated	O
into	O
an	O
index	O
which	O
is	O
concatenated	O
to	O
all	O
input	O
contexts	O
so	O
the	O
model	O
can	O
leverage	O
this	O
information	O
.	O

Second	O
,	O
the	O
intent	O
classifier	O
is	O
directly	O
fed	O
the	O
solution	O
,	O
which	O
is	O
what	O
allows	O
it	O
to	O
trivially	O
reach	O
perfect	O
accuracy	B-MetricName
.	O

We	O
leverage	O
the	O
Agent	O
Guidelines	O
by	O
using	O
it	O
to	O
mask	O
invalid	O
action	O
predictions	O
.	O

More	O
specifically	O
,	O
given	O
a	O
predicted	O
subflow	O
,	O
the	O
guidelines	O
outline	O
all	O
possible	O
actions	O
and	O
values	O
within	O
that	O
subflow	O
.	O

With	O
this	O
information	O
,	O
a	O
mask	O
is	O
created	O
before	O
training	O
and	O
applied	O
during	O
evaluation	O
to	O
only	O
allow	O
valid	O
actions	O
.	O

Since	O
ABCD	B-DatasetName
was	O
collected	O
using	O
Expert	O
Live	O
Chat	O
rather	O
than	O
templates	O
,	O
we	O
observe	O
various	O
linguistic	O
diversity	O
in	O
the	O
chats	O
.	O

These	O
phenomena	O
limit	O
the	O
ability	O
of	O
models	O
to	O
memorize	O
artificial	O
patterns	O
when	O
making	O
predictions	O
.	O

Co	O
-	O
reference	O
CUS	O
:	O
I	O
'd	O
like	O
to	O
return	O
something	O
AGT	O
:	O
OK	O
AGT	O
:	O
Can	O
I	O
get	O
your	O
full	O
name	O
AGT	O
:	O
Also	O
user	O
name	O
,	O
email	O
address	O
,	O
order	O
i	O
d	O
AGT	O
:	O
Membershp	O
level	O
and	O
reason	O
for	O
return	O
CUS	O
:	O
Alessandro	O
Phoenix	O
,	O
aphoenix872@email.com	O
,	O
order	O
ID	O
is	O
4024067912	O
CUS	O
:	O
I	O
'	O
m	O
at	O
the	O
Gold	O
level	O
.	O

I	O
'	O
m	O
returning	O
it	O
because	O
it	O
's	O
the	O
wrong	O
size	O
Chit	O
-	O
Chat	O
AGT	O
:	O
Do	O
you	O
need	O
any	O
more	O
help	O
?	O
CUS	O
:	O
a	O
break	O
,	O
I	O
need	O
a	O
coffee	O
break	O
CUS	O
:	O
but	O
no	O
,	O
nothing	O
from	O
you	O
CUS	O
:	O
thanks	O
for	O
the	O
save	O
AGT	O
:	O
Haha	O
have	O
a	O
good	O
break	O
!	O
And	O
have	O
an	O
even	O
better	O
day	O
.	O

Emotion	O
AGT	O
:	O
Ok	O
,	O
there	O
was	O
a	O
mistake	O
made	O
.	O

Do	O
you	O
have	O
the	O
Shipping	O
Status	O
?	O
CUS	O
:	O
It	O
's	O
in	O
transit	O
AGT	O
:	O
Ok	O
,	O
that	O
means	O
it	O
's	O
already	O
out	O
for	O
shipment	O
CUS	O
:	O
so	O
two	O
are	O
being	O
sent	O
?	O
AGT	O
:	O
Yes	O
.	O

Unfortunately	O
that	O
means	O
when	O
you	O
get	O
the	O
item	O
you	O
will	O
need	O
to	O
call	O
back	O
and	O
make	O
a	O
return	O
CUS	O
:	O
oh	O
you	O
got	O
ta	O
be	O
kidding	O
me	O
!	O
shows	O
an	O
on	O
-	O
going	O
conversation	O
with	O
customer	O
messages	O
in	O
grey	O
,	O
agent	O
messages	O
in	O
blue	O
,	O
and	O
actions	O
in	O
green	O
.	O

The	O
customer	O
prompt	O
(	O
right	O
top	O
)	O
grounds	O
the	O
customer	O
to	O
a	O
specific	O
issue	O
and	O
backstory	O
.	O

The	O
info	O
sections	O
(	O
right	O
middle	O
and	O
bottom	O
)	O
contains	O
values	O
that	O
the	O
customer	O
has	O
to	O
provide	O
in	O
the	O
conversation	O
as	O
well	O
as	O
other	O
meta	O
-	O
data	O
such	O
as	O
product	O
information	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Tao	O
Lei	O
,	O
Felix	O
Wu	O
and	O
Anmol	O
Kabra	O
for	O
their	O
feedback	O
and	O
support	O
.	O

We	O
would	O
also	O
like	O
to	O
thank	O
the	O
anonymous	O
NAACL	O
2021	O
reviewers	O
for	O
pointing	O
out	O
specific	O
areas	O
of	O
confusion	O
in	O
our	O
submission	O
,	O
which	O
we	O
have	O
tried	O
our	O
best	O
to	O
clarify	O
.	O

This	O
paper	O
presents	O
a	O
new	O
dataset	O
which	O
was	O
collected	O
through	O
the	O
use	O
of	O
crowdworkers	O
.	O

All	O
agent	O
workers	O
were	O
compensated	O
a	O
fair	O
wage	O
based	O
on	O
their	O
local	O
standard	O
of	O
living	O
,	O
where	O
their	O
location	O
was	O
determined	O
during	O
the	O
vetting	O
process	O
.	O
(	O

Please	O
refer	O
to	O
Appendix	O
A	O
for	O
more	O
details	O
.	O
)	O

Nowadays	O
,	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
which	O
aims	O
to	O
verify	O
whether	O
a	O
news	O
document	O
is	O
trusted	O
or	O
fake	O
,	O
has	O
become	O
urgent	O
and	O
important	O
.	O

Most	O
existing	O
methods	O
rely	O
heavily	O
on	O
linguistic	O
and	O
semantic	O
features	O
from	O
the	O
news	O
content	O
,	O
and	O
fail	O
to	O
effectively	O
exploit	O
external	O
knowledge	O
which	O
could	O
help	O
determine	O
whether	O
the	O
news	O
document	O
is	O
trusted	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
called	O
CompareNet	B-MethodName
,	O
which	O
compares	O
the	O
news	O
to	O
the	O
knowledge	O
base	O
(	O
KB	O
)	O
through	O
entities	O
for	B-TaskName
fake	I-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Considering	O
that	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
is	O
correlated	O
with	O
topics	O
,	O
we	O
also	O
incorporate	O
topics	O
to	O
enrich	O
the	O
news	O
representation	O
.	O

Specifically	O
,	O
we	O
first	O
construct	O
a	O
directed	B-MethodName
heterogeneous	I-MethodName
document	I-MethodName
graph	I-MethodName
for	O
each	O
news	O
incorporating	O
topics	O
and	O
entities	O
.	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
for	O
learning	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
content	O
.	O

The	O
contextual	O
entity	O
representations	O
are	O
then	O
compared	O
to	O
the	O
corresponding	O
KB	O
-	O
based	O
entity	O
representations	O
through	O
a	O
carefully	O
designed	O
entity	O
comparison	O
network	O
,	O
to	O
capture	O
the	O
consistency	O
between	O
the	O
news	O
content	O
and	O
KB	O
.	O

Finally	O
,	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
combining	O
the	O
entity	O
comparison	O
features	O
are	O
fed	O
into	O
a	O
fake	O
news	O
classifier	O
.	O

Experimental	O
results	O
on	O
two	O
benchmark	O
datasets	O
demonstrate	O
that	O
CompareNet	B-MethodName
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

With	O
the	O
rapid	O
development	O
of	O
the	O
Internet	O
,	O
there	O
are	O
increasingly	O
huge	O
opportunities	O
for	O
fake	O
news	O
*	O
The	O
work	O
was	O
done	O
while	O
visiting	O
Micorosft	O
Research	O
Asia.production	O
,	O
dissemination	O
and	O
consumption	O
.	O

Fake	O
news	O
are	O
news	O
documents	O
that	O
are	O
intentionally	O
and	O
verifiably	O
false	O
,	O
and	O
could	O
mislead	O
readers	O
(	O
Allcott	O
and	O
Gentzkow	O
,	O
2017	O
)	O
.	O

Fake	O
news	O
can	O
easily	O
misguide	O
public	O
opinion	O
,	O
cause	O
the	O
crisis	O
of	O
confidence	O
,	O
and	O
disturb	O
the	O
social	O
order	O
(	O
Vosoughi	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

It	O
is	O
well	O
known	O
that	O
fake	O
news	O
exerted	O
an	O
influence	O
in	O
the	O
past	O
2016	O
US	O
presidential	O
elections	O
(	O
Allcott	O
and	O
Gentzkow	O
,	O
2017	O
)	O
.	O

Thus	O
,	O
it	O
is	O
very	O
important	O
to	O
develop	O
effective	O
methods	O
for	O
early	O
fake	O
news	O
detection	O
based	O
on	O
the	O
textual	O
content	O
of	O
the	O
news	O
document	O
.	O

Some	O
existing	O
fake	O
news	O
detection	O
methods	O
rely	O
heavily	O
on	O
various	O
hand	O
-	O
crafted	O
linguistic	O
and	O
semantic	O
features	O
for	O
differentiating	O
between	O
news	O
documents	O
(	O
Conroy	O
et	O
al	O
.	O
,	O

2015;Rubin	O
et	O
al	O
.	O
,	O

2016;Rashkin	O
et	O
al	O
.	O
,	O

2017;Khurana	O
and	O
Intelligentie	O
,	O
2017;Shu	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

To	O
avoid	O
feature	O
engineering	O
,	O
deep	O
neural	O
models	O
such	O
as	O
Bi	B-MethodName
-	I-MethodName
LSTM	I-MethodName
and	I-MethodName
convolutional	I-MethodName
neural	I-MethodName
networks	I-MethodName
(	O
CNN	B-MethodName
)	O
have	O
been	O
employed	O
(	O
Oshikawa	O
et	O
al	O
.	O
,	O

2020;Wang	O
,	O
2017;Rodríguez	O
and	O
Iglesias	O
,	O
2019	O
)	O
.	O

However	O
,	O
they	O
fail	O
to	O
consider	O
the	O
sentence	O
interactions	O
in	O
the	O
document	O
.	O

Vaibhav	O
et	O
al	O
.	O

showed	O
that	O
trusted	O
news	O
and	O
fake	O
news	O
have	O
different	O
patterns	O
of	O
sentence	O
interactions	O
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

They	O
modeled	O
a	O
news	O
document	O
as	O
a	O
fully	O
connected	O
sentence	O
graph	O
and	O
proposed	O
a	O
graph	B-MethodName
attention	I-MethodName
model	I-MethodName
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Although	O
these	O
existing	O
approaches	O
can	O
be	O
effective	O
,	O
they	O
fail	O
to	O
fully	O
exploit	O
external	O
KB	O
which	O
could	O
help	O
determine	O
whether	O
the	O
news	O
is	O
fake	O
or	O
trusted	O
.	O

External	O
KB	O
such	O
as	O
Wikipedia	B-DatasetName
contains	O
a	O
large	O
amount	O
of	O
high	O
-	O
quality	O
structured	O
subjectpredicate	O
-	O
object	O
triplets	O
and	O
unstructured	O
entity	O
descriptions	O
,	O
which	O
could	O
serve	O
as	O
evidence	O
for	O
detecting	O
fake	O
news	O
.	O

As	O
shown	O
in	O
Figure	O
4	O
,	O
the	O
news	O
document	O
about	O
"	O
mammograms	O
are	O
not	O
effective	O
at	O
detecting	O
breast	O
tumors	O
"	O
is	O
likely	O
to	O
be	O
detected	O
as	O
fake	O
news	O
with	O
the	O
knowledge	O
that	O
"	O
The	O
goal	O
of	O
mammography	O
is	O
the	O
early	O
detection	O
of	O
breast	O
cancer	O
"	O
in	O
the	O
Wikipedia	O
entity	O
description	O
page	O
1	O
.	O

Pan	O
et	O
al	O
.	O

proposed	O
to	O
construct	O
knowledge	O
graphs	O
from	O
positive	O
and	O
negative	O
news	O
,	O
and	O
apply	O
TransE	B-MethodName
to	O
learn	O
triplet	O
scores	O
for	O
fake	O
news	O
detection	O
(	O
Pan	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Nevertheless	O
,	O
the	O
performance	O
is	O
largely	O
influenced	O
by	O
construction	O
of	O
the	O
knowledge	O
graph	O
.	O

In	O
this	O
paper	O
,	O
to	O
take	O
full	O
advantage	O
of	O
the	O
external	O
knowledge	O
,	O
we	O
propose	O
a	O
novel	O
endto	B-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
CompareNet	I-MethodName
which	O
directly	O
compares	O
the	O
news	O
to	O
the	O
KB	O
through	O
entities	O
for	O
fake	O
news	O
detection	O
.	O

In	O
CompareNet	B-MethodName
,	O
we	O
also	O
consider	O
using	O
topics	O
to	O
enrich	O
the	O
news	O
document	O
representation	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
since	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
and	O
topics	O
are	O
highly	O
correlated	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020;Jin	O
et	O
al	O
.	O
,	O

2016	O
)	O
.	O

For	O
example	O
,	O
the	O
news	O
documents	O
in	O
the	O
"	O
health	O
"	O
topic	O
are	O
inclined	O
towards	O
false	O
,	O
while	O
the	O
documents	O
belonging	O
to	O
the	O
"	O
economy	O
"	O
topic	O
are	O
biased	O
to	O
be	O
trusted	O
instead	O
.	O

Particularly	O
,	O
we	O
first	O
construct	O
a	O
directed	O
heterogeneous	O
document	O
graph	O
for	O
each	O
news	O
document	O
,	O
containing	O
sentences	O
,	O
topics	O
and	O
entities	O
as	O
nodes	O
.	O

The	O
sentences	O
are	O
fully	O
connected	O
in	O
bidirection	O
.	O

Each	O
sentence	O
is	O
also	O
connected	O
with	O
its	O
top	O
relevant	O
topics	O
in	O
bi	O
-	O
direction	O
.	O

If	O
a	O
sentence	O
contains	O
an	O
entity	O
,	O
one	O
directed	O
link	O
is	O
built	O
from	O
the	O
sentence	O
to	O
the	O
entity	O
.	O

The	O
reason	O
for	O
building	O
one	O
-	O
way	O
links	O
from	O
sentences	O
to	O
entities	O
is	O
to	O
ensure	O
that	O
we	O
can	O
learn	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
,	O
while	O
avoiding	O
the	O
influence	O
of	O
the	O
true	O
entity	O
knowledge	O
to	O
the	O
news	O
representation	O
.	O

Based	O
on	O
the	O
directed	O
heterogeneous	O
document	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
topic	O
-	O
enriched	O
news	O
representations	O
and	O
contextual	O
entity	O
representations	O
.	O

The	O
learned	O
contextual	O
entity	O
representations	O
are	O
then	O
compared	O
to	O
the	O
corresponding	O
KB	O
-	O
based	O
entity	O
representations	O
with	O
a	O
carefully	O
designed	O
entity	O
comparison	O
network	O
,	O
in	O
order	O
to	O
capture	O
the	O
semantic	O
consistency	O
between	O
the	O
news	O
content	O
and	O
external	O
KB	O
.	O

Finally	O
,	O
the	O
topic	O
-	O
enriched	O
news	O
representations	O
and	O
the	O
entity	O
comparison	O
features	O
are	O
combined	O
for	O
fake	O
news	O
classification	O
.	O

To	O
facilitate	O
related	O
researches	O
,	O
we	O
release	O
both	O
our	O
code	O
and	O
dataset	O
to	O
the	O
public	O
2	O
.1	O
https://en.wikipedia.org/wiki/Mammography	O
2	O
https://github.com/ytc272098215/FakeNewsDetection	O
In	O
summary	O
,	O
our	O
main	O
contributions	O
include	O
:	O
1	O
)	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
CompareNet	I-MethodName
which	O
compares	O
the	O
news	O
to	O
the	O
external	O
knowledge	O
through	O
entities	O
for	O
fake	O
news	O
detection.2	O
)	O
In	O
CompareNet	B-MethodName
,	O
we	O
also	O
consider	O
the	O
useful	O
topic	O
information	O
.	O

We	O
construct	O
a	O
directed	O
heterogeneous	O
document	O
graph	O
incorporating	O
topics	O
and	O
entities	O
.	O

Then	O
we	O
develop	O
heterogeneous	O
graph	O
attention	O
networks	O
to	O
learn	O
topicenriched	O
news	O
representations	O
.	O

A	O
novel	O
entity	O
comparison	O
network	O
is	O
designed	O
to	O
compare	O
the	O
news	O
to	O
the	O
KB.3	O
)	O
Extensive	O
experiments	O
on	O
two	O
benchmark	O
datasets	O
demonstrate	O
that	O
our	O
model	O
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
fake	O
news	O
detection	O
by	O
effectively	O
incorporating	O
external	O
knowledge	O
and	O
topic	O
information	O
.	O

Fake	O
news	O
detection	O
has	O
attracted	O
much	O
attention	O
in	O
recent	O
years	O
(	O
Zhou	O
and	O
Zafarani	O
,	O
2020;Oshikawa	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

A	O
lot	O
of	O
works	O
also	O
focus	O
on	O
the	O
related	O
problem	O
,	O
i.e.	O
,	O
fact	O
checking	O
,	O
which	O
aims	O
to	O
search	O
evidence	O
from	O
external	O
knowledge	O
to	O
verify	O
the	O
veracity	O
of	O
a	O
claim	O
(	O
e.g.	O
,	O
a	O
subject	O
-	O
predicateobject	O
triple	O
)	O
(	O
Thorne	O
et	O
al	O
.	O
,	O

2018;Zhong	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Generally	O
,	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
usually	O
focuses	O
on	O
news	O
events	O
while	O
fact	O
-	O
checking	O
is	O
broader	O
(	O
Oshikawa	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
approaches	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
can	O
be	O
divided	O
into	O
two	O
categories	O
:	O
social	O
-	O
based	O
and	O
content	O
-	O
based	O
.	O

Social	O
context	O
related	O
to	O
news	O
documents	O
contains	O
rich	O
information	O
such	O
as	O
user	O
profiles	O
and	O
social	O
relationships	O
to	O
help	O
detect	O
fake	O
news	O
.	O

Social	B-MethodName
based	I-MethodName
models	O
basically	O
include	O
stance	B-MethodName
-	I-MethodName
based	I-MethodName
and	O
propagation	B-MethodName
-	I-MethodName
based	I-MethodName
.	O

Stance	B-MethodName
-	I-MethodName
based	I-MethodName
models	O
utilize	O
users	O
'	O
opinions	O
to	O
infer	O
news	O
veracity	O
(	O
Jin	O
et	O
al	O
.	O
,	O

2016;Wu	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Tacchini	O
et	O
al	O
.	O

constructed	O
a	O
bipartite	O
network	O
of	O
user	O
and	O
posts	O
with	O
'	O
like	O
'	O
stance	O
information	O
,	O
and	O
proposed	O
a	O
semisupervised	O
probabilistic	O
model	O
to	O
predict	O
the	O
likelihood	O
of	O
posts	O
being	O
hoaxes	O
(	O
Tacchini	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

Propagation	B-MethodName
-	I-MethodName
based	I-MethodName
approaches	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
are	O
based	O
on	O
the	O
basic	O
assumption	O
that	O
the	O
credibility	O
of	O
a	O
news	O
event	O
is	O
highly	O
related	O
to	O
the	O
credibilities	O
of	O
relevant	O
social	O
media	O
posts	O
.	O

Both	O
homogeneous	B-MethodName
(	O
Jin	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
heterogeneous	B-MethodName
credibility	I-MethodName
networks	I-MethodName
(	O
Gupta	O
et	O
al	O
.	O
,	O

2012;Shu	O
et	O
al	O
.	O
,	O

2019;Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
have	O
been	O
built	O
to	O
model	O
the	O
propagation	O
process	O
.	O

For	O
instance	O
,	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
constructed	O
a	O
heterogeneous	O
network	O
of	O
news	O
articles	O
,	O
creators	O
and	O
news	O
subjects	O
,	O
and	O
proposed	O
a	O
deep	B-MethodName
diffusive	I-MethodName
network	I-MethodName
model	O
for	O
incorporating	O
the	O
network	O
structure	O
information	O
to	O
simultaneously	O
detect	B-TaskName
fake	I-TaskName
news	I-TaskName
articles	I-TaskName
,	O
creators	O
and	O
subjects	O
.	O

On	O
the	O
other	O
hand	O
,	O
news	O
contents	O
contain	O
the	O
clues	O
to	O
differentiate	O
fake	O
and	O
trusted	O
news	O
.	O

A	O
lot	O
of	O
existing	O
works	O
extract	O
specific	O
writing	O
styles	O
such	O
as	O
lexical	O
and	O
syntactic	O
features	O
(	O
Conroy	O
et	O
al	O
.	O
,	O

2015;Rubin	O
et	O
al	O
.	O
,	O

2016;Khurana	O
and	O
Intelligentie	O
,	O
2017;Rashkin	O
et	O
al	O
.	O
,	O

2017;Shu	O
et	O
al	O
.	O
,	O

2020;Oshikawa	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
sensational	O
headlines	O
(	O
Potthast	O
et	O
al	O
.	O
,	O

2018;Sitaula	O
et	O
al	O
.	O
,	O

2019	O
)	O
for	O
fake	O
news	O
classifier	O
.	O

To	O
avoid	O
hand	O
-	O
crafted	O
feature	O
engineering	O
,	O
neural	O
models	O
have	O
been	O
proposed	O
(	O
Wang	O
,	O
2017;Rodríguez	O
and	O
Iglesias	O
,	O
2019	O
2019	O
)	O
.	O

However	O
,	O
these	O
works	O
fail	O
to	O
consider	O
different	O
sentence	O
interaction	O
patterns	O
between	O
trusted	O
and	O
fake	O
news	O
documents	O
.	O

Vaibhav	O
et	O
al	O
.	O

proposed	O
to	O
model	O
a	O
document	O
as	O
a	O
sentence	O
graph	O
capturing	O
the	O
sentence	O
interactions	O
and	O
applied	O
graph	O
attention	O
networks	O
for	O
learning	O
document	O
representation	O
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Pan	O
et	O
al	O
.	O

proposed	O
to	O
construct	O
knowledge	O
graphs	O
from	O
positive	O
and	O
negative	O
news	O
,	O
and	O
apply	O
TransE	B-MethodName
to	O
learn	O
triplet	O
scores	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
(	O
Pan	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Nevertheless	O
,	O
they	O
relied	O
heavily	O
on	O
the	O
quality	O
of	O
the	O
construction	O
of	O
knowledge	O
graphs	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
graph	O
neural	O
model	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
which	O
directly	O
compares	O
the	O
news	O
to	O
external	O
knowledge	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Considering	O
that	O
the	O
detection	B-TaskName
of	I-TaskName
fake	I-TaskName
news	I-TaskName
is	O
correlated	O
with	O
topics	O
,	O
we	O
also	O
use	O
topics	O
to	O
enrich	O
the	O
news	O
representation	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Some	O
works	O
(	O
Wang	O
,	O
2017;Khattar	O
et	O
al	O
.	O
,	O

2019	O
;)	O
also	O
consider	O
incorporating	O
multi	O
-	O
modal	O
features	O
such	O
as	O
images	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
detail	O
our	O
proposed	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
model	O
CompareNet	B-MethodName
,	O
which	O
directly	O
compares	O
the	O
news	O
to	O
external	O
knowledge	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

As	O
shown	O
in	O
Figure	O
2	O
,	O
we	O
also	O
consider	O
topics	O
for	O
enriching	O
news	O
representation	O
since	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
is	O
highly	O
correlated	O
with	O
topics	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Specifically	O
,	O
we	O
first	O
construct	O
a	O
directed	O
heterogeneous	O
document	O
graph	O
for	O
each	O
news	O
document	O
incorporating	O
topics	O
and	O
entities	O
as	O
shown	O
in	O
Figure	O
1	O
.	O

The	O
graph	O
well	O
captures	O
the	O
interactions	O
among	O
sentences	O
,	O
topics	O
and	O
entities	O
.	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
document	O
.	O

To	O
fully	O
leverage	O
external	O
KB	O
,	O
we	O
take	O
the	O
entities	O
as	O
the	O
bridge	O
between	O
the	O
news	O
document	O
and	O
the	O
KB	O
.	O

We	O
compare	O
the	O
contextual	O
entity	O
representations	O
with	O
the	O
corresponding	O
KB	O
-	O
based	O
entity	O
representations	O
using	O
a	O
carefully	O
designed	O
entity	O
comparison	O
network	O
.	O

Finally	O
,	O
the	O
obtained	O
entity	O
comparison	O
features	O
are	O
combined	O
with	O
the	O
topic	O
-	O
enriched	O
news	O
document	O
representation	O
for	O
fake	O
news	O
detection	O
.	O

For	O
each	O
news	O
document	O
d	O
,	O
we	O
construct	O
a	O
directed	O
heterogeneous	O
document	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
incorporating	O
topics	O
and	O
entities	O
,	O
as	O
shown	O
in	O
Figure	O
1	O
.	O

There	O
are	O
three	O
kinds	O
of	O
nodes	O
in	O
the	O
graph	O
:	O
sentences	O
S	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
•	O
•	O
•	O
,	O
s	O
m	O
}	O
,	O
topics	O
T	O
=	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
•	O
•	O
•	O
,	O
t	O
K	O
}	O
and	O
entities	O
E	O
=	O
{	O
e	O
1	O
,	O
e	O
2	O
,	O
•	O
•	O
•	O
,	O
e	O
n	O
}	O
,	O
i.e.	O
,	O
V	O
=	O
S	O
∪	O
T	O
∪	O
E.The	O
set	O
of	O
edges	O
E	O
represent	O
the	O
relations	O
among	O
sentences	O
,	O
topics	O
and	O
entities	O
.	O

The	O
details	O
of	O
constructing	O
the	O
graph	O
are	O
described	O
as	O
follows	O
.	O

We	O
first	O
split	O
the	O
news	O
document	O
as	O
a	O
set	O
of	O
sentences	O
.	O

Sentences	O
are	O
bidirectionally	O
connected	O
with	O
each	O
other	O
in	O
the	O
graph	O
,	O
capturing	O
the	O
interaction	O
of	O
each	O
sentence	O
with	O
every	O
other	O
sentence	O
.	O

Since	O
topic	O
information	O
is	O
important	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
apply	O
the	O
unsupervised	B-MethodName
LDA	I-MethodName
(	O
Blei	O
et	O
al	O
.	O
,	O

2003	O
)	O
(	O
the	O
total	B-HyperparameterName
topic	I-HyperparameterName
number	I-HyperparameterName
K	B-HyperparameterName
is	O
set	O
as	O
100	B-HyperparameterValue
)	O
to	O
mine	O
the	O
latent	O
topics	O
T	O
from	O
all	O
the	O
sentences	O
of	O
all	O
the	O
documents	O
in	O
our	O
dataset	O
.	O

Specifically	O
,	O
each	O
sentence	O
is	O
taken	O
as	O
a	O
pseudo	O
-	O
document	O
and	O
is	O
assigned	O
to	O
the	O
top	B-HyperparameterName
P	I-HyperparameterName
relevant	I-HyperparameterName
topics	I-HyperparameterName
with	O
the	O
largest	O
probabilities	O
.	O

Thus	O
,	O
each	O
sentence	O
is	O
also	O
connected	O
with	O
its	O
top	B-HyperparameterName
P	I-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
in	O
bi	O
-	O
direction	O
,	O
allowing	O
the	O
useful	O
topic	O
information	O
to	O
propagate	O
among	O
the	O
sentences	O
.	O

Note	O
that	O
we	O
can	O
also	O
deal	O
with	O
new	O
coming	O
news	O
documents	O
by	O
inferring	O
the	O
topics	O
with	O
trained	O
LDA	B-MethodName
.	O

We	O
identify	O
the	O
entities	O
E	O
in	O
the	O
document	O
d	O
and	O
map	O
them	O
to	O
Wikipedia	B-DatasetName
using	O
the	O
entity	O
linking	O
tool	O
TAGME	O
3	O
.	O

If	O
a	O
sentence	O
s	O
contains	O
an	O
entity	O
e	O
,	O
we	O
build	O
a	O
one	O
-	O
way	O
directed	O
edge	O
from	O
a	O
sentence	O
to	O
the	O
entity	O
e	O
,	O
in	O
order	O
to	O
allow	O
only	O
information	O
propagation	O
from	O
sentences	O
to	O
entities	O
.	O

In	O
this	O
way	O
,	O
we	O
can	O
avoid	O
integrating	O
true	O
entity	O
knowledge	O
directly	O
into	O
news	O
representation	O
,	O
which	O
may	O
mislead	O
the	O
detection	O
of	O
fake	O
news	O
.	O

Based	O
on	O
the	O
above	O
directed	O
heterogeneous	O
document	O
graph	O
G	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
for	O
learning	O
the	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
.	O

It	O
considers	O
not	O
only	O
the	O
weights	O
of	O
different	O
nodes	O
with	O
different	O
types	O
(	O
Hu	O
et	O
al	O
.	O
,	O

2019	O
)	O
but	O
also	O
the	O
edge	O
directions	O
in	O
the	O
heterogeneous	O
graph	O
.	O

Formally	O
,	O
we	O
have	O
three	O
types	O
T	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
τ	O
3	O
}	O
of	O
nodes	O
:	O
sentences	O
S	O
,	O
topics	O
T	O
and	O
entities	O
E	O
with	O
different	O
feature	O
spaces	O
.	O

We	O
apply	O
LSTM	O
to	O
encode	O
a	O
sentence	O
s	O
=	O
{	O
w	O
1	O
,	O
•	O
•	O
•	O
,	O
w	O
m	O
}	O
and	O
get	O
its	O
feature	O
vector	O
x	O
s	O
∈	O
R	O
M	O
.	O

The	O
entity	O
e	O
∈	O
E	O
is	O
initialized	O
with	O
the	O
entity	O
representations	O
e	O
KB	O
∈	O
R	O
M	O
learned	O
from	O
the	O
external	O
KB	O
(	O
see	O
Subsection	O
3.3.1	O
)	O
.	O

The	O
topic	O
t	O
∈	O
T	O
is	O
initialized	O
with	O
one	O
-	O
hot	O
vector	O
x	O
t	O
∈	O
R	O
K	O
.Next	O
,	O
consider	O
the	O
graph	O
G	O
=	O
(	O
V	O
,	O
E	O
)	O
where	O
V	O
and	O
E	O
represent	O
the	O
set	O
of	O
nodes	O
and	O
edges	O
respectively	O
.	O

Let	O
X	O
∈	O
R	O
|V|×M	O
be	O
a	O
matrix	O
containing	O
the	O
nodes	O
with	O
their	O
features	O
x	O
v	O
∈	O
R	O
M	O
(	O
each	O
row	O
x	O
v	O
is	O
a	O
feature	O
vector	O
for	O
a	O
node	O
v	O
)	O
.	O

A	O
and	O
D	O
are	O
the	O
adjacency	O
matrix	O
and	O
the	O
degree	O
matrix	O
,	O
respectively	O
.	O

The	O
heterogeneous	O
convolution	O
layer	O
updates	O
the	O
(	O
l	O
+	O
1)-th	O
layer	O
representation	O
of	O
the	O
nodes	O
H	O
(	O
l+1	O
)	O
by	O
aggregating	O
the	O
features	O
of	O
their	O
neighboring	O
nodes	O
H	O
(	O
l	O
)	O
τ	O
with	O
different	O
types	O
τ	O
.	O
(	O

Initially	O
,	O
H	O
(	O
0	O
)	O
=	O
X):H	O
(	O
l+1	O
)	O
=	O
σ	O
(	O
τ	O
∈T	O
B	O
τ	O
•	O
H	O
(	O
l	O
)	O
τ	O
•	O
W	O
(	O
l	O
)	O
τ	O
)	O
,	O
(	O
1)where	O
σ(•	O
)	O
denotes	O
the	O
activation	O
function	O
.	O

Nodes	O
with	O
different	O
types	O
τ	O
have	O
different	O
transformation	O
matrix	O
W(l)τ	O
.	O

The	O
transformation	O
matrix	O
W	O
(	O
l)τ	O
considers	O
the	O
different	O
feature	O
spaces	O
and	O
projects	O
them	O
into	O
an	O
implicit	O
common	O
space	O
.	O

B	O
τ	O
∈	O
R	O
|V|×|Vτ	O
|	O
is	O
the	O
attention	O
matrix	O
,	O
whose	O
rows	O
represent	O
all	O
the	O
nodes	O
and	O
columns	O
represent	O
their	O
neighboring	O
nodes	O
with	O
the	O
type	O
τ	O
.	O

Its	O
element	O
β	O
vv	O
in	O
the	O
v	O
-	O
th	O
row	O
and	O
the	O
v	O
-th	O
column	O
is	O
computed	O
as	O
follows	O
:	O
β	O
vv	O
=	O
Softmax	O
v	O
(	O
σ(ν	O
T	O
•	O
α	O
τ	O
[	O
h	O
v	O
,	O
h	O
v	O
]	O
)	O
)	O
,	O
(	O
2)where	O
ν	O
is	O
the	O
attention	O
vector	O
and	O
α	O
τ	O
is	O
the	O
typelevel	O
attention	O
weight	O
.	O

h	O
v	O
and	O
h	O
v	O
are	O
respectively	O
the	O
representation	O
of	O
the	O
current	O
node	O
v	O
and	O
its	O
neighboring	O
node	O
v	O
.	O

Softmax	O
function	O
is	O
applied	O
to	O
normalize	O
across	O
the	O
neighboring	O
nodes	O
of	O
node	O
v.	O
We	O
calculate	O
the	O
type	O
-	O
level	O
attention	O
weights	O
α	O
τ	O
based	O
on	O
the	O
current	O
node	O
embedding	O
h	O
v	O
and	O
the	O
type	O
embedding	O
h	O
τ	O
=	O
v	O
Ã	O
vv	O
h	O
v	O
(	O
the	O
weighted	O
sum	O
of	O
the	O
neighboring	O
node	O
embeddings	O
h	O
v	O
with	O
the	O
type	O
τ	O
,	O
where	O
the	O
weight	O
matrixÃ	O
=	O
D	O
−	O
1	O
2	O
(	O
A+	O
I)D	O
−	O
1	O
2	O
is	O
the	O
normalized	O
adjacency	O
matrix	O
with	O
added	O
self	O
-	O
connections	O
)	O
as	O
follows	O
:	O
α	O
τ	O
=	O
Softmax	O
τ	O
(	O
σ(µ	O
T	O
τ	O
•	O
[	O
h	O
v	O
,	O
h	O
τ	O
]	O
)	O
)	O
,	O
(	O
3)where	O
µ	O
τ	O
is	O
the	O
attention	O
vector	O
for	O
the	O
type	O
τ	O
.	O

Softmax	O
function	O
is	O
applied	O
to	O
normalize	O
across	O
all	O
the	O
types	O
.	O

After	O
L	O
-	O
layer	O
heterogeneous	O
graph	O
convolution	O
,	O
we	O
can	O
finally	O
get	O
all	O
the	O
node	O
(	O
including	O
sentences	O
and	O
entities	O
)	O
representations	O
aggregating	O
neighborhood	O
semantics	O
.	O

We	O
use	O
max	O
pooling	O
over	O
the	O
representations	O
of	O
the	O
sentence	O
nodes	O
H	O
s	O
∈	O
R	O
N	O
to	O
obtain	O
the	O
final	O
topic	O
-	O
enriched	O
news	O
document	O
embedding	O
H	O
d	O
∈	O
R	O
N	O
.	O

The	O
learned	O
entity	O
representations	O
that	O
encode	O
the	O
contextual	O
semantics	O
of	O
the	O
document	O
are	O
taken	O
as	O
contextual	O
entity	O
representations	O
e	O
c	O
∈	O
R	O
N	O
.	O

In	O
this	O
subsection	O
,	O
we	O
detail	O
our	O
entity	O
comparison	O
network	O
which	O
compares	O
the	O
learned	O
contextual	O
entity	O
embeddings	O
e	O
c	O
to	O
the	O
corresponding	O
KBbased	O
entity	O
embeddings	O
e	O
KB	O
.	O

We	O
believe	O
entity	O
comparison	O
features	O
could	O
improve	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
based	O
on	O
the	O
assumption	O
that	O
e	O
c	O
learned	O
from	O
trusted	O
news	O
document	O
can	O
be	O
better	O
aligned	O
with	O
the	O
corresponding	O
e	O
KB	O
;	O
while	O
inverse	O
for	O
fake	O
news	O
.	O

We	O
first	O
illustrate	O
how	O
to	O
take	O
full	O
advantage	O
of	O
both	O
structured	O
subject	O
-	O
predicate	O
-	O
object	O
triplets	O
and	O
unstructured	O
textual	O
entity	O
descriptions	O
in	O
the	O
KB	O
(	O
i.e.	O
,	O
Wikipedia	O
)	O
to	O
learn	O
KB	O
-	O
based	O
entity	O
representations	O
e	O
KB	O
.Structural	O
Embedding	O
.	O

A	O
wide	O
range	O
of	O
knowledge	O
graph	O
embedding	O
methods	O
can	O
be	O
applied	O
to	O
obtain	O
structured	O
entity	O
embeddings	O
.	O

Due	O
to	O
the	O
simplicity	O
of	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
we	O
adopted	O
TransE	B-MethodName
to	O
learn	O
entity	O
representations	O
e	O
s	O
∈	O
R	O
M	O
from	O
the	O
triplets	O
.	O

Formally	O
,	O
given	O
a	O
triplet	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
TransE	B-MethodName
regards	O
a	O
relationship	O
r	O
as	O
a	O
translation	O
vector	O
r	O
from	O
the	O
head	O
entity	O
h	O
to	O
the	O
tail	O
entity	O
t	O
,	O
namely	O
h	O
+	O
r	O
=	O
t.	O
Textual	O
Embedding	O
.	O

For	O
each	O
entity	O
,	O
we	O
take	O
the	O
first	O
paragraph	O
of	O
the	O
corresponding	O
Wikipedia	O
page	O
as	O
its	O
text	O
description	O
.	O

Then	O
we	O
apply	O
LSTM	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
to	O
learn	O
entity	O
representations	O
e	O
d	O
∈	O
R	O
M	O
that	O
encode	O
the	O
entity	O
descriptions	O
.	O

Gating	O
Integration	O
.	O

Since	O
both	O
the	O
structural	O
triplets	O
and	O
textual	O
description	O
provide	O
valuable	O
information	O
for	O
an	O
entity	O
,	O
we	O
integrate	O
these	O
information	O
into	O
a	O
joint	O
representation	O
.	O

Particularly	O
,	O
as	O
we	O
have	O
the	O
structural	O
embedding	O
e	O
s	O
and	O
textual	O
embedding	O
e	O
d	O
,	O
we	O
adopt	O
a	O
learnable	O
gating	O
function	O
to	O
integrate	O
entity	O
embeddings	O
from	O
the	O
two	O
sources	O
.	O

Formally	O
,	O
e	O
KB	O
=	O
g	O
e	O
e	O
s	O
+	O
(	O
1	O
−	O
g	O
e	O
)	O
e	O
d	O
,	O
(	O
4)where	O
g	O
e	O
∈	O
R	O
M	O
is	O
a	O
gating	O
vector	O
(	O
w.r.t	O
.	O

the	O
entity	O
e	O
)	O
to	O
trade	O
-	O
off	O
information	O
from	O
the	O
two	O
sources	O
and	O
its	O
elements	O
are	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O

denotes	O
elementwise	O
multiplication	O
.	O

The	O
gating	O
vector	O
g	O
e	O
means	O
that	O
each	O
dimension	O
of	O
e	O
s	O
and	O
e	O
d	O
are	O
summed	O
by	O
different	O
weights	O
.	O

To	O
constrain	O
the	O
value	O
of	O
each	O
element	O
in	O
[	O
0	O
,	O
1	O
]	O
,	O
we	O
compute	O
the	O
gate	O
g	O
e	O
with	O
the	O
Sigmoid	O
function	O
:	O
g	O
e	O
=	O
σ(g	O
e	O
)	O
,	O
(	O
5)whereg	O
e	O
∈	O
R	O
M	O
is	O
a	O
real	O
-	O
value	O
vector	O
and	O
is	O
learned	O
in	O
the	O
training	O
process	O
.	O

After	O
fusing	O
the	O
two	O
types	O
of	O
embeddings	O
with	O
the	O
gating	O
function	O
,	O
we	O
obtain	O
the	O
final	O
KB	O
-	O
based	O
entity	O
embeddings	O
e	O
KB	O
∈	O
R	O
M	O
which	O
encode	O
both	O
structural	O
information	O
from	O
the	O
triplets	O
and	O
textual	O
information	O
from	O
the	O
entity	O
descriptions	O
in	O
the	O
KB	O
.	O

We	O
then	O
perform	O
entity	O
-	O
to	O
-	O
entity	O
comparison	O
between	O
the	O
news	O
document	O
and	O
the	O
KB	O
,	O
to	O
capture	O
the	O
semantic	O
consistency	O
between	O
the	O
news	O
content	O
and	O
the	O
KB	O
.	O

We	O
calculate	O
a	O
comparison	O
vector	O
a	O
i	O
between	O
each	O
contextual	O
entity	O
representation	O
e	O
c	O
∈	O
R	O
N	O
and	O
its	O
corresponding	O
KB	O
-	O
based	O
entity	O
embedding	O
e	O
KB	O
∈	O
R	O
M	O
.a	O
i	O
=	O
f	O
cmp	O
(	O
e	O
c	O
,	O
W	O
e	O
•	O
e	O
KB	O
)	O
,	O
(	O
6)where	O
f	O
cmp	O
(	O
)	O
denotes	O
the	O
comparison	O
function	O
,	O
and	O
W	O
e	O
∈	O
R	O
N	O
×M	O
is	O
a	O
transformation	O
matrix	O
.	O

To	O
measure	O
the	O
embedding	O
closeness	O
and	O
relevance	O
(	O
Shen	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
we	O
design	O
our	O
comparison	O
function	O
as	O
:	O
f	O
cmp	O
(	O
x	O
,	O
y	O
)	O
=	O
W	O
a	O
[	O
x	O
−	O
y	O
,	O
x	O
y],(7)where	O
W	O
a	O
∈	O
R	O
N	O
×2N	O
is	O
a	O
transformation	O
matrix	O
and	O
is	O
hadamard	O
product	O
,	O
i.e.	O
,	O
element	O
-	O
wise	O
product	O
.	O

The	O
final	O
output	O
comparison	O
feature	O
vector	O
C	O
∈	O
R	O
N	O
is	O
obtained	O
by	O
the	O
max	O
pooling	O
over	O
the	O
alignment	O
vectors	O
A	O
=	O
[	O
a	O
1	O
,	O
a	O
2	O
,	O
...	O
,	O
a	O
n	O
]	O
of	O
all	O
the	O
entities	O
E	O
=	O
{	O
e	O
1	O
,	O
e	O
2	O
,	O
...	O
,	O
e	O
n	O
}	O
in	O
the	O
news	O
document	O
.	O

After	O
obtaining	O
the	O
comparison	O
vector	O
C	O
∈	O
R	O
N	O
and	O
the	O
final	O
news	O
document	O
representation	O
vector	O
H	O
d	O
∈	O
R	O
N	O
,	O
we	O
concatenate	O
and	O
feed	O
them	O
into	O
a	O
Softmax	O
layer	O
for	O
fake	O
news	O
classification	O
.	O

Formally	O
,	O
Z	O
=	O
Softmax(W	O
o	O
[	O
H	O
d	O
,	O
C	O
]	O
+	O
b	O
o	O
)	O
,	O
(	O
8)where	O
L	O
=	O
−	O
i∈D	O
train	O
j=1	O
Y	O
ij	O
•	O
log	O
Z	O
ij	O
+	O
η	O
Θ	O
2	O
,	O
(	O
9)where	O
D	O
train	O
is	O
the	O
set	O
of	O
news	O
documents	O
for	O
training	O
,	O
Y	O
is	O
the	O
corresponding	O
label	O
indicator	O
matrix	O
,	O
Θ	O
is	O
the	O
model	O
parameters	O
,	O
and	O
η	O
is	O
regularization	O
factor	O
.	O

For	O
model	O
optimization	O
,	O
we	O
adopt	O
the	O
gradient	O
descent	O
algorithm	O
.	O

We	O
conduct	O
extensive	O
experiments	O
across	O
various	O
settings	O
and	O
datasets	O
.	O

Following	O
the	O
previous	O
work	O
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
we	O
use	O
SLN	B-DatasetName
:	O
Satirical	B-DatasetName
and	I-DatasetName
Legitimate	I-DatasetName
News	I-DatasetName
Database	I-DatasetName
(	O
Rubin	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
LUN	B-DatasetName
:	O
Labeled	B-DatasetName
Unreliable	I-DatasetName
News	I-DatasetName
Dataset	O
(	O
Rashkin	O
et	O
al	O
.	O
,	O

2017	O
)	O
for	O
our	O
experiments	O
.	O

Table	O
1	O
shows	O
the	O
statistics	O
.	O

Our	O
baseline	O
models	O
include	O
deep	O
neural	O
models	O
:	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
CNN	B-MethodName
(	O
Kim	O
,	O
2014	O
)	O
,	O
BERT+LSTM	B-MethodName
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
(	O
BERT	B-MethodName
for	O
sentence	O
encoder	O
and	O
then	O
LSTM	B-MethodName
for	O
document	O
encoder	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
(	O
directly	O
for	O
document	O
encoder	O
)	O
.	O

We	O
also	O
compare	O
our	O
model	O
with	O
graph	O
neural	O
models	O
:	O
GCN	B-MethodName
and	O
GAT	B-MethodName
based	O
on	O
an	O
undirected	O
fully	O
-	O
connected	O
sentence	O
graph	O
,	O
which	O
use	O
attention	O
pooling	O
or	O
max	O
pooling	O
for	O
learning	O
news	O
document	O
representation	O
.	O

For	O
fair	O
comparison	O
with	O
the	O
previous	O
work	O
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
we	O
use	O
LSTM	B-MethodName
to	O
encode	O
sentences	O
with	O
randomly	O
initialized	O
word	O
embeddings	O
,	O
which	O
is	O
the	O
same	O
as	O
all	O
the	O
graph	O
neural	O
baselines	O
.	O

We	O
run	O
our	O
model	O
5	O
times	O
and	O
report	O
the	O
micro	O
-	O
averaged	O
(	O
Precision	O
=	O
Recall	O
=	O
F1	O
)	O
and	O
macro	O
-	O
averaged	O
scores	O
(	O
Precision	O
,	O
Recall	O
,	O
F1	O
)	O
in	O
all	O
the	O
settings	O
including	O
2	O
-	O
way	O
and	O
4	O
-	O
way	O
classification.2	O
-	O
way	O
classification	O
:	O
We	O
use	O
the	O
satirical	O
and	O
trusted	O
news	O
articles	O
from	O
LUN	B-DatasetName
-	I-DatasetName
train	I-DatasetName
for	O
training	O
,	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
for	O
validation	O
and	O
evaluate	O
our	O
model	O
on	O
the	O
entire	O
SLN	B-DatasetName
dataset	O
.	O

This	O
is	O
done	O
to	O
emulate	O
a	O
real	O
-	O
world	O
scenario	O
where	O
we	O
want	O
to	O
see	O
the	O
performance	O
of	O
our	O
model	O
on	O
an	O
out	O
-	O
of	O
-	O
domain	O
dataset.4	O
-	O
way	O
classification	O
:	O
We	O
split	O
the	O
LUN	B-DatasetName
-	I-DatasetName
train	I-DatasetName
into	O
a	O
80:20	O
split	O
to	O
create	O
our	O
training	O
and	O
validation	O
set	O
.	O

We	O
use	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
as	O
our	O
in	O
-	O
domain	O
test	O
set	O
.	O

Experimental	O
Setting	O
.	O

In	O
our	O
experiments	O
,	O
we	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
topics	I-HyperparameterName
K	I-HyperparameterName
=	O
100	B-HyperparameterValue
in	O
LDA	B-MethodName
.	O

Each	O
sentence	O
is	O
assigned	O
to	O
top	B-HyperparameterName
P	I-HyperparameterName
=	O
2	B-HyperparameterValue
topics	O
with	O
the	O
largest	O
probabilities	O
.	O

The	O
layer	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
our	I-HyperparameterName
heterogeneous	I-HyperparameterName
graph	I-HyperparameterName
convolution	I-HyperparameterName
is	O
set	O
as	B-HyperparameterName
L	I-HyperparameterName
=	O
1	B-HyperparameterValue
.	O

These	O
parameters	O
are	O
chosen	O
according	O
to	O
the	O
best	O
experimental	O
results	O
on	O
validation	O
set	O
.	O

The	O
other	O
hyper	O
-	O
parameters	O
are	O
set	O
as	O
the	O
same	O
as	O
the	O
baseline	O
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
for	O
fair	O
comparison	O
.	O

Specifically	O
,	O
all	O
the	O
hidden	B-HyperparameterName
dimensions	I-HyperparameterName
used	O
in	O
our	O
model	O
are	O
set	O
as	O
M	B-HyperparameterName
=	O
100	B-HyperparameterValue
.	O

The	O
node	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
N	B-HyperparameterName
=	O
32	B-HyperparameterValue
.	O

For	O
GCN	B-HyperparameterName
,	O
GAT	B-HyperparameterName
and	O
CompareNet	B-HyperparameterName
,	O
we	O
set	O
the	O
activation	B-HyperparameterValue
function	I-HyperparameterValue
as	O
LeakyRelU	B-HyperparameterValue
with	O
slope	B-HyperparameterName
0.2	B-HyperparameterValue
.	O

For	O
model	O
training	O
,	O
we	O
train	O
the	O
models	O
for	O
a	O
maximum	O
of	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	B-HyperparameterValue
.	O

We	O
set	O
L2	B-HyperparameterName
normalization	I-HyperparameterName
factor	I-HyperparameterName
η	B-HyperparameterName
as	O
1e-6	B-HyperparameterValue
.	O

Table	O
2	O
shows	O
the	O
results	O
for	O
the	O
two	O
-	O
way	O
classification	O
between	O
satirical	O
and	O
trusted	O
news	O
articles	O
.	O

We	O
report	O
only	O
micro	B-MetricName
F1	I-MetricName
since	O
micro	B-MetricName
Precision	I-MetricName
=	O
Recall	B-MetricName
=	O
F1	B-MetricName
.	O

As	O
we	O
can	O
see	O
,	O
our	O
proposed	O
model	O
CompareNet	B-HyperparameterName
significantly	O
outperforms	O
all	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
in	O
terms	O
of	O
all	O
the	O
metrics	O
.	O

Compared	O
to	O
the	O
best	O
baseline	O
model	O
,	O
CompareNet	B-HyperparameterName
improves	O
both	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
by	O
nearly	O
3	B-MetricValue
%	I-MetricValue
.	O

We	O
can	O
also	O
find	O
that	O
the	O
graph	O
neural	O
network	O
based	O
models	O
GCN	B-MethodName
and	O
GAT	B-MethodName
all	O
perform	O
better	O
than	O
the	O
deep	O
neural	O
models	O
including	O
CNN	B-MethodName
,	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
.	O

The	O
reason	O
is	O
that	O
the	O
deep	O
neural	O
models	O
fail	O
to	O
consider	O
the	O
interactions	O
between	O
sentences	O
,	O
which	O
is	O
important	O
for	O
fake	O
news	O
detection	O
since	O
different	O
interaction	O
patterns	O
are	O
observed	O
in	O
trusted	O
and	O
fake	O
news	O
documents	O
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Our	O
model	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
further	O
improves	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
by	O
effectively	O
exploiting	O
the	O
topics	O
as	O
well	O
as	O
the	O
external	O
KB	O
.	O

The	O
topics	O
enrich	O
the	O
news	O
representation	O
,	O
and	O
the	O
external	O
KB	O
offers	O
evidences	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

We	O
also	O
present	O
the	O
results	O
of	O
four	O
-	O
way	O
classification	O
in	O
Table	O
3	O
.	O

Consistently	O
,	O
all	O
graph	O
neural	O
models	O
capturing	O
sentence	O
interactions	O
outperform	O
the	O
deep	O
neural	O
models	O
.	O

Our	O
model	O
CompareNet	B-MethodName
achieves	O
the	O
best	O
performance	O
in	O
terms	O
of	O
all	O
metrics	O
.	O

We	O
believe	O
that	O
our	O
model	O
CompareNet	B-MethodName
benefits	O
from	O
the	O
topics	O
and	O
external	O
knowledge	O
.	O

In	O
this	O
subsection	O
,	O
we	O
conduct	O
experiments	O
to	O
study	O
the	O
effectiveness	O
of	O
each	O
module	O
in	O
CompareNet	B-MethodName
and	O
the	O
way	O
we	O
incorporate	O
external	O
knowledge	O
.	O

We	O
study	O
the	O
average	O
performance	O
of	O
5	O
runs	O
on	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
set	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
we	O
test	O
the	O
performance	O
of	O
CompareNet	B-MethodName
removing	O
structured	O
triplets	O
,	O
removing	O
the	O
entire	O
external	O
knowledge	O
,	O
removing	O
topics	O
,	O
and	O
removing	O
both	O
topics	O
and	O
external	O
knowledge	O
.	O

In	O
the	O
last	O
two	O
rows	O
,	O
we	O
further	O
information	O
is	O
as	O
important	O
as	O
the	O
external	O
knowledge	O
.	O

Removing	O
both	O
topics	O
and	O
external	O
knowledge	O
(	O
i.e.	O
,	O
w/o	O
Both	O
)	O
will	O
lead	O
to	O
substantial	O
performance	O
drop	O
(	O
4.0	B-MetricValue
-	I-MetricValue
5.0	I-MetricValue
%	I-MetricValue
)	O
.	O

It	O
demonstrates	O
the	O
importance	O
of	O
both	O
topics	O
and	O
external	O
knowledge	O
.	O

The	O
variant	O
model	O
CompareNet	B-MethodName
(	I-MethodName
undirected	I-MethodName
)	I-MethodName
although	O
incorporating	O
both	O
topics	O
and	O
external	O
knowledge	O
achieves	O
lower	O
performance	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
and	O
CompareNet	B-MethodName
w/o	I-MethodName
Topics	I-MethodName
.	O

The	O
reason	O
could	O
be	O
that	O
CompareNet	B-MethodName
(	I-MethodName
undirected	I-MethodName
)	I-MethodName
directly	O
aggregates	O
the	O
true	O
entity	O
knowledge	O
into	O
the	O
news	O
representation	O
in	O
graph	O
convolution	O
without	O
considering	O
the	O
directed	O
edges	O
,	O
which	O
misleads	O
the	O
classifier	O
for	O
differentiating	O
fake	O
news	O
.	O

This	O
verifies	O
the	O
appropriateness	O
of	O
our	O
constructed	O
directed	O
heterogeneous	O
document	O
graph	O
.	O

The	O
last	O
variant	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
(	I-MethodName
concatenation	I-MethodName
)	I-MethodName
also	O
performs	O
lower	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
,	O
further	O
indicating	O
that	O
directly	O
concatenating	O
true	O
entity	O
knowledge	O
is	O
not	O
a	O
good	O
way	O
for	O
incorporating	O
entity	O
knowledge	O
.	O

Its	O
performance	O
drops	O
by	O
around	O
2.0	B-MetricValue
%	I-MetricValue
compared	O
to	O
CompareNet	B-MethodName
.	O

These	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
carefully	O
designed	O
entity	O
comparison	O
network	O
in	O
CompareNet	B-MethodName
.	O

Figure	O
3	O
shows	O
the	O
performance	O
(	O
micro	B-MetricName
and	I-MetricName
macro	I-MetricName
F1	I-MetricName
)	O
of	O
our	O
model	O
CompareNet	B-MethodName
on	O
LUN	B-DatasetName
validation	I-DatasetName
set	O
with	O
different	O
number	O
of	O
top	B-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
P	I-HyperparameterName
to	O
each	O
sentence	O
.	O

As	O
we	O
can	O
see	O
clearly	O
,	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
first	O
consistently	O
rises	O
with	O
the	O
increase	O
of	O
P	B-HyperparameterName
and	O
then	O
drops	O
when	O
P	B-HyperparameterName
is	O
larger	O
than	O
2	O
.	O

This	O
may	O
because	O
that	O
connecting	O
too	O
many	O
lowprobability	O
topics	O
will	O
introduce	O
some	O
noise	O
.	O

Thus	O
,	O
in	O
our	O
experiments	O
,	O
we	O
set	O
P	B-HyperparameterName
=	O
2	B-HyperparameterValue
.	O

To	O
further	O
illustrate	O
why	O
our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baseline	O
GAT+Attn	B-MethodName
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
4	O
,	O
the	O
content	O
of	O
the	O
news	O
document	O
is	O
in	O
conflict	O
with	O
the	O
entity	O
description	O
from	O
Wikipedia	O
.	O

Specifically	O
,	O
the	O
news	O
about	O
"	O
FDA	O
target	O
and	O
threaten	O
the	O
natural	O
health	O
community	O
"	O
delivers	O
contrary	O
meaning	O
from	O
the	O
entity	O
description	O
that	O
"	O
FDA	O
is	O
responsible	O
for	O
protecting	O
and	O
promoting	O
public	O
health	O
"	O
4	O
.	O

Similarly	O
,	O
the	O
news	O
document	O
about	O
"	O
mammograms	O
are	O
not	O
effective	O
at	O
detecting	O
breast	O
tumors	O
"	O
conveys	O
different	O
meaning	O
from	O
the	O
entity	O
description	O
of	O
"	O
mammograms	O
"	O
.	O

We	O
believe	O
that	O
our	O
model	O
CompareNet	B-MethodName
benefits	O
from	O
the	O
comparison	O
to	O
Wikipedia	B-DatasetName
knowledge	I-DatasetName
by	O
the	O
entity	O
comparison	O
network	O
.	O

We	O
find	O
there	O
are	O
also	O
unsuccessful	O
cases	O
since	O
an	O
entity	O
could	O
be	O
mistakenly	O
linked	O
to	O
a	O
wrong	O
entity	O
in	O
the	O
Wikipedia	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
graph	O
neural	O
model	O
CompareNet	B-MethodName
which	O
compares	O
the	O
news	O
to	O
the	O
external	O
knowledge	O
for	O
fake	O
news	O
detection	O
.	O

Considering	O
that	O
the	O
detection	O
of	O
fake	O
news	O
is	O
correlated	O
with	O
topics	O
,	O
in	O
our	O
model	O
,	O
we	O
also	O
use	O
topics	O
to	O
enrich	O
the	O
news	O
document	O
representation	O
for	O
improving	O
fake	O
news	O
detection	O
.	O

Particularly	O
,	O
we	O
first	O
construct	O
a	O
directed	O
heterogeneous	O
document	O
graph	O
for	O
each	O
news	O
document	O
capturing	O
the	O
interactions	O
among	O
sentences	O
,	O
topics	O
and	O
entities	O
.	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	O
graph	O
attention	O
network	O
for	O
learning	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
content	O
of	O
the	O
news	O
document	O
.	O

To	O
capture	O
the	O
semantic	O
consistency	O
of	O
the	O
news	O
content	O
and	O
the	O
KB	O
,	O
the	O
learned	O
contextual	O
entity	O
representations	O
are	O
then	O
compared	O
to	O
the	O
KB	O
-	O
based	O
entity	O
representations	O
,	O
with	O
a	O
carefully	O
designed	O
entity	O
comparison	O
network	O
.	O

Finally	O
,	O
the	O
obtained	O
entity	O
comparison	O
features	O
are	O
combined	O
with	O
the	O
news	O
representation	O
for	O
an	O
improved	O
fake	O
news	O
classifier	O
.	O

Experiments	O
on	O
two	O
benchmark	O
datasets	O
have	O
demonstrated	O
the	O
effectiveness	O
of	O
the	O
way	O
we	O
incorporate	O
the	O
external	O
knowledge	O
and	O
topics	O
.	O

In	O
future	O
work	O
,	O
we	O
will	O
explore	O
a	O
better	O
way	O
to	O
combine	O
multi	O
-	O
modal	O
data	O
(	O
e.g.	O
,	O
images	O
)	O
and	O
external	O
knowledge	O
for	O
fake	O
news	O
detection	O
.	O

The	O
work	O
is	O
supported	O
by	O
the	O
National	O
Natural	O
Science	O
Fundation	O
of	O
China	O
(	O
No	O
.	O

61806020	O
,	O
U1936220	O
,	O
61972047	O
,	O
62076245	O
)	O
and	O
the	O
Microsoft	O
Research	O
Asia	O
's	O
Star	O
Track	O
project	O
.	O

This	O
paper	O
describes	O
three	O
open	O
access	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
corpora	O
and	O
presents	O
the	O
results	O
and	O
implications	O
of	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
for	I-TaskName
endangered	I-TaskName
language	I-TaskName
documentation	I-TaskName
.	O

Two	O
issues	O
are	O
addressed	O
.	O

First	O
,	O
the	O
advantage	O
for	O
ASR	B-TaskName
accuracy	O
of	O
targeting	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
in	O
addition	O
to	O
,	O
or	O
in	O
substitution	O
of	O
,	O
linguistic	O
units	O
(	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
morae	B-MethodName
)	O
and	O
then	O
using	O
ROVER	B-MethodName
for	O
system	O
combination	O
.	O

BPE	B-MethodName
units	O
consistently	O
outperform	O
linguistic	O
units	O
although	O
the	O
best	O
results	O
are	O
obtained	O
by	O
system	O
combination	O
of	O
different	O
BPE	B-MethodName
targets	O
.	O

Second	O
,	O
a	O
case	O
is	O
made	O
that	O
for	O
endangered	O
language	O
documentation	O
,	O
ASR	B-TaskName
contributions	O
should	O
be	O
evaluated	O
according	O
to	O
extrinsic	O
criteria	O
(	O
e.g.	O
,	O
positive	B-MetricName
impact	I-MetricName
on	I-MetricName
downstream	I-MetricName
tasks	I-MetricName
)	O
and	O
not	O
simply	O
intrinsic	O
metrics	O
(	O
e.g.	O
,	O
CER	B-MetricName
and	O
WER	B-MetricName
)	O
.	O

The	O
extrinsic	O
metric	O
chosen	O
is	O
the	O
level	O
of	O
reduction	O
in	O
the	O
human	O
effort	O
needed	O
to	O
produce	O
high	O
-	O
quality	O
transcriptions	O
for	O
permanent	O
archiving	O
.	O

1	O
Introduction	O
:	O
Endangered	O
language	O
documentation	O
history	O
and	O
contextEndangered	O
language	O
(	O
EL	O
)	O
documentation	O
emerged	O
as	O
a	O
field	O
of	O
linguistic	O
activity	O
in	O
the	O
1990s	O
,	O
as	O
reflected	O
in	O
several	O
seminal	O
moments	O
.	O

In	O
1991	O
the	O
Linguistic	O
Society	O
of	O
America	O
held	O
a	O
symposium	O
entitled	O
"	O
Endangered	O
Languages	O
and	O
their	O
Preservation	O
"	O
;	O
in	O
1992	O
Hale	O
et	O
al	O
.	O
(	O

1992	O
)	O
published	O
a	O
seminal	O
article	O
on	O
endangered	O
languages	O
in	O
Language	O
,	O
the	O
LSA	O
's	O
flagship	O
journal	O
.	O
,	O

Himmelmann	O
(	O
1998	O
argued	O
for	O
the	O
development	O
of	O
documentary	O
linguistics	O
as	O
an	O
endeavor	O
separate	O
from	O
and	O
complementary	O
to	O
descriptive	O
linguistics	O
.	O

By	O
the	O
early	O
years	O
of	O
the	O
present	O
millennium	O
,	O
infrastructure	O
efforts	O
were	O
being	O
developed	O
:	O
metadata	O
standards	O
and	O
best	O
practices	O
for	O
archiving	O
(	O
Bird	O
and	O
Simons	O
,	O
2003	O
)	O
;	O
tools	O
for	O
lexicography	O
and	O
corpus	O
developments	O
such	O
as	O
Shoebox	O
,	O
Transcriber	O
(	O
Barras	O
et	O
al	O
.	O
,	O

1998	O
)	O
,	O
and	O
ELAN	O
(	O
Wittenburg	O
et	O
al	O
.	O
,	O

2006	O
)	O
,	O
and	O
financial	O
support	O
for	O
endangered	O
language	O
documentation	O
(	O
the	O
Volkswagen	O
Foundation	O
,	O
the	O
NSF	O
Documenting	O
Endangered	O
Language	O
Program	O
,	O
and	O
the	O
SOAS	O
Endangered	O
Language	O
Documentation	O
Programme	O
)	O
.	O

Recent	O
retrospectives	O
on	O
the	O
impact	O
of	O
Hale	O
et	O
al	O
.	O
(	O

1992	O
)	O
and	O
Himmelmann	O
(	O
1998	O
)	O
have	O
been	O
published	O
by	O
Seifart	O
et	O
al	O
.	O
(	O

2018	O
)	O
and	O
McDonnell	O
et	O
al	O
.	O
(	O

2018	O
)	O
.	O

Within	O
the	O
last	O
decade	O
,	O
the	O
National	O
Science	O
Foundation	O
supported	O
a	O
series	O
of	O
three	O
workshops	O
,	O
under	O
the	O
acronym	O
AARDVARC	O
(	O
Automatically	O
Annotated	O
Repository	O
of	O
Digital	O
Audio	O
and	O
Video	O
Resources	O
Community	O
)	O
to	O
bring	O
together	O
field	O
linguists	O
working	O
on	O
endangered	O
languages	O
and	O
computational	O
linguists	O
working	O
on	O
automatic	O
annotation	O
-	O
particularly	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR)-to	B-TaskName
address	O
the	O
impact	O
of	O
what	O
has	O
been	O
called	O
the	O
"	O
transcription	O
bottleneck	O
"	O
(	O
Whalen	O
and	O
Damir	O
,	O
2012	O
)	O
.	O

Interest	O
in	O
applying	O
machine	O
learning	O
to	O
endangered	O
language	O
documentation	O
is	O
also	O
manifested	O
in	O
four	O
biennial	O
workshops	O
on	O
this	O
topic	O
,	O
the	O
first	O
in	O
2014	O
(	O
Good	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

Finally	O
,	O
articles	O
directly	O
referencing	O
ASR	B-TaskName
of	O
endangered	O
languages	O
have	O
become	O
increasingly	O
common	O
over	O
the	O
last	O
five	O
years	O
(	O
Adams	O
et	O
al	O
.	O
,	O
,	O

2020Ćavar	O
et	O
al	O
.	O
,	O

2016;Foley	O
et	O
al	O
.	O
,	O

2018Foley	O
et	O
al	O
.	O
,	O
,	O

2019Gupta	O
and	O
Boulianne	O
,	O
2020;Michaud	O
et	O
al	O
.	O
,	O

2018;Mitra	O
et	O
al	O
.	O
,	O

2016;Shi	O
et	O
al	O
.	O
,	O

2021).This	O
article	O
continues	O
work	O
on	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
ASR	B-TaskName
(	O
Mitra	O
et	O
al	O
.	O
,	O

2016;Shi	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

The	O
most	O
recent	O
efforts	O
(	O
2020	O
and	O
2021	O
)	O
have	O
adopted	O
the	O
ESPNet	O
toolkit	O
for	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
E2E	B-TaskName
ASR	I-TaskName
)	O
.	O

This	O
approach	O
has	O
proven	O
to	O
be	O
very	O
efficient	O
in	O
terms	O
of	O
time	O
needed	O
to	O
develop	O
the	O
ASR	B-TaskName
recipe	O
(	O
Shi	O
et	O
al	O
.	O
,	O

2021	O
)	O
and	O
in	O
yielding	O
ASR	B-TaskName
hypotheses	O
of	O
an	O
accuracy	O
capable	O
of	O
significantly	O
reducing	O
the	O
extent	O
of	O
human	O
effort	O
needed	O
to	O
finalize	O
accurate	O
transcribed	O
audio	O
for	O
permanent	O
archiving	O
as	O
here	O
demonstrated	O
.	O

Section	O
2	O
discusses	O
the	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
corpora	O
,	O
and	O
Section	O
3	O
explores	O
the	O
general	O
goals	O
of	O
EL	O
documentation	O
.	O

Section	O
4	O
reviews	O
the	O
E2E	B-TaskName
ASR	I-TaskName
and	O
corresponding	O
results	O
using	O
ESPNet	O
.	O

The	O
conclusion	O
is	O
offered	O
in	O
Section	O
5.2	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
:	O
Corpus	O
characteristics	O
and	O
development	O
Much	O
work	O
on	O
computer	O
-	O
assisted	O
EL	O
documentation	O
is	O
closely	O
related	O
to	O
work	O
on	O
low	O
-	O
resource	O
languages	O
,	O
for	O
the	O
obvious	O
reason	O
that	O
most	O
ELs	O
have	O
limited	O
resources	O
,	O
be	O
they	O
time	O
-	O
coded	O
transcriptions	O
,	O
interlinearized	O
texts	O
,	O
or	O
corpora	O
in	O
parallel	O
translation	O
.	O

The	O
resources	O
for	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
,	O
the	O
language	O
targeted	O
in	O
this	O
present	O
study	O
,	O
are	O
,	O
however	O
,	O
relatively	O
abundant	O
by	O
EL	O
standards	O
(	O
119.32	O
hours	O
over	O
three	O
corpora	O
)	O
,	O
the	O
result	O
of	O
over	O
a	O
decade	O
of	O
linguistic	O
and	O
anthropological	O
research	O
by	O
Amith	O
and	O
Castillo	O
García	O
(	O
2020	O
)	O
.	O

Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
(	O
henceforth	O
YM	B-DatasetName
)	O
,	O
an	O
endangered	O
Mixtecan	O
language	O
spoken	O
in	O
the	O
municipality	O
of	O
San	O
Luis	O
Acatlán	O
,	O
Guerrero	O
,	O
Mexico	O
,	O
is	O
one	O
of	O
some	O
50	O
languages	O
in	O
the	O
Mixtec	O
language	O
family	O
,	O
which	O
is	O
within	O
a	O
larger	O
unit	O
,	O
Otomanguean	O
,	O
that	O
Suárez	O
(	O
1983	O
)	O
considers	O
a	O
hyper	O
-	O
family	O
or	O
stock	O
.	O

Mixtec	O
languages	O
(	O
spoken	O
in	O
Oaxaca	O
,	O
Guerrero	O
,	O
and	O
Puebla	O
)	O
are	O
highly	O
varied	O
,	O
the	O
result	O
of	O
approximately	O
2,000	O
years	O
of	O
diversification	O
.	O

YM	B-DatasetName
is	O
spoken	O
in	O
four	O
communities	O
:	O
Yoloxóchitl	O
,	O
Cuanacaxtitlan	O
,	O
Arroyo	O
Cumiapa	O
,	O
and	O
Buena	O
Vista	O
.	O

Mutual	O
intelligibility	O
among	O
the	O
four	O
communities	O
is	O
high	O
despite	O
differences	O
in	O
phonology	O
,	O
morphology	O
,	O
and	O
syntax	O
.	O

All	O
villages	O
have	O
a	O
simple	O
common	O
segmental	O
inventory	O
but	O
apparently	O
significant	O
though	O
still	O
undocumented	O
variation	O
in	O
tonal	O
phonology	O
;	O
only	O
Cuanacaxtitlan	O
manifests	O
tone	O
sandhi	O
.	O

YMC	B-DatasetName
(	O
referring	O
only	O
to	O
the	O
Mixtec	O
of	O
the	O
community	O
of	O
Yoloxóchitl	O
[	O
16.81602	O
,	O
-98.68597	O
]	O
)	O
manifests	O
28	O
distinct	O
tonal	O
patterns	O
on	O
1,451	O
to	O
-	O
date	O
identified	O
bimoraic	O
lexical	O
stems	O
.	O

The	O
tonal	O
patterns	O
carry	O
a	O
significant	O
functional	O
load	O
regarding	O
the	O
lexicon	O
and	O
inflection	O
(	O
Palancar	O
et	O
al	O
.	O
,	O

2016	O
)	O
.	O

For	O
example	O
,	O
24	O
distinct	O
tonal	O
patterns	O
on	O
the	O
bimoraic	O
segmental	O
sequence	O
[	O
nama	O
]	O
yield	O
30	O
words	O
(	O
including	O
five	O
homophones	O
)	O
.	O

The	O
three	O
principal	O
aspectual	O
forms	O
(	O
irrealis	O
,	O
incompletive	O
,	O
and	O
completive	O
)	O
are	O
almost	O
invariably	O
marked	O
by	O
a	O
tonal	O
variation	O
on	O
the	O
first	O
mora	O
of	O
the	O
verbal	O
stem	O
(	O
1	O
or	O
3	O
for	O
the	O
irrealis	O
,	O
4	O
for	O
the	O
incompletive	O
,	O
and	O
13	O
for	O
the	O
completive	O
;	O
in	O
addition	O
14	O
on	O
the	O
initial	O
mora	O
almost	O
always	O
indicates	O
negation	O
of	O
the	O
irrealis	O
1	O
)	O
.	O

In	O
a	O
not	O
-	O
insignificant	O
number	O
of	O
cases	O
,	O
suppletive	O
stems	O
exist	O
,	O
generally	O
manifesting	O
variation	O
in	O
a	O
stem	O
-	O
initial	O
consonant	O
and	O
often	O
the	O
stem	O
-	O
initial	O
vowel	O
.	O

The	O
ample	O
tonal	O
inventory	O
of	O
YMC	B-DatasetName
presents	O
obstacles	O
to	O
native	O
speaker	O
literacy	O
and	O
an	O
ASR	B-TaskName
system	O
learning	O
to	O
convert	O
an	O
acoustic	O
signal	O
to	O
text	O
.	O

It	O
also	O
complicates	O
the	O
construction	O
of	O
a	O
language	O
lexicon	O
for	O
HMM	B-MethodName
-	I-MethodName
based	I-MethodName
systems	I-MethodName
,	O
a	O
lexicon	O
that	O
is	O
not	O
required	O
in	O
E2E	B-TaskName
ASR	I-TaskName
.	O

The	O
phonological	O
and	O
morphological	O
differences	O
between	O
YMC	B-DatasetName
and	O
the	O
Mixtec	O
of	O
the	O
three	O
other	O
YM	B-DatasetName
communities	O
create	O
challenges	O
for	O
transcription	O
and	O
,	O
by	O
extension	O
,	O
for	O
applying	O
YMC	B-DatasetName
ASR	B-TaskName
to	O
speech	O
recordings	O
from	O
these	O
other	O
villages	O
.	O

To	O
accomplish	O
this	O
,	O
it	O
will	O
be	O
necessary	O
first	O
to	O
learn	O
the	O
phonology	O
and	O
morphology	O
of	O
these	O
variants	O
and	O
then	O
use	O
this	O
as	O
input	O
into	O
a	O
transfer	O
learning	O
scenario	O
.	O

Intralanguage	O
variation	O
among	O
distinct	O
communities	O
(	O
see	O
Hildebrandt	O
et	O
al	O
.	O
,	O

2017b	O
andother	O
articles	O
in	O
Hildebrandt	O
et	O
al	O
.	O
,	O

2017a	O
)	O
is	O
an	O
additional	O
factor	O
that	O
can	O
negatively	O
impact	O
computer	O
-	O
assisted	O
EL	O
documentation	O
efforts	O
in	O
both	O
intra	O
-	O
and	O
intercommunity	O
contexts	O
.	O

Ćavar	O
et	O
al	O
.	O
,	O

2016	O
;	O
.	O

This	O
ample	O
size	O
has	O
yielded	O
lower	O
character	B-MetricName
(	O
CER	B-MetricName
)	O
and	O
word	B-MetricName
(	O
WER	B-MetricName
)	O
error	O
rates	O
than	O
would	O
usually	O
occur	O
with	O
truly	O
low	O
-	O
resource	O
EL	O
documentation	O
projects	O
.	O

Amith	O
and	O
Castillo	O
García	O
recorded	O
the	O
corpus	O
at	O
a	O
48KHz	O
sampling	O
rate	O
and	O
16	O
-	O
bits	O
(	O
usually	O
with	O
a	O
Marantz	O
PMD	O
671	O
recorder	O
,	O
Shure	O
SM-10a	O
dynamic	O
headset	O
mics	O
,	O
and	O
separate	O
channels	O
for	O
each	O
speaker	O
)	O
.	O

The	O
entire	O
corpus	O
was	O
transcribed	O
by	O
Castillo	O
,	O
a	O
native	O
speaker	O
linguist	O
(	O
García	O
,	O
2007	O
)	O
.	O

A	O
second	O
YMC	B-DatasetName
corpus	O
(	B-DatasetName
YMC	I-DatasetName
-	I-DatasetName
FB	I-DatasetName
;	O
for	O
'	O
field	O
botany	O
'	O
)	O
was	O
developed	O
during	O
ethno	O
-	O
botanical	O
fieldwork	O
.	O

Kenia	O
Velasco	O
Gutiérrez	O
(	O
a	O
Spanish	O
-	O
speaking	O
botanist	O
)	O
and	O
Esteban	O
Guadalupe	O
Sierra	O
(	O
a	O
native	O
speaker	O
from	O
Yoloxóchitl	O
)	O
led	O
105	O
days	O
of	O
fieldwork	O
that	O
yielded	O
888	O
distinct	O
plant	O
collections	O
.	O

A	O
total	O
of	O
584	O
recordings	O
were	O
made	O
in	O
all	O
four	O
YM	O
communities	O
;	O
only	O
452	O
were	O
in	O
Yoloxóchitl	O
,	O
and	O
of	O
these	O
,	O
435	O
,	O
totaling	O
15.17	O
hours	O
with	O
only	O
three	O
speakers	O
,	O
were	O
used	O
as	O
a	O
second	O
test	O
case	O
for	O
E2E	B-TaskName
ASR	I-TaskName
.	O

Recordings	O
were	O
done	O
outdoors	O
at	O
the	O
plant	O
collection	O
site	O
with	O
a	O
Zoom	O
H4n	O
handheld	O
digital	O
recorder	O
.	O

The	O
Zoom	O
H4n	O
internal	O
mic	O
was	O
used	O
;	O
recordings	O
were	O
48KHz	O
,	O
16	O
-	O
bit	O
,	O
a	O
single	O
channel	O
with	O
one	O
speaker	O
talking	O
after	O
another	O
(	O
no	O
overlap	O
)	O
.	O

Each	O
recording	O
has	O
a	O
short	O
introduction	O
by	O
Velasco	O
describing	O
,	O
in	O
Spanish	O
,	O
the	O
plant	O
being	O
collected	O
.	O

This	O
Spanish	O
section	O
has	O
not	O
been	O
factored	O
into	O
the	O
duration	O
of	O
the	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
,	O
nor	O
has	O
it	O
been	O
evaluated	O
for	O
character	O
and	O
word	O
error	O
rates	O
at	O
this	O
time	O
(	O
pending	O
future	O
implementation	O
of	O
a	O
multilingual	O
model	O
)	O
.	O

The	O
processing	O
of	O
the	O
435	O
recordings	O
falls	O
into	O
two	O
groups	O
.	O

9	O
of	O
Shi	O
et	O
al	O
.	O
(	O

2021).•	O
178	O
recordings	O
(	O
6.81	O
hours	O
)	O
were	O
processed	O
by	O
E2E	B-TaskName
ASR	I-TaskName
,	O
then	O
corrected	O
by	O
Castillo	O
.	O

This	O
set	O
was	O
not	O
used	O
to	O
teach	O
or	O
evaluate	O
novice	O
trainee	O
transcription	O
skills	O
but	O
only	O
to	O
determine	O
CER	B-MetricName
and	O
WER	B-MetricName
for	O
E2E	B-TaskName
ASR	I-TaskName
with	O
the	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
.	O

No	O
training	O
or	O
validation	O
sets	O
were	O
created	O
from	O
this	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
,	O
which	O
for	O
this	O
present	O
paper	O
was	O
used	O
solely	O
to	O
test	O
E2E	B-TaskName
ASR	I-TaskName
efficiency	O
using	O
the	O
recipe	O
developed	O
from	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
scores	O
for	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
were	O
only	O
produced	O
after	O
Castillo	O
used	O
the	O
ELAN	O
interface	O
to	O
correct	O
the	O
ASR	B-TaskName
hypotheses	O
for	O
this	O
corpus	O
(	O
see	O
Appendix	O
A	O
for	O
an	O
example	O
ASR	B-TaskName
output	O
)	O
.	O

The	O
final	O
corpus	O
is	O
a	O
set	O
of	O
24	O
narratives	O
made	O
to	O
provide	O
background	O
information	O
and	O
off	O
-	O
camera	O
voice	O
for	O
a	O
documentary	O
video	O
.	O

The	O
recordings	O
involved	O
some	O
speakers	O
not	O
represented	O
in	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
.	O

All	O
recordings	O
(	O
5.16	O
hours	O
)	O
were	O
made	O
at	O
44.1kHz	O
,	O
16	O
-	O
bit	O
with	O
a	O
boom	O
-	O
held	O
microphone	O
and	O
a	O
Tascam	O
portable	O
digital	O
recorder	O
in	O
a	O
hotel	O
room	O
.	O

This	O
environment	O
may	O
have	O
introduced	O
reverb	O
or	O
other	O
effects	O
that	O
might	O
have	O
negatively	O
affected	O
ASR	B-TaskName
CER	B-MetricName
and	O
WER.Accessibility	B-MetricName
:	O
All	O
three	O
corpora	O
(	O
119.32	O
hours	O
)	O
are	O
available	O
at	O
the	O
OpenSLR	O
data	O
portal	O
(	O
Amith	O
and	O
Castillo	O
García	O
,	O
2020	O
)	O
3	O
Goals	O
and	O
challenges	O
of	O
corpora	O
-	O
based	O
endangered	O
language	O
documentation	O
The	O
oft	O
-	O
cited	O
Boasian	O
trilogy	O
of	O
grammar	O
,	O
dictionaries	O
,	O
and	O
texts	O
is	O
a	O
common	O
foundation	O
for	O
EL	O
documentation	O
.	O

Good	O
(	O
2018	O
,	O
p.	O
14	O
)	O
parallels	O
this	O
classic	O
conception	O
with	O
a	O
"	O
Himmelmannian	O
"	O
trilogy	O
of	O
recordings	O
,	O
metadata	O
,	O
and	O
annotations	O
(	O
see	O
Himmelmann	O
2018	O
)	O
.	O

For	O
the	O
purpose	O
of	O
the	O
definition	O
proposed	O
here	O
,	O
EL	O
documentation	O
is	O
considered	O
to	O
be	O
based	O
on	O
the	O
Boasian	O
trilogy	O
of	O
(	O
1	O
)	O
corpus	O
,	O
(	O
2	O
)	O
lexicon	O
(	O
in	O
the	O
sense	O
of	O
dictionary	O
)	O
,	O
and	O
(	O
3	O
)	O
grammar	O
.	O

In	O
turn	O
,	O
each	O
element	O
in	O
the	O
trilogy	O
is	O
molded	O
by	O
a	O
series	O
of	O
expectations	O
and	O
best	O
practices	O
.	O

An	O
audio	O
corpus	O
,	O
for	O
example	O
,	O
would	O
best	O
be	O
presented	O
interlinearized	O
with	O
(	O
a	O
)	O
lines	O
corresponding	O
to	O
the	O
transcription	O
(	O
often	O
in	O
a	O
practical	O
orthography	O
or	O
IPA	O
transcription	O
)	O
,	O
(	O
b	O
)	O
morphological	O
segmentation	O
(	O
often	O
called	O
a	O
'	O
parse	O
'	O
)	O
,	O
(	O
c	O
)	O
parallel	O
glossing	O
of	O
each	O
morpheme	O
,	O
(	O
d	O
)	O
a	O
free	O
translation	O
into	O
a	O
target	O
,	O
often	O
colonial	O
language	O
,	O
and	O
(	O
e	O
)	O
metadata	O
about	O
recording	O
conditions	O
and	O
participants	O
.	O

This	O
is	O
effectively	O
the	O
Himmelmannian	O
trilogy	O
referenced	O
by	O
Good	O
.	O

A	O
dictionary	O
should	O
contain	O
certain	O
minimum	O
fields	O
(	O
e.g.	O
,	O
part	O
of	O
speech	O
,	O
etymology	O
,	O
illustrative	O
sentences	O
)	O
.	O

Grammatical	O
descriptions	O
(	O
books	O
and	O
articles	O
)	O
are	O
more	O
openly	O
defined	O
(	O
e.g.	O
,	O
a	O
reference	O
vs.	O
a	O
pedagogical	O
grammar	O
)	O
and	O
may	O
treat	O
only	O
parts	O
of	O
the	O
language	O
(	O
e.g.	O
,	O
verb	O
morphology).In	O
a	O
best	O
-	O
case	O
scenario	O
,	O
these	O
three	O
elements	O
of	O
the	O
Boasian	O
trilogy	O
are	O
interdependent	O
.	O

Corpusbased	O
lexicography	O
clearly	O
requires	O
ample	O
interlinearized	O
transcriptions	O
(	O
IGT	O
)	O
of	O
natural	O
speech	O
that	O
can	O
be	O
used	O
to	O
(	O
a	O
)	O
develop	O
concordances	O
mapped	O
to	O
lemmas	O
(	O
not	O
word	O
forms	O
)	O
;	O
(	O
b	O
)	O
enrich	O
a	O
dictionary	O
by	O
finding	O
lemmas	O
in	O
the	O
corpus	O
that	O
are	O
absent	O
from	O
an	O
extant	O
set	O
of	O
dictionary	O
headwords	O
;	O
and	O
(	O
c	O
)	O
discover	O
patterns	O
in	O
the	O
corpus	O
suggestive	O
of	O
multiword	O
lemmas	O
(	O
e.g.	O
,	O
ku	O
3	O
-na	O
3	O
a	O
4	O
followed	O
by	O
i	O
3	O
ni	O
2	O
(	O
lit	O
.	O
,	O
'	O

darken	O
heart	O
'	O
but	O
meaning	O
'	O
to	O
faint	O
'	O
)	O
.	O

A	O
grammar	O
will	O
inform	O
decisions	O
about	O
morphological	O
segmentation	O
used	O
in	O
the	O
IGT	O
as	O
well	O
as	O
part	O
-	O
of	O
-	O
speech	O
tags	O
and	O
other	O
glosses	O
.	O

And	O
a	O
grammar	O
itself	O
would	O
benefit	O
greatly	O
from	O
a	O
large	O
set	O
of	O
annotated	O
natural	O
speech	O
recordings	O
not	O
simply	O
to	O
provide	O
examples	O
of	O
particular	O
structures	O
but	O
to	O
facilitate	O
a	O
statistical	O
analysis	O
of	O
speech	O
patterns	O
(	O
e.g.	O
,	O
for	O
YMC	B-DatasetName
,	O
the	O
relative	O
frequency	O
of	O
completive	O
verbs	O
marked	O
solely	O
by	O
tone	O
vs.	O
those	O
marked	O
by	O
the	O
prefix	O
ni	O
1	O
-	O
)	O
.	O

This	O
integration	O
of	O
elements	O
into	O
one	O
"	O
hypertextual	O
"	O
documentation	O
effort	O
is	O
proposed	O
by	O
Musgrave	O
and	O
Thieberger	O
(	O
2021	O
)	O
,	O
who	O
note	O
the	O
importance	O
of	O
spontaneous	O
text	O
(	O
i.e.	O
,	O
corpora	O
,	O
which	O
they	O
separate	O
into	O
two	O
elements	O
,	O
media	O
,	O
and	O
text	O
)	O
and	O
comment	O
that	O
"	O
all	O
examples	O
[	O
in	O
the	O
dictionary	O
and	O
grammar	O
]	O
should	O
come	O
from	O
the	O
spontaneous	O
text	O
and	O
should	O
be	O
viewed	O
in	O
context	O
"	O
(	O
p.	O
6	O
)	O
.	O

Musgrave	O
and	O
Thieberger	O
is	O
central	O
to	O
effective	O
endangered	O
language	O
documentation	O
based	O
on	O
natural	O
speech	O
and	O
that	O
textual	O
transcription	O
of	O
multimedia	O
recordings	O
of	O
natural	O
speech	O
is	O
,	O
therefore	O
,	O
the	O
foundation	O
for	O
a	O
dictionary	O
and	O
grammar	O
based	O
on	O
actual	O
language	O
use	O
.	O

End	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
is	O
used	O
to	O
rapidly	O
increase	O
corpus	O
size	O
while	O
offering	O
the	O
opportunity	O
to	O
target	O
certain	O
genres	O
(	O
such	O
as	O
expert	O
conversations	O
on	O
the	O
nomenclature	O
,	O
classification	O
,	O
and	O
use	O
of	O
local	O
flora	O
and	O
fauna	O
;	O
ritual	O
discourse	O
;	O
material	O
cultural	O
production	O
;	O
techniques	O
for	O
fishing	O
and	O
hunting	O
)	O
that	O
are	O
of	O
ethnographic	O
interest	O
but	O
are	O
often	O
insufficiently	O
covered	O
in	O
EL	O
documentation	O
projects	O
that	O
struggle	O
to	O
produce	O
large	O
and	O
varied	O
corpora	O
.	O

With	O
the	O
human	O
effortreducing	O
advances	O
in	O
ASR	B-TaskName
for	O
YMC	B-DatasetName
presented	O
in	O
this	O
paper	O
,	O
such	O
extensive	O
targeted	O
recording	O
of	O
endangered	O
cultural	O
knowledge	O
can	O
now	O
easily	O
be	O
included	O
in	O
the	O
documentation	O
effort	O
.	O

The	O
present	O
paper	O
focuses	O
on	O
end	O
-	O
to	O
-	O
end	O
automatic	O
speech	O
recognition	O
using	O
the	O
ESPNet	O
toolkit	O
Shi	O
et	O
al	O
.	O
,	O

2021;Watanabe	O
et	O
al	O
.	O
,	O

2020Watanabe	O
et	O
al	O
.	O
,	O
,	O

2017Watanabe	O
et	O
al	O
.	O
,	O
,	O

2018	O
.	O

The	O
basic	O
goal	O
is	O
simple	O
:	O
To	O
develop	O
computational	O
tools	O
that	O
reduce	O
the	O
amount	O
of	O
human	O
effort	O
required	O
to	O
produce	O
accurate	O
transcriptions	O
in	O
time	O
-	O
coded	O
interlinearized	O
format	O
that	O
will	O
serve	O
a	O
wide	O
range	O
of	O
potential	O
stakeholders	O
,	O
from	O
native	O
and	O
heritage	O
speakers	O
to	O
specialized	O
academics	O
in	O
institutions	O
of	O
higher	O
learning	O
,	O
in	O
the	O
present	O
and	O
future	O
generations	O
.	O

The	O
evaluation	O
metric	O
,	O
therefore	O
,	O
is	O
not	O
intrinsic	O
(	O
e.g.	O
,	O
reduced	B-MetricName
CER	I-MetricName
and	O
WER	B-MetricName
)	O
but	O
rather	O
extrinsic	O
:	O
the	O
impact	B-MetricName
of	I-MetricName
ASR	I-MetricName
on	I-MetricName
the	I-MetricName
downstream	I-MetricName
task	I-MetricName
of	I-MetricName
creating	I-MetricName
a	I-MetricName
large	I-MetricName
and	I-MetricName
varied	I-MetricName
corpus	I-MetricName
of	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
.	O

ASR	B-TaskName
for	I-TaskName
endangered	I-TaskName
languages	I-TaskName
is	O
made	O
difficult	O
not	O
simply	O
because	O
of	O
limited	O
resources	O
for	O
training	O
a	O
robust	O
system	O
but	O
by	O
a	O
series	O
of	O
factors	O
briefly	O
discussed	O
in	O
this	O
section	O
.	O

Recording	O
conditions	O
:	O
Noisy	O
environments	O
,	O
including	O
overlapping	O
speech	O
,	O
reverberation	O
in	O
indoor	O
recordings	O
,	O
natural	O
sounds	O
in	O
outdoor	O
recordings	O
,	O
less	O
than	O
optimal	O
microphone	O
placement	O
(	O
e.g.	O
,	O
a	O
boom	O
mic	O
in	O
video	O
recordings	O
)	O
,	O
and	O
failure	O
to	O
separately	O
mike	O
speakers	O
for	O
multichannel	O
recordings	O
all	O
negatively	O
impact	O
the	O
accuracy	B-MetricName
of	O
ASR	B-TaskName
output	O
.	O

Also	O
to	O
the	O
point	O
,	O
field	O
recordings	O
are	O
seldom	O
made	O
with	O
an	O
eye	O
to	O
seeding	O
a	O
corpus	O
in	O
ways	O
that	O
would	O
specifically	O
benefit	O
ASR	B-TaskName
results	O
(	O
e.g.	O
,	O
recording	O
a	O
large	O
number	O
of	O
speakers	O
for	O
shorter	O
durations	O
,	O
rather	O
than	O
fewer	O
speakers	O
for	O
longer	O
times	O
)	O
.	O

To	O
date	O
,	O
then	O
,	O
processing	O
a	O
corpus	O
through	O
ASR	B-TaskName
techniques	O
of	O
any	O
nature	O
(	O
HMM	O
,	O
end	O
-	O
to	O
-	O
end	O
)	O
has	O
been	O
more	O
of	O
an	O
afterthought	O
than	O
planned	O
at	O
project	O
beginning	O
.	O

Development	O
of	O
a	O
corpus	O
from	O
the	O
beginning	O
with	O
an	O
eye	O
to	O
subsequent	O
ASR	B-TaskName
potential	O
would	O
be	O
immensely	O
helpful	O
to	O
these	O
computational	O
efforts	O
.	O

It	O
could	O
,	O
perhaps	O
should	O
,	O
be	O
increasingly	O
considered	O
in	O
the	O
initial	O
project	O
design	O
.	O

Indeed	O
,	O
just	O
as	O
funding	O
agencies	O
such	O
as	O
NSF	O
require	O
that	O
projects	O
address	O
data	O
management	O
issues	O
,	O
it	O
might	O
be	O
worth	O
considering	O
the	O
suggested	O
inclusion	O
of	O
how	O
to	O
make	O
documentation	O
materials	O
more	O
amenable	O
to	O
ASR	B-TaskName
and	O
NLP	O
processing	O
as	O
machine	O
learning	O
technologies	O
are	O
getting	O
more	O
robust	O
.	O

Colonialization	O
of	O
language	O
:	O
Endangered	O
languages	O
do	O
not	O
die	O
,	O
to	O
paraphrase	O
Dorian	O
(	O
1978	O
)	O
,	O
with	O
their	O
"	O
boots	O
on	O
.	O
"	O

Rather	O
,	O
in	O
the	O
colonialized	O
situation	O
in	O
which	O
most	O
ELs	O
are	O
immersed	O
,	O
there	O
are	O
multiple	O
phonological	O
,	O
morphological	O
,	O
and	O
syntactic	O
influences	O
from	O
a	O
dominant	O
language	O
.	O

The	O
incidence	O
of	O
a	O
colonial	O
language	O
in	O
native	O
language	O
recordings	O
runs	O
a	O
gamut	O
from	O
multilanguage	O
situations	O
(	O
e.g.	O
,	O
each	O
speaker	O
using	O
a	O
distinct	O
language	O
,	O
as	O
often	O
occurs	O
in	O
elicitation	O
sessions	O
:	O
'	O
How	O
would	O
you	O
translate	O
_	O
_	O
_	O
into	O
Mixtec	O
?	O
'	O
)	O
,	O
to	O
code	O
-	O
switching	O
and	O
borrowing	O
or	O
relexification	O
in	O
the	O
speech	O
of	O
single	O
individuals	O
.	O

In	O
some	O
languages	O
(	O
e.g.	O
,	O
Nahuatl	O
)	O
,	O
a	O
single	O
word	O
may	O
easily	O
combine	O
stems	O
from	O
both	O
native	O
and	O
colonial	O
languages	O
.	O

Preliminary	O
,	O
though	O
not	O
quantified	O
,	O
CER	O
analysis	O
for	O
YMC	B-DatasetName
ASR	B-TaskName
suggests	O
that	O
"	O
Spanish	O
-	O
origin	O
"	O
words	O
provoke	O
a	O
significantly	O
higher	O
error	O
rate	O
than	O
the	O
YMC	B-DatasetName
lexicon	O
uninfluenced	O
by	O
Spanish	O
.	O

It	O
is	O
also	O
not	O
clear	O
that	O
a	O
multilingual	O
phone	O
recognition	O
system	O
is	O
the	O
solution	O
to	O
character	O
errors	O
(	O
such	O
as	O
ASR	O
hypothesis	O
'	O
cereso	O
'	O
for	O
Spanish	O
'	O
cerezo	O
'	O
)	O
that	O
may	O
derive	O
from	O
an	O
orthographic	O
system	O
,	O
such	O
as	O
that	O
for	O
Spanish	O
,	O
that	O
is	O
not	O
designed	O
,	O
as	O
many	O
EL	O
orthographies	O
are	O
,	O
for	O
consistency	O
.	O

Phonological	O
shifts	O
in	O
borrowed	O
terms	O
also	O
preclude	O
the	O
simple	O
application	O
of	O
lexical	O
tools	O
to	O
correct	O
misspellings	O
(	O
as	O
'	O
agustu	O
'	O
for	O
the	O
Spanish	O
month	O
'	O
agosto').Orthographic	O
conventions	O
:	O
The	O
practical	O
deep	O
orthography	O
developed	O
by	O
Amith	O
and	O
Castillo	O
marks	O
off	O
boundaries	O
of	O
affixes	O
(	O
with	O
a	O
hyphen	O
)	O
and	O
clitics	O
(	O
with	O
an	O
=	O
sign	O
)	O
.	O

Tones	O
are	O
indicated	O
by	O
superscript	O
numbers	O
,	O
from	O
1	O
low	O
to	O
4	O
high	O
,	O
with	O
five	O
common	O
rising	O
and	O
falling	O
tones	O
.	O

Stem	O
-	O
final	O
elided	O
tones	O
are	O
enclosed	O
in	O
parentheses	O
(	O
e.g.	O
,	O
underlying	O
form	O
be	O
'	O
3	O
e	O
(	O
3	O
)	O
=	O
2	O
;	O
house=1sgPoss	O
,	O
'	O
my	O
house	O
'	O
;	O
surface	O
form	O
be	O
'	O
3	O
e	O
2	O
)	O
.	O

Tone	O
-	O
based	O
inflectional	O
morphology	O
is	O
not	O
separated	O
in	O
any	O
YMC	B-DatasetName
transcriptions	O
.	O

2	O
The	O
transcription	O
strategy	O
for	O
YMC	B-DatasetName
was	O
unusual	O
in	O
that	O
the	O
practical	O
orthography	O
was	O
a	O
deep	O
,	O
underlying	O
system	O
that	O
represented	O
segmental	O
morpheme	O
boundaries	O
and	O
showed	O
elided	O
tones	O
in	O
parentheses	O
.	O

The	O
original	O
plans	O
of	O
Amith	O
and	O
Castillo	O
were	O
to	O
use	O
the	O
transcribed	O
audio	O
as	O
primary	O
data	O
for	O
a	O
corpus	O
-	O
based	O
dictionary	O
.	O

A	O
deep	O
orthography	O
facilitates	O
discovery	O
(	O
without	O
recourse	O
to	O
a	O
morphological	O
analyzer	O
)	O
of	O
lemmas	O
that	O
may	O
be	O
altered	O
in	O
surface	O
pronunciations	O
by	O
the	O
effect	O
of	O
personmarking	O
enclitics	O
and	O
certain	O
common	O
verbal	O
prefixes	O
(	O
see	O
Shi	O
et	O
al	O
.	O
,	O

2021	O
,	O
§	O
2.3).Only	O
after	O
documentation	O
(	O
recording	O
and	O
timecoded	O
transcriptions	O
)	O
was	O
well	O
advanced	O
did	O
work	O
begin	O
on	O
a	O
finite	O
state	O
transducer	O
for	O
the	O
YMC	B-DatasetName
corpus	O
.	O

this	O
was	O
made	O
possible	O
by	O
collaboration	O
with	O
another	O
NSF	O
-	O
DEL	O
sponsored	O
project	O
.	O

3	O
The	O
code	O
was	O
written	O
by	O
Jason	O
Lilley	O
in	O
consultation	O
with	O
Amith	O
and	O
Castillo	O
.	O

As	O
the	O
FOMA	O
FST	O
was	O
being	O
built	O
,	O
FST	O
output	O
was	O
repeatedly	O
checked	O
against	O
expectations	O
based	O
on	O
the	O
morphological	O
grammar	O
until	O
no	O
discrepancies	O
were	O
noted	O
.	O

The	O
FST	O
,	O
however	O
,	O
only	O
generates	O
surface	O
forms	O
consistent	O
with	O
Castillo	O
's	O
grammar	O
.	O

If	O
speakers	O
varied	O
,	O
for	O
example	O
,	O
in	O
the	O
extent	O
of	O
vowel	O
harmonization	O
or	O
regressive	O
nasalization	O
,	O
the	O
FST	O
would	O
yield	O
only	O
one	O
surface	O
form	O
,	O
that	O
suggested	O
by	O
Castillo	O
to	O
be	O
the	O
most	O
common	O
.	O

For	O
example	O
,	O
underlying	O
be	O
3	O
e	O
(	O
3	O
)	O
=	O
an	O
4	O
(	O
house=3sgFem	O
;	O
'	O
her	O
house	O
'	O
)	O
surfaces	O
as	O
be	O
3ã4	O
even	O
though	O
for	O
some	O
speakers	O
nasalization	O
spreads	O
to	O
the	O
stem	O
initial	O
vowel	O
.	O

Note	O
,	O
then	O
,	O
that	O
the	O
surface	O
forms	O
in	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
are	O
based	O
on	O
FST	O
generation	O
from	O
an	O
underlying	O
transcription	O
as	O
input	O
and	O
not	O
from	O
the	O
direct	O
transcription	O
of	O
the	O
acoustic	O
signal	O
.	O

It	O
is	O
occasionally	O
the	O
case	O
that	O
different	O
speakers	O
might	O
extend	O
vowel	O
harmonization	O
or	O
nasalization	O
leftward	O
to	O
different	O
degrees	O
.	O

This	O
could	O
increase	O
the	O
CER	B-MetricName
and	O
WER	B-MetricName
for	O
ASR	B-TaskName
of	O
surface	O
forms	O
,	O
given	O
that	O
the	O
reference	O
for	O
evaluation	O
is	O
not	O
directly	O
derived	O
from	O
the	O
acoustic	O
signal	O
while	O
the	O
ASR	B-TaskName
hypothesis	O
is	O
so	O
derived	O
.	O

In	O
an	O
evaluation	O
across	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
development	O
and	O
test	O
sets	O
(	O
total	O
6.53	O
hours	O
)	O
of	O
the	O
relative	O
accuracy	O
of	O
ASR	B-TaskName
when	O
using	O
underlying	O
versus	O
surface	O
orthography	O
,	O
it	O
was	O
found	O
that	O
training	O
on	O
underlying	O
orthography	O
produced	O
slightly	O
greater	O
accuracy	O
than	O
training	O
on	O
surface	O
forms	O
:	O
Underlying	O
=	O
7.7/16.0	B-MetricValue
[	O
CER	B-MetricName
/	O
WER	B-MetricName
]	O
compared	O
to	O
Surface	O
=	O
7.8/16.5	B-MetricValue
[	O
CER	B-MetricName
/	O
WER	B-MetricName
]	O
(	O
Shi	O
et	O
al	O
.	O
,	O

2021	O
,	O
see	O
Table	O
4	O
)	O
.	O

The	O
decision	O
to	O
use	O
underlying	O
representations	O
in	O
ASR	B-TaskName
training	O
has	O
,	O
however	O
,	O
several	O
more	O
important	O
advantages	O
.	O

First	O
,	O
for	O
native	O
speakers	O
,	O
the	O
process	O
of	O
learning	O
a	O
deep	O
practical	O
orthography	O
means	O
that	O
one	O
learns	O
segmental	O
morphology	O
as	O
one	O
learns	O
to	O
write	O
.	O

For	O
the	O
purposes	O
of	O
YMC	O
language	O
documentation	O
,	O
the	O
ability	O
of	O
a	O
neural	O
network	O
to	O
directly	O
learn	O
segmental	O
morphology	O
as	O
part	O
of	O
ASR	O
training	O
has	O
resulted	O
in	O
a	O
YMC	O
ASR	O
output	O
across	O
all	O
three	O
corpora	O
with	O
affixes	O
and	O
clitics	O
separated	O
and	O
stem	O
-	O
final	O
elided	O
tones	O
marked	O
in	O
parentheses	O
.	O

Semi	O
-	O
or	O
un	O
-	O
supervised	O
morphological	O
learning	O
as	O
a	O
separate	O
NLP	O
task	O
is	O
unnecessary	O
when	O
ASR	O
training	O
and	O
testing	O
was	O
successfully	O
carried	O
out	O
on	O
a	O
corpus	O
with	O
basic	O
morphological	O
segmentation	O
.	O

As	O
the	O
example	O
in	O
Appendix	O
A	O
demonstrates	O
,	O
ASR	O
output	O
includes	O
basic	O
segmentation	O
at	O
the	O
morphological	O
level	O
.	O

3.3	O
Intrinsic	O
metrics	O
:	O
CER	B-MetricName
,	O
WER	B-MetricName
,	O
and	O
consistency	B-MetricName
in	I-MetricName
transcriptions	I-MetricName
used	I-MetricName
as	I-MetricName
reference	I-MetricName
:	O
Although	O
both	O
CER	B-MetricName
and	O
WER	B-MetricName
reference	O
"	O
error	O
rate	O
"	O
in	O
regards	O
to	O
character	O
and	O
word	O
,	O
respectively	O
,	O
the	O
question	O
of	O
the	O
accuracy	O
of	O
the	O
reference	O
itself	O
is	O
rarely	O
explored	O
(	O
but	O
cf	O
.	O

Saon	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

For	O
YMC	B-DatasetName
,	O
only	O
one	O
speaker	O
,	O
Castillo	O
García	O
,	O
is	O
capable	O
of	O
accurate	O
transcription	O
,	O
which	O
in	O
YMC	B-DatasetName
is	O
the	O
sole	O
gold	O
standard	O
for	O
ASR	B-TaskName
training	O
,	O
validation	O
,	O
and	O
testing	O
.	O

Thus	O
there	O
is	O
a	O
consistency	O
to	O
the	O
transcription	O
used	O
as	O
a	O
reference	O
.	O

In	O
comparison	O
,	O
for	O
Highland	O
Puebla	O
Nahuat	O
(	O
another	O
language	O
that	O
the	O
present	O
team	O
is	O
exploring	O
)	O
,	O
the	O
situation	O
is	O
distinct	O
.	O

Three	O
native	O
speaker	O
experts	O
have	O
worked	O
with	O
Amith	O
on	O
transcription	O
for	O
over	O
six	O
years	O
,	O
but	O
the	O
reference	O
for	O
ASR	B-TaskName
development	O
are	O
native	O
-	O
speaker	O
transcriptions	O
carefully	O
proofed	O
by	O
Amith	O
,	O
a	O
process	O
that	O
both	O
corrected	O
simple	O
errors	O
and	O
applied	O
a	O
single	O
standard	O
implemented	O
by	O
one	O
researcher	O
.	O

When	O
all	O
three	O
native	O
speaker	O
experts	O
were	O
asked	O
to	O
transcribe	O
the	O
same	O
90	O
minutes	O
or	O
recordings	O
,	O
and	O
the	O
results	O
were	O
compared	O
,	O
there	O
was	O
not	O
an	O
insignificant	O
level	O
of	O
variation	O
(	O
9%).The	O
aforementioned	O
scenario	O
suggests	O
the	O
impact	O
on	O
ASR	O
intrinsic	O
metrics	O
of	O
variation	O
in	O
transcriptions	O
across	O
multiple	O
annotators	O
,	O
or	O
even	O
inconsistencies	O
of	O
one	O
skilled	O
annotator	O
in	O
the	O
context	O
of	O
incipient	O
writing	O
systems	O
.	O

This	O
affects	O
not	O
only	O
ASR	B-TaskName
output	O
but	O
also	O
the	O
evaluation	O
of	O
ASR	B-TaskName
accuracy	O
via	O
character	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rates	I-MetricName
.	O

It	O
may	O
be	O
that	O
rather	O
than	O
character	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
,	O
it	O
would	O
be	O
advisable	O
to	O
consider	O
the	O
character	O
and	O
word	O
discrepancy	O
rate	O
a	O
change	O
in	O
terminology	O
that	O
perhaps	O
better	O
communicates	O
the	O
idea	O
that	O
the	O
differences	O
between	O
REF	O
and	O
HYP	O
are	O
often	O
as	O
much	O
a	O
matter	O
of	O
opinion	O
as	O
fact	O
.	O

The	O
nature	O
and	O
value	O
of	O
utilizing	O
intrinsic	O
metrics	O
(	O
e.g.	O
,	O
CER	B-MetricName
and	O
WER	B-MetricName
)	O
for	O
evaluating	O
ASR	B-TaskName
effectiveness	I-TaskName
for	I-TaskName
endangered	I-TaskName
language	I-TaskName
documentation	I-TaskName
merits	O
rethinking	O
.	O

An	O
additional	O
factor	O
that	O
has	O
emerged	O
in	O
the	O
YMC	B-DatasetName
corpora	I-DatasetName
,	O
which	O
contains	O
very	O
rapid	O
speech	O
,	O
is	O
what	O
may	O
be	O
called	O
"	O
hypercorrection	O
"	O
.	O

This	O
is	O
not	O
uncommon	O
and	O
may	O
occur	O
with	O
lenited	O
forms	O
(	O
e.g.	O
,	O
writing	O
ndi	O
1	O
ku	O
4	O
chi	O
4	O
when	O
close	O
examination	O
of	O
the	O
acoustic	O
signal	O
reveals	O
that	O
the	O
speaker	O
used	O
the	O
fully	O
acceptable	O
lenited	O
form	O
ndiu	O
14	O
chi	O
4	O
)	O
or	O
when	O
certain	O
function	O
words	O
are	O
reduced	O
,	O
at	O
times	O
effectively	O
disappearing	O
from	O
the	O
acoustic	O
signal	O
though	O
not	O
from	O
the	O
mind	O
of	O
a	O
fluent	O
speaker	O
transcriber	O
.	O

In	O
both	O
cases	O
,	O
ASR	B-TaskName
"	O
errors	O
"	O
might	O
represent	O
a	O
more	O
accurate	O
representation	O
of	O
the	O
acoustic	O
signal	O
than	O
the	O
transcription	O
of	O
even	O
the	O
most	O
highly	O
capable	O
native	O
speakers	O
.	O

The	O
above	O
discussion	O
also	O
brings	O
into	O
question	O
what	O
it	O
means	O
to	O
achieve	O
human	O
parity	O
via	O
an	O
ASR	O
system	O
.	O

Parity	O
could	O
perhaps	O
best	O
be	O
considered	O
as	O
not	O
based	O
on	O
CER	B-MetricName
and	O
WER	B-MetricName
alone	O
but	O
on	O
whether	O
ASR	B-TaskName
output	O
achieves	O
a	O
lower	O
error	O
rate	O
in	O
these	O
two	O
measurements	O
as	O
compared	O
to	O
what	O
another	O
skilled	O
human	O
transcriber	O
might	O
achieve	O
.	O

Given	O
the	O
nature	O
of	O
EL	O
documentation	O
,	O
which	O
requires	O
high	O
levels	O
of	O
accuracy	O
if	O
the	O
corpus	O
is	O
to	O
be	O
easily	O
used	O
for	O
future	O
linguistic	O
research	O
,	O
it	O
is	O
essential	O
that	O
ASR	B-TaskName
-	O
generated	O
hypotheses	O
be	O
reviewed	O
by	O
an	O
expert	O
human	O
annotator	O
before	O
permanent	O
archiving	O
.	O

Certainly	O
,	O
audio	O
can	O
be	O
archived	O
with	O
metadata	O
alone	O
or	O
with	O
unchecked	O
ASR	B-TaskName
transcriptions	O
(	O
see	O
Michaud	O
et	O
al	O
.	O
,	O

2018	O
,	O
§	O
4.3	O
and	O
4.4	O
)	O
,	O
but	O
the	O
workflow	O
envisioned	O
for	O
YMC	B-DatasetName
is	O
to	O
use	O
ASR	O
to	O
reduce	O
human	O
effort	O
while	O
the	O
archived	O
corpus	O
of	O
audio	O
and	O
text	O
maintains	O
results	O
equivalent	O
to	O
those	O
that	O
would	O
be	O
obtained	O
by	O
careful	O
,	O
and	O
laborintensive	O
,	O
expert	O
transcription	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
were	O
measured	O
for	O
YMC	B-DatasetName
corpora	O
with	O
training	O
sets	O
of	O
10	O
,	O
20	O
,	O
50	O
,	O
and	O
92	O
hours	O
.	O

The	O
CER	O
/	O
WER	O
were	O
as	O
follows	O
:	O
19.5/39.2	O
(	O
10	O
hrs	O
.	O
)	O
,	O

12.7/26.2	O
(	O
20	O
hrs	O
.	O
)	O
,	O

10.2/24.9	O
(	O
50	O
hrs	O
.	O
)	O
,	O

and	O
7.7/16.1	O
(	O
92	O
hrs	O
.	O
)	O
;	O

Table	O
5	O
in	O
Shi	O
et	O
al	O
.	O
(	O

2021	O
)	O
.	O

Measurement	O
of	O
human	O
effort	O
reduction	O
suggests	O
that	O
with	O
a	O
corpus	O
of	O
30	O
-	O
50	O
hours	O
,	O
even	O
for	O
a	O
relatively	O
challenging	O
language	O
such	O
as	O
YMC	B-DatasetName
,	O
E2E	B-TaskName
ASR	I-TaskName
can	O
achieve	O
the	O
level	O
of	O
accuracy	O
that	O
allows	O
a	O
reduction	O
of	O
human	O
effort	O
by	O
>	O
75	O
percent	O
(	O
e.g.	O
,	O
from	O
40	O
to	O
10	O
hours	O
,	O
approximately	O
)	O
.	O

These	O
totals	O
are	O
derived	O
from	O
measurements	O
with	O
the	O
FB	O
and	O
VN	O
corpora	O
,	O
the	O
two	O
corpora	O
for	O
which	O
ASR	B-TaskName
provided	O
the	O
initial	O
transcription	O
,	O
and	O
Castillo	O
subsequently	O
corrected	O
the	O
output	O
,	O
keeping	O
track	O
of	O
the	O
time	O
he	O
spent	O
.	O

For	O
the	O
first	O
corpus	O
,	O
Castillo	O
required	O
58.20	O
hours	O
to	O
correct	O
6.65	O
hours	O
of	O
audio	O
(	O
from	O
173	O
of	O
the	O
178	O
files	O
that	O
had	O
not	O
been	O
first	O
transcribed	O
by	O
a	O
speaker	O
trainee	O
)	O
.	O

This	O
yields	O
8.76	O
hours	O
of	O
effort	O
per	O
hour	O
of	O
recording	O
.	O

The	O
5.16	O
hours	O
(	O
in	O
24	O
files	O
)	O
of	O
the	O
VN	O
corpus	O
required	O
53.07	O
hours	O
to	O
correct	O
,	O
a	O
ratio	O
of	O
10.28	O
hours	O
of	O
effort	O
to	O
finalize	O
1	O
hour	O
of	O
speech	O
.	O

Over	O
the	O
entire	O
set	O
of	O
197	O
files	O
(	O
11.81	O
hours	O
)	O
,	O
human	O
effort	O
was	O
111.27	O
hours	O
,	O
or	O
9.42	O
hours	O
to	O
correct	O
1	O
hour	O
of	O
audio	O
.	O

Given	O
that	O
the	O
ASR	B-TaskName
system	I-TaskName
was	O
trained	O
on	O
an	O
underlying	O
orthography	O
,	O
the	O
final	O
result	O
of	O
<	O
10	O
hours	O
of	O
human	O
effort	O
per	O
hour	O
of	O
audio	O
is	O
a	O
transcribed	O
and	O
partially	O
parsed	O
corpus	O
.	O

Table	O
3	O
presents	O
an	O
analysis	O
of	O
two	O
lines	O
of	O
a	O
recording	O
that	O
was	O
first	O
processed	O
by	O
E2E	B-TaskName
ASR	I-TaskName
and	O
corrected	O
by	O
Castillo	O
García	O
.	O

A	O
fuller	O
presentation	O
and	O
analysis	O
are	O
offered	O
in	O
the	O
Appendix	O
.	O

This	O
focus	O
on	O
extrinsic	O
metrics	O
reflects	O
the	O
realization	O
that	O
the	O
ultimate	O
goal	O
of	O
computational	O
systems	O
is	O
not	O
to	O
achieve	O
the	O
lowest	O
CER	B-MetricName
and	O
WER	B-MetricName
but	O
to	O
help	O
documentation	O
initiatives	O
more	O
efficiently	O
produce	O
results	O
that	O
will	O
benefit	O
future	O
stakeholders	O
.	O

Recently	O
,	O
E2E	B-TaskName
ASR	I-TaskName
has	O
reached	O
comparable	O
or	O
better	O
performances	O
than	O
conventional	O
Hidden	B-MethodName
-	I-MethodName
Markov	I-MethodName
-	I-MethodName
Model	I-MethodName
-	O
based	O
ASR	B-TaskName
(	O
Graves	O
and	O
Jaitly	O
,	O
2014;Chiu	O
et	O
al	O
.	O
,	O

2018;Pham	O
et	O
al	O
.	O
,	O

2019;Karita	O
et	O
al	O
.	O
,	O

2019a;Shi	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

In	O
practice	O
,	O
E2E	B-TaskName
ASR	I-TaskName
systems	I-TaskName
are	O
less	O
affected	O
by	O
linguistic	O
constraints	O
and	O
are	O
generally	O
easier	O
to	O
train	O
.	O

The	O
benefits	O
of	O
such	O
systems	O
are	O
reflected	O
in	O
the	O
recent	O
trends	O
of	O
using	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
for	O
EL	O
documentation	O
(	O
Adams	O
et	O
al	O
.	O
,	O

2020;Thai	O
et	O
al	O
.	O
,	O

2020;Matsuura	O
et	O
al	O
.	O
,	O

2020;Hjortnaes	O
et	O
al	O
.	O
,	O

2020;Shi	O
et	O
al	O
.	O
,	O

2021).In	O
developing	O
E2E	B-TaskName
ASR	I-TaskName
recipes	O
for	O
YMC	B-DatasetName
,	O
we	O
have	O
adopted	O
transformer	B-MethodName
and	I-MethodName
conformerbased	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
networks	I-MethodName
with	I-MethodName
hybrid	I-MethodName
CTC	I-MethodName
/	I-MethodName
attention	I-MethodName
training	O
(	O
Karita	O
et	O
al	O
.	O
,	O

2019b;Watanabe	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

We	O
used	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
(	O
trainsplit	O
)	O
for	O
training	O
and	O
other	O
YMC	B-DatasetName
corpora	O
for	O
evaluation	O
.	O

The	O
hyper	O
-	O
parameters	O
for	O
the	O
training	O
and	O
decoding	O
follow	O
Shi	O
et	O
al	O
.	O
(	O

2021	O
)	O
.	O

Seven	O
systems	O
with	O
different	O
modeling	O
units	O
are	O
examined	O
in	O
the	O
experiments	O
.	O

Four	O
systems	O
employ	O
the	O
byte	B-MethodName
-	I-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
method	O
trained	O
from	O
unigram	O
language	O
models	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
,	O
with	O
transcription	B-HyperparameterName
alphabets	I-HyperparameterName
limited	O
to	O
the	O
150	B-HyperparameterName
,	O
500	B-HyperparameterName
,	O
1000	B-HyperparameterName
,	O
and	O
1500	B-HyperparameterName
most	O
frequent	O
byte	O
-	O
pairs	O
in	O
the	O
training	O
set	O
.	O

The	O
other	O
three	O
ASR	O
systems	O
adopt	O
linguistic	O
units	O
,	O
including	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
mora	B-MethodName
.	O

The	O
YM	B-DatasetName
word	O
is	O
defined	O
as	O
a	O
stem	O
with	O
all	O
prefixes	O
(	O
such	O
as	O
completetive	O
ni	O
1	O
-	O
,	O
causative	O
sa	O
4	O
-	O
,	O
and	O
iterative	O
nda	O
3	O
-	O
)	O
separated	O
from	O
the	O
stem	O
by	O
a	O
hyphen	O
;	O
and	O
all	O
enclitics	O
(	O
particularly	O
person	O
markers	O
for	O
subjects	O
,	O
objects	O
,	O
and	O
possessors	O
,	O
such	O
as	O
=	O
yu	O
3	O
,	O
1sg	O
;	O
=	O
un	O
4	O
,	O
2sg	O
;	O
=	O
an	O
4	O
,	O
3sgFem	O
;	O
=	O
o	O
4	O
,	O
1plIncl	O
;	O
as	O
well	O
as	O
=	O
lu	O
3	O
,	O
augmentive	O
)	O
.	O

Many	O
vowel	O
-	O
initial	O
enclitics	O
have	O
alternative	O
vowels	O
,	O
and	O
many	O
encl	O
-	O
ASR	O
yo	O
'	O
3	O
o	O
4	O
xi	O
13	O
i	O
2	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
yo	O
4	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
Exp	O
yo	O
'	O
3	O
o	O
4	O
xi	O
1	O
i	O
32	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
ji	O
'	O
4	O
in	O
(	O
4	O
)	O
=	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
Note	O
ASR	O
missed	O
the	O
word	O
ji	O
'	O
4	O
in	O
4	O
(	O
'	O
with	O
'	O
,	O
comitative	O
)	O
and	O
as	O
a	O
result	O
wrote	O
the	O
1plInclusive	O
as	O
an	O
independent	O
pronoun	O
and	O
not	O
an	O
enclitic	O
.	O

ASR	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
coco	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
Exp	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
ko	O
4	O
ko	O
13	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
Note	O
ASR	B-TaskName
suggested	O
Spanish	O
'	O
coco	O
'	O
coconut	O
for	O
Mixtec	O
ko	O
4	O
ko	O
13	O
(	O
'	O
to	O
be	O
abundant[plants	O
]	O
'	O
)	O
itics	O
have	O
alternative	O
tones	O
,	O
depending	O
on	O
stemfinal	O
vowel	O
and	O
tone	O
,	O
respectively	O
.	O

Morphemes	O
are	O
stems	O
,	O
prefixes	O
,	O
and	O
enclitics	O
.	O

The	O
inflectional	O
tone	O
is	O
not	O
segmented	O
out	O
.	O

The	O
right	O
boundary	O
of	O
a	O
mora	O
is	O
a	O
vowel	O
or	O
dipthong	O
(	O
with	O
an	O
optional	O
<	O
n	O
>	O
to	O
indicate	O
a	O
nasalized	O
vowel	O
)	O
followed	O
by	O
a	O
tone	O
.	O

The	O
left	O
boundary	O
is	O
a	O
preceding	O
mora	O
or	O
word	O
boundary	O
.	O

Thus	O
the	O
word	O
ni	O
1	O
-xa	O
3	O
nda	O
2	O
=	O
e	O
4	O
(	O
completive	O
-	O
play(guitar)-1plIncl	O
)	O
would	O
be	O
divided	O
into	O
three	O
morphemes	O
ni	O
1	O
-	O
,	O
xa	O
3	O
nda	O
2	O
,	O
=	O
e	O
4	O
and	O
into	O
four	O
morae	O
given	O
that	O
xa	O
3	O
nda	O
2	O
would	O
be	O
segmented	O
as	O
xa	O
3	O
,	O
nda	O
2	O
.We	O
adopt	O
recognizer	O
output	O
voting	O
error	O
reduction	O
(	O
ROVER	O
)	O
for	O
the	O
hypotheses	O
combination	O
(	O
Fiscus	O
,	O
1997	O
)	O
.	O

Three	O
combinations	O
have	O
been	O
evaluated	O
:	O
(	O
1	O
)	O
ROVER	B-MethodName
among	O
only	O
linguistic	O
units	O
(	O
i.e.	O
,	O
morae	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
word	B-MethodName
)	O
,	O
(	O
2	O
)	O
ROVER	B-MethodName
among	O
only	O
sub	O
-	O
word	O
units	O
(	O
in	O
this	O
case	O
BPE	B-MethodName
)	O
;	O
and	O
(	O
3	O
)	O
ROVER	B-MethodName
combination	O
utilizing	O
all	O
seven	O
systems	O
.	O

Experimental	O
results	O
are	O
presented	O
in	O
two	O
subsections	O
.	O

The	O
first	O
addresses	O
the	O
performance	O
of	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
across	O
three	O
corpora	O
,	O
each	O
with	O
slightly	O
different	O
recording	O
systems	O
and	O
content	O
.	O

As	O
clear	O
from	O
the	O
preceding	O
discussion	O
and	O
illustrated	O
in	O
Table	O
2	O
,	O
in	O
addition	O
to	O
training	O
on	O
the	O
word	O
unit	O
,	O
the	O
YMC	O
E2E	B-TaskName
ASR	I-TaskName
system	I-TaskName
was	O
trained	O
on	O
six	O
additional	O
linguistic	O
and	O
informational	O
sub	O
-	O
word	O
units	O
.	O

ROVER	B-MethodName
was	O
then	O
used	O
to	O
produce	O
composite	O
systems	O
in	O
which	O
the	O
outputs	O
of	O
all	O
seven	O
systems	O
were	O
combined	O
in	O
three	O
distinct	O
manners	O
.	O

In	O
all	O
cases	O
,	O
ROVER	B-MethodName
combinations	O
improved	O
the	O
result	O
of	O
any	O
individual	O
system	O
,	O
including	O
the	O
averages	O
for	O
either	O
of	O
the	O
two	O
types	O
of	O
units	O
:	O
linguistic	O
and	O
informational	O
.	O

4	O
Those	O
interested	O
in	O
the	O
recordings	O
and	O
associated	O
ELAN	O
files	O
may	O
visit	O
Amith	O
and	O
Castillo	O
García	O
(	O
2020	O
)	O
.	O

As	O
evident	O
in	O
Table	O
2	O
,	O
across	O
all	O
corpora	O
,	O
informational	O
units	O
(	O
BPE	O
)	O
are	O
more	O
efficient	O
than	O
linguistic	O
units	O
(	O
word	O
,	O
morpheme	O
,	O
morae	O
)	O
in	O
regards	O
to	O
ASR	O
accuracy	O
.	O

The	O
average	O
CER	B-MetricName
/	O
WER	B-MetricName
for	O
linguistic	O
units	O
(	O
rows	O
A	O
-	O
C	O
)	O
was	O
10.4/	B-MetricValue
19.5	B-MetricValue
(	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
,	O
13.6/23.3	B-MetricValue
(	O
FB	B-DatasetName
)	O
,	O
and	O
10.7/21.7	B-MetricValue
(	O
VN	B-DatasetName
)	O
.	O

The	O
corresponding	O
figures	O
for	O
the	O
BPE	O
units	O
(	O
rows	O
D	O
-	O
G	O
)	O
were	O
7.7/16.0	B-MetricValue
(	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
,	O
9.7/19.5	B-MetricValue
(	O
FB	B-DatasetName
)	O
,	O
and	O
6.8/16.8	B-MetricValue
(	O
VN	B-DatasetName
)	O
.	O

In	O
terms	O
of	O
percentage	O
differences	O
between	O
the	O
two	O
types	O
of	O
units	O
,	O
the	O
numbers	O
are	O
not	O
insignificant	O
.	O

In	O
regards	O
to	O
CER	B-MetricName
,	O
performance	O
improved	O
from	O
linguistic	O
to	O
informational	O
units	O
by	O
26.0	B-MetricValue
,	O
28.7	B-MetricValue
,	O
and	O
36.4	B-MetricValue
percent	O
across	O
the	O
Exp(Test	B-DatasetName
)	I-DatasetName
,	O
FB	B-DatasetName
,	O
and	O
VN	B-DatasetName
corpora	O
.	O

In	O
regards	O
to	O
WER	B-MetricName
,	O
performance	O
improved	O
by	O
17.9	B-MetricValue
,	O
16.3	B-MetricValue
,	O
and	O
22.6	B-MetricValue
percent	O
across	O
the	O
same	O
three	O
corpora	O
.	O

The	O
experiments	O
also	O
addressed	O
two	O
remaining	O
questions	O
:	O
(	O
1	O
)	O
does	O
unweighted	O
ROVER	B-MethodName
combination	O
improve	O
the	O
accuracy	O
of	O
ASR	B-TaskName
results	O
;	O
(	O
2	O
)	O
does	O
adding	O
linguistic	O
unit	O
performance	O
units	O
to	O
the	O
ROVER	B-MethodName
"	O
voting	O
pool	O
"	O
improve	O
results	O
over	O
a	O
combination	O
of	O
only	O
BPE	B-MethodName
units	I-MethodName
.	O

In	O
regards	O
to	O
the	O
first	O
question	O
:	O
ROVER	B-MethodName
always	O
improves	O
results	O
over	O
any	O
individual	O
system	O
(	O
compare	O
row	O
H	O
to	O
rows	O
A	O
,	O
B	O
,	O
and	O
C	O
,	O
and	O
row	O
I	O
to	O
rows	O
D	O
,	O
E	O
,	O
F	O
,	O
and	O
G	O
)	O
.	O

The	O
second	O
question	O
is	O
addressed	O
by	O
comparing	O
rows	O
I	O
(	O
ROVER	B-MethodName
applied	O
only	O
to	O
the	O
four	O
BPE	B-MethodName
results	O
)	O
to	O
J	O
(	O
adding	O
the	O
ASR	O
results	O
for	O
the	O
three	O
linguistic	O
units	O
into	O
the	O
combination	O
)	O
.	O

In	O
only	O
one	O
of	O
the	O
six	O
cases	O
(	O
CER	B-MetricName
of	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
does	O
including	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
morae	O
lower	O
the	O
error	O
rate	O
from	O
the	O
results	O
of	O
a	O
simple	O
combination	O
of	O
the	O
four	O
BPE	B-MethodName
results	O
(	O
in	O
this	O
case	O
from	O
7.6	B-MetricValue
[	O
row	O
I	O
]	O
to	O
7.4	B-MetricValue
[	O
row	O
J	O
]	O
)	O
.	O

In	O
one	O
case	O
,	O
there	O
is	O
no	O
change	O
(	O
CER	B-MetricName
for	O
the	O
VN	B-DatasetName
corpus	O
)	O
and	O
in	O
four	O
cases	O
,	O
including	O
linguistic	O
units	O
slightly	O
worsens	O
the	O
score	O
from	O
the	O
combination	O
of	O
BPE	B-MethodName
units	I-MethodName
alone	O
(	O
row	O
I	O
with	O
bold	O
numbers	O
)	O
.	O

The	O
implication	O
of	O
the	O
preceding	O
is	O
that	O
ASR	O
using	O
linguistic	O
units	O
yields	O
significantly	O
lower	O
accuracy	O
than	O
ASR	B-TaskName
that	O
uses	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
.	O

Combining	O
the	O
former	O
with	O
the	O
latter	O
in	O
an	O
unweighted	O
ROVER	B-MethodName
system	O
in	O
most	O
cases	O
does	O
not	O
improve	O
results	O
.	O

Whether	O
a	O
weighted	O
combinatory	O
system	O
would	O
do	O
better	O
is	O
a	O
question	O
that	O
will	O
need	O
to	O
be	O
explored	O
.	O

A	O
fundamental	O
element	O
of	O
endangered	O
language	O
documentation	O
is	O
the	O
creation	O
of	O
an	O
extensive	O
corpus	O
of	O
audio	O
recordings	O
accompanied	O
by	O
timecoded	O
annotations	O
in	O
interlinear	O
format	O
.	O

In	O
the	O
best	O
of	O
cases	O
,	O
such	O
annotations	O
include	O
an	O
accurate	O
transcription	O
aligned	O
with	O
morphological	O
segmentation	O
,	O
glossing	O
,	O
and	O
free	O
translations	O
.	O

The	O
degree	O
to	O
which	O
such	O
corpus	O
creation	O
is	O
facilitated	O
is	O
the	O
extrinsic	O
metric	O
by	O
which	O
ASR	O
contributions	O
to	O
EL	O
documentation	O
should	O
be	O
considered	O
.	O

The	O
project	O
here	O
discussed	O
suggests	O
a	O
path	O
to	O
creating	O
such	O
corpora	O
using	O
end	O
-	O
to	O
-	O
end	O
ASR	O
technology	O
to	O
build	O
up	O
the	O
resources	O
(	O
30	O
-	O
50	O
hours	O
)	O
necessary	O
to	O
train	O
an	O
ASR	B-TaskName
system	O
with	O
perhaps	O
a	O
6	B-MetricValue
-	I-MetricValue
10	I-MetricValue
percent	I-MetricValue
CER	B-MetricName
.	O

Once	O
this	O
threshold	O
is	O
reached	O
,	O
it	O
is	O
unlikely	O
that	O
further	O
improvement	O
will	O
significantly	O
reduce	O
the	O
human	O
effort	O
needed	O
to	O
check	O
the	O
ASR	B-TaskName
output	O
for	O
accuracy	O
.	O

Indeed	O
,	O
even	O
if	O
there	O
are	O
no	O
"	O
errors	O
"	O
in	O
the	O
ASR	B-TaskName
output	O
,	O
confirmation	O
of	O
this	O
through	O
careful	O
revision	O
of	O
the	O
recording	O
of	O
the	O
transcription	O
would	O
probably	O
still	O
take	O
3	O
-	O
4	O
hours	O
.	O

The	O
effort	O
reduction	O
of	O
75	O
percent	O
documented	O
here	O
for	O
YMC	B-DatasetName
is	O
,	O
therefore	O
,	O
approaching	O
what	O
may	O
be	O
considered	O
the	O
minimum	O
amount	O
of	O
time	O
to	O
proofread	O
transcription	O
of	O
natural	O
speech	O
in	O
an	O
endangered	O
language	O
.	O

This	O
project	O
has	O
also	O
demonstrated	O
the	O
advantage	O
of	O
using	O
a	O
practical	O
orthography	O
that	O
separates	O
affixes	O
and	O
clitics	O
.	O

In	O
a	O
relatively	O
isolating	O
language	O
such	O
as	O
YM	O
,	O
such	O
a	O
system	O
is	O
not	O
difficult	O
for	O
native	O
speakers	O
to	O
write	O
nor	O
for	O
ASR	B-TaskName
systems	O
to	O
learn	O
.	O

It	O
has	O
the	O
advantage	O
of	O
creating	O
a	O
workflow	O
in	O
which	O
parsed	O
text	O
is	O
the	O
direct	O
output	O
of	O
E2E	B-TaskName
ASR	I-TaskName
.	O

The	O
error	O
rate	O
evaluations	O
across	O
the	O
spectrum	O
of	O
corpora	O
and	O
CER	B-MetricName
/	O
WER	B-MetricName
also	O
demonstrate	O
the	O
advantage	O
of	O
using	O
subword	O
units	O
such	O
as	O
BPE	O
and	O
subsequent	O
processing	O
by	O
ROVER	B-MethodName
for	O
system	O
combination	O
(	O
see	O
above	O
and	O
Table	O
2	O
)	O
.	O

The	O
error	O
rates	O
could	O
perhaps	O
be	O
lowered	O
further	O
as	O
the	O
corpus	O
increases	O
in	O
size	O
,	O
as	O
more	O
care	O
is	O
placed	O
on	O
recording	O
environments	O
,	O
and	O
as	O
normalization	O
eliminates	O
reported	O
errors	O
for	O
minor	O
discrepancies	O
such	O
as	O
in	O
transcription	O
of	O
back	O
-	O
channel	O
cues	O
.	O

But	O
such	O
lower	O
error	O
rates	O
will	O
probably	O
not	O
significantly	O
reduce	O
the	O
time	O
for	O
final	O
revision	O
.	O

A	O
final	O
question	O
concerns	O
additional	O
steps	O
once	O
CER	B-MetricName
is	O
reduced	O
to	O
6	B-MetricValue
-	I-MetricValue
8	I-MetricValue
percent	I-MetricValue
,	O
and	O
additional	O
improvements	O
to	O
ASR	B-TaskName
would	O
not	O
significantly	O
affect	O
the	O
human	O
effort	O
needed	O
to	O
produce	O
a	O
high	O
-	O
quality	O
time	O
-	O
coded	O
transcription	O
and	O
segmentation	O
.	O

Four	O
topics	O
are	O
suggested	O
:	O
(	O
1	O
)	O
address	O
issues	O
of	O
noise	O
,	O
overlapping	O
speech	O
,	O
and	O
other	O
challenging	O
recording	O
situations	O
;	O
(	O
2	O
)	O
focus	O
on	O
transfer	O
learning	O
to	O
related	O
languages	O
;	O
(	O
3	O
)	O
explore	O
the	O
impact	O
of	O
"	O
colonialization	O
"	O
by	O
a	O
dominant	O
language	O
;	O
and	O
(	O
4	O
)	O
focus	O
additional	O
ASR	B-TaskName
-	O
supported	O
corpus	O
development	O
on	O
producing	O
material	O
for	O
documentation	O
of	O
endangered	O
cultural	O
knowledge	O
,	O
a	O
facet	O
of	O
documentation	O
that	O
is	O
often	O
absent	O
from	O
endangered	O
language	O
documentation	O
projects	O
.	O

A	O
Analysis	O
of	O
ASR	B-TaskName
errors	O
in	O
one	O
recording	O
from	O
the	O
FB	O
corpus	O
Unique	O
identifier	O
:	O
2017	O
-	O
12	O
-	O
01	O
-	O
b	O
Speakers	O
:	O
Constantino	O
Teodoro	O
Bautista	O
and	O
Esteban	O
Guadalupe	O
Sierra	O
Spanish	O
:	O
The	O
first	O
13	O
seconds	O
(	O
3	O
segments	O
)	O
of	O
the	O
recording	O
were	O
of	O
a	O
Spanish	O
speaker	O
describing	O
the	O
plant	O
being	O
collected	O
(	O
Passiflora	O
biflora	O
Lam	O
.	O
)	O

and	O
have	O
not	O
been	O
included	O
below	O
.	O

Note	O
:	O
A	O
total	O
16	O
out	O
of	O
33	O
segments	O
/	O
utterances	O
are	O
without	O
ASR	B-TaskName
error	O
.	O

These	O
are	O
marked	O
with	O
an	O
asterisk	O
.	O

Original	O
recording	O
and	O
ELAN	O
file	O
:	O
Download	O
at	O
http://www.balsas-nahuatl.org/NLP	O
4	O
*	O
.	O

00:00:13.442	O
-	O
>	O
00:00:17.105	O
ASR	O
constantino	O
teodoro	O
bautista	O
Exp	O
Constantino	O
Teodoro	O
Bautista	O
.	O

Notes	O
:	O
ASR	B-TaskName
does	O
not	O
output	O
caps	O
or	O
punctuation	O
.	O

5	O
*	O
.	O

00:00:17.105	O
-	O
>	O
00:00:19.477	O
ASR	B-TaskName
ya	O
1	O
mi	O
4	O
i	O
4	O
tu	O
1	O
tu	O
'	O
4	O
un	O
4	O
ku	O
3	O
rra	O
42	O
Exp	O
Ya	O
1	O
mi	O
4	O
i	O
4	O
tu	O
1	O
tu	O
'	O
4	O
un	O
4	O
ku	O
3	O
rra	O
42	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.6	O
.	O

00:00:19.477	O
-	O
>	O
00:00:23.688	O
ASR	O
ta	O
1	O
mas	O
4	O
tru	O
2	O
tela	O
ya	O
1	O
i	O
3	O
chi	O
4	O
ya	O
3	O
tin	O
3	O
ye	O
'	O
1	O
4e	O
4	O
ku	O
3	O
rra	O
42	O
ndi	O
4	O
covalentín	O
yo	O
'	O
4	O
o	O
4	O
Exp	O
ta	O
1	O
mas	O
4	O
tru	O
2	O
Tele	O
ya	O
1	O
i	O
3	O
chi	O
4	O
ya	O
3	O
tin	O
3	O
ye	O
'	O
1	O
4e	O
4	O
ku	O
3	O
rra	O
42	O
Nicu	O
Valentín	O
yo	O
'	O
4	O
o	O
4	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
the	O
proper	O
name	O
,	O
Nicu	O
Valentín	O
(	O
short	O
for	O
Nicolás	O
Valentín	O
)	O
but	O
did	O
get	O
the	O
accent	O
on	O
Valentín	O
,	O
while	O
mistaking	O
the	O
first	O
name	O
Nicu	O
for	O
ndi	O
4	O
co[valentín	O
]	O
7	O
*	O
.	O

00:00:23.688	O
-	O
>	O
00:00:31.086	O
ASR	B-TaskName
ya	O
1	O
i	O
3	O
chi	O
4	O
kwa	O
'	O
1	O
an	O
(	O
1	O
)	O
=	O
e	O
4	O
tan	O
3	O
xa	O
1	O
a	O
(	O
1	O
)	O
=	O
e	O
4	O
ku	O
3	O
rra	O
42	O
chi	O
4	O
ñu	O
3	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
ya	O
1	O
kwa	O
'	O
1	O
an	O
1	O
ni	O
1	O
nu	O
3	O
yo	O
'	O
4	O
o	O
4	O
ju	O
13	O
ta	O
'	O
3	O
an	O
2	O
=	O
ndu	O
1	O
ya	O
1	O
ko	O
4	O
ndo	O
3	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
Exp	O
ya	O
1	O
i	O
3	O
chi	O
4	O
kwa	O
'	O
1	O
an	O
(	O
1	O
)	O
=	O
e	O
4	O
tan	O
3	O
xa	O
1	O
a	O
(	O
1	O
)	O
=	O
e	O
4	O
ku	O
3	O
rra	O
42	O
chi	O
4	O
ñu	O
3	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
ya	O
1	O
kwa	O
'	O
1	O
an	O
1	O
ni	O
1	O
nu	O
3	O
yo	O
'	O
4	O
o	O
4	O
ju	O
13	O
ta	O
'	O
3	O
an	O
2	O
=	O
ndu	O
1	O
ya	O
1	O
ko	O
4	O
ndo	O
3	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.8	O
*	O
.	O

00:00:31.086	O
-	O
>	O
00:00:37.318	O
ASR	B-TaskName
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
kwi	O
4	O
i	O
24	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
ya	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
i	O
4	O
yo	O
(	O
2	O
)	O
=	O
a	O
2	O
mi	O
4	O
i	O
4	O
bi	O
1	O
xin	O
3	O
tan	O
3	O
Exp	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
kwi	O
4	O
i	O
24	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
ya	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
i	O
4	O
yo	O
(	O
2	O
)	O
=	O
a	O
2	O
mi	O
4	O
i	O
4	O
bi	O
1	O
xin	O
3	O
tan	O
3	O
Notes	O
:	O
The	O
ASR	B-TaskName
hypothesis	O
missed	O
the	O
inanimate	O
enclitic	O
after	O
the	O
verb	O
su	O
4	O
kun	O
1	O
and	O
as	O
a	O
result	O
failed	O
to	O
mark	O
the	O
elision	O
of	O
the	O
stem	O
-	O
final	O
low	O
tone	O
as	O
would	O
occur	O
before	O
a	O
following	O
low	O
-	O
tone	O
enclitic.9	O
.	O

00:00:37.318	O
-	O
>	O
00:00:42.959	O
ASR	O
yo	O
'	O
3	O
o	O
4	O
xi	O
13	O
i	O
2	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
yo	O
4	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
Exp	O
yo	O
'	O
3	O
o	O
4	O
xi	O
1	O
i	O
32	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
ji	O
'	O
4	O
in	O
(	O
4	O
)	O
=	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
,	O
Notes	O
:	O
ASR	O
missed	O
the	O
word	O
ji	O
'	O
4	O
in	O
4	O
(	O
'	O
with	O
'	O
,	O
comitative	O
)	O
and	O
as	O
a	O
result	O
wrote	O
the	O
1plInclusive	O
as	O
an	O
independent	O
pronoun	O
and	O
not	O
an	O
enclitic.10	O
.	O

00:00:42.959	O
-	O
>	O
00:00:49.142	O
ASR	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
coco	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2Exp	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
ko	O
4	O
ko	O
13	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
,	O
Notes	O
:	O
ASR	B-TaskName
suggested	O
Spanish	O
'	O
coco	O
'	O
coconut	O
for	O
Mixtec	O
ko	O
4	O
ko	O
13	O
(	O
'	O
to	O
be	O
abundant[plants	O
]	O
'	O
)	O
.	O

Note	O
that	O
'	O
coco	O
'	O
was	O
spelled	O
as	O
it	O
is	O
in	O
Spanish	O
and	O
no	O
tones	O
were	O
included	O
in	O
the	O
ASR	B-TaskName
output.11	O
.	O

00:00:49.142	O
-	O
>	O
00:00:53.458	O
ASR	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
43	O
Exp	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
42	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
tone	O
42	O
,	O
writing	O
43	O
instead	O
.	O

Note	O
that	O
the	O
two	O
tone	O
patterns	O
are	O
alternate	O
forms	O
of	O
the	O
same	O
word	O
,	O
the	O
copula	O
used	O
in	O
regards	O
to	O
objects.12	O
*	O
.	O

00:00:53.458	O
-	O
>	O
00:00:57.279	O
ASR	O
tan	O
3	O
o	O
4	O
ra	O
2	O
chi	O
4	O
chi	O
13	O
=	O
a	O
2	O
ndi	O
4	O
ndu	O
1	O
u	O
4	O
nde	O
'	O
3	O
e	O
4	O
ku	O
4	O
u	O
4	O
ndu	O
1	O
u	O
4	O
=	O
a	O
3	O
Exp	O
tan	O
3	O
o	O
4	O
ra	O
2	O
chi	O
4	O
chi	O
13	O
=	O
a	O
2	O
ndi	O
4	O
ndu	O
1	O
u	O
4	O
nde	O
'	O
3	O
e	O
4	O
ku	O
4	O
u	O
4	O
ndu	O
1	O
u	O
4	O
=	O
a	O
3	O
.	O

Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	O
hypothesis.13	O
*	O
.	O

00:00:57.279	O
-	O
>	O
00:01:02.728	O
ASR	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
(	O
4	O
)	O
=	O
a	O
2	O
ni	O
1	O
-xa	O
'	O
3	O
nda	O
2	O
=	O
e	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
tun	O
4	O
si	O
13	O
su	O
2	O
kan	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
tan	O
3	O
ndi	O
4	O
Exp	O
Yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
(	O
4	O
)	O
=	O
a	O
2	O
ni	O
1	O
-xa	O
'	O
3	O
nda	O
2	O
=	O
e	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
tun	O
4	O
si	O
13	O
su	O
2	O
kan	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
tan	O
3	O
ndi	O
4	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.14	O
.	O

00:01:02.728	O
-	O
>	O
00:01:06.296	O
ASR	O
su	O
14	O
u	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
1	O
ba	O
42	O
ndi	O
4	O
su	O
14	O
u	O
3	O
ki	O
3	O
ti	O
4	O
ja	O
4	O
xi	O
24	O
=	O
ri	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
mi	O
4	O
i	O
4	O
ba	O
(	O
3	O
)	O
=	O
e	O
3	O
Exp	O
su	O
14	O
u	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
1	O
ba	O
42	O
tan	O
3	O
ni	O
4	O
su	O
14	O
u	O
3	O
ki	O
3	O
ti	O
4	O
ja	O
4	O
xi	O
24	O
=	O
ri	O
4	O
,	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
mi	O
4	O
i	O
4	O
ba	O
(	O
3	O
)	O
=	O
e	O
3	O
,	O
Notes	O
:	O
ASR	B-TaskName
mistakenly	O
proposed	O
ndi	O
4	O
for	O
tan	O
3	O
ni	O
4	O
.15	O
*	O
.	O

00:01:06.296	O
-	O
>	O
00:01:10.981	O
ASR	O
tan	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
sa	O
3	O
ba	O
3	O
xia	O
4	O
an	O
4	O
ku	O
3	O
ta	O
'	O
3	O
an	O
2	O
=	O
e	O
4	O
=	O
e	O
2	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
(	O
3	O
)	O
=	O
a	O
2	O
kwa	O
1	O
nda	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
nda	O
'	O
3	O
a	O
4	O
i	O
3	O
tun	O
4	O
Exp	O
tan	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
sa	O
3	O
ba	O
3	O
xia	O
4	O
an	O
4	O
ku	O
3	O
ta	O
'	O
3	O
an	O
2	O
=	O
e	O
4	O
=	O
e	O
2	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
(	O
3	O
)	O
=	O
a	O
2	O
kwa	O
1	O
nda	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
nda	O
'	O
3	O
a	O
4	O
i	O
3	O
tun	O
4	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	O
hypothesis.16	O
.	O

00:01:10.981	O
-	O
>	O
00:01:14.768	O
ASR	O
u	O
1	O
xi	O
1	O
an	O
4	O
nda	O
1	O
xa	O
'	O
1	O
un	O
1	O
metru	O
ka	O
1	O
a	O
3	O
mi	O
4	O
i	O
4	O
i	O
4	O
yo	O
2	O
i	O
3	O
tun	O
4	O
ndo	O
3	O
o	O
3	O
tan	O
3	O
ko	O
4	O
ko	O
13	O
=	O
a	O
2	O
kwa	O
1	O
nde	O
3	O
e	O
3	O
ni	O
1	O
nu	O
3	O
Exp	O
u	O
1	O
xi	O
1	O
an	O
4	O
nda	O
1	O
xa	O
'	O
1	O
un	O
1	O
metru	O
ka	O
1	O
a	O
3	O
mi	O
4	O
i	O
4	O
i	O
4	O
yo	O
2	O
i	O
3	O
tun	O
4	O
ndo	O
3	O
o	O
3	O
tan	O
3	O
ko	O
4	O
ko	O
13	O
=	O
a	O
2	O
kwa	O
1	O
nda	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
ni	O
1	O
nu	O
3	O
,	O
Notes	O
:	O
Not	O
only	O
did	O
ASR	O
recognize	O
the	O
Spanish	O
metru	O
borrowing	O
but	O
wrote	O
it	O
according	O
to	O
our	O
conventions	O
,	O
without	O
tone	O
.	O

Note	O
that	O
the	O
correct	O
underlying	O
form	O
kwa	O
1	O
nda	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
(	O
progressive	O
of	O
'	O
to	O
climb	O
[	O
e.g.	O
,	O
a	O
vine	O
]	O
'	O
with	O
3sg	O
enclitic	O
for	O
inanimates	O
=	O
e	O
2	O
)	O
surfaces	O
as	O
kwa	O
1	O
nde	O
3	O
e	O
2	O
quite	O
close	O
to	O
the	O
ASR	O
hypothesis	O
of	O
kwa	O
1	O
nde	O
3	O
e	O
3	O
,	O
which	O
exists	O
,	O
but	O
as	O
a	O
distinct	O
word	O
(	O
progressive	O
of	O
'	O
to	O
enter[pl]').17	O
*	O
.	O

00:01:14.768	O
-	O
>	O
00:01:18.281	O
ASR	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
Exp	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
,	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis	O
.	O

The	O
authors	O
gratefully	O
acknowledge	O
the	O
following	O
support	O
for	O
documenting	O
and	O
studying	O
Yoloxóchitl	O
Mixtec	O
:	O
National	O
Science	O
Foundation	O
,	O
Documenting	O
Endangered	O
Languages	O
(	O
DEL	O
):	O
Awards	O
1761421	O
,	O
1500595	O
,	O
0966462	O
(	O
Amith	O
,	O
PI	O
on	O
all	O
three	O
;	O
the	O
second	O
was	O
a	O
collaborative	O
project	O
with	O
SRI	O
International	O
,	O
Award	O
1500738	O
,	O
Andreas	O
Kathol	O
,	O
PI	O
)	O
;	O
Endangered	O
Language	O
Documentation	O
Programme	O
:	O
Awards	O
MDP0201	O
,	O
PPG0048	O
(	O
Amith	O
,	O
PI	O
on	O
both	O
)	O
.	O

The	O
following	O
support	O
is	O
acknowledged	O
for	O
documenting	O
and	O
studying	O
Highland	O
Puebla	O
Nahuat	O
:	O
NSF	O
DEL	O
:	O
Awards	O
:	O
1401178	O
,	O
0756536	O
(	O
Amith	O
,	O
PI	O
on	O
both	O
awards	O
)	O
;	O
National	O
Endowment	O
for	O
the	O
Humanities	O
,	O
Preservation	O
and	O
Access	O
:	O
PD-50031	O
-	O
14	O
(	O
Amith	O
,	O
PI	O
)	O
;	O
Endangered	O
Language	O
Documentation	O
Programme	O
:	O
Award	O
MDP0272	O
(	O
Amith	O
,	O
PI	O
)	O
;	O
and	O
the	O
Comisión	O
Nacional	O
para	O
el	O
Conocimiento	O
y	O
Uso	O
de	O
la	O
Biodiversidad	O
,	O
Mexico	O
(	O
Gerardo	O
Salazar	O
,	O
PI	O
;	O
Amith	O
,	O
co	O
-	O
PI	O
)	O
.	O

The	O
FOMA	O
FST	O
for	O
Yoloxóchitl	O
Mixtec	O
was	O
built	O
by	O
Jason	O
Lilley	O
,	O
Amith	O
,	O
and	O
Castillo	O
with	O
support	O
from	O
NSF	O
DEL	O
Award	O
1360670	O
(	O
Christian	O
DiCanio	O
,	O
PI).Finally	O
,	O
the	O
authors	O
thank	O
Shinji	O
Watanabe	O
both	O
for	O
his	O
advice	O
and	O
guidance	O
and	O
for	O
the	O
key	O
role	O
he	O
played	O
in	O
bringing	O
together	O
a	O
field	O
linguist	O
,	O
a	O
native	O
speaker	O
,	O
and	O
a	O
computational	O
linguist	O
for	O
this	O
project	O
.	O

Most	O
language	O
understanding	O
models	O
in	O
taskoriented	O
dialog	O
systems	O
are	O
trained	O
on	O
a	O
small	O
amount	O
of	O
annotated	O
training	O
data	O
,	O
and	O
evaluated	O
in	O
a	O
small	O
set	O
from	O
the	O
same	O
distribution	O
.	O

However	O
,	O
these	O
models	O
can	O
lead	O
to	O
system	O
failure	O
or	O
undesirable	O
output	O
when	O
being	O
exposed	O
to	O
natural	O
language	O
perturbation	O
or	O
variation	O
in	O
practice	O
.	O

In	O
this	O
paper	O
,	O
we	O
conduct	O
comprehensive	O
evaluation	O
and	O
analysis	O
with	O
respect	O
to	O
the	O
robustness	B-TaskName
of	I-TaskName
natural	I-TaskName
language	I-TaskName
understanding	I-TaskName
models	I-TaskName
,	O
and	O
introduce	O
three	O
important	O
aspects	O
related	O
to	O
language	O
understanding	O
in	O
realworld	O
dialog	O
systems	O
,	O
namely	O
,	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

We	O
propose	O
a	O
model	O
-	O
agnostic	O
toolkit	O
LAUG	B-MethodName
to	O
approximate	O
natural	O
language	O
perturbations	O
for	O
testing	O
the	O
robustness	B-TaskName
issues	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
.	O

Four	O
data	O
augmentation	O
approaches	O
covering	O
the	O
three	O
aspects	O
are	O
assembled	O
in	O
LAUG	B-MethodName
,	O
which	O
reveals	O
critical	O
robustness	O
issues	O
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

The	O
augmented	O
dataset	O
through	O
LAUG	B-MethodName
can	O
be	O
used	O
to	O
facilitate	O
future	O
research	O
on	O
the	O
robustness	B-TaskName
testing	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
.	O

Recently	O
task	O
-	O
oriented	O
dialog	O
systems	O
have	O
been	O
attracting	O
more	O
and	O
more	O
research	O
efforts	O
(	O
Gao	O
et	O
al	O
.	O
,	O

2019;Zhang	O
et	O
al	O
.	O
,	O

2020b	O
)	O
,	O
where	O
understanding	O
user	O
utterances	O
is	O
a	O
critical	O
precursor	O
to	O
the	O
success	O
of	O
such	O
dialog	O
systems	O
.	O

While	O
modern	O
neural	O
networks	O
have	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
language	O
understanding	O
(	O
LU	O
)	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2018;Zhao	O
and	O
Feng	O
,	O
2018;Goo	O
et	O
al	O
.	O
,	O

2018;Liu	O
et	O
al	O
.	O
,	O

2019;Shah	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
their	O
robustness	O
to	O
changes	O
in	O
the	O
input	O
distribution	O
is	O
still	O
one	O
of	O
the	O
biggest	O
challenges	O
in	O
practical	O
use	O
.	O

Real	O
dialogs	O
between	O
human	O
participants	O
involve	O
language	O
phenomena	O
that	O
do	O
not	O
contribute	O
so	O
much	O
to	O
the	O
intent	O
of	O
communication	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

1	O
,	O
user	O
expressions	O
can	O
be	O
of	O
high	O
lexical	O
and	O
syntactic	O
diversity	O
when	O
a	O
system	O
is	O
deployed	O
to	O
users	O
;	O
typed	O
texts	O
may	O
differ	O
significantly	O
from	O
those	O
recognized	O
from	O
voice	O
speech	O
;	O
interaction	O
environments	O
may	O
be	O
full	O
of	O
chaos	O
and	O
even	O
users	O
themselves	O
may	O
introduce	O
irrelevant	O
noises	O
such	O
that	O
the	O
system	O
can	O
hardly	O
get	O
clean	O
user	O
input	O
.	O

Unfortunately	O
,	O
neural	O
LU	O
models	O
are	O
vulnerable	O
to	O
these	O
natural	O
perturbations	O
that	O
are	O
legitimate	O
inputs	O
but	O
not	O
observed	O
in	O
training	O
data	O
.	O

For	O
example	O
,	O
Bickmore	O
et	O
al	O
.	O
(	O

2018	O
)	O
found	O
that	O
popular	O
conversational	O
assistants	O
frequently	O
failed	O
to	O
understand	O
real	O
health	O
-	O
related	O
scenarios	O
and	O
were	O
unable	O
to	O
deliver	O
adequate	O
responses	O
on	O
time	O
.	O

Although	O
many	O
studies	O
have	O
discussed	O
the	O
LU	B-TaskName
robustness	I-TaskName
(	O
Ray	O
et	O
al	O
.	O
,	O

2018;Zhu	O
et	O
al	O
.	O
,	O

2018;Iyyer	O
et	O
al	O
.	O
,	O

2018;Yoo	O
et	O
al	O
.	O
,	O

2019;Ren	O
et	O
al	O
.	O
,	O

2019;Jin	O
et	O
al	O
.	O
,	O

2020;He	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
there	O
is	O
a	O
lack	O
of	O
systematic	O
studies	O
for	O
real	O
-	O
life	O
robustness	O
issues	O
and	O
corresponding	O
benchmarks	O
for	O
evaluating	O
task	O
-	O
oriented	O
dialog	O
systems	O
.	O

In	O
order	O
to	O
study	O
the	O
real	O
-	O
world	O
robustness	O
issues	O
,	O
we	O
define	O
the	O
LU	B-TaskName
robustness	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
.	O

While	O
collecting	O
dialogs	O
from	O
deployed	O
systems	O
could	O
obtain	O
realistic	O
data	O
distribution	O
,	O
it	O
is	O
quite	O
costly	O
and	O
not	O
scalable	O
since	O
a	O
large	O
number	O
of	O
conversational	O
interactions	O
with	O
real	O
users	O
are	O
required	O
.	O

Therefore	O
,	O
we	O
propose	O
an	O
automatic	O
method	O
LAUG	B-MethodName
for	O
Language	B-MethodName
understanding	I-MethodName
AUGmentation	I-MethodName
in	O
this	O
paper	O
to	O
approximate	O
the	O
natural	O
perturbations	O
to	O
existing	O
data	O
.	O

LAUG	B-MethodName
is	O
a	O
black	O
-	O
box	O
testing	O
toolkit	O
on	O
LU	B-TaskName
robustness	I-TaskName
composed	O
of	O
four	O
data	O
augmentation	O
methods	O
,	O
including	O
word	B-TaskName
perturbation	I-TaskName
,	O
text	B-TaskName
paraphrasing	I-TaskName
,	O
speech	B-TaskName
recognition	I-TaskName
,	O
and	O
speech	B-TaskName
disfluency	I-TaskName
.	O

We	O
instantiate	O
LAUG	B-MethodName
on	O
two	O
dialog	O
corporaFrames	B-DatasetName
(	O
El	O
Asri	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MultiWOZ	B-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

2018	O
)	O
to	O
demonstrate	O
the	O
toolkit	O
's	O
effectiveness	O
.	O

Quality	O
evaluation	O
by	O
annotators	O
indicates	O
that	O
the	O
utterances	O
augmented	O
by	O
LAUG	B-MethodName
are	O
reasonable	O
and	O
appropriate	O
with	O
regards	O
to	O
each	O
augmentation	O
approach	O
's	O
target	O
.	O

A	O
number	O
of	O
LU	O
models	O
with	O
different	O
categories	O
and	O
training	O
paradigms	O
are	O
tested	O
as	O
base	O
models	O
with	O
in	O
-	O
depth	O
analysis	O
.	O

Experiments	O
indicate	O
a	O
sharp	O
performance	O
decline	O
in	O
most	O
baselines	O
in	O
terms	O
of	O
each	O
robustness	O
aspect	O
.	O

Real	O
user	O
evaluation	O
further	O
verifies	O
that	O
LAUG	B-MethodName
well	O
reflects	O
real	O
-	O
world	O
robustness	O
issues	O
.	O

Since	O
our	O
toolkit	O
is	O
model	O
-	O
agnostic	O
and	O
does	O
not	O
require	O
model	O
parameters	O
or	O
gradients	O
,	O
the	O
augmented	O
data	O
can	O
be	O
easily	O
obtained	O
for	O
both	O
training	O
and	O
testing	O
to	O
build	O
a	O
robust	O
dialog	O
system	O
.	O

Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
classify	O
the	O
LU	B-TaskName
robustness	I-TaskName
systematically	O
into	O
three	O
aspects	O
that	O
occur	O
in	O
real	O
-	O
world	O
dialog	O
,	O
including	O
linguistic	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
;	O
(	O
2	O
)	O
We	O
propose	O
a	O
general	O
and	O
model	O
-	O
agnostic	O
toolkit	O
,	O
LAUG	B-MethodName
,	O
which	O
is	O
an	O
integration	O
of	O
four	O
data	O
augmentation	O
methods	O
on	O
LU	O
that	O
covers	O
the	O
three	O
aspects	O
.	O
(	O

3	O
)	O
We	O
conduct	O
an	O
in	O
-	O
depth	O
analysis	O
of	O
LU	B-TaskName
robustness	I-TaskName
on	O
two	O
dialog	O
corpora	O
with	O
a	O
variety	O
of	O
baselines	O
and	O
standardized	O
evaluation	O
measures	O
.	O
(	O

4	O
)	O
Quality	O
and	O
user	O
evaluation	O
results	O
demonstrate	O
that	O
the	O
augmented	O
data	O
are	O
representative	O
of	O
real	O
-	O
world	O
noisy	O
data	O
,	O
therefore	O
can	O
be	O
used	O
for	O
future	O
research	O
to	O
test	O
the	O
LU	B-TaskName
robustness	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
1	O
.	O

We	O
summarize	O
several	O
common	O
interleaved	O
challenges	O
in	O
language	O
understanding	O
from	O
three	O
aspects	O
,	O
as	O
shown	O
in	O
Fig	O
.	O

1b	O
:	O
Language	O
Variety	O
A	O
modern	O
dialog	O
system	O
in	O
a	O
text	O
form	O
has	O
to	O
interact	O
with	O
a	O
large	O
variety	O
of	O
real	O
users	O
.	O

The	O
user	O
utterances	O
can	O
be	O
characterized	O
by	O
a	O
series	O
of	O
linguistic	O
phenomena	O
with	O
a	O
long	O
tail	O
of	O
variations	O
in	O
terms	O
of	O
spelling	O
,	O
vocabulary	O
,	O
lexical	O
/	O
syntactic	O
/	O
pragmatic	O
choice	O
(	O
Ray	O
et	O
al	O
.	O
,	O

2018;Jin	O
et	O
al	O
.	O
,	O

2020;He	O
et	O
al	O
.	O
,	O

2020;Zhao	O
et	O
al	O
.	O
,	O

2019;Ganhotra	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
dialog	O
system	O
can	O
take	O
voice	O
input	O
or	O
typed	O
text	O
,	O
but	O
these	O
two	O
differ	O
in	O
many	O
ways	O
.	O

For	O
example	O
,	O
written	O
language	O
1	O
The	O
data	O
,	O
toolkit	O
,	O
and	O
codes	O
are	O
available	O
at	O
https	O
:	O
//github.com	O
/	O
thu	O
-	O
coai	O
/	O
LAUG	O
,	O
and	O
will	O
be	O
merged	O
into	O
https://github.com/thu-coai/ConvLab-2	O
.	O

tends	O
to	O
be	O
more	O
complex	O
and	O
intricate	O
with	O
longer	O
sentences	O
and	O
many	O
subordinate	O
clauses	O
,	O
whereas	O
spoken	O
language	O
can	O
contain	O
repetitions	O
,	O
incomplete	O
sentences	O
,	O
self	O
-	O
corrections	O
and	O
interruptions	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2020a;Park	O
et	O
al	O
.	O
,	O

2019;Wang	O
et	O
al	O
.	O
,	O

2020b;Honal	O
and	O
Schultz	O
,	O
2003;Zhu	O
et	O
al	O
.	O
,	O

2018).Noise	O
Perturbation	O
Most	O
dialog	O
systems	O
are	O
trained	O
only	O
on	O
noise	O
-	O
free	O
interactions	O
.	O

However	O
,	O
there	O
are	O
various	O
noises	O
in	O
the	O
real	O
world	O
,	O
including	O
background	O
noise	O
,	O
channel	O
noise	O
,	O
misspelling	O
,	O
and	O
grammar	O
mistakes	O
(	O
Xu	O
and	O
Sarikaya	O
,	O
2014;Li	O
and	O
Qiu	O
,	O
2020;Yoo	O
et	O
al	O
.	O
,	O

2019;Henderson	O
et	O
al	O
.	O
,	O

2012;Ren	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

This	O
section	O
introduces	O
commonly	O
observed	O
out	O
-	O
ofdistribution	O
data	O
in	O
real	O
-	O
world	O
dialog	O
into	O
existing	O
corpora	O
.	O

We	O
approximate	O
natural	O
perturbations	O
in	O
an	O
automatic	O
way	O
instead	O
of	O
collecting	O
real	O
data	O
by	O
asking	O
users	O
to	O
converse	O
with	O
a	O
dialog	O
system	O
.	O

values	O
of	O
dialog	O
acts	O
are	O
not	O
modified	O
in	O
these	O
four	O
operations	O
.	O

Additionally	O
,	O
we	O
design	O
slot	O
value	O
replacement	O
,	O
which	O
changes	O
the	O
utterance	O
and	O
label	O
at	O
the	O
same	O
time	O
to	O
test	O
model	O
's	O
generalization	O
to	O
unseen	O
entities	O
.	O

Some	O
randomly	O
picked	O
slot	O
values	O
are	O
replaced	O
by	O
unseen	O
values	O
with	O
the	O
same	O
slot	O
name	O
in	O
the	O
database	O
or	O
crawled	O
from	O
web	O
sources	O
.	O

For	O
example	O
in	O
Table	O
2	O
,	O
"	O
Cambridge	O
"	O
is	O
replaced	O
by	O
"	O
Liverpool	O
"	O
,	O
where	O
both	O
belong	O
to	O
the	O
same	O
slot	O
name	O
"	O
dest	O
"	O
(	O
destination).Synonym	O
replacement	O
and	O
slot	O
value	O
replacement	O
aim	O
at	O
increasing	O
the	O
language	O
variety	O
,	O
while	O
random	O
word	O
insertion	O
/	O
deletion	O
/	O
swap	O
test	O
the	O
robustness	O
of	O
noise	O
perturbation	O
.	O

From	O
another	O
perspective	O
,	O
four	O
operations	O
from	O
EDA	O
perform	O
an	O
Invariance	O
test	O
,	O
while	O
slot	O
value	O
replacement	O
conducts	O
a	O
Directional	O
Expectation	O
test	O
according	O
to	O
CheckList	O
(	O
Ribeiro	O
et	O
al	O
.	O
,	O

2020).Text	O
Paraphrasing	O
The	O
target	O
of	O
text	O
paraphrasing	O
is	O
to	O
generate	O
a	O
new	O
utterance	O
x	O
=	O
x	O
while	O
maintaining	O
its	O
dialog	O
act	O
unchanged	O
,	O
i.e.	O
y	O
=	O
y.	O
We	O
applied	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
,	O
a	O
finetuned	O
language	O
model	O
conditioned	O
on	O
the	O
dialog	O
acts	O
,	O
to	O
paraphrase	O
the	O
sentences	O
as	O
data	O
augmentation	O
.	O

Specifically	O
,	O
it	O
characterizes	O
the	O
conditional	O
probability	O
p	O
θ	O
(	O
x|y	O
)	O
=	O
K	O
k=1	O
p	O
θ	O
(	O
x	O
k	O
|x	O
<	O
k	O
,	O
y	O
)	O
,	O
where	O
x	O
<	O
k	O
denotes	O
all	O
the	O
tokens	O
before	O
the	O
k	O
-	O
th	O
position	O
.	O

The	O
model	O
parameters	O
θ	O
are	O
trained	O
by	O
maximizing	O
the	O
log	O
-	O
likelihood	O
of	O
p	O
θ	O
.	O

of	O
examples	O
that	O
consider	O
contextual	O
resolution	O
or	O
not	O
.	O

In	O
the	O
second	O
example	O
,	O
the	O
user	O
omits	O
to	O
claim	O
that	O
he	O
wants	O
a	O
train	O
in	O
the	O
second	O
utterance	O
since	O
he	O
has	O
mentioned	O
this	O
before	O
.	O

We	O
observe	O
that	O
co	O
-	O
reference	O
and	O
ellipsis	O
frequently	O
occurs	O
in	O
user	O
utterances	O
.	O

Therefore	O
,	O
we	O
propose	O
different	O
encoding	O
strategies	O
during	O
paraphrasing	O
to	O
further	O
evaluate	O
each	O
model	O
's	O
capacity	O
for	O
context	O
resolution	O
.	O

In	O
particular	O
,	O
if	O
the	O
user	O
mentions	O
a	O
certain	O
domain	O
for	O
the	O
first	O
time	O
in	O
a	O
dialog	O
,	O
we	O
will	O
insert	O
a	O
"	O
*	O
"	O
mark	O
into	O
the	O
sequential	O
dialog	O
act	O
y	O
to	O
indicate	O
that	O
the	O
user	O
tends	O
to	O
express	O
without	O
co	O
-	O
references	O
or	O
ellipsis	O
,	O
as	O
shown	O
in	O
Table	O
3	O
.	O

Then	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
is	O
finetuned	O
on	O
the	O
processed	O
data	O
so	O
that	O
it	O
can	O
be	O
aware	O
of	O
dialog	O
context	O
when	O
generating	O
paraphrases	O
.	O

As	O
a	O
result	O
,	O
we	O
find	O
that	O
the	O
average	O
token	O
length	O
of	O
generated	O
utterances	O
with	O
/	O
without	O
"	O
*	O
"	O
is	O
15.96/12.67	O
respectively	O
after	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
's	O
finetuning	O
on	O
MultiWOZ.It	B-DatasetName
should	O
be	O
noted	O
that	O
slot	O
values	O
of	O
an	O
utterance	O
can	O
be	O
paraphrased	O
by	O
models	O
,	O
resulting	O
in	O
a	O
different	O
semantic	O
meaning	O
y	O
.	O

To	O
prevent	O
generating	O
irrelevant	O
sentences	O
,	O
we	O
apply	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
in	O
paraphrases	O
with	O
original	O
slot	O
values	O
by	O
fuzzy	O
matching	O
3	O
,	O
and	O
replace	O
the	O
detected	O
values	O
in	O
bad	O
paraphrases	O
with	O
original	O
values	O
.	O

In	O
addition	O
,	O
we	O
filter	O
out	O
paraphrases	O
that	O
have	O
missing	O
or	O
redundant	O
information	O
compared	O
to	O
the	O
original	O
utterance	O
.	O

Speech	B-TaskName
Recognition	I-TaskName
We	O
simulate	O
the	O
speech	B-TaskName
recognition	I-TaskName
(	O
SR	B-TaskName
)	O
process	O
with	O
a	O
TTS	B-MethodName
-	I-MethodName
ASR	I-MethodName
pipeline	I-MethodName
(	O
Park	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

First	O
we	O
transfer	O
textual	O
user	O
utterance	O
x	O
to	O
its	O
audio	O
form	O
a	O
using	O
gTTS	B-MethodName
4	I-MethodName
(	O
Oord	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
a	O
Text	O
-	O
to	O
-	O
Speech	O
system	O
.	O

Then	O
audio	O
data	O
is	O
translated	O
back	O
into	O
text	O
x	O
by	O
DeepSpeech2	B-MethodName
(	O
Amodei	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
an	O
Automatic	B-MethodName
Speech	I-MethodName
Recognition	I-MethodName
(	I-MethodName
ASR	I-MethodName
)	I-MethodName
system	I-MethodName
.	O

We	O
directly	O
use	O
the	O
released	O
models	O
in	O
the	O
DeepSpeech2	B-MethodName
repository	O
5	O
with	O
the	O
original	O
configuration	O
,	O
where	O
the	O
speech	O
model	O
is	O
trained	O
on	O
Baidu	B-DatasetName
Internal	I-DatasetName
English	I-DatasetName
Dataset	O
,	O
and	O
the	O
language	O
model	O
is	O
trained	O
on	O
CommonCrawl	B-DatasetName
Data	I-DatasetName
.	O

Table	O
4	O
shows	O
some	O
typical	O
examples	O
of	O
our	O
SR	O
augmentation	O
.	O

ASR	B-TaskName
sometimes	O
wrongly	O
identifies	O
one	O
word	O
as	O
another	O
with	O
similar	O
pronunciation	O
.	O

Liaison	O
constantly	O
occurs	O
between	O
successive	O
words	O
.	O

Expressions	O
with	O
numbers	O
including	O
time	O
and	O
price	O
are	O
written	O
in	O
numerical	O
form	O
but	O
different	O
in	O
spoken	O
language	O
.	O

Since	O
SR	O
may	O
modify	O
the	O
slot	O
values	O
in	O
the	O
translated	O
utterances	O
,	O
fuzzy	O
value	O
detection	O
is	O
employed	O
here	O
to	O
handle	O
similar	O
sounds	O
and	O
liaison	O
problems	O
when	O
it	O
extracts	O
slot	O
values	O
to	O
obtain	O
a	O
semantic	O
label	O
y	O
.	O

However	O
,	O
we	O
do	O
not	O
replace	O
the	O
noisy	O
value	O
with	O
the	O
original	O
value	O
as	O
we	O
encourage	O
such	O
misrecognition	O
in	O
SR	O
,	O
thus	O
y	O
=	O
y	O
is	O
allowed	O
.	O

Moreover	O
,	O
numerical	O
terms	O
are	O
normalized	O
to	O
deal	O
with	O
the	O
spoken	O
number	O
problem	O
.	O

Most	O
slot	O
values	O
could	O
be	O
relocated	O
by	O
our	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
rules	O
.	O

The	O
remainder	O
slot	O
values	O
which	O
vary	O
too	O
much	O
to	O
recognize	O
are	O
discarded	O
along	O
with	O
their	O
corresponding	O
labels	O
.	O

Speech	O
Disfluency	O
Disfluency	O
is	O
a	O
common	O
feature	O
of	O
spoken	O
language	O
.	O

We	O
follow	O
the	O
categorization	O
of	O
disfluency	O
in	O
previous	O
works	O
(	O
Lickley	O
,	O
1995;Wang	O
et	O
al	O
.	O
,	O

2020b	O
):	O
filled	O
pauses	O
,	O
repeats	O
,	O
restarts	O
,	O
and	O
repairs	O
.	O

Original	O
I	O
want	O
to	O
go	O
to	O
Cambridge	O
.	O

Pauses	O
I	O
want	O
to	O
um	O
go	O
to	O
uh	O
Cambridge	O
.	O

Repeats	O
I	O
,	O
I	O
want	O
to	O
go	O
to	O
,	O
go	O
to	O
Cambridge	O
.	O

Restarts	O
I	O
just	O
I	O
want	O
to	O
go	O
to	O
Cambridge	O
.	O

Repairs	O
I	O
want	O
to	O
go	O
to	O
Liverpool	O
,	O
sorry	O
I	O
mean	O
Cambridge	O
.	O

We	O
present	O
some	O
examples	O
of	O
SD	B-TaskName
in	O
Table	O
5	O
.	O

Filler	O
words	O
(	O
"	O
um	O
"	O
,	O
"	O
uh	O
"	O
)	O
are	O
injected	O
into	O
the	O
sentence	O
to	O
present	O
pauses	O
.	O

Repeats	O
are	O
inserted	O
by	O
repeating	O
the	O
previous	O
word	O
.	O

In	O
order	O
to	O
approximate	O
the	O
real	O
distribution	O
of	O
disfluency	O
,	O
the	O
interruption	O
points	O
of	O
filled	O
pauses	O
and	O
repeats	O
are	O
predicted	O
by	O
a	O
Bi	B-MethodName
-	I-MethodName
LSTM+CRF	I-MethodName
model	O
(	O
Zayats	O
et	O
al	O
.	O
,	O

2016	O
)	O
trained	O
on	O
an	O
annotated	O
dataset	O
SwitchBoard	B-DatasetName
(	O
Godfrey	O
et	O
al	O
.	O
,	O

1992	O
)	O
,	O
which	O
was	O
collected	O
from	O
real	O
human	O
talks	O
.	O

For	O
restarts	O
,	O
we	O
insert	O
false	O
start	O
terms	O
(	O
"	O
I	O
just	O
"	O
)	O
as	O
a	O
prefix	O
of	O
the	O
utterance	O
to	O
simulate	O
self	O
-	O
correction	O
.	O

In	O
LU	O
task	O
,	O
we	O
apply	O
repairs	O
on	O
slot	O
values	O
to	O
fool	O
the	O
models	O
to	O
predict	O
wrong	O
labels	O
.	O

We	O
take	O
the	O
original	O
slot	O
value	O
as	O
Repair	O
(	O
"	O
Cambridge	O
"	O
)	O
and	O
take	O
another	O
value	O
with	O
the	O
same	O
slot	O
name	O
as	O
Reparandum	O
(	O
"	O
Liverpool	O
"	O
)	O
.	O

An	O
edit	O
term	O
(	O
"	O
sorry	O
,	O
I	O
mean	O
"	O
)	O
is	O
inserted	O
between	O
Repair	O
and	O
Reparandum	O
to	O
construct	O
a	O
correction	O
.	O

The	O
filler	O
words	O
,	O
restart	O
terms	O
,	O
and	O
edit	O
terms	O
and	O
their	O
occurrence	O
frequency	O
are	O
all	O
sampled	O
from	O
their	O
distribution	O
in	O
SwitchBoard	B-DatasetName
.	O

In	O
order	O
to	O
keep	O
the	O
spans	O
of	O
slot	O
values	O
intact	O
,	O
each	O
span	O
is	O
regarded	O
as	O
one	O
whole	O
word	O
.	O

No	O
insertions	O
are	O
allowed	O
to	O
operate	O
inside	O
the	O
span	O
.	O

Therefore	O
,	O
SD	B-TaskName
augmentation	O
do	O
not	O
change	O
the	O
original	O
semantic	O
and	O
labels	O
of	O
the	O
utterance	O
,	O
i.e.	O
y	O
=	O
y.	O
In	O
our	O
experiments	O
we	O
adopt	O
Frames	B-DatasetName
6	O
(	O
El	O
Asri	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MultiWOZ	B-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
which	O
are	O
two	O
task	O
-	O
oriented	O
dialog	O
datasets	O
where	O
semantic	O
labels	O
of	O
user	O
utterances	O
are	O
annotated	O
.	O

In	O
particular	O
,	O
MultiWOZ	O
is	O
one	O
of	O
the	O
most	O
challenging	O
datasets	O
due	O
to	O
its	O
multi	O
-	O
domain	O
setting	O
and	O
complex	O
ontology	O
,	O
and	O
we	O
conduct	O
our	O
experiments	O
on	O
the	O
latest	O
annotation	O
-	O
enhanced	O
version	O
MultiWOZ	B-DatasetName
2.3	I-DatasetName
(	O
Han	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
which	O
provides	O
cleaned	O
annotations	O
of	O
user	O
dialog	O
acts	O
(	O
i.e.	O
semantic	O
labels	O
)	O
.	O

The	O
dialog	O
act	O
consists	O
of	O
four	O
parts	O
:	O
domain	O
,	O
intent	O
,	O
slot	O
names	O
,	O
and	O
slot	O
values	O
.	O

The	O
statistics	O
of	O
two	O
datasets	O
are	O
shown	O
in	O
Table	O
6	O
The	O
data	O
are	O
augmented	O
with	O
the	O
inclusion	O
of	O
its	O
copies	O
,	O
leading	O
to	O
a	O
composite	O
of	O
all	O
4	O
augmentation	O
types	O
with	O
equal	O
proportion	O
.	O

Other	O
setups	O
are	O
described	O
in	O
each	O
experiment	O
7	O
.	O

Table	O
7	O
shows	O
the	O
change	O
rates	O
in	O
different	O
as-	O
7	O
See	O
appendix	O
for	O
the	O
hyperparameter	O
setting	O
of	O
LAUG.pects	B-MethodName
by	O
comparing	O
our	O
augmented	O
utterances	O
with	O
the	O
original	O
counterparts	O
.	O

We	O
could	O
find	O
each	O
augmentation	O
method	O
has	O
a	O
distinct	O
effect	O
on	O
the	O
data	O
.	O

For	O
instance	O
,	O
TP	B-TaskName
rewrites	O
the	O
text	O
without	O
changing	O
the	O
original	O
meaning	O
,	O
thus	O
lexical	O
and	O
syntactic	O
representations	O
dramatically	O
change	O
,	O
while	O
most	O
slot	O
values	O
remain	O
unchanged	O
.	O

In	O
contrast	O
,	O
SR	O
makes	O
the	O
lowest	O
change	O
rate	O
in	O
characters	O
and	O
words	O
but	O
modifies	O
the	O
most	O
slot	O
values	O
due	O
to	O
the	O
speech	O
misrecognition	O
.	O

To	O
ensure	O
the	O
quality	O
of	O
our	O
augmented	O
test	O
set	O
,	O
we	O
conduct	O
human	O
annotation	O
on	O
1,000	O
sampled	O
utterances	O
in	O
each	O
augmented	O
test	O
set	O
of	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
.	O

We	O
ask	O
annotators	O
to	O
check	O
whether	O
our	O
augmented	O
utterances	O
are	O
reasonable	O
and	O
our	O
autodetected	O
value	O
annotations	O
are	O
correct	O
(	O
two	O
true	O
-	O
orfalse	O
questions	O
)	O
.	O

According	O
to	O
the	O
feature	O
of	O
each	O
augmentation	O
method	O
,	O
different	O
evaluation	O
protocols	O
are	O
used	O
.	O

For	O
TP	B-TaskName
and	O
SD	B-TaskName
,	O
annotators	O
check	O
whether	O
the	O
meaning	O
of	O
utterances	O
and	O
dialog	O
acts	O
are	O
unchanged	O
.	O

For	O
WP	B-TaskName
,	O
changing	O
slot	O
values	O
is	O
allowed	O
due	O
to	O
slot	O
value	O
replacement	O
,	O
but	O
the	O
slot	O
name	O
should	O
be	O
the	O
same	O
.	O

For	O
SR	O
,	O
annotators	O
are	O
asked	O
to	O
judge	O
on	O
the	O
similarity	O
of	O
pronunciation	O
rather	O
than	O
semantics	O
.	O

In	O
summary	O
,	O
all	O
the	O
high	O
scores	O
in	O
els	O
(	O
Liu	O
and	O
Lane	O
,	O
2016;Zhao	O
and	O
Feng	O
,	O
2018	O
)	O
generate	O
a	O
dialog	O
act	O
containing	O
intent	O
and	O
slot	O
values	O
.	O

They	O
treat	O
LU	O
as	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
problem	O
and	O
transform	O
a	O
dialog	O
act	O
into	O
a	O
sequential	O
structure	O
as	O
shown	O
in	O
Fig	O
.	O

2b	O
.	O

Five	O
base	O
models	O
with	O
different	O
categories	O
are	O
used	O
in	O
the	O
experiments	O
,	O
as	O
shown	O
in	O
Table	O
9	O
.	O

To	O
support	O
a	O
multi	O
-	O
intent	O
setting	O
in	O
classificationbased	O
models	O
,	O
we	O
decouple	O
the	O
LU	B-TaskName
process	O
as	O
follows	O
:	O
first	O
perform	O
domain	O
classification	O
and	O
intent	O
detection	O
,	O
then	O
concatenate	O
two	O
special	O
tokens	O
which	O
indicate	O
the	O
detected	O
domain	O
and	O
intent	O
(	O
e.g.[restaurant][inf	O
orm	O
]	O
)	O
at	O
the	O
beginning	O
of	O
the	O
input	O
sequence	O
,	O
and	O
last	O
encode	O
the	O
new	O
sequence	O
to	O
predict	O
slot	O
tags	O
.	O

In	O
this	O
way	O
,	O
the	O
model	O
can	O
address	O
overlapping	O
slot	O
values	O
when	O
values	O
are	O
shared	O
in	O
different	O
dialog	O
acts	O
.	O

We	O
conduct	O
robustness	O
testing	O
on	O
all	O
three	O
capacities	O
for	O
five	O
base	O
models	O
using	O
four	O
augmentation	O
methods	O
in	O
LAUG	B-MethodName
.	O

All	O
baselines	O
are	O
first	O
trained	O
on	O
the	O
original	O
datasets	O
,	O
then	O
finetuned	O
on	O
the	O
augmented	O
datasets	O
.	O

Overall	O
F1	B-MetricName
-	O
measure	O
performance	O
on	O
Frames	B-DatasetName
and	O
MultiWOZ	B-DatasetName
is	O
shown	O
in	O
Table	O
8	O
.	O

All	O
experiments	O
are	O
conducted	O
over	O
5	O
runs	O
,	O
and	O
averaged	O
results	O
are	O
reported	O
.	O

Robustness	O
for	O
each	O
capacity	O
can	O
be	O
measured	O
by	O
performance	O
drops	O
on	O
the	O
corresponding	O
augmented	O
test	O
sets	O
.	O

All	O
models	O
achieve	O
some	O
performance	O
recovery	O
on	O
augmented	O
test	O
sets	O
after	O
trained	O
on	O
the	O
augmented	O
data	O
,	O
while	O
keeping	O
a	O
comparable	O
result	O
on	O
the	O
original	O
test	O
set	O
.	O

This	O
indicates	O
the	O
effectiveness	O
of	O
LAUG	B-MethodName
in	O
improving	O
the	O
model	O
's	O
robustness	O
.	O

We	O
observe	O
that	O
pre	O
-	O
trained	O
models	O
outperform	O
non	O
-	O
pre	O
-	O
trained	O
ones	O
on	O
both	O
original	O
and	O
augmented	O
test	O
sets	O
.	O

Classification	O
-	O
based	O
models	O
have	O
better	O
performance	O
and	O
are	O
more	O
robust	O
than	O
generation	O
-	O
based	O
models	O
.	O

ToD	B-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
the	O
state-	O
of	O
-	O
the	O
-	O
art	O
model	O
which	O
was	O
further	O
pre	O
-	O
trained	O
on	O
task	O
-	O
oriented	O
dialog	O
data	O
,	O
has	O
comparable	O
performance	O
with	O
BERT	B-MethodName
.	O

With	O
most	O
augmentation	O
methods	O
,	O
ToD	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
slightly	O
better	O
robustness	O
than	O
BERT.Since	B-MethodName
the	O
data	O
volume	O
of	O
Frames	B-DatasetName
is	O
far	O
less	O
than	O
that	O
of	O
MultiWOZ	B-DatasetName
,	O
the	O
performance	O
improvement	O
of	O
pre	O
-	O
trained	O
models	O
on	O
Frames	B-DatasetName
is	O
larger	O
than	O
that	O
on	O
MultiWOZ	B-DatasetName
.	O

Due	O
to	O
the	O
same	O
reason	O
,	O
augmented	O
training	O
data	O
benefits	O
the	O
non	O
-	O
pre	O
-	O
trained	O
models	O
performance	O
of	O
on	O
Ori	O
.	O

test	O
set	O
more	O
remarkably	O
in	O
Frames	B-DatasetName
where	O
data	O
is	O
not	O
sufficient	O
.	O

Among	O
the	O
four	O
augmentation	O
methods	O
,	O
SR	O
has	O
the	O
largest	O
impact	O
on	O
the	O
models	O
'	O
performance	O
,	O
and	O
SD	O
comes	O
the	O
second	O
.	O

The	O
dramatic	O
performance	O
drop	O
when	O
testing	O
on	O
SR	O
and	O
SD	O
data	O
indicates	O
that	O
robustness	O
for	O
speech	O
characteristics	O
may	O
be	O
the	O
most	O
challenging	O
issue	O
.	O

Fig	O
.	O

3	O
shows	O
how	O
the	O
performance	O
of	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
changes	O
on	O
MultiWOZ	B-DatasetName
when	O
the	O
ratio	O
of	O
augmented	O
training	O
data	O
to	O
the	O
original	O
data	O
varies	O
from	O
0.1	O
to	O
4.0	O
.	O

F1	B-MetricName
scores	O
on	O
augmented	O
test	O
sets	O
increase	O
when	O
there	O
are	O
more	O
augmented	O
data	O
for	O
training	O
.	O

The	O
performance	O
of	O
BERT	B-MethodName
on	O
augmented	O
test	O
sets	O
is	O
improved	O
when	O
augmentation	O
ratio	O
is	O
less	O
than	O
0.5	O
but	O
becomes	O
almost	O
unchanged	O
after	O
0.5	O
while	O
GPT-2	B-MethodName
keeps	O
increasing	O
stably	O
.	O

This	O
result	O
shows	O
the	O
different	O
characteristics	O
between	O
classification	O
-	O
based	O
models	O
and	O
generation	O
-	O
based	O
models	O
when	O
finetuned	O
with	O
augmented	O
data	O
.	O

Between	O
augmentation	O
approaches	O
In	O
order	O
to	O
study	O
the	O
influence	O
of	O
each	O
augmentation	O
approach	O
in	O
LAUG	B-MethodName
,	O
we	O
test	O
the	O
performance	O
changes	O
when	O
one	O
augmentation	O
approach	O
is	O
removed	O
from	O
constructing	O
augmented	O
training	O
data	O
.	O

Results	O
on	O
Mul	B-DatasetName
-	I-DatasetName
tiWOZ	I-DatasetName
are	O
shown	O
in	O
Table	O
10	O
.	O

Original	O
EDA	O
consists	O
of	O
four	O
functions	O
as	O
described	O
in	O
Table	O
2	O
.	O

Performance	O
differences	O
(	O
Diff	O
.	O
)	O

can	O
reflect	O
the	O
influences	O
of	O
those	O
components	O
in	O
Table	O
11a	O
.	O

The	O
additional	O
function	O
of	O
our	O
SC	O
-	O
EDA	O
is	O
slot	O
value	O
replacement	O
.	O

We	O
can	O
also	O
observe	O
an	O
increase	O
in	O
performance	O
when	O
it	O
is	O
removed	O
,	O
especially	O
for	O
MILU	B-MethodName
.	O

This	O
implies	O
a	O
lack	O
of	O
LU	B-TaskName
robustness	I-TaskName
in	O
detecting	O
unseen	O
entities	O
.	O

Table	O
11b	O
shows	O
the	O
results	O
of	O
ablation	O
study	O
on	O
SD	B-TaskName
.	O

Among	O
the	O
four	O
types	O
of	O
disfluencies	O
described	O
in	O
Table	O
5	O
,	O
repairs	O
has	O
the	O
largest	O
impact	O
on	O
models	O
'	O
performance	O
.	O

The	O
performance	O
is	O
also	O
affected	O
by	O
pauses	O
but	O
to	O
a	O
less	O
extent	O
.	O

The	O
influences	O
of	O
repeats	O
and	O
restarts	O
are	O
small	O
,	O
which	O
indicates	O
that	O
neural	O
models	O
are	O
robust	O
to	O
handle	O
these	O
two	O
problems	O
.	O

In	O
order	O
to	O
test	O
whether	O
the	O
data	O
automatically	O
augmented	O
by	O
LAUG	O
can	O
reflect	O
and	O
alleviate	O
practical	O
robustness	O
problems	O
,	O
we	O
conduct	O
a	O
real	O
user	O
evaluation	O
.	O

We	O
collected	O
240	O
speech	O
utterances	O
from	O
real	O
humans	O
as	O
follows	O
:	O
First	O
,	O
we	O
sampled	O
120	O
combinations	O
of	O
DA	O
from	O
the	O
test	O
set	O
of	O
MultiWOZ	B-DatasetName
.	O

Given	O
a	O
combination	O
,	O
each	O
user	O
was	O
asked	O
to	O
speak	O
two	O
utterances	O
with	O
different	O
expressions	O
,	O
in	O
their	O
own	O
language	O
habits	O
.	O

Then	O
the	O
audio	O
signals	O
were	O
recognized	O
into	O
text	O
using	O
DeepSpeech2	O
,	O
thereby	O
constructing	O
a	O
new	O
test	O
set	O
in	O
real	O
scenarios	O
8	O
.	O

Results	O
on	O
this	O
real	O
test	O
set	O
are	O
shown	O
in	O
Table	O
12	O
.	O

The	O
performance	O
on	O
the	O
real	O
test	O
set	O
is	O
substantially	O
lower	O
than	O
that	O
on	O
Ori	O
.	O

and	O
Avg	O
.	O
,	O

indicating	O
that	O
real	O
user	O
evaluation	O
is	O
much	O
more	O
challenging	O
.	O

This	O
is	O
because	O
multiple	O
robustness	O
issues	O
may	O
be	O
included	O
in	O
one	O
real	O
case	O
,	O
while	O
each	O
augmentation	O
method	O
in	O
LAUG	B-MethodName
evaluates	O
them	O
separately	O
.	O

Despite	O
the	O
difference	O
,	O
model	O
performance	O
on	O
the	O
real	O
data	O
is	O
remarkably	O
improved	O
after	O
every	O
model	O
is	O
finetuned	O
on	O
the	O
augmented	O
data	O
,	O
verifying	O
that	O
LAUG	B-MethodName
effectively	O
enhances	O
the	O
model	O
's	O
real	O
-	O
world	O
robustness	O
.	O

Table	O
13	O
investigates	O
which	O
error	O
type	O
the	O
model	O
has	O
made	O
on	O
the	O
real	O
test	O
set	O
by	O
manually	O
checking	O
all	O
the	O
error	O
outputs	O
of	O
BERT	B-MethodName
Ori	O
.	O
"	O

Others	O
"	O
are	O
the	O
error	O
cases	O
which	O
are	O
not	O
caused	O
by	O
robustness	O
issues	O
,	O
for	O
example	O
,	O
because	O
of	O
the	O
model	O
's	O
poor	O
performance	O
.	O

It	O
can	O
be	O
observed	O
that	O
the	O
model	O
seriously	O
suffers	O
to	O
LU	B-TaskName
robustness	I-TaskName
(	O
over	O
70	O
%	O
)	O
,	O
and	O
that	O
almost	O
half	O
of	O
the	O
error	O
is	O
due	O
to	O
Language	O
Variety	O
.	O

We	O
find	O
that	O
this	O
is	O
because	O
there	O
are	O
more	O
diverse	O
expressions	O
in	O
real	O
user	O
evaluation	O
than	O
in	O
the	O
original	O
data	O
.	O

After	O
augmented	O
training	O
,	O
we	O
can	O
observe	O
that	O
the	O
number	O
of	O
error	O
cases	O
of	O
Speech	O
Characteristics	O
and	O
Noise	O
Perturbation	O
is	O
relatively	O
decreased	O
.	O

This	O
shows	O
that	O
BERT	B-MethodName
Aug.	O
can	O
solve	O
these	O
two	O
kinds	O
of	O
problems	O
better	O
.	O

Noting	O
that	O
the	O
sum	O
of	O
four	O
percentages	O
is	O
over	O
100	O
%	O
since	O
25	O
%	O
error	O
cases	O
involve	O
multiple	O
robustness	O
issues	O
.	O

This	O
again	O
demonstrates	O
that	O
real	O
user	O
evaluation	O
is	O
more	O
challenging	O
than	O
the	O
original	O
test	O
set	O
9	O
.	O

Robustness	B-TaskName
in	I-TaskName
LU	I-TaskName
has	O
always	O
been	O
a	O
challenge	O
in	O
task	O
-	O
oriented	O
dialog	O
.	O

Several	O
studies	O
have	O
investigated	O
the	O
model	O
's	O
sensitivity	O
to	O
the	O
collected	O
data	O
distribution	O
,	O
in	O
order	O
to	O
prevent	O
models	O
from	O
overfitting	O
to	O
the	O
training	O
data	O
and	O
improve	O
robustness	O
in	O
the	O
real	O
world	O
.	O

Kang	O
et	O
al	O
.	O
(	O

2018	O
)	O
collected	O
dialogs	O
with	O
templates	O
and	O
paraphrased	O
with	O
crowdsourcing	O
to	O
achieve	O
high	O
coverage	O
and	O
diversity	O
in	O
training	O
data	O
.	O

Dinan	O
et	O
al	O
.	O
(	O

2019	O
)	O
proposed	O
a	O
training	O
schema	O
that	O
involves	O
human	O
in	O
the	O
loop	O
in	O
dialog	O
systems	O
to	O
enhance	O
the	O
model	O
's	O
defense	O
against	O
human	O
attack	O
in	O
an	O
iterative	O
way	O
.	O

Ganhotra	O
et	O
al	O
.	O
(	O

2020	O
)	O
injected	O
natural	O
perturbation	O
into	O
the	O
dialog	O
history	O
manually	O
to	O
refine	O
over	O
-	O
controlled	O
data	O
generated	O
through	O
crowd	O
-	O
sourcing	O
.	O

All	O
these	O
methods	O
require	O
laborious	O
human	O
intervention	O
.	O

This	O
paper	O
aims	O
to	O
provide	O
an	O
automatic	O
way	O
to	O
test	O
the	O
LU	B-TaskName
robustness	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
.	O

Various	O
textual	O
adversarial	O
attacks	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020a	O
)	O
have	O
been	O
proposed	O
and	O
received	O
increasing	O
attentions	O
these	O
years	O
to	O
measure	O
the	O
robustness	O
of	O
a	O
victim	O
model	O
.	O

Most	O
attack	O
methods	O
perform	O
whitebox	O
attacks	O
(	O
Papernot	O
et	O
al	O
.	O
,	O

2016;Ebrahimi	O
et	O
al	O
.	O
,	O

2018	O
)	O
based	O
on	O
the	O
model	O
's	O
internal	O
structure	O
or	O
gradient	O
signals	O
.	O

Even	O
some	O
black	O
-	O
box	O
attack	O
models	O
are	O
not	O
purely	O
"	O
black	O
-	O
box	O
"	O
,	O
which	O
require	O
the	O
prediction	O
scores	O
(	O
classification	O
probabilities	O
)	O
of	O
the	O
victim	O
model	O
(	O
Jin	O
et	O
al	O
.	O
,	O

2020;Ren	O
et	O
al	O
.	O
,	O

2019;Alzantot	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

However	O
,	O
all	O
these	O
methods	O
address	O
random	O
perturbation	O
but	O
do	O
not	O
consider	O
linguistic	O
phenomena	O
to	O
evaluate	O
the	O
real	O
-	O
life	O
generalization	O
of	O
LU	O
models	O
.	O

While	O
data	O
augmentation	O
can	O
be	O
an	O
efficient	O
method	O
to	O
address	O
data	O
sparsity	O
,	O
it	O
can	O
improve	O
the	O
generalization	O
abilities	O
and	O
measure	O
the	O
model	B-TaskName
robustness	I-TaskName
as	O
well	O
(	O
Eshghi	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

Paraphrasing	O
that	O
rewrites	O
the	O
utterances	O
in	O
dialog	O
has	O
been	O
used	O
to	O
get	O
diverse	O
representation	O
and	O
thus	O
enhancing	O
robustness	O
(	O
Ray	O
et	O
al	O
.	O
,	O

2018;Zhao	O
et	O
al	O
.	O
,	O

2019;Iyyer	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Word	O
-	O
level	O
operations	O
(	O
Kolomiyets	O
et	O
al	O
.	O
,	O

2011;Li	O
and	O
Qiu	O
,	O
2020;Wei	O
and	O
Zou	O
,	O
2019	O
)	O
including	O
replacement	O
,	O
insertion	O
,	O
and	O
deletion	O
were	O
also	O
proposed	O
to	O
increase	O
language	O
variety	O
.	O

Other	O
studies	O
(	O
Shah	O
et	O
al	O
.	O
,	O

2019;Xu	O
and	O
Sarikaya	O
,	O
2014	O
)	O
worked	O
on	O
the	O
out	O
-	O
of	O
-	O
vocabulary	O
problem	O
when	O
facing	O
unseen	O
user	O
expression	O
.	O

Some	O
other	O
research	O
focused	O
on	O
building	O
robust	O
spoken	O
language	O
understanding	O
(	O
Zhu	O
et	O
al	O
.	O
,	O

2018;Henderson	O
et	O
al	O
.	O
,	O

2012;Huang	O
and	O
Chen	O
,	O
2019	O
)	O
from	O
audio	O
signals	O
beyond	O
text	O
transcripts	O
.	O

Simulating	O
ASR	B-TaskName
errors	O
(	O
Schatzmann	O
et	O
al	O
.	O
,	O

2007;Park	O
et	O
al	O
.	O
,	O

2019;Wang	O
et	O
al	O
.	O
,	O

2020a	O
)	O
and	O
speaker	O
disfluency	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2020b;Qader	O
et	O
al	O
.	O
,	O

2018	O
)	O
can	O
be	O
promising	O
solutions	O
to	O
enhance	O
robustness	O
to	O
voice	O
input	O
when	O
only	O
textual	O
data	O
are	O
provided	O
.	O

As	O
most	O
work	O
tackles	O
LU	O
robustness	O
from	O
only	O
one	O
perspective	O
,	O
we	O
present	O
a	O
comprehensive	O
study	O
to	O
reveal	O
three	O
critical	O
issues	O
in	O
this	O
paper	O
,	O
and	O
shed	O
light	O
on	O
a	O
thorough	O
robustness	O
evaluation	O
of	O
LU	B-TaskName
in	I-TaskName
dialog	I-TaskName
systems	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
systematic	O
robustness	B-TaskName
evaluation	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
(	I-TaskName
LU	I-TaskName
)	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

Accordingly	O
,	O
we	O
develop	O
four	O
data	O
augmentation	O
methods	O
to	O
approximate	O
these	O
language	O
phenomena	O
.	O

In	O
-	O
depth	O
experiments	O
and	O
analysis	O
are	O
conducted	O
on	O
MultiWOZ	B-DatasetName
and	O
Frames	B-DatasetName
,	O
with	O
both	O
classification	O
-	O
and	O
generation	O
-	O
based	O
LU	O
models	O
.	O

The	O
performance	O
drop	O
of	O
all	O
models	O
on	O
augmented	O
test	O
data	O
indicates	O
that	O
these	O
robustness	O
issues	O
are	O
challenging	O
and	O
critical	O
,	O
while	O
pre	O
-	O
trained	O
models	O
are	O
relatively	O
more	O
robust	O
to	O
LU	O
.	O

Ablation	O
studies	O
are	O
carried	O
out	O
to	O
show	O
the	O
effect	O
and	O
orthogonality	O
of	O
each	O
augmentation	O
approach	O
.	O

We	O
also	O
conduct	O
a	O
real	O
user	O
evaluation	O
and	O
verifies	O
that	O
our	O
augmentation	O
methods	O
can	O
reflect	O
and	O
help	O
alleviate	O
real	O
robustness	O
problems	O
.	O

Existing	O
and	O
future	O
dialog	O
models	O
can	O
be	O
evaluated	O
in	O
terms	O
of	O
robustness	O
with	O
our	O
toolkit	O
and	O
data	O
,	O
as	O
our	O
augmentation	O
model	O
does	O
not	O
depend	O
on	O
any	O
particular	O
LU	O
models	O
.	O

Moreover	O
,	O
our	O
proposed	O
robustness	O
evaluation	O
scheme	O
is	O
extensible	O
.	O

In	O
addition	O
to	O
the	O
four	O
approaches	O
in	O
LAUG	B-MethodName
,	O
more	O
methods	O
to	O
evaluate	O
LU	O
robustness	O
can	O
be	O
considered	O
in	O
the	O
future	O
.	O

As	O
for	O
hyperparameters	O
in	O
LAUG	B-MethodName
,	O
we	O
set	O
the	O
ratio	B-HyperparameterName
of	I-HyperparameterName
perturbation	I-HyperparameterName
number	I-HyperparameterName
to	I-HyperparameterName
text	I-HyperparameterName
length	I-HyperparameterName
α	B-HyperparameterName
=	O
n	O
/	O
l	O
=	O
0.1	B-HyperparameterValue
in	O
EDA	O
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
used	O
to	O
finetune	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
in	O
TP	O
is	O
1e-4	B-HyperparameterValue
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epoch	I-HyperparameterName
is	O
5	B-HyperparameterValue
,	O
and	O
the	O
beam	B-HyperparameterName
size	I-HyperparameterName
during	O
inference	O
is	O
5	B-HyperparameterValue
.	O

In	O
SR	B-TaskName
,	O
the	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
the	O
language	O
model	O
in	O
DeepSpeech2	O
is	O
set	O
to	O
50	B-HyperparameterValue
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
Bi	B-MethodName
-	I-MethodName
LSTM+CRF	I-MethodName
in	O
SD	B-TaskName
is	O
1e-3	B-HyperparameterValue
.	O

The	O
threshold	B-HyperparameterName
of	I-HyperparameterName
fuzzy	I-HyperparameterName
matching	I-HyperparameterName
in	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
is	O
set	O
to	O
0.9	B-HyperparameterValue
in	O
TP	B-TaskName
and	O
0.7	B-HyperparameterValue
in	O
SR.For	B-TaskName
hyperparameters	O
of	O
base	O
models	O
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
1e-4	B-HyperparameterValue
for	O
BERT	B-MethodName
,	O
1e-5	B-HyperparameterValue
for	O
GPT2	B-MethodName
,	O
and	O
1e-3	B-HyperparameterName
for	O
MILU	B-MethodName
and	O
CopyNet	B-MethodName
.	O

The	O
beam	B-HyperparameterName
-	I-HyperparameterName
size	I-HyperparameterName
of	O
GPT2	B-MethodName
and	O
CopyNet	B-MethodName
is	O
5	B-HyperparameterValue
during	O
the	O
decoding	O
step	O
.	O

Among	O
the	O
120	O
sampled	O
DA	O
combinations	O
,	O
each	O
combination	O
contains	O
1	O
to	O
3	O
DAs	O
.	O

Users	O
can	O
organize	O
the	O
DAs	O
in	O
any	O
order	O
provided	O
that	O
they	O
describe	O
DAs	O
with	O
the	O
correct	O
meaning	O
so	O
as	O
to	O
imitate	O
diverse	O
user	O
expressions	O
in	O
real	O
scenarios	O
.	O

Users	O
are	O
also	O
asked	O
to	O
keep	O
natural	O
in	O
both	O
intonation	O
and	O
expression	O
,	O
and	O
communication	O
noise	O
caused	O
by	O
users	O
in	O
speech	O
and	O
language	O
is	O
included	O
during	O
collection	O
.	O

The	O
audios	O
are	O
recorded	O
by	O
users	O
'	O
PCs	O
under	O
their	O
real	O
environmental	O
noises	O
.	O

We	O
use	O
the	O
same	O
settings	O
of	O
DeepSpeech2	B-MethodName
in	O
SR	O
to	O
recognize	O
the	O
collected	O
audios	O
.	O

After	O
automatic	B-TaskName
span	I-TaskName
detection	I-TaskName
(	O
also	O
the	O
same	O
as	O
SR	O
's	O
)	O
are	O
applied	O
,	O
we	O
conduct	O
human	O
check	O
and	O
annotation	O
to	O
ensure	O
the	O
quality	O
of	O
labels	O
.	O

14	O
:	O
Robustness	O
on	O
different	O
schemes	O
on	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
.	O

The	O
coupled	O
scheme	O
predicts	O
dialog	O
acts	O
with	O
a	O
joint	O
tagging	O
scheme	O
;	O
the	O
decoupled	O
scheme	O
first	O
detects	O
domains	O
and	O
intents	O
,	O
then	O
recognizes	O
the	O
slot	O
tags	O
.	O

In	O
this	O
section	O
,	O
we	O
study	O
the	O
influence	O
of	O
training	O
/	O
prediction	O
schemes	O
on	O
LU	B-TaskName
robustness	I-TaskName
.	O

As	O
described	O
in	O
Sec	O
.	O

4.3	O
of	O
the	O
main	O
paper	O
,	O
the	O
process	O
of	O
classification	O
-	O
based	O
LU	O
models	O
is	O
decoupled	O
into	O
two	O
steps	O
to	O
handle	O
multiple	O
labels	O
:	O
one	O
for	O
domain	O
/	O
intent	O
classification	O
and	O
the	O
other	O
for	O
slot	O
tagging	O
.	O

Another	O
strategy	O
is	O
to	O
use	O
the	O
cartesian	O
product	O
of	O
all	O
the	O
components	O
of	O
dialog	O
acts	O
,	O
which	O
yields	O
a	O
joint	O
tagging	O
scheme	O
as	O
presented	O
in	O
Con	O
-	O
vLab	O
.	O

To	O
give	O
an	O
intuitive	O
illustration	O
,	O
the	O
slot	O
tag	O
of	O
the	O
token	O
"	O
Los	O
"	O
becomes	O
"	O
Train	O
-	O
Inform	O
-	O
Depart	O
-	O
B	O
"	O
in	O
the	O
example	O
described	O
in	O
Fig	O
.	O

2	O
of	O
the	O
main	O
paper	O
.	O

The	O
classificationbased	O
models	O
can	O
predict	O
the	O
dialog	O
acts	O
within	O
a	O
single	O
step	O
in	O
this	O
way	O
.	O

Table	O
14	O
shows	O
that	O
MILU	B-MethodName
and	O
BERT	B-MethodName
gain	O
from	O
the	O
decoupled	O
scheme	O
on	O
the	O
original	O
test	O
set	O
.	O

This	O
indicates	O
that	O
the	O
decoupled	O
scheme	O
decreases	O
the	O
model	O
complexity	O
by	O
decomposing	O
the	O
output	O
space	O
.	O

Interestingly	O
,	O
there	O
is	O
no	O
consistency	O
between	O
two	O
models	O
in	O
terms	O
of	O
robustness	O
.	O

MILU	B-MethodName
via	O
the	O
coupled	O
scheme	O
behaves	O
more	O
robustly	O
than	O
the	O
decoupled	O
counterpart	O
(	O
-2.61	B-MetricValue
vs.-7.05	B-MetricValue
)	O
,	O
while	O
BERT	B-MethodName
with	O
the	O
decoupled	O
scheme	O
outperforms	O
its	O
coupled	O
version	O
in	O
robustness	O
(	O
-6.45	B-MetricValue
vs.	O
-8.61	B-MetricValue
)	O
.	O

Meanwhile	O
,	O
BERT	B-MethodName
benefits	O
from	O
the	O
decoupled	O
scheme	O
and	O
still	O
achieves	O
86.95	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
but	O
BERT	B-MethodName
training	O
with	O
the	O
coupled	O
scheme	O
seems	O
more	O
susceptible	O
.	O

In	O
addition	O
,	O
both	O
MILU	B-MethodName
and	O
BERT	B-MethodName
recover	O
more	O
performance	O
by	O
the	O
proposed	O
decoupled	O
scheme	O
.	O

All	O
these	O
results	O
demonstrate	O
the	O
superiority	O
of	O
the	O
decoupled	O
scheme	O
in	O
classification	B-TaskName
-	I-TaskName
based	I-TaskName
LU	I-TaskName
models	O
.	O

In	O
Table	O
15	O
,	O
we	O
present	O
some	O
examples	O
of	O
augmented	O
utterances	O
in	O
MultiWOZ	B-DatasetName
.	O

In	O
terms	O
of	O
model	O
performance	O
,	O
MILU	B-MethodName
,	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
perform	O
well	O
on	O
WP	B-TaskName
and	O
TP	B-TaskName
in	O
the	O
example	O
while	O
Copy	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
misses	O
some	O
dialog	O
acts	O
.	O

For	O
the	O
SR	O
utterance	O
,	O
only	O
BERT	B-MethodName
obtains	O
all	O
the	O
correct	O
labels	O
.	O

MILU	B-MethodName
and	O
Copynet	O
both	O
fail	O
to	O
find	O
the	O
changed	O
value	O
spans	O
"	O
lester	O
"	O
and	O
"	O
thirteen	O
forty	O
five	O
"	O
.	O

Copynet	B-MethodName
's	O
copy	O
mechanism	O
is	O
fully	O
confused	O
by	O
recognition	O
error	O
and	O
even	O
predicts	O
discontinuous	O
slot	O
values	O
.	O

GPT-2	B-MethodName
successfully	O
finds	O
the	O
non	O
-	O
numerical	O
time	O
but	O
misses	O
"	O
leseter	O
"	O
.	O

In	O
the	O
SD	O
utterance	O
,	O
the	O
repair	O
term	O
fools	O
all	O
the	O
models	O
.	O

Overall	O
,	O
in	O
this	O
example	O
,	O
BERT	B-MethodName
performs	O
quite	O
well	O
while	O
MILU	B-MethodName
and	O
Copy	B-MethodName
-	I-MethodName
Net	I-MethodName
expose	O
some	O
of	O
their	O
defects	O
in	O
robustness	O
.	O

Table	O
16	O
shows	O
some	O
examples	O
from	O
real	O
user	O
evaluation	O
.	O

In	O
case-1	O
,	O
the	O
user	O
says	O
"	O
seventeen	O
o'clock	O
"	O
while	O
time	O
is	O
always	O
represented	O
in	O
numeric	O
formats	O
(	O
e.g.	O
"	O
17:00	O
"	O
)	O
in	O
the	O
dataset	O
,	O
which	O
is	O
a	O
typical	O
Speech	O
Characteristics	O
problem	O
.	O

Case-2	O
could	O
be	O
regarded	O
as	O
a	O
Speech	O
Characteristics	O
or	O
Noise	O
Perturbation	O
case	O
because	O
"	O
please	O
"	O
is	O
wrongly	O
recognized	O
as	O
"	O
police	O
"	O
by	O
ASR	B-TaskName
models	O
.	O

Case-3	O
is	O
an	O
example	O
of	O
Language	O
Variety	O
,	O
the	O
user	O
expresses	O
the	O
request	O
of	O
getting	O
ticket	O
price	O
in	O
a	O
different	O
way	O
comparing	O
to	O
the	O
dataset	O
.	O

MILU	B-MethodName
and	O
BERT	B-MethodName
failed	O
in	O
most	O
of	O
these	O
cases	O
but	O
fixed	O
some	O
error	O
after	O
augmented	O
training	O
.	O

This	O
work	O
was	O
partly	O
supported	O
by	O
the	O
NSFC	O
projects	O
(	O
Key	O
project	O
with	O
No	O
.	O

61936010	O
and	O
regular	O
project	O
with	O
No	O
.	O

61876096	O
)	O
.	O

This	O
work	O
was	O
also	O
supported	O
by	O
the	O
Guoqiang	O
Institute	O
of	O
Tsinghua	O
University	O
,	O
with	O
Grant	O
No	O
.	O

2019GQG1	O
and	O
2020GQG0005	O
.	O

We	O
would	O
like	O
to	O
thank	O
colleagues	O
from	O
HUAWEI	O
for	O
their	O
constant	O
support	O
and	O
valuable	O
discussion	O
.	O

Modern	O
pre	O
-	O
trained	O
language	O
models	O
are	O
mostly	O
built	O
upon	O
backbones	O
stacking	O
selfattention	O
and	O
feed	O
-	O
forward	O
layers	O
in	O
an	O
interleaved	O
order	O
.	O

In	O
this	O
paper	O
,	O
beyond	O
this	O
stereotyped	O
layer	O
pattern	O
,	O
we	O
aim	O
to	O
improve	O
pre	O
-	O
trained	O
models	O
by	O
exploiting	O
layer	O
variety	O
from	O
two	O
aspects	O
:	O
the	O
layer	O
type	O
set	O
and	O
the	O
layer	O
order	O
.	O

Specifically	O
,	O
besides	O
the	O
original	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
,	O
we	O
introduce	O
convolution	O
into	O
the	O
layer	O
type	O
set	O
,	O
which	O
is	O
experimentally	O
found	O
beneficial	O
to	O
pre	O
-	O
trained	O
models	O
.	O

Furthermore	O
,	O
beyond	O
the	O
original	O
interleaved	O
order	O
,	O
we	O
explore	O
more	O
layer	O
orders	O
to	O
discover	O
more	O
powerful	O
architectures	O
.	O

However	O
,	O
the	O
introduced	O
layer	O
variety	O
leads	O
to	O
a	O
large	O
architecture	O
space	O
of	O
more	O
than	O
billions	O
of	O
candidates	O
,	O
while	O
training	O
a	O
single	O
candidate	O
model	O
from	O
scratch	O
already	O
requires	O
huge	O
computation	O
cost	O
,	O
making	O
it	O
not	O
affordable	O
to	O
search	O
such	O
a	O
space	O
by	O
directly	O
training	O
large	O
amounts	O
of	O
candidate	O
models	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
first	O
pre	O
-	O
train	O
a	O
supernet	O
from	O
which	O
the	O
weights	O
of	O
all	O
candidate	O
models	O
can	O
be	O
inherited	O
,	O
and	O
then	O
adopt	O
an	O
evolutionary	O
algorithm	O
guided	O
by	O
pre	O
-	O
training	O
accuracy	O
to	O
find	O
the	O
optimal	O
architecture	O
.	O

Extensive	O
experiments	O
show	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
model	O
obtained	O
by	O
our	O
method	O
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
on	O
various	O
downstream	O
tasks	O
.	O

For	O
example	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
achieves	O
78.8	B-MetricValue
on	O
the	O
GLUE	B-MetricName
testing	O
set	O
,	O
1.8	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
.	O

1	O
In	O
recent	O
years	O
,	O
pre	O
-	O
trained	O
language	O
models	O
,	O
such	O
as	O
the	O
representative	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
GPT-3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
have	O
gained	O
great	O
success	O
in	O
natural	O
language	O
processing	O
tasks	O
(	O
Peters	O
et	O
al	O
.	O
,	O

2018a;Radford	O
et	O
al	O
.	O
,	O

2018;Yang	O
et	O
al	O
.	O
,	O

2019;Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
backbone	O
architectures	O
of	O
these	O
models	O
mostly	O
adopt	O
a	O
stereotyped	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
development	O
set	O
.	O

Except	O
BERT	B-MethodName
pre	O
-	O
trained	O
with	O
the	O
Masked	O
Language	O
Modeling	O
objective	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
the	O
other	O
models	O
are	O
pre	O
-	O
trained	O
with	O
Replaced	O
Token	O
Detection	O
objective	O
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
to	O
save	O
computation	O
cost	O
.	O

layer	O
pattern	O
,	O
in	O
which	O
the	O
self	O
-	O
attention	O
and	O
feedforward	O
layers	O
are	O
arrayed	O
in	O
an	O
interleaved	O
order	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

However	O
,	O
there	O
is	O
no	O
evidence	O
supporting	O
that	O
this	O
layer	O
pattern	O
is	O
optimal	O
(	O
Press	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

We	O
then	O
consider	O
a	O
straightforward	O
and	O
interesting	O
question	O
:	O
Could	O
we	O
change	O
the	O
layer	O
pattern	O
to	O
improve	O
pre	O
-	O
trained	O
models?We	O
attempt	O
to	O
answer	O
this	O
question	O
by	O
exploiting	O
more	O
layer	O
variety	O
from	O
two	O
aspects	O
,	O
as	O
shown	O
in	O
Figure	O
1(a	O
):	O
the	O
layer	O
type	O
set	O
and	O
the	O
layer	O
order	O
.	O

We	O
first	O
consider	O
the	O
layer	O
types	O
.	O

In	O
previous	O
pre	O
-	O
trained	O
language	O
models	O
,	O
the	O
most	O
widely	O
-	O
used	O
layer	O
set	O
contains	O
the	O
self	O
-	O
attention	O
layer	O
for	O
capturing	O
global	O
information	O
and	O
the	O
feed	O
-	O
forward	O
layer	O
for	O
non	O
-	O
linear	O
transformation	O
.	O

However	O
,	O
some	O
recent	O
works	O
have	O
unveiled	O
that	O
some	O
self	O
-	O
attention	O
heads	O
in	O
pre	O
-	O
trained	O
models	O
tend	O
to	O
learn	O
local	O
dependencies	O
due	O
to	O
the	O
inherent	O
property	O
of	O
natural	O
language	O
(	O
Kovaleva	O
et	O
al	O
.	O
,	O

2019;Brunner	O
et	O
al	O
.	O
,	O

2020	O
;	O
,	O
incurring	O
computation	O
redundancy	O
for	O
capturing	O
local	O
information	O
.	O

In	O
contrast	O
,	O
convolution	O
is	O
a	O
local	O
operator	O
(	O
LeCun	O
et	O
al	O
.	O
,	O

1998;Krizhevsky	O
et	O
al	O
.	O
,	O

2012;Simonyan	O
and	O
Zisserman	O
,	O
2015;He	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
has	O
shown	O
effectiveness	O
on	O
extracting	O
local	O
information	O
for	O
language	O
models	O
(	O
Zeng	O
et	O
al	O
.	O
,	O

2014;Kim	O
,	O
2014;Kalchbrenner	O
et	O
al	O
.	O
,	O

2014;Wu	O
et	O
al	O
.	O
,	O

2018Wu	O
et	O
al	O
.	O
,	O
,	O

2019b	O
.	O

Thus	O
,	O
we	O
propose	O
to	O
augment	O
the	O
layer	O
set	O
by	O
including	O
convolution	O
for	O
local	O
information	O
extraction	O
.	O

For	O
layer	O
orders	O
,	O
most	O
of	O
the	O
existing	O
pre	O
-	O
trained	O
models	O
adopt	O
an	O
interleaved	O
order	O
to	O
arrange	O
the	O
different	O
types	O
of	O
layers	O
.	O

Differently	O
,	O
Press	O
et	O
al	O
.	O
(	O

2020	O
)	O
presented	O
the	O
sandwich	O
order	O
,	O
i.e.	O
,	O
stacking	O
consecutive	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
at	O
the	O
bottom	O
and	O
top	O
,	O
respectively	O
,	O
while	O
keeping	O
the	O
interleaved	O
order	O
in	O
the	O
middle	O
.	O

It	O
has	O
been	O
shown	O
that	O
the	O
sandwich	O
order	O
can	O
bring	O
improvement	O
on	O
language	O
modeling	O
task	O
,	O
indicating	O
the	O
layer	O
order	O
contributes	O
to	O
model	O
performance	O
.	O

However	O
,	O
Press	O
et	O
al	O
.	O
(	O

2020	O
)	O
did	O
not	O
show	O
the	O
generalization	O
capability	O
of	O
this	O
order	O
to	O
other	O
tasks	O
.	O

There	O
is	O
still	O
a	O
large	O
room	O
for	O
exploring	O
more	O
effective	O
orders	O
for	O
pre	O
-	O
trained	O
models	O
.	O

We	O
show	O
the	O
different	O
layer	O
variety	O
designs	O
of	O
existing	O
models	O
in	O
Figure	O
1(b	O
)	O
,	O
including	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019)/ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
Dynamic	B-MethodName
-	I-MethodName
Conv	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
Sandwich	B-MethodName
(	O
Press	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Their	O
performance	O
is	O
summarized	O
in	O
Figure	O
1(c	O
)	O
.	O

It	O
can	O
be	O
seen	O
that	O
layer	O
variety	O
significantly	O
influences	O
model	O
performance	O
.	O

We	O
thus	O
claim	O
it	O
is	O
necessary	O
to	O
investigate	O
layer	O
variety	O
for	O
promoting	O
pre	O
-	O
trained	O
models	O
.	O

However	O
,	O
to	O
perform	O
such	O
investigation	O
for	O
a	O
common	O
model	O
backbone	O
,	O
e.g.	O
,	O
with	O
24	O
layers	O
,	O
we	O
need	O
to	O
evaluate	O
performance	O
of	O
every	O
candidate	O
within	O
an	O
architecture	O
space	O
of	O
3	O
24	O
≈	O
2.8	O
×	O
10	O
11	O
candidates	O
.	O

Pre	O
-	O
training	O
a	O
single	O
language	O
model	O
already	O
needs	O
to	O
consume	O
a	O
large	O
amount	O
of	O
computation	O
,	O
e.g.	O
,	O
2400	O
P100	O
GPU	O
days	O
for	O
pre	O
-	O
training	O
BERT	B-MethodName
.	O

It	O
is	O
barely	O
affordable	O
to	O
pre	O
-	O
train	O
such	O
a	O
large	O
amount	O
of	O
model	O
candidates	O
from	O
scratch	O
.	O

To	O
reduce	O
the	O
computation	O
cost	O
,	O
inspired	O
by	O
recent	O
works	O
on	O
Neural	O
Architecture	O
Search	O
(	O
NAS	O
)	O
(	O
Guo	O
et	O
al	O
.	O
,	O

2020;Cai	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
we	O
construct	O
a	O
supernet	O
according	O
to	O
the	O
layer	O
variety	O
discussed	O
above	O
and	O
pre	O
-	O
train	O
it	O
with	O
Masked	O
Language	O
Modeling	O
(	O
MLM	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
objective	O
.	O

After	O
obtaining	O
the	O
pre	O
-	O
trained	O
supernet	O
,	O
we	O
develop	O
an	O
evolutionary	O
algorithm	O
guided	O
by	O
MLM	O
evaluation	O
accuracy	B-MetricName
to	O
search	O
an	O
effective	O
architecture	O
with	O
specific	O
layer	O
variety	O
.	O

We	O
call	O
the	O
resulted	O
model	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
.	O

Extensive	O
experiments	O
show	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
two	O
-	O
fold	O
.	O

Firstly	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
work	O
is	O
the	O
first	O
to	O
exploit	O
layer	O
variety	O
w.r.t	O
.	O

both	O
layer	O
types	O
and	O
orders	O
for	O
pretrained	O
language	O
models	O
.	O

We	O
found	O
convolutions	O
and	O
layer	O
orders	O
both	O
benefit	O
pre	O
-	O
trained	O
model	O
performance	O
.	O

We	O
hope	O
our	O
observations	O
would	O
facilitate	O
the	O
development	O
of	O
pre	O
-	O
trained	O
lauguage	O
models	O
.	O

Secondly	O
,	O
our	O
obtained	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
superiority	O
over	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

For	O
example	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	O
small	O
achieves	O
79.8	O
on	O
GLUE	B-DatasetName
testing	O
set	O
,	O
1.8	O
higher	O
than	O
the	O
baseline	O
ELECTRAsmall	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Pre	O
-	O
trained	O
Language	O
Models	O
Pre	O
-	O
trained	O
language	O
models	O
have	O
achieved	O
great	O
success	O
and	O
promoted	O
the	O
development	O
of	O
NLP	O
techniques	O
.	O

Instead	O
of	O
separate	O
word	O
representation	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

2013a	O
,	O
b	O
)	O
,	O
McCann	O
et	O
al	O
.	O
(	O

2017	O
and	O
Peters	O
et	O
al	O
.	O
(	O

2018b	O
)	O
propose	O
CoVe	B-MethodName
and	O
ELMo	B-MethodName
respectively	O
which	O
both	O
utilize	O
LSTM	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
to	O
generate	O
contextualized	O
word	O
representations	O
.	O

Later	O
,	O
Radford	O
et	O
al	O
.	O
(	O

2018	O
)	O
introduce	O
GPT	B-MethodName
that	O
changes	O
the	O
backbone	O
to	O
transformers	O
where	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
are	O
arrayed	O
interleavedly	O
.	O

They	O
also	O
propose	O
generative	O
pre	O
-	O
training	O
objectives	O
.	O

BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
continues	O
to	O
use	O
the	O
same	O
layer	O
set	O
and	O
order	O
for	O
backbone	O
but	O
employs	O
different	O
pre	O
-	O
training	O
objectives	O
,	O
i.e.	O
,	O
Masked	O
Language	O
Modeling	O
and	O
Next	O
Sentence	O
Prediction	O
.	O

Then	O
more	O
works	O
introduce	O
new	O
effective	O
pre	O
-	O
training	O
objectives	O
,	O
like	O
Generalized	O
Autoregressive	O
Pretraining	O
(	O
Yang	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
Span	O
Boundary	O
Objective	O
(	O
Joshi	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
Replaced	O
Token	O
Detection	O
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Besides	O
designing	O
pre	O
-	O
training	O
objectives	O
,	O
some	O
other	O
works	O
try	O
to	O
extend	O
BERT	B-MethodName
by	O
incorporating	O
knowledge	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2019;Peters	O
et	O
al	O
.	O
,	O

2019;Xiong	O
et	O
al	O
.	O
,	O

2020	O
)	O
or	O
with	O
multiple	O
languages	O
Conneau	O
and	O
Lample	O
,	O
2019;Chi	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

These	O
works	O
utilize	O
the	O
stereotyped	O
layer	O
pattern	O
,	O
which	O
is	O
unnecessarily	O
optimal	O
(	O
Press	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
inspiring	O
us	O
to	O
further	O
investigate	O
more	O
layer	O
variety	O
to	O
improve	O
pre	O
-	O
trained	O
models	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
we	O
are	O
the	O
first	O
to	O
exploit	O
layer	O
variety	O
from	O
both	O
the	O
layer	O
type	O
set	O
and	O
the	O
layer	O
order	O
for	O
pre	O
-	O
trained	O
language	O
models	O
.	O

Neural	O
Architecture	O
Search	O
Manually	O
designing	O
neural	O
architecture	O
is	O
a	O
time	O
-	O
consuming	O
and	O
error	O
-	O
prone	O
process	O
(	O
Elsken	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

To	O
solve	O
this	O
,	O
many	O
neural	O
architecture	O
search	O
algorithms	O
are	O
proposed	O
.	O

Pioneering	O
works	O
utilize	O
reinforcement	O
learning	O
(	O
Zoph	O
and	O
Le	O
,	O
2017;Baker	O
et	O
al	O
.	O
,	O

2017	O
)	O
or	O
evolutionary	O
algorithm	O
(	O
Real	O
et	O
al	O
.	O
,	O

2017	O
)	O
to	O
sample	O
architecture	O
candidates	O
and	O
train	O
them	O
from	O
scratch	O
,	O
which	O
demand	O
huge	O
computation	O
that	O
ordinary	O
researchers	O
can	O
not	O
afford	O
.	O

To	O
reduce	O
computation	O
cost	O
,	O
recent	O
methods	O
(	O
Pham	O
et	O
al	O
.	O
,	O

2018;Xie	O
et	O
al	O
.	O
,	O

2018;Brock	O
et	O
al	O
.	O
,	O

2018;Cai	O
et	O
al	O
.	O
,	O

2018;Bender	O
et	O
al	O
.	O
,	O

2018;Wu	O
et	O
al	O
.	O
,	O

2019a;Guo	O
et	O
al	O
.	O
,	O

2020	O
)	O
adopt	O
a	O
weight	O
sharing	O
strategy	O
that	O
a	O
supernet	O
subsuming	O
all	O
architectures	O
is	O
trained	O
only	O
once	O
and	O
all	O
architecture	O
candidates	O
can	O
inherit	O
their	O
weights	O
from	O
the	O
supernet	O
.	O

Despite	O
the	O
boom	O
of	O
NAS	O
research	O
,	O
most	O
works	O
focus	O
on	O
computer	O
vision	O
tasks	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2019;Ghiasi	O
et	O
al	O
.	O
,	O

2019;Liu	O
et	O
al	O
.	O
,	O

2019a	O
)	O
,	O
while	O
NAS	O
on	O
NLP	O
is	O
not	O
fully	O
investigated	O
.	O

Recently	O
,	O
So	O
et	O
al	O
.	O
(	O

2019	O
)	O
and	O
search	O
architectures	O
of	O
transformers	O
for	O
translation	O
tasks	O
.	O

leverage	O
differentiable	O
neural	O
architecture	O
to	O
automatically	O
compress	O
BERT	B-MethodName
with	O
task	O
-	O
oriented	O
knowledge	O
distillation	O
for	O
specific	O
tasks	O
.	O

Zhu	O
et	O
al	O
.	O
(	O

2020	O
)	O
utilize	O
architecture	O
search	O
to	O
improve	O
models	O
based	O
on	O
pre	O
-	O
trained	O
BERT	B-MethodName
for	O
the	O
relation	O
classification	O
task	O
.	O

However	O
,	O
these	O
methods	O
only	O
focus	O
on	O
specific	O
tasks	O
or	O
the	O
fine	O
-	O
tuning	O
phase	O
.	O

Besides	O
,	O
Khetan	O
and	O
Karnin	O
(	O
2020	O
)	O
employ	O
pre	O
-	O
training	O
loss	O
to	O
help	O
prune	O
BERT	B-MethodName
,	O
but	O
their	O
method	O
can	O
not	O
find	O
new	O
architectures	O
.	O

Different	O
from	O
them	O
,	O
our	O
work	O
is	O
the	O
first	O
to	O
use	O
NAS	O
to	O
help	O
explore	O
new	O
architectures	O
in	O
a	O
pre	O
-	O
training	O
scenario	O
for	O
general	O
language	O
understanding	O
.	O

An	O
overview	O
of	O
our	O
approach	O
is	O
shown	O
in	O
Figure	O
2	O
.	O

We	O
first	O
define	O
the	O
layer	O
variety	O
to	O
introduce	O
a	O
large	O
architecture	O
search	O
space	O
,	O
and	O
then	O
pre	O
-	O
train	O
a	O
supernet	O
subsuming	O
all	O
candidate	O
architectures	O
,	O
followed	O
by	O
an	O
evolutionary	O
algorithm	O
guided	O
by	O
pre	O
-	O
training	O
MLM	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
accuracy	O
to	O
search	O
an	O
effective	O
model	O
.	O

In	O
what	O
follows	O
,	O
we	O
will	O
give	O
detailed	O
descriptions	O
.	O

As	O
shown	O
in	O
Figure	O
1(a	O
)	O
,	O
the	O
proposed	O
layer	O
variety	O
contains	O
two	O
aspects	O
:	O
layer	O
type	O
and	O
layer	O
order	O
,	O
both	O
of	O
which	O
are	O
important	O
for	O
the	O
performance	O
of	O
pre	O
-	O
trained	O
models	O
but	O
not	O
exploited	O
before	O
.	O

The	O
layer	O
type	O
set	O
of	O
current	O
BERTlike	B-MethodName
models	O
consists	O
of	O
self	O
-	O
attention	O
for	O
information	O
communication	O
and	O
feed	O
-	O
forward	O
for	O
nonlinear	O
transformation	O
.	O

However	O
,	O
as	O
a	O
global	O
operator	O
,	O
self	O
-	O
attention	O
needs	O
to	O
take	O
as	O
input	O
all	O
tokens	O
to	O
compute	O
attention	O
weights	O
for	O
each	O
token	O
,	O
which	O
is	O
inefficient	O
in	O
capturing	O
local	O
information	O
(	O
Wu	O
et	O
al	O
.	O
,	O

2019b	O
;	O
.	O

We	O
notice	O
that	O
convolution	O
(	O
LeCun	O
et	O
al	O
.	O
,	O

1998;Krizhevsky	O
et	O
al	O
.	O
,	O

2012	O
)	O
,	O
as	O
a	O
local	O
operator	O
,	O
has	O
been	O
successfully	O
applied	O
in	O
language	O
models	O
(	O
Zeng	O
et	O
al	O
.	O
,	O

2014;Kim	O
,	O
2014;Kalchbrenner	O
et	O
al	O
.	O
,	O

2014;Wu	O
et	O
al	O
.	O
,	O

2018Wu	O
et	O
al	O
.	O
,	O
,	O

2019b	O
.	O

A	O
typical	O
example	O
is	O
the	O
dynamic	O
convolution	O
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
for	O
machine	O
translation	O
,	O
language	O
modeling	O
and	O
summarization	O
.	O

Therefore	O
,	O
we	O
augment	O
the	O
layer	O
type	O
set	O
by	O
introducing	O
dynamic	O
convolution	O
as	O
a	O
new	O
layer	O
type	O
.	O

The	O
layer	O
set	O
considered	O
in	O
this	O
work	O
thus	O
contains	O
three	O
types	O
of	O
layers	O
,	O
L	O
type	O
=	O
{	O
L	O
SA	O
,	O
L	O
FF	O
,	O
L	O
DC	O
}	O
,	O
(	O
1)where	O
the	O
set	O
elements	O
denote	O
self	O
-	O
attention	O
,	O
feedforward	O
and	O
dynamic	O
convolution	O
layers	O
respectively	O
.	O

See	O
Appendix	O
for	O
more	O
detailed	O
formulation	O
description	O
on	O
them	O
.	O

Layer	O
Order	O
The	O
other	O
variety	O
aspect	O
is	O
layer	O
order	O
.	O

The	O
most	O
widely	O
-	O
used	O
order	O
for	O
pre	O
-	O
trained	O
models	O
is	O
the	O
interleaved	O
order	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017;Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

For	O
a	O
model	O
with	O
24	B-HyperparameterValue
layers	B-HyperparameterName
,	O
the	O
interleaved	O
order	O
can	O
be	O
expressed	O
by	O
the	O
following	O
list,[L	O
SA	O
1	O
,	O
L	O
FF	O
2	O
,	O
L	O
SA	O
3	O
,	O
L	O
FF	O
4	O
,	O
...	O
,	O
L	O
SA	O
23	O
,	O
L	O
FF	O
24	O
]	O
.	O
(	O

2)Similarly	O
,	O
the	O
sandwich	O
order	O
(	O
Press	O
et	O
al	O
.	O
,	O

2020	O
)	O
can	O
be	O
expressed	O
as	O
}	O
The	O
accuracy	B-MetricName
is	O
used	O
to	O
guide	O
the	O
evolutionary	O
algorithm	O
for	O
generating	O
new	O
candidate	O
models.~After	O
T	O
iterations	O
,	O
the	O
candidate	O
with	O
best	O
pre	O
-	O
training	O
accuracy	B-MetricName
is	O
output	O
as	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
.	O

LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
can	O
be	O
scaled	O
up	O
to	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
medium	I-MethodName
/	I-MethodName
base	I-MethodName
with	O
larger	O
hidden	O
size	O
.	O

Beyond	O
the	O
above	O
manually	O
designed	O
orders	O
,	O
we	O
take	O
advantage	O
of	O
neural	O
architecture	O
search	O
to	O
identify	O
more	O
effective	O
layer	O
orders	O
for	O
pre	O
-	O
trained	O
models	O
.	O

The	O
order	O
to	O
be	O
discovered	O
can	O
be	O
expressed	O
as[L	O
1	O
,	O
L	O
2	O
,	O
...	O
,	O
L	O
i	O
,	O
...	O
,	O
L	O
N	O
]	O
,	O
(	O
4)where	O
L	B-HyperparameterName
i	O
∈	O
L	O
type	O
and	O
N	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
.	O

Here	O
,	O
N	B-HyperparameterName
is	O
set	O
to	O
24	B-HyperparameterValue
,	O
following	O
common	O
practice	O
.	O

The	O
layer	O
variety	O
introduced	O
above	O
leads	O
to	O
a	O
huge	O
architecture	O
space	O
of	O
3	O
24	O
≈	O
2.8	O
×	O
10	O
11	O
candidate	O
models	O
to	O
be	O
explored	O
.	O

Thus	O
,	O
it	O
is	O
not	O
affordable	O
to	O
pre	O
-	O
train	O
every	O
candidate	O
model	O
in	O
the	O
space	O
from	O
scratch	O
to	O
evaluate	O
their	O
performance	O
since	O
the	O
pre	O
-	O
training	O
procedure	O
requires	O
huge	O
computations	O
.	O

To	O
reduce	O
the	O
search	O
computations	O
,	O
recent	O
NAS	O
works	O
(	O
Pham	O
et	O
al	O
.	O
,	O

2018;Guo	O
et	O
al	O
.	O
,	O

2020;Cai	O
et	O
al	O
.	O
,	O

2019	O
)	O
exploit	O
a	O
weight	O
sharing	O
strategy	O
.	O

It	O
first	O
trains	O
a	O
supernet	O
subsuming	O
all	O
candidate	O
architectures	O
,	O
and	O
then	O
each	O
candidate	O
architecture	O
can	O
inherit	O
its	O
weights	O
from	O
the	O
trained	O
supernet	O
to	O
avoid	O
training	O
from	O
scratch	O
.	O

Inspired	O
by	O
this	O
strategy	O
,	O
we	O
construct	O
a	O
supernet	O
where	O
each	O
layer	O
contains	O
all	O
types	O
of	O
layers	O
,	O
i.e.	O
,	O
self	O
-	O
attention	O
,	O
feedforward	O
,	O
and	O
dynamic	O
convolution	O
.	O

The	O
supernet	O
architecture	O
can	O
be	O
expressed	O
asA	O
=	O
[	O
{	O
L	O
SA	O
1	O
,	O
L	O
FF	O
1	O
,	O
L	O
DC	O
1	O
}	O
,	O
{	O
L	O
SA	O
2	O
,	O
L	O
FF	O
2	O
,	O
L	O
DC	O
2	O
}	O
,	O
...	O
,	O
{	O
L	O
SA	O
N	O
,	O
L	O
FF	O
N	O
,	O
L	O
DC	O
N	O
}	O
]	O
.(5)Masked	O
Language	O
Modeling	O
(	O
MLM	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
is	O
utilized	O
as	O
the	O
pre	O
-	O
training	O
objective	O
to	O
pretrain	O
the	O
supernet	O
since	O
MLM	O
accuracy	B-MetricName
can	O
reflect	O
the	O
model	O
performance	O
on	O
downstream	O
tasks	O
(	O
Lan	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Most	O
weight	O
sharing	O
approaches	O
on	O
NAS	O
(	O
Wu	O
et	O
al	O
.	O
,	O

2019a	O
;	O
train	O
and	O
optimize	O
the	O
full	O
supernet	O
:	O
the	O
output	O
of	O
each	O
layer	O
is	O
the	O
weighted	O
sum	O
of	O
all	O
types	O
of	O
candidate	O
layers	O
.	O

However	O
,	O
it	O
can	O
not	O
guarantee	O
the	O
sampled	O
single	O
type	O
of	O
layer	O
also	O
works	O
(	O
Guo	O
et	O
al	O
.	O
,	O

2020).To	O
handle	O
this	O
issue	O
,	O
we	O
propose	O
to	O
randomly	O
sample	O
a	O
submodel	O
from	O
the	O
supernet	O
to	O
participate	O
in	O
forward	O
and	O
backward	O
propagation	O
per	O
training	O
step	O
(	O
Cai	O
et	O
al	O
.	O
,	O

2018;Guo	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
sampled	O
submodel	O
architecture	O
can	O
be	O
expressed	O
asa	O
=	O
[	O
L	O
1	O
,	O
L	O
2	O
,	O
...	O
,	O
L	O
i	O
,	O
...	O
,	O
L	O
N	O
]	O
,	O
(	O
6)where	O
L	O
i	O
∈	O
L	O
type	O
∼	O
U	O
with	O
uniform	O
probability	O
distribution	O
P	O
r	O
=	O
1/3	O
.	O

In	O
this	O
pre	O
-	O
training	O
method	O
,	O
the	O
optimized	O
supernet	O
weights	O
can	O
be	O
expressed	O
for	O
i	O
=	O
1	O
:	O
T	O
do	O
S	O
MLM	O
i−1	O
:	O
=	O
∅	O
;	O
for	O
a	O
in	O
S	O
i−1	O
do	O
MLM	O
a	O
val	O
:	O
=	O
Inference(N	O
(	O
a	O
,	O
W	O
A	O
(	O
a	O
)	O
)	O
,	O
D	O
val	O
)	O
;	O
S	O
MLM	O
i−1	O
:	O
=	O
S	O
MLM	O
i−1	O
∪	O
MLM	O
a	O
val	O
;	O
S	O
topk	O
:	O
=	O
Update(S	O
topk	O
,	O
S	O
i−1	O
,	O
S	O
MLM	O
i−1	O
)	O
;	O
S	O
cro	O
:	O
=	O
Crossover(S	O
topk	O
,	O
N	O
cro	O
)	O
;	O
S	O
mut	O
:	O
=	O
Mutation(S	O
topk	O
,	O
N	O
mut	O
,	O
p	O
)	O
;	O
S	O
i	O
:	O
=	O
S	O
cro	O
∪	O
S	O
mut	O
;	O
return	O
a	O
*	O
=	O
argmax	O
a∈S	O
topk	O
MLM	O
a	O
val	O
;	O
as	O
W	O
A	O
=	O
argmin	O
W	O
E	O
a∼U	O
(	O
A	O
)	O
[	O
L	O
pre−train	O
(	O
N	O
(	O
a	O
,	O
W	O
(	O
a)))],(7)where	O
W	O
(	O
a	O
)	O
denotes	O
the	O
submodel	O
weights	O
inherited	O
from	O
the	O
supernet	O
,	O
N	O
means	O
the	O
submodel	O
with	O
specific	O
architecture	O
and	O
weights	O
,	O
L	O
pre−train	O
denotes	O
the	O
pre	O
-	O
training	O
MLM	O
loss	O
and	O
a	O
∼	O
U	O
(	O
A	O
)	O
means	O
a	O
is	O
uniformly	O
sampled	O
from	O
A.	O
Inspired	O
by	O
the	O
recent	O
NAS	O
works	O
(	O
Elsken	O
et	O
al	O
.	O
,	O

2019;Ren	O
et	O
al	O
.	O
,	O

2020;Guo	O
et	O
al	O
.	O
,	O

2020	O
;	O
,	O
we	O
adopt	O
an	O
evolutionary	O
algorithm	O
(	O
EA	O
)	O
to	O
search	O
the	O
model	O
.	O

Previously	O
Real	O
et	O
al	O
.	O
(	O

2017	O
)	O
utilized	O
an	O
evolutionary	O
method	O
in	O
NAS	O
but	O
they	O
trained	O
each	O
candidate	O
model	O
from	O
scratch	O
which	O
is	O
costly	O
and	O
inefficient	O
.	O

Instead	O
,	O
thanks	O
to	O
the	O
supernet	O
mentioned	O
above	O
,	O
we	O
do	O
not	O
need	O
to	O
train	O
the	O
candidate	O
models	O
from	O
scratch	O
since	O
their	O
weights	O
can	O
be	O
inherited	O
from	O
the	O
supernet	O
.	O

Next	O
problem	O
is	O
how	O
to	O
select	O
indicator	O
of	O
the	O
candidate	O
models	O
to	O
guide	O
the	O
EA	O
.	O

Note	O
that	O
our	O
goal	O
is	O
to	O
search	O
a	O
general	O
pre	O
-	O
trained	O
model	O
to	O
benefit	O
a	O
variety	O
of	O
downstream	O
tasks	O
instead	O
of	O
a	O
specific	O
task	O
.	O

Traditional	O
NAS	O
methods	O
Zhu	O
et	O
al	O
.	O
,	O

2020	O
)	O
use	O
downstream	O
task	O
performance	O
as	O
the	O
objective	O
to	O
search	O
for	O
task	O
-	O
specific	O
models	O
.	O

Instead	O
,	O
similar	O
to	O
the	O
work	O
by	O
Khetan	O
and	O
Karnin	O
(	O
2020	O
)	O
et	O
al	O
.	O
,	O

2015	O
)	O
.	O

However	O
,	O
BooksCorpus	B-DatasetName
is	O
no	O
longer	O
publicly	O
available	O
.	O

To	O
ease	O
reproduction	O
,	O
we	O
train	O
models	O
on	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
that	O
is	O
open	O
-	O
sourced	O
and	O
of	O
similar	O
size	O
with	O
the	O
corpus	O
used	O
by	O
BERT	B-DatasetName
.	O

When	O
pre	O
-	O
training	O
the	O
supernet	O
,	O
we	O
leave	O
2	O
%	O
data	O
as	O
our	O
validation	O
set	O
for	O
evolutionary	O
search	O
.	O

Fine	O
-	O
tuning	O
Datasets	O
To	O
compare	O
our	O
model	O
with	O
other	O
pre	O
-	O
trained	O
models	O
,	O
we	O
fine	O
-	O
tune	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
including	O
various	O
tasks	O
for	O
general	O
language	O
understanding	O
,	O
and	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016(Rajpurkar	O
et	O
al	O
.	O
,	O
,	O

2018	O
for	O
question	B-TaskName
answering	I-TaskName
.	O

See	O
Appendix	O
for	O
more	O
details	O
of	O
all	O
tasks	O
.	O

2020	O
)	O
,	O
we	O
define	O
different	O
model	O
sizes	O
,	O
i.e.	O
,	O
"	O
small	O
"	O
,	O
"	O
medium	O
"	O
and	O
"	O
base	O
"	O
,	O
with	O
the	O
same	O
layer	O
number	O
of	O
24	B-HyperparameterValue
but	O
different	O
hidden	B-HyperparameterName
sizes	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
384	B-HyperparameterValue
,	O
and	O
768	B-HyperparameterValue
,	O
respectively	O
.	O

The	O
detailed	O
hyperparameters	O
are	O
shown	O
in	O
Appendix	O
.	O

To	O
reduce	O
training	O
cost	O
,	O
we	O
construct	O
the	O
supernet	O
only	O
in	O
small	O
size	O
.	O

Since	O
the	O
layer	O
number	O
of	O
models	O
in	O
medium	O
and	O
base	O
sizes	O
are	O
the	O
same	O
as	O
that	O
of	O
the	O
small	O
-	O
sized	O
one	O
,	O
the	O
obtained	O
architecture	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
can	O
be	O
easily	O
scaled	O
up	O
to	O
the	O
ones	O
of	O
medium	O
and	O
base	O
sizes	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
to	O
pre	O
-	O
train	O
the	O
supernet	O
with	O
MLM	O
loss	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-4	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	B-HyperparameterValue
,	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
128	B-HyperparameterValue
and	O
pre	B-HyperparameterName
-	I-HyperparameterName
training	I-HyperparameterName
step	I-HyperparameterName
number	I-HyperparameterName
of	O
2	B-HyperparameterValue
million	I-HyperparameterValue
.	O

See	O
Appendix	O
for	O
more	O
details	O
.	O

Evaluation	O
Setup	O
To	O
compare	O
with	O
other	O
pretrained	O
models	O
,	O
we	O
pre	O
-	O
train	O
the	O
searched	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
architecture	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
from	O
scratch	O
on	O
the	O
Open	B-DatasetName
-	I-DatasetName
WebText	I-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
using	O
Re	O
-	O
placed	O
Token	O
Detection	O
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
since	O
it	O
can	O
save	O
computation	O
cost	O
.	O

We	O
fine	O
-	O
tune	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016(Rajpurkar	O
et	O
al	O
.	O
,	O
,	O

2018	O
downstream	O
tasks	O
with	O
most	O
hyperparameters	O
the	O
same	O
as	O
those	O
of	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
for	O
fair	O
comparison	O
.	O

For	O
GLUE	B-DatasetName
tasks	O
,	O
the	O
evaluation	O
metrics	O
are	O
Matthews	B-MetricName
correlation	I-MetricName
for	O
CoLA	B-DatasetName
,	O
Spearman	B-MetricName
correlation	I-MetricName
for	O
STS	B-DatasetName
,	O
and	O
accuracy	B-MetricName
for	O
other	O
tasks	O
,	O
which	O
are	O
averaged	O
to	O
get	O
GLUE	O
score	O
.	O

We	O
utilize	O
evaluation	O
metrics	O
of	O
Exact	B-MetricName
-	I-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
for	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
.	O

Some	O
of	O
the	O
fine	O
-	O
tuning	O
datasets	O
are	O
small	O
,	O
and	O
consequently	O
,	O
the	O
results	O
may	O
vary	O
substantially	O
for	O
different	O
random	O
seeds	O
.	O

Similar	O
to	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
report	O
the	O
median	O
of	O
10	O
fine	O
-	O
tuning	O
runs	O
from	O
the	O
same	O
pretrained	O
model	O
for	O
each	O
result	O
.	O

See	O
Appendix	O
for	O
more	O
evaluation	O
details	O
.	O

Layer	O
Variety	O
Various	O
models	O
are	O
constructed	O
with	O
different	O
layer	O
variety	O
designs	O
,	O
and	O
their	O
results	O
on	O
GLUE	B-DatasetName
development	O
set	O
are	O
shown	O
in	O
Table	O
1	O
.	O

For	O
the	O
layer	O
types	O
,	O
if	O
only	O
two	O
layer	O
types	O
are	O
provided	O
,	O
selecting	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
yields	O
the	O
best	O
result	O
,	O
which	O
can	O
always	O
achieve	O
performance	O
higher	O
than	O
80	B-MetricValue
under	O
different	O
search	O
methods	O
.	O

With	O
only	O
dynamic	O
convolution	O
and	O
feedforward	O
,	O
the	O
performance	O
drops	O
dramatically	O
to	O
around	O
65	B-MetricValue
.	O

Surprisingly	O
,	O
without	O
feed	O
-	O
forward	O
,	O
the	O
layer	O
set	O
of	O
dynamic	O
convolution	O
and	O
self	O
-	O
attention	O
can	O
still	O
achieve	O
relatively	O
good	O
score	O
,	O
near	O
80	B-MetricValue
.	O

When	O
using	O
all	O
the	O
three	O
layer	O
types	O
,	O
we	O
can	O
obtain	O
the	O
best	O
81.8	B-MetricValue
score	O
,	O
1.4	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
(	O
80.4	B-MetricValue
)	O
and	O
0.6	B-MetricValue
higher	O
than	O
the	O
model	O
searched	O
with	O
only	O
self	O
-	O
attention	O
and	O
feedforward	O
(	O
81.2	B-MetricValue
)	O
.	O

This	O
indicates	O
that	O
it	O
is	O
effective	O
to	O
augment	O
the	O
layer	O
type	O
set	O
by	O
including	O
convolution	O
to	O
extract	O
local	O
information	O
for	O
pre	O
-	O
trained	O
models	O
.	O

For	O
layer	O
orders	O
,	O
with	O
the	O
same	O
layer	O
types	O
,	O
the	O
models	O
with	O
either	O
EA	O
or	O
randomly	O
searched	O
orders	O
perform	O
better	O
than	O
those	O
with	O
randomly	O
sampled	O
orders	O
,	O
reflecting	O
the	O
importance	O
of	O
investigating	O
layer	O
orders	O
.	O

For	O
example	O
,	O
with	O
the	O
same	O
layer	O
types	O
of	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
,	O
the	O
EA	O
searched	O
model	O
obtains	O
81.2	B-MetricValue
score	O
,	O
improving	O
BERT	B-MethodName
/	O
ELECTRA	B-MethodName
by	O
6.1/0.8	B-MetricValue
as	O
well	O
as	O
Sandwich	B-MethodName
by	O
2.6	B-MetricValue
.	O

Blue	O
and	O
yellow	O
dots	O
denote	O
the	O
accuracy	B-MetricName
of	O
top	O
10	O
candidates	O
for	O
each	O
method	O
respectively	O
,	O
while	O
the	O
plots	O
mean	O
their	O
averages	O
.	O

each	O
design	O
of	O
layer	O
type	O
set	O
,	O
the	O
order	O
is	O
the	O
best	O
one	O
among	O
5	O
randomly	O
generated	O
orders	O
that	O
are	O
estimated	O
by	O
training	O
models	O
from	O
scratch	O
.	O
"	O

Randomly	O
searched	O
"	O
and	O
"	O
EA	O
searched	O
"	O
are	O
both	O
supernet	O
-	O
based	O
methods	O
,	O
in	O
which	O
the	O
weights	O
of	O
candidate	O
models	O
are	O
inherited	O
from	O
the	O
supernet	O
.	O
"	O

Randomly	O
searched	O
"	O
produces	O
candidate	O
models	O
at	O
random	O
for	O
estimation	O
while	O
"	O
EA	O
searched	O
"	O
generates	O
candidate	O
models	O
with	O
evolutionary	O
algorithm	O
guided	O
by	O
the	O
pre	O
-	O
training	O
MLM	O
accuracy	B-MetricName
.	O

With	O
the	O
same	O
layer	O
types	O
,	O
EA	O
searched	O
orders	O
are	O
generally	O
better	O
than	O
randomly	O
searched	O
ones	O
while	O
the	O
randomly	O
searched	O
ones	O
are	O
generally	O
better	O
than	O
random	O
ones	O
.	O

Figure	O
3	O
plots	O
the	O
pre	O
-	O
trianing	O
MLM	O
evaluation	O
accuracy	B-MetricName
over	O
search	O
iterations	O
with	O
both	O
random	O
and	O
evolutionary	O
search	O
methods	O
.	O

It	O
shows	O
that	O
the	O
accuracy	B-MetricName
of	O
evolutionary	O
search	O
is	O
obviously	O
higher	O
than	O
that	O
of	O
random	O
search	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
evolutionary	O
search	O
.	O

As	O
shown	O
in	O
When	O
running	O
the	O
evolutionary	O
method	O
with	O
different	O
seeds	O
,	O
we	O
see	O
that	O
the	O
resulting	O
models	O
prefer	O
stacking	O
dynamic	O
convolutions	O
at	O
the	O
bottom	O
two	O
layers	O
for	O
extracting	O
local	O
information	O
and	O
self	O
-	O
attention	O
at	O
the	O
top	O
layer	O
to	O
fuse	O
the	O
global	O
information	O
.	O

According	O
to	O
these	O
observation	O
,	O
for	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
,	O
if	O
we	O
replace	O
the	O
bottom	O
two	O
layers	O
with	O
dynamic	O
convolutions	O
or	O
the	O
top	O
layer	O
with	O
self	O
-	O
attention	O
,	O
the	O
performance	O
can	O
be	O
improved	O
by	O
0.3	B-MetricValue
or	O
0.5	B-MetricValue
respectively	O
on	O
GLUE	B-DatasetName
development	O
set	O
.	O

If	O
we	O
replace	O
the	O
bottom	O
8	B-HyperparameterValue
layers	B-HyperparameterName
with	O
manually	O
designed	O
'	O
ccsfccsf	O
'	O
(	O
'	O
c	O
'	O
,	O
's	O
'	O
and	O
'	O
f	O
'	O
denote	O
dynamic	O
convolution	O
,	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
,	O
respectively	O
)	O
and	O
replace	O
the	O
top	O
8	B-HyperparameterValue
layers	B-HyperparameterName
with	O
manually	O
designed	O
'	O
ssfsssfs	O
'	O
together	O
,	O
we	O
observe	O
0.7	B-MetricValue
performance	O
improvement	O
.	O

These	O
results	O
show	O
that	O
it	O
is	O
helpful	O
to	O
stack	O
dynamic	O
convolution	O
at	O
the	O
bottom	O
and	O
self	O
-	O
attention	O
at	O
the	O
top	O
.	O

We	O
only	O
investigate	O
layer	O
variety	O
and	O
search	O
models	O
in	O
a	O
small	O
-	O
sized	O
setting	O
to	O
save	O
computation	O
cost	O
.	O

It	O
is	O
interesting	O
to	O
know	O
whether	O
the	O
searched	O
models	O
can	O
be	O
generalized	O
to	O
larger	O
models	O
with	O
large	O
hidden	O
size	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
2	O
.	O

For	O
larger	O
model	O
size	O
"	O
medium	O
"	O
and	O
"	O
base	O
"	O
,	O
LV	B-MethodName
-	I-MethodName
BERTs	I-MethodName
still	O
outperform	O
other	O
baseline	O
models	O
,	O
demonstrating	O
the	O
good	O
generalization	O
in	O
terms	O
of	O
model	O
size	O
.	O

We	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pretrained	O
models	O
(	O
Radford	O
et	O
al	O
.	O
,	O

2018;Devlin	O
et	O
al	O
.	O
,	O

2019;Clark	O
et	O
al	O
.	O
,	O

2020;Sanh	O
et	O
al	O
.	O
,	O

2019;Jiao	O
et	O
al	O
.	O
,	O

2020	O
;	O
on	O
GLUE	B-DatasetName
testing	O
set	O
and	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
to	O
show	O
its	O
advantages	O
.	O

Although	O
more	O
pre	O
-	O
training	O
data	O
/	O
steps	O
and	O
lager	O
model	O
size	O
can	O
significantly	O
help	O
improve	O
performance	O
(	O
Yang	O
et	O
al	O
.	O
,	O

2019;Lan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
due	O
to	O
the	O
computation	O
resource	O
limit	O
,	O
we	O
only	O
pre	O
-	O
train	O
our	O
models	O
in	O
small	O
/	O
medium	O
/	O
base	O
sizes	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
.	O

We	O
leave	O
evaluating	O
models	O
with	O
more	O
pre	O
-	O
training	O
data	O
/	O
steps	O
and	O
larger	O
model	O
size	O
for	O
future	O
work	O
.	O

We	O
also	O
list	O
some	O
knowledge	O
distillation	O
methods	O
for	O
comparison	O
.	O

However	O
,	O
note	O
that	O
these	O
methods	O
rely	O
on	O
a	O
pre	O
-	O
trained	O
large	O
teacher	O
network	O
and	O
thus	O
are	O
orthogonal	O
to	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
methods	O
.	O

Table	O
3	O
presents	O
the	O
performance	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
pre	O
-	O
trained	O
models	O
on	O
GLUE	B-DatasetName
testing	O
set	O
.	O

It	O
shows	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
other	O
pre	O
-	O
trained	O
models	O
with	O
similar	O
model	O
size	O
.	O

Remarkably	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
/	I-MethodName
base	I-MethodName
achieve	O
79.8/85.1	B-MetricValue
,	O
1.8/1.6	B-MetricValue
higher	O
than	O
strong	O
baselines	O
ELECTRAsmall	B-MethodName
/	I-MethodName
base	I-MethodName
.	O

Even	O
compared	O
with	O
knowledge	O
distillation	O
based	O
model	O
MobileBERT	B-MethodName
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
medium	I-MethodName
still	O
outperforms	O
it	O
by	O
0.3	B-MetricValue
.	O

Since	O
there	O
is	O
nearly	O
no	O
single	O
model	O
submission	O
on	O
SQuAD	B-DatasetName
leaderboard	O
2	O
,	O
we	O
only	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
other	O
pre	O
-	O
trained	O
models	O
on	O
the	O
development	O
sets	O
.	O

The	O
results	O
are	O
shown	O
in	O
Table	O
4	O
.	O

We	O
find	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
outperforms	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
significantly	O
,	O
like	O
F1	B-MetricName
score	O
73.7	B-MetricName
versus	O
69.4	B-MetricName
on	O
SQuAD	B-DatasetName
2.0	I-DatasetName
.	O

However	O
,	O
when	O
we	O
generalize	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
to	O
base	O
size	O
,	O
the	O
gap	O
between	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
ELECTRA	B-MethodName
with	O
base	O
size	O
is	O
narrower	O
than	O
that	O
with	O
small	O
size	O
.	O

One	O
reason	O
may	O
be	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
is	O
searched	O
by	O
our	O
method	O
while	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
base	I-MethodName
is	O
only	O
generalized	O
from	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
with	O
larger	O
hidden	O
size	O
.	O

We	O
are	O
the	O
first	O
to	O
exploit	O
layer	O
variety	O
for	O
improving	O
pre	O
-	O
trained	O
language	O
models	O
,	O
from	O
two	O
aspects	O
,	O
i.e.	O
,	O
layer	O
types	O
and	O
layer	O
orders	O
.	O

For	O
layer	O
types	O
,	O
we	O
augment	O
the	O
layer	O
type	O
set	O
by	O
including	O
convolution	O
for	O
local	O
information	O
extraction	O
.	O

For	O
layer	O
orders	O
,	O
beyond	O
the	O
stereotyped	O
interleaved	O
one	O
,	O
we	O
explore	O
more	O
effective	O
orders	O
by	O
using	O
an	O
evolutionary	O
based	O
search	O
algorithm	O
.	O

Experiment	O
results	O
show	O
our	O
obtained	O
model	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
on	O
various	O
downstream	O
tasks.program	O
and	O
Google	O
Cloud	O
Research	O
Credits	O
Program	O
for	O
the	O
support	O
of	O
computational	O
resources	O
.	O

Daojian	O
Zeng	O
,	O
Kang	O
Liu	O
,	O
Siwei	O
Lai	O
,	O
Guangyou	O
Zhou	O
,	O
and	O
Jun	O
Zhao	O
.	O

2014	O
For	O
a	O
layer	O
,	O
assume	O
its	O
input	O
is	O
I	O
∈	O
R	O
s×c	O
and	O
output	O
is	O
O	O
∈	O
R	O
s×c	O
,	O
where	O
s	O
is	O
the	O
sequence	O
length	O
and	O
c	O
is	O
the	O
hidden	O
size	O
(	O
channel	O
dimension	O
)	O
.	O

For	O
simplicity	O
,	O
c	O
takes	O
the	O
same	O
value	O
for	O
the	O
input	O
and	O
output	O
.	O

Self	O
-	O
Attention	O
The	O
self	O
-	O
Attention	O
layer	O
,	O
also	O
known	O
as	O
multi	O
-	O
head	O
self	O
-	O
attention	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
transforms	O
the	O
input	O
by	O
three	O
linear	O
transformations	O
into	O
the	O
key	O
K	O
,	O
query	O
Q	O
and	O
value	O
V	O
vectors	O
respectively	O
,	O
K	O
=	O
Reshape(IW	O
K	O
+	O
b	O
K	O
)	O
Q	O
=	O
Reshape(IW	O
Q	O
+	O
b	O
Q	O
)	O
V	O
=	O
Reshape(IW	O
V	O
+	O
b	O
V	O
)	O
,	O
(	O
9)whereK	O
,	O
Q	O
,	O
V	O
∈	O
R	O
h×s×d	O
,	O
W	O
K	O
,	O
W	O
Q	O
,	O
W	O
V	O
∈	O
R	O
c×c	O
,	O
and	O
b	O
K	O
,	O
b	O
Q	O
,	O
b	O
V	O
∈	O
R	O
c	O
.	O

Notice	O
that	O
h	B-HyperparameterName
×	O
d	B-HyperparameterName
=	O
cwhere	O
h	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
heads	I-HyperparameterName
and	O
d	B-HyperparameterName
is	O
the	O
head	B-HyperparameterName
dimension	I-HyperparameterName
.	O

The	O
above	O
K	O
and	O
Q	O
are	O
used	O
to	O
compute	O
their	O
similarity	O
matrix	O
M	O
which	O
is	O
then	O
used	O
to	O
generate	O
new	O
value	O
V	O
:	O
M	O
=	O
Softmax(KQ	B-HyperparameterValue
/	O
√	O
d	O
)	O
V	O
=	O
Reshape(M	O
V	O
)	O
,	O
(	O
10)where	O
M	O
∈	O
R	O
h×s×s	O
and	O
V	O
∈	O
R	O
s×c	O
.	O

Finally	O
,	O
a	O
linear	O
transformation	O
is	O
used	O
to	O
exchange	O
information	O
between	O
different	O
heads	O
,	O
followed	O
by	O
shortcut	O
connection	O
and	O
layer	O
normalization	O
,	O
O	O
=	O
Norm(V	B-HyperparameterValue
W	O
O	O
+	O
b	O
O	O
+	O
I),(11)whereW	O
O	O
∈	O
R	O
c×c	O
and	O
b	O
O	O
∈	O
R	O
c	O
.Feed	O
-	O
Forward	O
The	O
feed	O
-	O
forward	O
layer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
includes	O
two	O
linear	O
transformations	O
with	O
a	O
non	O
-	O
linear	O
activation	O
,	O
followed	O
by	O
a	O
shortcut	O
connection	O
and	O
layer	O
normalization	O
,	O
N	O
=	O
GELU(IW	B-HyperparameterValue
1	O
+	O
b	O
1	O
)	O
O	O
=	O
Norm(N	O
W	O
2	O
+	O
b	O
2	O
+	O
I),(12)where	O
W	O
1	O
∈	O
R	O
c×rc	O
and	O
W	O
2	O
∈	O
R	O
rc×c	O
with	O
a	O
ratio	O
r.	O
GELU(•	B-HyperparameterValue
)	O
denotes	O
the	O
Gaussian	O
Error	O
Linear	O
Unit	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
.	O

Different	O
from	O
the	O
vanilla	O
dynamic	O
convolution	O
that	O
directly	O
generates	O
dynamic	O
kernel	O
from	O
V	O
∈	O
R	O
s×c	O
,	O
in	O
this	O
work	O
,	O
we	O
supplement	O
a	O
separate	O
convolution	O
(	O
Howard	O
et	O
al	O
.	O
,	O

2017	O
)	O
with	O
depthwise	O
weights	O
W	O
Dep	O
∈	O
R	O
k×c	O
(	O
k	O
is	O
the	O
convolution	O
kernel	O
size	O
,	O
set	O
as	O
9	O
in	O
this	O
paper	O
)	O
and	O
pointwise	O
weights	O
W	O
Poi	O
∈	O
R	O
c×c	O
to	O
extract	O
local	O
information	O
to	O
help	O
the	O
following	O
kernel	O
generation	O
.	O

Denoting	O
the	O
output	O
as	O
S	O
∈	O
R	O
s×c	O
,	O
the	O
separate	O
convolution	O
can	O
be	O
formulated	O
asS	O
i	O
,	O
:	O
=	O
	O
	O
k	O
j=1	O
W	O
Dep	O
j	O
,	O
:	O
•	O
V	O
i+j−	O
k+1	O
2	O
,	O
:	O
	O
	O
W	O
Poi	O
.	O
(	O

14)Then	O
the	O
output	O
of	O
separate	O
convolution	O
is	O
used	O
to	O
generate	O
dynamic	O
kernels	O
,	O
D	O
=	O
Softmax(Reshape(SW	O
Dyn	O
)	O
)	O
,	O
(	O
15)where	O
W	O
Dyn	O
∈	O
R	O
c×hk	O
and	O
D	O
∈	O
R	O
h×s×k	O
.	O

Then	O
lightweight	O
convolution	O
is	O
applied	O
to	O
the	O
reshaped	O
V	O
=	O
Reshape(V	O
)	O
∈	O
R	O
h×s×d	O
.	O

The	O
output	O
C	O
∈	O
R	O
h×s×d	O
can	O
be	O
expressed	O
asC	O
p	O
,	O
i	O
,	O
:	O
=	O
k	O
j=1	O
D	O
p	O
,	O
i	O
,	O
j	O
•	O
V	O
p	O
,	O
i+j−	O
k+1	O
2	O
,	O
:	O
.(16)Finally	O
,	O
C	O
is	O
reshaped	O
to	O
C	O
=	O
Reshape(C	O
)	O
∈	O
R	O
s×c	O
and	O
a	O
linear	O
transformer	O
is	O
applied	O
to	O
fuse	O
the	O
information	O
among	O
multiple	O
heads	O
,	O
followed	O
by	O
a	O
short	O
connection	O
and	O
layer	O
normalization	O
,	O
O	O
=	O
Norm(C	O
W	O
Out	O
+	O
b	O
Out	O
+	O
I),(17)where	O
W	O
Out	O
∈	O
R	O
c×c	O
and	O
b	O
Out	O
∈	O
R	O
c	O
.	O

Introduced	O
by	O
Wang	O
et	O
al	O
.	O
(	O

2018	O
)	O
,	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
GLUE	B-DatasetName
)	O
benchmark	O
is	O
a	O
collection	O
of	O
nine	O
tasks	O
for	O
natural	O
language	O
understanding	O
,	O
where	O
testing	O
set	O
labels	O
are	O
hidden	O
and	O
predictions	O
need	O
to	O
be	O
submitted	O
to	O
the	O
evaluation	O
server	O
3	O
.	O

We	O
provide	O
details	O
about	O
the	O
GLUE	B-DatasetName
tasks	O
below	O
.	O

CoLA	B-DatasetName
The	I-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Linguistic	I-DatasetName
Acceptability	I-DatasetName
(	O
Warstadt	O
et	O
al	O
.	O
,	O

2019	O
)	O
is	O
a	O
binary	B-TaskName
single	I-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
dataset	O
for	O
predicting	O
whether	O
an	O
sentence	O
is	O
grammatical	O
or	O
not	O
.	O

The	O
samples	O
are	O
from	O
books	O
and	O
journal	O
articles	O
on	O
linguistic	O
theory	O
.	O

MRPC	O
The	O
Microsoft	B-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
is	O
a	O
dataset	O
for	O
the	O
task	O
to	O
predict	O
whether	O
two	O
sentences	O
are	O
semantically	O
equivalent	O
or	O
not	O
.	O

It	O
is	O
extracted	O
from	O
online	O
news	O
sources	O
with	O
human	O
annotations	O
.	O

MNLI	B-DatasetName
The	I-DatasetName
Multi	I-DatasetName
-	I-DatasetName
Genre	I-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
Corpus	I-DatasetName
(	O
Williams	O
et	O
al	O
.	O
,	O

2018	O
)	O
is	O
a	O
dataset	O
of	O
sentence	O
pairs	O
.	O

Each	O
pair	O
has	O
a	O
premise	O
sentence	O
and	O
a	O
hypothesis	O
sentence	O
,	O
requiring	O
models	O
to	O
predict	O
its	O
relationships	O
containing	O
ententailment	O
,	O
contradiction	O
or	O
neutral	O
.	O

It	O
is	O
from	O
ten	O
distinct	O
genres	O
of	O
spoken	O
and	O
written	O
English	O
.	O

SST	B-DatasetName
The	O
Stanford	B-DatasetName
Sentiment	I-DatasetName
Treebank	I-DatasetName
(	O
Socher	O
et	O
al	O
.	O
,	O

2013	O
)	O
is	O
a	O
dataset	O
for	O
the	O
task	O
to	O
predict	O
whether	O
a	O
sentence	O
is	O
positive	O
or	O
negative	O
in	O
sentiment	O
.	O

The	O
dataset	O
is	O
from	O
movie	O
reviews	O
with	O
human	O
annotations	O
.	O

(	O
Giampiccolo	O
et	O
al	O
.	O
,	O

2007	O
)	O
,	O
and	O
RTE5	B-DatasetName
(	O
Bentivogli	O
et	O
al	O
.	O
,	O

2009	O
)	O
.	O

QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
dataset	O
converted	O
from	O
The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
.	O

An	O
example	O
is	O
a	O
pair	O
of	O
a	O
context	O
sentence	O
and	O
a	O
question	O
,	O
requiring	O
to	O
predict	O
whether	O
the	O
context	O
sentence	O
contains	O
the	O
answer	O
to	O
the	O
given	O
question	O
.	O

QQP	B-DatasetName
The	O
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
dataset	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2018	O
)	O
is	O
the	O
dataset	O
from	O
Quora	O
,	O
requiring	O
to	O
determine	O
whether	O
a	O
pair	O
of	O
questions	O
are	O
semantically	O
equivalent	O
or	O
not	O
.	O

STS	B-DatasetName
The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	O
(	O
Cer	O
et	O
al	O
.	O
,	O

2017	O
)	O
is	O
a	O
collection	O
of	O
sentence	O
pairs	O
with	O
human	O
-	O
annotated	O
similarity	O
score	O
on	O
a	O
1	O
-	O
5	O
scare	O
.	O

WNLI	B-DatasetName
Winograd	B-DatasetName
NLI	I-DatasetName
(	O
Levesque	O
et	O
al	O
.	O
,	O

2012	O
)	O
is	O
a	O
small	O
dataset	O
for	O
natural	O
language	O
inference	O
.	O

However	O
,	O
there	O
are	O
issues	O
with	O
the	O
construction	O
of	O
this	O
dataset	O
4	O
.	O

Therefore	O
,	O
this	O
dataset	O
is	O
exclude	O
in	O
this	O
paper	O
for	O
comparison	O
as	O
BERT	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
etc	O
.	O

The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	O
(	O
SQuAD	O
1.1	O
)	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
is	O
a	O
dataset	O
of	O
more	O
than	O
100	O
K	O
questions	O
which	O
all	O
can	O
be	O
answered	O
by	O
locating	O
a	O
span	O
of	O
text	O
from	O
the	O
corresponding	O
context	O
passage	O
.	O

Besides	O
this	O
data	O
,	O
the	O
upgraded	O
version	O
SQuAD	B-DatasetName
2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2018	O
)	O
supplements	O
it	O
with	O
over	O
50	O
K	O
unanswerable	O
questions	O
.	O

For	O
supernet	O
,	O
We	O
pre	O
-	O
train	O
it	O
for	O
2	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
hyperparameters	O
listed	O
in	O
Table	O
5	O
,	O
using	O
Masked	O
Language	O
Modeling	O
(	O
MLM	O
)	O
pre	O
-	O
training	O
objective	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

This	O
objective	O
masks	O
15	O
%	O
input	O
tokens	O
that	O
require	O
the	O
model	O
to	O
predict	O
.	O

The	O
reason	O
to	O
use	O
this	O
objective	O
is	O
that	O
the	O
MLM	O
valida-4	O
https://gluebenchmark.com/faq	O
tion	O
accuracy	B-MetricName
can	O
reflect	O
the	O
performance	O
of	O
models	O
on	O
downstream	O
tasks	O
(	O
Lan	O
et	O
al	O
.	O
,	O

2020).For	O
pre	O
-	O
training	O
LV	B-MethodName
-	I-MethodName
BERTs	I-MethodName
and	O
other	O
compared	O
baselines	O
like	O
DynamicConv	B-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
Sandwich	B-MethodName
(	O
Press	O
et	O
al	O
.	O
,	O

2020	O
)	O
from	O
scratch	O
,	O
we	O
utilize	O
Replaced	O
Token	O
Detection	O
(	O
RTE	O
)	O
pre	O
-	O
training	O
objective	O
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

This	O
objective	O
employs	O
a	O
small	O
generator	O
to	O
predict	O
masked	O
tokens	O
and	O
utilize	O
a	O
larger	O
discriminator	O
to	O
determine	O
predicted	O
tokens	O
from	O
the	O
generator	O
are	O
the	O
same	O
as	O
original	O
ones	O
or	O
not	O
.	O

RTE	O
can	O
help	O
save	O
computation	O
cost	O
but	O
achieve	O
good	O
performance	O
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

We	O
pre	O
-	O
train	O
the	O
models	O
for	O
1	O
M	O
steps	O
,	O
mostly	O
using	O
the	O
same	O
hyperparameters	O
as	O
ELEC	B-MethodName
-	I-MethodName
TRA	I-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

We	O
set	O
the	O
pre	O
-	O
training	O
sequence	O
length	O
128	O
that	O
can	O
help	O
us	O
save	O
computation	O
cost	O
.	O

For	O
downstream	O
task	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
that	O
needs	O
longer	O
input	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
,	O
we	O
pre	O
-	O
train	O
more	O
10	B-HyperparameterValue
%	I-HyperparameterValue
steps	B-HyperparameterName
with	O
the	O
sequence	B-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
to	O
learn	O
the	O
position	O
embedding	O
before	O
fine	O
-	O
tuning	O
.	O

The	O
hyperparameters	O
are	O
listed	O
in	O
Table	O
5	O
.	O

For	O
fine	O
-	O
tuning	O
on	O
downstream	O
tasks	O
,	O
most	O
of	O
the	O
hyperparameters	O
are	O
the	O
same	O
as	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

See	O
Table	O
6	O
.	O

The	O
different	O
searched	O
architectures	O
are	O
listed	O
in	O
Table	O
7	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O

This	O
research	O
/	O
project	O
is	O
supported	O
by	O
the	O
National	O
Research	O
Foundation	O
,	O
Singapore	O
under	O
its	O
AI	O
Singapore	O
Programme	O
(	O
AISG	O
Award	O
No	O
:	O
AISG-100E/-2019	O
-	O
035	O
)	O
.	O

Jiashi	O
Feng	O
was	O
partially	O
supported	O
by	O
MOE2017	O
-	O
T2	O
-	O
2	O
-	O
151	O
,	O
NUS	O
ECRA	O
FY17	O
P08	O
and	O
CRP20	O
-	O
2017	O
-	O
0006	O
.	O

The	O
authors	O
also	O
thank	O
Quanhong	O
Fu	O
and	O
Jian	O
Liang	O
for	O
the	O
help	O
to	O
improve	O
the	O
technical	O
writing	O
aspect	O
of	O
this	O
paper	O
.	O

The	O
computational	O
work	O
for	O
this	O
article	O
was	O
partially	O
performed	O
on	O
resources	O
of	O
the	O
National	O
Supercomputing	O
Centre	O
,	O
Singapore	O
(	O
https://www.nscc.sg	O
)	O
.	O

Weihao	O
Yu	O
would	O
like	O
to	O
thank	O
TPU	O
Research	O
Cloud	O
(	O
TRC	O
)	O

This	O
paper	O
presents	O
a	O
technical	O
report	O
of	O
our	O
submission	O
to	O
the	O
4th	O
task	O
of	O
SemEval-2021	O
,	O
titled	O
:	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
.	O

In	O
this	O
task	O
,	O
we	O
want	O
to	O
predict	O
the	O
correct	O
answer	O
based	O
on	O
a	O
question	O
given	O
a	O
context	O
.	O

Usually	O
,	O
contexts	O
are	O
very	O
lengthy	O
and	O
require	O
a	O
large	O
receptive	O
field	O
from	O
the	O
model	O
.	O

Thus	O
,	O
common	O
contextualized	O
language	O
models	O
like	O
BERT	B-MethodName
miss	O
fine	O
representation	O
and	O
performance	O
due	O
to	O
the	O
limited	O
capacity	O
of	O
the	O
input	O
tokens	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
used	O
the	O
longformer	B-MethodName
model	O
to	O
better	O
process	O
the	O
sequences	O
.	O

Furthermore	O
,	O
we	O
utilized	O
the	O
method	O
proposed	O
in	O
the	O
longformer	B-MethodName
benchmark	O
on	O
wikihop	O
dataset	O
which	O
improved	O
the	O
accuracy	B-MetricName
on	O
our	O
task	O
data	O
from	O
(	O
23.01	B-MetricValue
%	I-MetricValue
and	O
22.95	B-MetricValue
%	I-MetricValue
)	O
achieved	O
by	O
the	O
baselines	O
for	O
subtask	O
1	O
and	O
2	O
,	O
respectively	O
,	O
to	O
(	O
70.30	B-MetricValue
%	I-MetricValue
and	O
64.38	B-MetricValue
%	I-MetricValue
)	O
.	O

Reading	B-TaskName
comprehension	I-TaskName
is	O
the	O
ability	O
to	O
understand	O
a	O
passage	O
either	O
by	O
human	O
or	O
machine	O
.	O

One	O
of	O
the	O
great	O
benchmarks	O
to	O
evaluate	O
this	O
ability	O
is	O
to	O
try	O
to	O
answer	O
specific	O
questions	O
related	O
to	O
the	O
passage	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
.	O

Generally	O
,	O
this	O
problem	O
can	O
contain	O
single	O
or	O
multiple	O
documents	O
as	O
context	O
(	O
containing	O
relevant	O
information	O
needed	O
to	O
understand	O
and	O
answer	O
the	O
question	O
)	O
,	O
a	O
question	O
(	O
a	O
sentence	O
with	O
at	O
least	O
one	O
asking	O
parameter	O
)	O
,	O
and	O
an	O
answer	O
(	O
which	O
is	O
the	O
parameter	O
value	O
of	O
the	O
question).In	O
the	O
Task	O
of	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
(	O
ReCAM	B-TaskName
)	O
,	O
we	O
have	O
one	O
passage	O
as	O
a	O
context	O
,	O
one	O
question	O
and	O
five	O
candidate	O
answers	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

The	O
goal	O
is	O
to	O
identify	O
the	O
correct	O
answer	O
based	O
on	O
the	O
context	O
and	O
the	O
given	O
question	O
.	O

You	O
can	O
see	O
a	O
sample	O
of	O
the	O
data	O
in	O
Table	O
1	O
.	O

For	O
each	O
instance	O
of	O
the	O
data	O
,	O
there	O
is	O
a	O
passage	O
,	O
a	O
question	O
with	O
a	O
missing	O
word	O
that	O
should	O
be	O
filled	O
based	O
on	O
the	O
passage	O
,	O
and	O
five	O
candidate	O
answers	O
to	O
the	O
question	O
.	O
...	O

observers	O
have	O
even	O
named	O
it	O
after	O
him	O
,	O
"	O
Abenomics	O
"	O
.	O

It	O
is	O
based	O
on	O
three	O
key	O
pillars	O
-the	O
"	O
three	O
arrows	O
"	O
of	O
monetary	O
policy	O
,	O
fiscal	O
stimulus	O
and	O
structural	O
reforms	O
in	O
order	O
to	O
ensure	O
long	O
-	O
term	O
sustainable	O
growth	O
in	O
the	O
world	O
's	O
third	O
-	O
largest	O
economy	O
.	O

In	O
this	O
weekend	O
's	O
upper	O
house	O
elections	O
...	O
Question	O
Abenomics	O
:	O
The	O
@Placeholder	O
and	O
the	O
risks	O
Answer	O
(	O
A	O
)	O
chances	O
(	O
B	O
)	O
prospective	O
(	O
C	O
)	O
security	O
(	O
D	O
)	O
objectives	O
(	O
E	O
)	O
threats	O
The	O
task	O
divides	O
into	O
two	O
subtasks	O
:	O
imperceptibility	O
and	O
non	O
-	O
specificity	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021).•	O
imperceptibility	O
:	O
this	O
level	O
of	O
abstract	O
words	O
refers	O
to	O
ideas	O
and	O
concepts	O
that	O
are	O
distant	O
from	O
immediate	O
perception	O
;	O
such	O
as	O
culture	O
,	O
economics	O
,	O
and	O
politics.•	O
non	O
-	O
specificity	O
:	O
In	O
contrast	O
to	O
concrete	O
words	O
,	O
this	O
subtask	O
includes	O
more	O
abstract	O
words	O
which	O
focus	O
on	O
a	O
different	O
type	O
of	O
definition	O
;	O
for	O
example	O
,	O
a	O
concrete	O
word	O
like	O
'	O
cow	O
'	O
could	O
be	O
interpreted	O
as	O
an	O
'	O
animal	O
'	O
which	O
is	O
considered	O
as	O
a	O
more	O
abstract	O
word	O
(	O
Changizi	O
,	O
2008).The	O
main	O
challenges	O
of	O
this	O
task	O
are	O
the	O
abstract	O
meaning	O
concept	O
representation	O
as	O
well	O
as	O
the	O
machine	O
reading	O
comprehension	O
.	O

This	O
is	O
the	O
main	O
reason	O
we	O
have	O
utilized	O
contextualized	O
language	O
representation	O
models	O
to	O
tackle	O
abstract	O
meaning	O
representation	O
problems	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
an	O
end	O
-	O
to	O
-	O
end	O
deep	O
contextualized	O
architecture	O
to	O
model	O
this	O
task	O
.	O

This	O
model	O
is	O
also	O
capable	O
of	O
considering	O
more	O
than	O
one	O
passage	O
as	O
the	O
context	O
,	O
and	O
more	O
than	O
five	O
candidate	O
answers	O
.	O

Since	O
we	O
use	O
the	O
long	B-MethodName
document	I-MethodName
transformer	I-MethodName
model	O
(	O
Longformer	B-MethodName
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
)	O
,	O
no	O
limitation	O
is	O
considered	O
in	O
context	O
passage	O
length	O
.	O

We	O
have	O
evaluated	O
this	O
model	O
both	O
on	O
subtask-1	O
and	O
subtask-2	O
which	O
resulted	O
in	O
70	B-MetricValue
%	I-MetricValue
and	O
64	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
respectively	O
.	O

Therefore	O
,	O
we	O
have	O
about	O
40	B-MetricValue
%	I-MetricValue
improvement	O
compared	O
to	O
the	O
baseline	O
,	O
which	O
is	O
a	O
Gated	B-MethodName
Attention	I-MethodName
(	O
GA	B-MethodName
)	O
model	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021).The	O
rest	O
of	O
the	O
paper	O
is	O
as	O
follows	O
:	O
Section	O
2	O
describes	O
the	O
related	O
works	O
and	O
the	O
background	O
.	O

Section	O
3	O
includes	O
the	O
description	O
of	O
the	O
proposed	O
method	O
.	O

Section	O
4	O
contains	O
the	O
evaluation	O
metrics	O
used	O
as	O
well	O
as	O
a	O
brief	O
discussion	O
,	O
which	O
is	O
then	O
followed	O
by	O
a	O
conclusion	O
and	O
future	O
works	O
in	O
section	O
5	O
.	O

Many	O
approaches	O
have	O
been	O
presented	O
in	O
the	O
literature	O
,	O
from	O
pipeline	O
-	O
based	O
models	O
to	O
end	O
-	O
to	O
-	O
end	O
ones	O
.	O

Each	O
module	O
is	O
also	O
well	O
-	O
investigated	O
from	O
rule	O
-	O
based	O
models	O
to	O
deep	O
learning	O
ones	O
.	O

Despite	O
various	O
configurations	O
presented	O
in	O
the	O
literature	O
to	O
model	O
this	O
problem	O
,	O
most	O
of	O
the	O
systems	O
consist	O
of	O
three	O
modules	O
(	O
Baradaran	O
et	O
al	O
.	O
,	O

2020):•	O
Language	O
representation	O
:	O
this	O
module	O
is	O
responsible	O
to	O
encode	O
the	O
inputs	O
.	O

Context	O
,	O
question	O
,	O
and	O
answer	O
need	O
to	O
be	O
represented	O
as	O
numeric	O
values	O
for	O
computational	O
algorithms	O
to	O
be	O
usable	O
on	O
them	O
.	O

Dense	O
vectorized	O
representations	O
are	O
the	O
most	O
popular	O
methods	O
,	O
which	O
allow	O
us	O
to	O
use	O
the	O
majority	O
of	O
machine	O
learning	O
algorithms.•	O
Reasoning	O
:	O
this	O
module	O
is	O
used	O
to	O
find	O
demonstrations	O
of	O
why	O
the	O
answer	O
is	O
assumed	O
to	O
be	O
valid	O
.	O

It	O
can	O
also	O
be	O
used	O
as	O
a	O
limiter	O
for	O
searchable	O
context.•	O
Prediction	O
:	O
this	O
module	O
aims	O
to	O
generate	O
,	O
retrieve	O
or	O
select	O
the	O
correct	O
answer	O
based	O
on	O
the	O
task	O
description	O
.	O

Recent	O
studies	O
are	O
provided	O
as	O
follows	O
with	O
respect	O
to	O
these	O
modules	O
that	O
the	O
last	O
two	O
modules	O
have	O
been	O
merged	O
.	O

In	O
the	O
end	O
,	O
the	O
longformer	O
model	O
is	O
presented	O
as	O
our	O
mainstay	O
in	O
this	O
paper	O
.	O

One	O
of	O
the	O
most	O
important	O
problems	O
in	O
NLP	O
is	O
representation	O
learning	O
.	O

The	O
earliest	O
models	O
for	O
word	O
representation	O
in	O
the	O
time	O
of	O
deep	O
learning	O
were	O
the	O
models	O
proposed	O
in	O
(	O
Pennington	O
et	O
al	O
.	O
,	O

2014	O
)	O
and	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
which	O
utilized	O
the	O
weights	O
learned	O
for	O
an	O
auxiliary	O
task	O
(	O
a	O
simplified	O
version	O
of	O
the	O
task	O
of	O
language	O
modeling	O
)	O
for	O
word	O
representation	O
.	O

Similarly	O
,	O
methods	O
proposed	O
in	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
and	O
(	O
Liu	O
et	O
al	O
.	O
,	O

2015	O
)	O
utilized	O
a	O
similar	O
structure	O
for	O
sentence	O
,	O
paragraph	O
,	O
or	O
document	O
representation	O
learning	O
.	O

While	O
these	O
methods	O
were	O
quite	O
effective	O
,	O
it	O
has	O
been	O
shown	O
that	O
using	O
neural	O
language	O
models	O
as	O
a	O
way	O
of	O
word	O
representation	O
results	O
in	O
much	O
better	O
,	O
and	O
context	O
-	O
aware	O
representations	O
.	O

In	O
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
it	O
has	O
been	O
shown	O
that	O
fine	O
-	O
tuning	O
language	O
models	O
as	O
sentence	O
encoders	O
result	O
in	O
a	O
significant	O
performance	O
improvement	O
.	O

At	O
the	O
same	O
time	O
,	O
(	O
Peters	O
et	O
al	O
.	O
,	O

2018	O
)	O
used	O
language	O
models	O
directly	O
as	O
word	O
representations	O
,	O
which	O
resulted	O
in	O
significant	O
improvements	O
.	O

In	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2018	O
)	O
a	O
transformer	O
model	O
was	O
trained	O
for	O
the	O
task	O
of	O
masked	O
language	O
models	O
,	O
which	O
resulted	O
in	O
significant	O
improvements	O
,	O
surpassing	O
human	O
performance	O
in	O
many	O
NLP	O
tasks	O
.	O

One	O
of	O
the	O
shortcomings	O
of	O
transformers	O
is	O
the	O
lack	O
of	O
a	O
memory	O
mechanism	O
,	O
which	O
results	O
in	O
(	O
theoretically	O
)	O
lower	O
receptive	O
field	O
compared	O
with	O
LSTMs	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
this	O
shortcoming	O
was	O
addressed	O
by	O
improving	O
the	O
self	O
attention	O
mechanism	O
in	O
transformers	O
so	O
that	O
it	O
would	O
have	O
a	O
(	O
theoretically	O
)	O
unbounded	O
receptive	O
field	O
.	O

More	O
details	O
are	O
presented	O
later	O
in	O
this	O
section	O
.	O

Natural	O
language	O
understanding	O
(	O
NLU	O
)	O
is	O
an	O
umbrella	O
term	O
,	O
referring	O
to	O
any	O
tasks	O
that	O
require	O
machine	O
comprehension	O
.	O

Compared	O
to	O
other	O
NLP	O
tasks	O
,	O
NLU	O
requires	O
the	O
model	O
to	O
be	O
able	O
to	O
understand	O
and	O
reason	O
about	O
the	O
data	O
(	O
Semaan	O
,	O
2012	O
)	O
.	O

While	O
great	O
progress	O
has	O
been	O
made	O
in	O
this	O
field	O
by	O
using	O
contextual	O
word	O
representation	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
it	O
has	O
been	O
found	O
that	O
designing	O
the	O
model	O
itself	O
must	O
not	O
be	O
neglected	O
(	O
Zhu	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
it	O
has	O
been	O
shown	O
that	O
utilizing	O
a	O
transfer	O
learning	O
setting	O
to	O
share	O
knowledge	O
between	O
different	O
NLU	O
tasks	O
results	O
in	O
better	O
performance	O
with	O
fewer	O
data	O
and	O
fewer	O
parameters	O
(	O
Pilault	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
which	O
proves	O
a	O
significant	O
similarity	O
between	O
these	O
tasks	O
.	O

Deep	O
contextualized	O
language	O
models	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
have	O
been	O
well	O
investi	O
-	O
gated	O
in	O
the	O
literature	O
and	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
various	O
tasks	O
.	O

However	O
,	O
these	O
models	O
suffer	O
from	O
performance	O
limitations	O
due	O
to	O
their	O
self	O
-	O
attention	O
layer	O
which	O
results	O
in	O
quadratic	O
space	O
and	O
time	O
complexity	O
concerning	O
the	O
sequence	O
length	O
.	O

In	O
contrast	O
,	O
this	O
model	O
removes	O
the	O
self	O
-	O
attention	O
layer	O
from	O
the	O
base	O
language	O
models	O
,	O
so	O
the	O
limitation	O
resolves	O
and	O
the	O
complexity	O
scales	O
to	O
linear	O
.	O

In	O
order	O
to	O
increase	O
the	O
quality	O
of	O
the	O
model	O
compared	O
to	O
basic	O
models	O
,	O
they	O
have	O
added	O
a	O
global	O
attention	O
layer	O
to	O
the	O
model	O
end	O
which	O
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
long	O
document	O
(	O
passage	O
)	O
tasks	O
and	O
competitive	O
on	O
normal	O
documents	O
.	O

Also	O
,	O
this	O
configuration	O
increases	O
the	O
performance	O
on	O
both	O
normal	O
and	O
lengthy	O
inputs	O
which	O
makes	O
it	O
a	O
good	O
alternative	O
for	O
tasks	O
containing	O
large	O
inputs	O
.	O

This	O
model	O
is	O
also	O
evaluated	O
on	O
a	O
similar	O
task	O
on	O
WikiHop	O
dataset	O
(	O
Welbl	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
improved	O
the	O
results	O
in	O
terms	O
of	O
accuracy	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

As	O
mentioned	O
in	O
section	O
1	O
,	O
given	O
a	O
passage	O
,	O
a	O
question	O
,	O
and	O
a	O
set	O
of	O
answers	O
to	O
the	O
question	O
,	O
the	O
goal	O
is	O
to	O
predict	O
the	O
correct	O
answer	O
among	O
the	O
candidates	O
,	O
which	O
can	O
be	O
seen	O
as	O
a	O
benchmark	O
to	O
evaluate	O
how	O
well	O
the	O
model	O
can	O
comprehend	O
the	O
abstract	O
meaning	O
.	O

To	O
do	O
so	O
,	O
we	O
considered	O
an	O
end	O
-	O
to	O
-	O
end	O
deep	O
learning	O
architecture	O
based	O
on	O
the	O
transformer	O
architecture	O
.	O

Specifically	O
,	O
we	O
used	O
contextual	O
word	O
embeddings	O
based	O
on	O
the	O
transformer	O
to	O
better	O
discover	O
and	O
encode	O
the	O
information	O
contained	O
in	O
the	O
passage	O
.	O

In	O
our	O
model	O
,	O
both	O
subtasks	O
use	O
the	O
same	O
architecture	O
as	O
shown	O
in	O
figure	O
1	O
,	O
although	O
we	O
did	O
not	O
experiment	O
on	O
the	O
possibility	O
of	O
multi	O
-	O
task	O
learning	O
.	O

The	O
word	O
representation	O
models	O
are	O
fine	O
-	O
tuned	O
on	O
the	O
data	O
for	O
better	O
performance	O
.	O

The	O
fine	O
-	O
tuning	O
procedure	O
could	O
allow	O
us	O
to	O
extract	O
additional	O
,	O
taskrelated	O
information	O
which	O
could	O
result	O
in	O
better	O
accuracy	O
in	O
the	O
evaluation	O
phase	O
.	O

To	O
model	O
this	O
problem	O
,	O
let	O
c	O
=	O
{	O
c	O
1	O
,	O
c	O
2	O
,	O
...	O
,	O
c	O
I	O
}	O
denote	O
the	O
passage	O
as	O
the	O
context	O
,	O
where	O
c	O
i	O
corresponds	O
to	O
the	O
i	O
th	O
token	O
(	O
word	O
or	O
subword	O
,	O
depending	O
on	O
the	O
tokenization	O
technique	O
used	O
)	O
and	O
I	O
is	O
the	O
number	O
of	O
tokens	O
in	O
the	O
passage	O
.	O

Similarly	O
,	O
the	O
question	O
is	O
considered	O
as	O
q	O
=	O
{	O
q	O
1	O
,	O
q	O
2	O
,	O
...	O
,	O
q	O
K	O
}	O
where	O
K	O
denotes	O
the	O
length	O
of	O
the	O
question	O
,	O
and	O
q	O
k	O
corresponds	O
to	O
the	O
k	O
th	O
token	O
of	O
the	O
question	O
.	O

Each	O
answer	O
also	O
denotes	O
as	O
e	O
j	O
which	O
is	O
only	O
one	O
abstract	O
word	O
(	O
j	O
∈	O
{	O
1	O
,	O
2	O
,	O
...	O
,	O
5	O
}	O
)	O
.	O

Then	O
we	O
concate	O
-	O
nate	O
the	O
question	O
and	O
the	O
candidates	O
as	O
:	O
a	O
=	O
[	O
q	O
;	O
e	O
1	O
;	O
e	O
2	O
;	O
...	O
;	O
e	O
5	O
]	O
.(1)The	O
size	O
of	O
this	O
sequence	O
is	O
A	O
=	O
K	O
+	O
5	O
as	O
we	O
have	O
only	O
5	O
candidates	O
.	O

Generally	O
,	O
this	O
can	O
be	O
an	O
arbitrary	O
length	O
based	O
on	O
the	O
dataset	O
.	O

Note	O
that	O
we	O
introduce	O
special	O
tokens	O
to	O
separate	O
the	O
context	O
,	O
the	O
question	O
,	O
and	O
the	O
candidates	O
,	O
similar	O
to	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Specifically	O
,	O
we	O
introduce	O
the	O
tokens	O
<	O
s	O
>	O
and	O
<	O
/s	O
>	O
for	O
separating	O
the	O
context	O
,	O
<	O
q	O
>	O
and	O
<	O
/q	O
>	O
for	O
separating	O
the	O
the	O
question	O
,	O
and	O
the	O
tokens	O
<	O
ent	O
>	O
and	O
<	O
/ent	O
>	O
for	O
separating	O
the	O
candidates	O
from	O
each	O
other	O
.	O

In	O
the	O
case	O
of	O
multiple	O
passages	O
,	O
all	O
passages	O
are	O
concatenated	O
to	O
form	O
a	O
single	O
context	O
.	O

These	O
tokens	O
are	O
randomly	O
initialized	O
and	O
fine	O
-	O
tuned	O
.	O

We	O
used	O
the	O
Longformer	B-MethodName
model	O
introduced	O
in	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
as	O
the	O
pre	O
-	O
trained	O
contextual	O
embedding	O
model	O
in	O
our	O
method	O
.	O

Since	O
the	O
context	O
could	O
be	O
too	O
long	O
,	O
we	O
split	O
the	O
context	O
sequence	O
to	O
separate	O
chunks	O
.	O

Each	O
chunk	O
length	O
is	O
equal	O
to	O
maximum	O
sequence	O
length	O
the	O
model	O
could	O
accept	O
appending	O
the	O
sequence	O
a	O
;	O
in	O
fact	O
,	O
model	O
max	O
length	O
=	O
len(chunk)+len(a	O
)	O
.	O

If	O
c	O
l	O
denote	O
each	O
chunk	O
,	O
this	O
sequence	O
could	O
be	O
showed	O
as	O
:	O
b	O
=	O
[	O
c	O
l	O
;	O
a](2)where	O
the	O
full	O
context	O
is	O
c	O
=	O
{	O
c	O
1	O
,	O
c	O
2	O
,	O
...	O
,	O
c	O
L	O
}	O
,	O
and	O
L	O
is	O
the	O
last	O
chunk	O
.	O

The	O
size	O
of	O
this	O
sequence	O
is	O
B	O
so	O
B	O
=	O
L	O
+	O
A.After	O
feeding	O
the	O
input	O
b	O
to	O
the	O
Longformer	O
model	O
,	O
we	O
apply	O
a	O
global	O
attention	O
only	O
on	O
a	O
(	O
concatenated	O
question	O
and	O
answer	O
candidates	O
)	O
,	O
and	O
the	O
rest	O
is	O
the	O
context	O
.	O

As	O
the	O
longformer	O
model	O
utilizes	O
a	O
base	O
model	O
(	O
like	O
RoBERTa	B-MethodName
without	O
the	O
self	O
-	O
attention	O
layer	O
,	O
in	O
our	O
case	O
)	O
,	O
we	O
denote	O
this	O
as	O
basemodel	O
function	O
that	O
outputs	O
the	O
encoded	O
sequence	O
of	O
the	O
input	O
.	O

If	O
GAttn	B-MethodName
denotes	O
the	O
global	O
attention	O
function	O
,	O
we	O
have	O
:	O
d	O
i	O
=	O
basemodel(b	O
)	O
(	O
3	O
)	O
g	O
i	O
=	O
GAttn(d	O
i	O
)	O
.1(i	O
∈	O
A	O
)	O
(	O
4)where	O
d	O
i	O
is	O
the	O
raw	O
output	O
vector	O
for	O
each	O
input	O
token	O
.	O

The	O
global	O
attention	O
function	O
is	O
applied	O
if	O
it	O
is	O
a	O
question	O
or	O
answer	O
candidate	O
token	O
.	O

Then	O
,	O
we	O
extract	O
the	O
outputs	O
corresponding	O
to	O
the	O
question	O
and	O
the	O
candidates	O
tokens	O
,	O
i.e.	O
we	O
have	O
:	O
h	O
j	O
=	O
GAttn(a	O
,	O
c	O
l	O
)	O
(	O
5)Figure	O
1	O
:	O
The	O
model	O
architecture	O
.	O

The	O
concatenated	O
input	O
vector	O
will	O
be	O
encoded	O
using	O
the	O
base	O
model	O
(	O
like	O
RoBERTa	B-MethodName
without	O
the	O
self	O
-	O
attention	O
layer	O
,	O
in	O
our	O
case	O
)	O
.	O

A	O
global	O
attention	O
(	O
Luong	O
et	O
al	O
.	O
,	O

2015	O
)	O
will	O
be	O
applied	O
to	O
the	O
question	O
and	O
the	O
candidate	O
answers	O
representations	O
with	O
respect	O
to	O
the	O
passage	O
as	O
the	O
context	O
.	O

The	O
logit	O
(	O
score	O
)	O
of	O
each	O
ent	O
token	O
will	O
be	O
calculated	O
using	O
a	O
linear	O
transformation	O
function	O
,	O
then	O
the	O
prediction	O
distribution	O
over	O
the	O
answer	O
candidates	O
(	O
ent	O
tokens	O
)	O
will	O
be	O
outputted	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
.	O

Finally	O
,	O
we	O
obtain	O
the	O
logit	O
of	O
each	O
candidate	O
(	O
<	O
ent	O
>	O
tokens	O
)	O
as	O
x	O
j	O
(	O
x	O
j	O
=	O
h	O
j	O
if	O
j	O
correspond	O
to	O
a	O
candidate	O
)	O
,	O
average	O
over	O
different	O
chunks	O
,	O
and	O
apply	O
a	O
linear	O
transformation	O
:	O
f	O
j	O
=	O
v	O
T	O
x	O
j	O
(	O
6)where	O
the	O
vector	O
v	O
is	O
trainable	O
,	O
and	O
f	O
j	O
is	O
the	O
score	O
of	O
each	O
candidate	O
.	O

And	O
the	O
probability	O
distribution	O
over	O
the	O
candidates	O
will	O
be	O
calculated	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
on	O
the	O
logits	O
.	O

The	O
predicted	O
answer	O
is	O
the	O
argmax	O
of	O
the	O
softmax	O
output	O
.	O

we	O
fine	O
-	O
tuned	O
the	O
model	O
using	O
the	O
cross	O
-	O
entropy	O
loss	O
.	O

Although	O
we	O
only	O
participated	O
in	O
the	O
second	O
subtask	O
,	O
we	O
will	O
evaluate	O
our	O
model	O
on	O
both	O
subtasks	O
here	O
.	O

We	O
will	O
explain	O
our	O
configurations	O
for	O
utilizing	O
the	O
model	O
on	O
the	O
task	O
as	O
well	O
as	O
other	O
baselines	O
which	O
are	O
the	O
BERT	B-MethodName
-	O
base	O
as	O
an	O
alternative	O
model	O
and	O
the	O
Gate	B-MethodName
-	I-MethodName
Attention	I-MethodName
(	I-MethodName
GA	I-MethodName
)	I-MethodName
as	O
our	O
task	O
baseline	O
.	O

Finally	O
,	O
a	O
brief	O
discussion	O
will	O
be	O
done	O
based	O
on	O
the	O
results	O
.	O

Popular	O
metrics	O
to	O
evaluate	O
these	O
models	O
are	O
F1	B-MetricName
,	O
EM	B-MetricName
(	O
Exact	B-MetricName
Match	I-MetricName
or	O
accuracy	B-MetricName
)	O
,	O
and	O
MRR	B-MetricName
(	O
Mean	B-MetricName
Reciprocal	I-MetricName
Rank	I-MetricName
)	O
.	O

As	O
the	O
precision	B-MetricName
and	O
recall	B-MetricName
in	O
our	O
task	O
are	O
equal	O
,	O
so	O
F1	B-MetricName
=	O
Precision	B-MetricName
=	O
Recall	B-MetricName
.	O

Also	O
,	O
F1	B-MetricName
and	O
EM	B-MetricName
are	O
the	O
same	O
.	O

And	O
,	O
the	O
use	O
of	O
MRR	B-MetricName
is	O
optional	O
,	O
so	O
the	O
metrics	O
used	O
to	O
evaluate	O
the	O
result	O
are	O
the	O
accuracy	O
and	O
the	O
F1	B-MetricName
.	O

The	O
baseline	O
model	O
(	O
GA	B-MethodName
)	O
is	O
trained	O
for	O
30	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
each	O
epoch	O
containing	O
101	B-HyperparameterValue
mini	B-HyperparameterName
-	I-HyperparameterName
batches	I-HyperparameterName
.	O

The	O
train	B-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	B-HyperparameterValue
.	O

Dropout	B-HyperparameterName
with	O
the	O
rate	O
of	O
0.5	B-HyperparameterValue
is	O
also	O
applied	O
to	O
the	O
hidden	O
states	O
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
0.001	B-HyperparameterValue
.	O

The	O
dimensionality	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
GloVe	I-HyperparameterName
embedding	I-HyperparameterName
is	O
300	B-HyperparameterValue
,	O
and	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
128	B-HyperparameterValue
.	O

Training	O
and	O
evaluation	O
take	O
about	O
2	O
hours	O
on	O
a	O
single	O
v100	O
GPU	O
.	O

We	O
use	O
the	O
same	O
configuration	O
as	O
our	O
method	O
except	O
for	O
the	O
global	O
attention	O
mechanism	O
.	O

In	O
fact	O
,	O
we	O
consider	O
the	O
output	O
vector	O
of	O
each	O
chunk	O
as	O
our	O
final	O
vector	O
to	O
be	O
linearly	O
transformed	O
into	O
single	O
logit	O
,	O
followed	O
by	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
using	O
the	O
crossentropy	O
loss	O
.	O

Similarly	O
,	O
the	O
logit	O
is	O
averaged	O
over	O
different	O
chunks	O
,	O
before	O
applying	O
the	O
linear	O
transformation	O
.	O

Note	O
that	O
the	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
here	O
is	O
bounded	O
to	O
512	B-HyperparameterValue
tokens	I-HyperparameterValue
,	O
and	O
the	O
model	O
includes	O
the	O
n	O
2	O
attention	O
mechanism	O
.	O

We	O
use	O
the	O
base	O
version	O
of	O
the	O
model	O
and	O
fine	O
-	O
tuned	O
it	O
on	O
each	O
subtask	O
.	O

We	O
used	O
the	O
same	O
model	O
introduced	O
in	O
section	O
3	O
for	O
both	O
subtasks	O
.	O

The	O
model	O
was	O
initialized	O
using	O
the	O
Longformer	B-MethodName
-	O
base	O
pre	O
-	O
training	O
weights	O
,	O
then	O
finetuned	O
in	O
each	O
of	O
the	O
subtasks	O
.	O

Due	O
to	O
the	O
performance	O
issues	O
,	O
the	O
model	B-HyperparameterName
max	I-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
is	O
set	O
to	O
4096	B-HyperparameterValue
tokens	I-HyperparameterValue
which	O
are	O
sufficient	O
in	O
our	O
case	O
.	O

We	O
also	O
used	O
the	O
RoBERTa	B-MethodName
-	O
large	O
tokenizer	O
to	O
tokenize	O
the	O
input	O
sequence	O
as	O
the	O
Longformer	B-MethodName
model	O
has	O
been	O
trained	O
on	O
using	O
this	O
configuration	O
.	O

We	O
used	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
and	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-5	B-HyperparameterValue
using	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
beta2=0.98	B-HyperparameterName
.	O

We	O
then	O
assumed	O
the	O
validation	O
check	O
interval	O
to	O
250	O
which	O
indicates	O
the	O
number	O
of	O
gradient	O
updates	O
between	O
checking	O
validation	O
loss	O
.	O

And	O
a	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
has	O
been	O
considered	O
to	O
regularize	O
the	O
model	O
and	O
avoid	O
overfitting	O
.	O

Our	O
proposed	O
model	O
is	O
trained	O
for	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
for	O
each	O
task	O
.	O

Fine	O
-	O
tuning	O
the	O
model	O
takes	O
about	O
six	O
hours	O
,	O
and	O
inference	O
takes	O
about	O
nine	O
seconds	O
for	O
each	O
sample	O
on	O
a	O
single	O
V100	O
GPU	O
.	O

Subtask1	O
measures	O
imperceptibility	O
abstract	O
level	O
of	O
language	O
understanding	O
.	O

This	O
subtask	O
includes	O
3227	O
training	O
samples	O
,	O
837	O
validation	O
samples	O
,	O
and	O
2025	O
test	O
samples	O
.	O

The	O
size	O
of	O
the	O
biggest	O
sample	O
in	O
terms	O
of	O
context	O
length	O
is	O
about	O
2000	O
tokens	O
.	O

We	O
have	O
achieved	O
an	O
accuracy	B-MetricName
of	O
70	B-MetricValue
%	I-MetricValue
on	O
the	O
validation	O
set	O
,	O
which	O
improves	O
our	O
baseline	O
by	O
about	O
40	B-MetricValue
percent	I-MetricValue
.	O

Table	O
2	O
showed	O
the	O
results	O
of	O
this	O
subtask	O
.	O

Subtask2	O
measures	O
the	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
level	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
in	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

It	O
in	O
-	O
cludes	O
3318	O
training	O
samples	O
,	O
851	O
validation	O
samples	O
,	O
and	O
2017	O
test	O
samples	O
.	O

The	O
best	O
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
is	O
64	B-MetricValue
%	I-MetricValue
.	O

Table	O
3	O
showed	O
the	O
results	O
of	O
this	O
subtask	O
.	O

We	O
used	O
two	O
baselines	O
to	O
find	O
out	O
the	O
effect	O
of	O
using	O
a	O
pre	O
-	O
trained	O
model	O
rather	O
than	O
a	O
simple	O
RNN	B-MethodName
model	O
.	O

Although	O
this	O
task	O
offers	O
a	O
higher	O
level	O
of	O
representation	O
,	O
using	O
the	O
pre	O
-	O
train	O
models	O
is	O
helpful	O
,	O
and	O
there	O
is	O
a	O
higher	O
chance	O
of	O
modeling	O
such	O
abstract	O
concepts	O
.	O

The	O
results	O
on	O
subtask2	O
are	O
weaker	O
than	O
subtask1	O
in	O
pre	O
-	O
trained	O
models	O
.	O

This	O
can	O
be	O
the	O
consequence	O
of	O
limited	O
semantic	O
representation	O
for	O
abstract	O
word	O
which	O
indicates	O
the	O
subtask2	O
includes	O
more	O
abstract	O
words	O
in	O
terms	O
of	O
abstract	O
level	O
;	O
for	O
example	O
,	O
the	O
word	O
'	O
animal	O
'	O
could	O
be	O
matched	O
to	O
any	O
animal	O
,	O
like	O
'	O
cat	O
'	O
or	O
'	O
dog	O
'	O
,	O
but	O
the	O
word	O
'	O
entity	O
'	O
is	O
hard	O
to	O
be	O
represented	O
as	O
it	O
could	O
be	O
matched	O
to	O
a	O
large	O
number	O
of	O
words	O
.	O

And	O
the	O
model	O
faces	O
a	O
limitation	O
in	O
the	O
knowledge	O
representation	O
.	O

Another	O
assumption	O
could	O
be	O
the	O
data	O
enrichment	O
that	O
these	O
model	O
has	O
been	O
trained	O
on	O
.	O

As	O
most	O
of	O
the	O
available	O
texts	O
for	O
training	O
consist	O
of	O
concrete	O
words	O
,	O
it	O
is	O
more	O
likely	O
to	O
leverage	O
the	O
language	O
understanding	O
to	O
less	O
abstract	O
words	O
to	O
achieve	O
a	O
better	O
result	O
.	O

Comparing	O
our	O
method	O
which	O
is	O
based	O
on	O
longformer	B-MethodName
model	O
to	O
usual	O
language	O
models	O
like	O
BERT	B-MethodName
indicates	O
a	O
new	O
insight	O
in	O
terms	O
of	O
passage	O
length	O
and	O
the	O
attention	O
mechanism	O
.	O

Popular	O
language	O
models	O
like	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
use	O
a	O
n	O
2	O
attention	O
which	O
requires	O
a	O
large	O
receptive	O
field	O
to	O
represent	O
long	O
passages	O
.	O

This	O
results	O
in	O
the	O
performance	O
limitation	O
which	O
bounds	O
the	O
input	O
sequence	O
up	O
to	O
512	O
tokens	O
.	O

In	O
contrast	O
,	O
the	O
longformer	B-MethodName
global	O
attention	O
mechanism	O
relaxes	O
this	O
limitation	O
as	O
we	O
only	O
need	O
to	O
pay	O
attention	O
to	O
a	O
small	O
factor	O
of	O
context	O
and	O
more	O
focus	O
on	O
the	O
local	O
window	O
.	O

So	O
the	O
receptive	O
field	O
will	O
not	O
overflow	O
and	O
saves	O
the	O
necessary	O
information	O
to	O
better	O
represent	O
the	O
language	O
.	O

We	O
have	O
analyzed	O
the	O
errors	O
that	O
mostly	O
affect	O
our	O
model	O
performance	O
.	O

We	O
think	O
that	O
the	O
problem	O
is	O
the	O
contextual	O
representation	O
of	O
the	O
language	O
modeling	O
,	O
which	O
is	O
not	O
well	O
-	O
suited	O
in	O
our	O
method	O
i.e.	O
concatenating	O
the	O
context	O
,	O
question	O
,	O
and	O
answer	O
.	O

The	O
main	O
disadvantage	O
of	O
concatenating	O
the	O
candidate	O
answers	O
to	O
each	O
other	O
is	O
the	O
missing	O
fine	O
contextual	O
representation	O
as	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
consume	O
the	O
position	O
embedding	O
.	O

Additionally	O
,	O
incorrect	O
candidates	O
register	O
additional	O
noise	O
to	O
each	O
word	O
representation	O
as	O
well	O
as	O
the	O
placeholder	O
in	O
the	O
question	O
.	O

We	O
have	O
shown	O
how	O
different	O
approaches	O
can	O
be	O
leveraged	O
to	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
.	O

We	O
reformulated	O
the	O
longformer	B-MethodName
model	O
to	O
learn	O
abstract	B-TaskName
meaning	I-TaskName
as	I-TaskName
a	I-TaskName
new	I-TaskName
level	I-TaskName
of	I-TaskName
semantic	I-TaskName
in	I-TaskName
machine	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

This	O
method	O
can	O
also	O
be	O
improved	O
by	O
taking	O
advantage	O
of	O
external	O
knowledge	O
and	O
task	O
-	O
specific	O
model	O
architectures	O
that	O
optimize	O
the	O
current	O
baseline	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
contribution	O
in	O
SemEval-2021	B-DatasetName
Task	O
1	O
:	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
,	O
where	O
we	O
integrate	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
properties	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
as	O
features	O
within	O
a	O
Machine	O
Learning	O
(	O
ML	O
)	O
framework	O
for	O
predicting	O
lexical	O
complexity	O
.	O

In	O
particular	O
,	O
we	O
use	O
BERT	B-MethodName
contextualized	I-MethodName
word	I-MethodName
embeddings	I-MethodName
to	O
represent	O
the	O
semantic	O
meaning	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
.	O

We	O
participated	O
in	O
the	O
sub	O
-	O
task	O
of	O
predicting	O
the	O
complexity	O
score	O
of	O
single	O
words	O
.	O

Over	O
the	O
last	O
decade	O
,	O
automated	O
methods	O
for	O
detecting	O
complex	O
words	O
have	O
been	O
developed	O
.	O

At	O
the	O
beginning	O
,	O
most	O
of	O
these	O
methods	O
assumed	O
that	B-MetricName
lexical	I-MetricName
complexity	I-MetricName
is	O
binary	O
,	O
words	O
are	O
either	O
"	O
difficult	O
"	O
or	O
"	O
not	O
difficult	O
"	O
.	O

Thus	O
,	O
the	O
first	O
Complex	B-TaskName
Word	I-TaskName
Identification	I-TaskName
(	O
CWI	O
)	O
shared	O
task	O
referred	O
to	O
binary	O
identification	O
of	O
complex	O
words	O
(	O
Zampieri	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

The	O
main	O
limitation	O
of	O
this	O
assumption	O
is	O
that	O
a	O
word	O
close	O
to	O
the	O
decision	O
boundary	O
is	O
considered	O
to	O
be	O
as	O
complex	O
as	O
one	O
farther	O
apart	O
.	O

Therefore	O
,	O
three	O
years	O
ago	O
,	O
the	O
CWI	B-TaskName
included	O
an	O
additional	O
probabilistic	O
classification	O
task	O
where	O
the	O
participants	O
were	O
asked	O
to	O
give	O
a	O
probability	O
of	O
the	O
given	O
target	O
word	O
in	O
particular	O
context	O
being	O
complex	O
(	O
Štajner	O
et	O
al	O
.	O
,	O

2018).Recently	O
,	O
CompLex	B-DatasetName
,	O
a	O
new	O
English	O
corpus	O
for	O
lexical	O
complexity	O
prediction	O
was	O
introduced	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
corpus	O
is	O
annotated	O
using	O
a	O
5	O
-	O
point	O
Likert	B-MetricName
scale	I-MetricName
(	O
1	O
-	O
5	O
)	O
(	O
corresponding	O
to	O
very	O
easy	O
,	O
easy	O
,	O
neutral	O
,	O
difficult	O
,	O
and	O
very	O
difficult	O
)	O
,	O
and	O
covers	O
3	O
genres	O
:	O
Bible	O
translation	O
,	O
European	O
Pariliament	O
proceedings	O
,	O
and	O
biomedical	O
articles	O
.	O

SemEval-2021	B-DatasetName
(	O
Task	O
1	O
)	O
shared	O
task	O
on	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
(	I-TaskName
LCP	I-TaskName
)	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

2021a	O
,	O
b	O
)	O
provided	O
the	O
participants	O
with	O
Complex	O
and	O
defined	O
two	O
sub	O
-	O
tasks	O
:	O
predicting	O
the	O
com	O
-	O
plexity	O
score	O
of	O
single	O
words	O
,	O
and	O
predicting	O
the	O
complexity	O
score	O
of	O
multi	O
-	O
word	O
expressions	O
.	O

We	O
present	O
our	O
system	O
for	O
the	O
first	O
sub	O
-	O
task	O
of	O
predicting	O
the	O
complexity	O
score	O
of	O
single	O
words	O
.	O

Our	O
system	O
incorporates	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
properties	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
as	O
features	O
within	O
a	O
Machine	O
Learning	O
(	O
ML	O
)	O
framework	O
for	O
predicting	O
lexical	O
complexity	O
.	O

This	O
paper	O
is	O
organized	O
as	O
follows	O
:	O
First	O
,	O
in	O
Section	O
2	O
,	O
we	O
describe	O
features	O
from	O
previous	O
works	O
that	O
we	O
have	O
adopted	O
.	O

Then	O
,	O
in	O
Section	O
3	O
,	O
we	O
describe	O
our	O
feature	O
sets	O
,	O
the	O
feature	O
selection	O
process	O
,	O
and	O
the	O
results	O
on	O
the	O
trial	O
data	O
.	O

Finally	O
,	O
Our	O
system	O
results	O
on	O
the	O
test	O
data	O
are	O
detailed	O
in	O
Section	O
4	O
,	O
followed	O
by	O
conclusions	O
in	O
Section	O
5	O
.	O

In	O
this	O
section	O
,	O
we	O
shortly	O
describe	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
features	O
which	O
were	O
encoded	O
as	O
features	O
in	O
previous	O
complexity	O
prediction	O
tasks	O
and	O
were	O
integrated	O
in	O
our	O
system	O
.	O

Linguistics	O
features	O
,	O
such	O
as	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tag	O
,	O
dependency	O
parsing	O
relations	O
,	O
and	O
syllable	O
counts	O
,	O
as	O
well	O
as	O
statistical	O
features	O
,	O
such	O
as	O
word	O
length	O
and	O
word	O
frequency	O
,	O
have	O
been	O
widely	O
used	O
for	O
predicting	O
lexical	B-MetricName
complexity	I-MetricName
(	O
Mukherjee	O
et	O
al	O
.	O
,	O

2016;Ronzano	O
et	O
al	O
.	O
,	O

2016;Alfter	O
and	O
Pilán	O
,	O
2018;Gooding	O
and	O
Kochmar	O
,	O
2018;Hartmann	O
and	O
Dos	O
Santos	O
,	O
2018;Kajiwara	O
and	O
Komachi	O
,	O
2018;Wani	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Some	O
of	O
these	O
works	O
found	O
WordNet	B-MethodName
(	O
Miller	O
,	O
1998	O
)	O
as	O
a	O
valuable	O
source	O
of	O
lexical	O
features	O
.	O

The	O
main	O
extracted	O
feature	O
is	O
the	O
number	O
of	O
synsets	O
,	O
but	O
also	O
information	O
on	O
hypernyms	O
,	O
hyponyms	O
,	O
holonym	O
,	O
and	O
meronym	O
is	O
useful	O
(	O
Gooding	O
and	O
Kochmar	O
,	O
2018;Hartmann	O
and	O
Dos	O
Santos	O
,	O
2018;Wani	O
et	O
al	O
.	O
,	O

2018).Semantic	O
features	O
were	O
commonly	O
encoded	O
using	O
word	O
embedding	O
representation	O
of	O
the	O
meaning	O
of	O
words	O
(	O
Kuru	O
,	O
2016;AbuRa'ed	O
and	O
Sag	O
-	O
gion	O
,	O
2018	O
)	O
.	O

These	O
word	O
embeddings	O
were	O
generated	O
using	O
Word2Vec	B-MethodName
context	O
-	O
independent	O
models	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

Word2Vec	B-MethodName
models	O
combine	O
different	O
senses	O
of	O
the	O
word	O
into	O
one	O
single	O
vector	O
.	O

However	O
,	O
recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
contextualized	O
word	O
representations	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

BERT	B-MethodName
model	O
generates	O
context	O
-	O
dependent	O
embeddings	O
that	O
allow	O
a	O
word	O
to	O
have	O
several	O
vector	O
representations	O
depending	O
on	O
the	O
context	O
in	O
which	O
it	O
is	O
used	O
.	O

In	O
contrast	O
to	O
previous	O
works	O
that	O
only	O
use	O
context	O
-	O
independent	O
embeddings	O
,	O
our	O
system	O
uses	O
the	O
BERT	B-MethodName
-	O
based	O
contextdependent	O
embeddings	O
.	O

We	O
adopt	O
a	O
supervised	O
Machine	O
Learning	O
(	O
ML	O
)	O
approach	O
for	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

The	O
first	O
step	O
in	O
a	O
classifier	O
training	O
is	O
to	O
determine	O
which	O
text	O
characteristics	O
are	O
relevant	O
and	O
how	O
those	O
features	O
are	O
coded	O
.	O

We	O
next	O
detail	O
how	O
the	O
semantic	O
properties	O
of	O
the	O
sentence	O
,	O
as	O
well	O
as	O
the	O
linguistic	O
and	O
statistical	O
properties	O
found	O
useful	O
in	O
prior	O
work	O
,	O
are	O
encoded	O
as	O
features	O
.	O

Then	O
,	O
in	O
Section	O
3.2	O
,	O
we	O
describe	O
our	O
feature	O
analysis	O
procedure	O
and	O
the	O
supervised	O
ML	O
model	O
.	O

The	O
features	O
in	O
our	O
model	O
are	O
divided	O
into	O
3	O
sets	O
:	O
linguistic	O
,	O
statistical	O
and	O
semantic	O
.	O

Our	O
dataset	O
contains	O
three	O
corpora	O
:	O
Bible	B-DatasetName
,	I-DatasetName
Europarl	I-DatasetName
,	I-DatasetName
and	I-DatasetName
Biomedical	I-DatasetName
,	O
to	O
add	O
variation	O
.	O

Since	O
each	O
corpus	O
has	O
its	O
own	O
unique	O
linguistic	O
features	O
,	O
we	O
first	O
encode	O
the	O
text	O
source	O
by	O
three	O
binary	O
features	O
.	O

Most	O
of	O
our	O
linguistic	O
features	O
are	O
based	O
on	O
information	O
extracted	O
from	O
a	O
POS	O
tagger	O
.	O

Our	O
linguistic	O
properties	O
include	O
two	O
families	O
of	O
properties	O
:	O
morphological	O
and	O
syntactical	O
.	O

First	O
,	O
we	O
encode	O
the	O
target	O
word	O
POS	O
.	O

The	O
POS	O
is	O
extracted	O
by	O
the	O
Spacy	B-MethodName
's	O
statistical	O
POS	O
tagger	O
1	O
.	O

Each	O
possible	O
POS	O
tag	O
is	O
represented	O
as	O
a	O
binary	O
feature	O
.	O

We	O
use	O
the	O
following	O
12	O
tags	O
from	O
the	O
Universal	O
POS	O
tags	O
2	O
:	O
ADJ	O
,	O
ADP	O
,	O
ADV	O
,	O
CONJ	O
,	O
DET	O
,	O
NOUN	O
,	O
NUM	O
,	O
PRT	O
,	O
PRON	O
,	O
VERB	O
and	O
X	O
(	O
other	O
)	O
.	O

As	O
an	O
additional	O
feature	O
,	O
the	O
number	O
of	O
syllables	O
in	O
the	O
target	O
word	O
is	O
encoded	O
3	O
.	O

Then	O
,	O
we	O
calculate	O
the	O
number	O
of	O
punctuation	O
marks	O
and	O
stopwords	O
in	O
the	O
sentence	O
(	O
two	O
features).Next	O
,	O
we	O
represent	O
syntactic	O
forms	O
by	O
POS	O
patterns	O
.	O

The	O
POS	O
pattern	O
refers	O
to	O
seven	O
words	O
,	O
the	O
target	O
word	O
and	O
three	O
words	O
before	O
and	O
after	O
it	O
.	O

Each	O
of	O
the	O
words	O
is	O
encoded	O
by	O
12	O
binary	O
features	O
,	O
resulting	O
with	O
84	O
features	O
.	O

We	O
also	O
measure	O
the	O
polysemy	O
degree	O
of	O
the	O
target	O
word	O
using	O
the	O
number	O
of	O
senses	O
in	O
WordNet	B-MethodName
.	O

We	O
obtain	O
two	O
lexical	O
features	O
:	O
number	O
of	O
synsets	O
for	O
the	O
target	O
word	O
and	O
number	O
of	O
synsets	O
for	O
the	O
target	O
word	O
given	O
its	O
POS	O
.	O

We	O
define	O
some	O
statistical	O
features	O
based	O
on	O
frequency	O
.	O

First	O
,	O
we	O
calculate	O
target	O
word	O
length	O
and	O
sentence	O
length	O
.	O

Then	O
,	O
we	O
extract	O
the	O
target	O
word	O
frequency	O
using	O
Google	B-MethodName
N	I-MethodName
-	I-MethodName
gram	I-MethodName
4	I-MethodName
word	I-MethodName
frequencies	I-MethodName
.	O

We	O
encode	O
the	O
logarithm	O
of	O
this	O
frequency	O
as	O
a	O
feature	O
to	O
speed	O
the	O
ML	O
algorithm	O
's	O
convergence	O
(	O
three	O
features	O
)	O
.	O

We	O
represent	O
the	O
meaning	O
of	O
the	O
surrounding	O
context	O
of	O
the	O
target	O
word	O
by	O
vectors	O
in	O
the	O
same	O
semantic	O
space	O
.	O

We	O
use	O
the	O
BERT	B-MethodName
semantic	O
space	O
.	O

BERT	B-MethodName
is	O
a	O
bidirectional	B-MethodName
transformer	I-MethodName
pre	O
-	O
trained	O
on	O
a	O
large	O
corpus	O
containing	O
the	O
Toronto	B-DatasetName
Book	I-DatasetName
Corpus	I-DatasetName
and	O
Wikipedia	B-DatasetName
using	O
a	O
combination	O
of	O
masked	O
language	O
modeling	O
objective	O
and	O
next	O
sentence	O
prediction	O
.	O

BERT	B-MethodName
contextualizing	O
vectors	O
are	O
used	O
to	O
represent	O
the	O
semantic	O
meaning	O
of	O
the	O
sentence	O
by	O
averaging	O
the	O
BERT	B-MethodName
vectors	O
of	O
seven	O
words	O
,	O
the	O
target	O
word	O
and	O
three	O
words	O
before	O
and	O
after	O
it	O
.	O

Thus	O
,	O
our	O
semantic	O
representation	O
add	O
768	O
features	O
(	O
the	O
size	O
of	O
BERT	O
output	O
layer).To	O
extract	O
additional	O
features	O
,	O
we	O
use	O
two	O
machine	O
learning	O
algorithm	O
:	O
K	B-MethodName
-	I-MethodName
Means	I-MethodName
and	I-MethodName
k	I-MethodName
-	I-MethodName
Nearest	I-MethodName
Neighbors	I-MethodName
(	I-MethodName
KNN	I-MethodName
)	I-MethodName
algorithm	I-MethodName
.	O

K	B-MethodName
-	I-MethodName
Means	I-MethodName
is	O
an	O
unsupervised	O
learning	O
algorithm	O
used	O
for	O
clustering	O
.	O

It	O
takes	O
the	O
unlabeled	O
dataset	O
and	O
tries	O
to	O
group	O
them	O
into	O
k	O
number	O
of	O
clusters	O
.	O

We	O
encode	O
the	O
K	B-MethodName
-	I-MethodName
Mean	I-MethodName
results	O
by	O
four	O
binary	O
features	O
,	O
a	O
feature	O
per	O
cluster	O
(	O
k=4	B-HyperparameterName
)	O
.	O

The	O
results	O
of	O
the	O
KNN	B-MethodName
algorithm	O
are	O
encoded	O
similarly	O
.	O

However	O
,	O
KNN	O
is	O
a	O
supervised	O
learning	O
algorithm	O
used	O
for	O
classification	O
.	O

It	O
takes	O
the	O
labeled	O
dataset	O
and	O
uses	O
it	O
to	O
learn	O
how	O
to	O
label	O
other	O
sentences	O
.	O

KNN	B-MethodName
classifies	O
an	O
unseen	O
sentence	O
using	O
it	O
k	B-MethodName
nearest	I-MethodName
neighbors	I-MethodName
voting	O
.	O

We	O
use	O
four	O
complexity	B-HyperparameterName
classes	O
:	O
0	B-HyperparameterValue
-	I-HyperparameterValue
0.25	I-HyperparameterValue
,	I-HyperparameterValue
0.26	I-HyperparameterValue
-	I-HyperparameterValue
0.5	I-HyperparameterValue
,	I-HyperparameterValue
0.51	I-HyperparameterValue
-	I-HyperparameterValue
0.75	I-HyperparameterValue
,	I-HyperparameterValue
0.76	I-HyperparameterValue
-	I-HyperparameterValue
1	I-HyperparameterValue
.	O

For	O
each	O
of	O
the	O
above	O
feature	O
sets	O
,	O
we	O
tried	O
to	O
filter	O
out	O
non	O
-	O
relevant	O
features	O
using	O
several	O
approaches	O
.	O

First	O
,	O
we	O
discharged	O
features	O
that	O
decrease	O
the	O
system	O
performance	O
on	O
the	O
training	O
set	O
,	O
namely	O
,	O
the	O
POS	B-MethodName
pattern	I-MethodName
features	O
,	O
the	O
WordNet	B-MethodName
features	O
,	O
and	O
the	O
K	B-MethodName
-	I-MethodName
Means	I-MethodName
and	I-MethodName
KNN	I-MethodName
features	O
.	O

We	O
were	O
left	O
with	O
794	O
features	O
.	O

These	O
features	O
were	O
selected	O
using	O
the	O
Linear	B-MethodName
Regression	I-MethodName
algorithm	O
,	O
which	O
was	O
also	O
selected	O
as	O
a	O
baseline	O
algorithm	O
by	O
the	O
task	O
organizers	O
.	O

To	O
further	O
improve	O
the	O
performance	O
of	O
our	O
systems	O
,	O
we	O
used	O
additional	O
ML	O
algorithms	O
,	O
such	O
as	O
SVM	B-MethodName
and	O
XGBoost	B-MethodName
(	O
see	O
more	O
details	O
in	O
Section	O
3.3).Next	O
,	O
since	O
correlated	O
features	O
do	O
not	O
carry	O
unique	O
information	O
and	O
may	O
interfere	O
the	O
learning	O
,	O
we	O
tried	O
to	O
discharge	O
highly	O
correlated	O
features	O
.	O

We	O
implemented	O
this	O
approach	O
using	O
the	O
following	O
iterative	O
process	O
.	O

The	O
input	O
is	O
the	O
desired	O
final	O
number	O
of	O
features	O
.	O

First	O
,	O
we	O
define	O
an	O
initial	O
correlation	O
threshold	O
(	O
0.9	O
)	O
.	O

Then	O
,	O
we	O
calculate	O
the	O
features	O
'	O
pairwise	O
correlation	O
and	O
features	O
with	O
correlation	O
above	O
the	O
threshold	O
are	O
removed	O
.	O

Next	O
,	O
if	O
we	O
still	O
have	O
more	O
features	O
than	O
desired	O
,	O
we	O
will	O
lower	O
the	O
correlation	B-HyperparameterName
threshold	I-HyperparameterName
(	O
by	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
and	O
repeat	O
the	O
process	O
.	O

This	O
approach	O
improved	O
the	O
performance	O
of	O
the	O
SVM	B-MethodName
and	O
Linear	B-MethodName
Regression	I-MethodName
models	O
(	O
selecting	O
97	O
features	O
)	O
,	O
but	O
did	O
not	O
increase	O
the	O
performance	O
of	O
the	O
XGBOOST	B-MethodName
method	O
.	O

We	O
note	O
that	O
we	O
also	O
tried	O
to	O
filter	O
out	O
feature	O
using	O
the	O
principal	B-MethodName
component	I-MethodName
analysis	I-MethodName
(	I-MethodName
PCA	I-MethodName
)	I-MethodName
feature	O
selection	O
method	O
(	O
Song	O
et	O
al	O
.	O
,	O

2010	O
)	O
.	O

PCA	B-MethodName
aims	O
to	O
pick	O
a	O
subset	O
of	O
features	O
that	O
retains	O
as	O
much	O
information	O
present	O
in	O
the	O
full	O
data	O
as	O
possible	O
.	O

PCA	B-MethodName
was	O
performed	O
both	O
on	O
the	O
full	O
feature	O
list	O
and	O
on	O
specific	O
features	O
,	O
such	O
as	O
BERT	B-MethodName
features	O
,	O
but	O
it	O
was	O
not	O
successful	O
.	O

Some	O
of	O
the	O
classification	O
models	O
had	O
low	O
performance	O
using	O
such	O
amount	O
of	O
features	O
(	O
794	O
features	O
)	O
.	O

Therefore	O
,	O
we	O
further	O
filleted	O
features	O
by	O
calculating	O
their	O
correlation	O
with	O
the	O
complexity	O
score	O
and	O
discarding	O
features	O
with	O
low	O
correlation	O
(	O
less	O
than	O
0.072	O
)	O
.	O

We	O
resulted	O
with	O
the	O
following	O
list	O
of	O
101	O
features:•	O
Biomedical	O
corpus	O
indicator	O
•	O
94	O
features	O
from	O
BERT	B-MethodName
vector	O
It	O
is	O
interesting	O
to	O
note	O
that	O
even	O
though	O
,	O
there	O
are	O
12	O
POS	O
tags	O
,	O
only	O
2	O
are	O
informative	O
for	O
the	O
complexity	O
prediction	O
task	O
.	O

Considering	O
the	O
source	O
text	O
indicators	O
,	O
the	O
third	O
Bible	O
indicator	O
is	O
not	O
useful	O
.	O

Out	O
of	O
the	O
BERT	B-MethodName
768	O
features	O
,	O
only	O
94	O
remained	O
(	O
12.2	O
%	O
of	O
the	O
vector).The	O
BERT	B-MethodName
representation	O
of	O
the	O
sentence	O
is	O
generated	O
by	O
pre	O
-	O
trained	O
language	O
representation	O
model	O
.	O

These	O
models	O
can	O
be	O
trained	O
on	O
different	O
datasets	O
of	O
various	O
domains	O
.	O

Since	O
one	O
of	O
our	O
corpora	O
is	O
from	O
the	O
Biomedical	O
domain	O
,	O
we	O
examined	O
the	O
system	O
performance	O
using	O
the	O
domain	O
specific	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Figure	O
1	O
shows	O
a	O
comparison	O
between	O
the	O
error	O
rate	O
of	O
our	O
system	O
using	O
the	O
classic	O
BERT	B-MethodName
and	O
BioBERT	B-MethodName
(	O
BERT	B-MethodName
on	O
the	O
left	O
and	O
BioBERT	B-MethodName
on	O
the	O
right	O
)	O
.	O

The	O
columns	O
show	O
the	O
error	O
rate	O
for	O
different	O
text	O
sources	O
.	O

The	O
red	O
line	O
is	O
the	O
average	O
error	O
rate	O
.	O

Columns	O
from	O
left	O
to	O
right	O
:	B-DatasetName
Bible	I-DatasetName
,	I-DatasetName
Biomedical	I-DatasetName
,	I-DatasetName
and	I-DatasetName
Europarl	I-DatasetName
.	O

Surprisingly	O
,	O
the	O
error	O
rate	O
of	O
the	O
BioBERT	B-MethodName
on	O
the	O
Biomedical	O
domain	O
is	O
higher	O
than	O
that	O
of	O
the	O
classic	O
BERT	B-MethodName
.	O

However	O
,	O
the	O
average	O
error	O
for	O
both	O
is	O
the	O
same	O
(	O
∼	O
0.69	O
)	O
.	O

We	O
combined	O
the	O
features	O
in	O
a	O
supervised	O
classification	O
framework	O
using	O
five	O
ML	O
methods	O
:	O
Linear	B-MethodName
Regression	I-MethodName
,	I-MethodName
Supported	I-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	I-MethodName
SVM	I-MethodName
)	I-MethodName
,	I-MethodName
XGBoost	I-MethodName
(	I-MethodName
XGB	I-MethodName
)	I-MethodName
,	I-MethodName
KNN	I-MethodName
,	I-MethodName
and	I-MethodName
Stacking	I-MethodName
(	I-MethodName
Stack	I-MethodName
)	I-MethodName
.	O

We	O
trained	O
the	O
ML	O
methods	O
on	O
the	O
train	O
set	O
and	O
evaluated	O
their	O
performances	O
on	O
the	O
trial	O
set	O
.	O

We	O
ran	O
these	O
ML	O
methods	O
by	O
the	O
scikit	B-MethodName
-	I-MethodName
learn	I-MethodName
open	O
-	O
source	O
machine	O
-	O
learning	O
package	O
in	O
python	O
5	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O

2011	O
)	O
using	O
the	O
default	O
parameters	O
.	O

Table	O
1	O
shows	O
the	O
performances	O
of	O
the	O
different	O
ML	O
methods	O
on	O
the	O
feature	O
set	O
of	O
101	O
features	O
,	O
as	O
described	O
above	O
.	O

The	O
MAE	B-MethodName
is	O
omitted	O
from	O
the	O
table	O
because	O
it	O
is	O
similar	O
for	O
all	O
the	O
ML	O
algorithms	O
(	O
0.01	O
)	O
.	O

The	O
performance	O
differences	O
between	O
the	O
algorithms	O
were	O
not	O
so	O
substantial	O
.	O

Therefore	O
,	O
we	O
next	O
report	O
the	O
performances	O
of	O
all	O
these	O
methods	O
on	O
the	O
test	O
set	O
.	O

To	O
increase	O
the	O
size	O
of	O
our	O
train	O
set	O
for	O
the	O
test	O
phase	O
of	O
the	O
task	O
,	O
we	O
used	O
both	O
the	O
train	O
and	O
trial	O
sets	O
to	O
train	O
the	O
final	O
model	O
.	O

To	O
analyze	O
our	O
results	O
,	O
we	O
converted	O
the	O
complexity	O
scores	O
to	O
labels	O
following	O
Shardlow	O
et	O
al	O
.	O
(	O

2020	O
)	O
descriptors	O
.	O

In	O
Figure	O
2	O
,	O
we	O
present	O
the	O
classification	O
confusion	O
matrix	O
of	O
the	O
XGBoost	B-MethodName
algorithm	O
.	O

Each	O
column	O
of	O
the	O
matrix	O
represents	O
the	O
instances	O
in	O
a	O
predicted	O
class	O
while	O
each	O
row	O
represents	O
the	O
instances	O
in	O
an	O
actual	O
class	O
.	O

Most	O
of	O
the	O
classification	B-MetricName
errors	I-MetricName
(	O
18.54	B-MetricValue
%	I-MetricValue
)	O
were	O
due	O
to	O
incorrect	O
classification	O
of	O
very	O
easy	O
words	O
as	O
easy	O
.	O

There	O
were	O
also	O
errors	O
in	O
the	O
opposite	O
direction	O
(	O
4.36	B-MetricValue
%	I-MetricValue
)	O
.	O

Most	O
of	O
the	O
rest	O
of	O
the	O
classifications	O
were	O
between	O
neutral	O
and	O
easy	O
in	O
both	O
directions	O
(	O
7.42	B-MetricValue
%	I-MetricValue
+	I-MetricValue
6.43	I-MetricValue
%	I-MetricValue
=	I-MetricValue
13.85	I-MetricValue
%	I-MetricValue
)	O
.	O

We	O
note	O
that	O
the	O
5	O
th	O
class	O
,	O
very	O
difficult	O
,	O
does	O
not	O
appear	O
in	O
the	O
confusion	O
matrix	O
since	O
there	O
are	O
not	O
any	O
very	O
difficult	O
words	O
in	O
the	O
test	O
set	O
and	O
the	O
system	O
did	O
not	O
classified	O
any	O
of	O
the	O
words	O
as	O
very	O
difficult	O
.	O

We	O
have	O
implemented	O
a	O
system	O
that	O
incorporates	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
features	O
to	O
predict	O
lexical	O
complexity	O
of	O
target	O
word	O
in	O
context	O
.	O

BERT	B-MethodName
semantic	O
space	O
was	O
used	O
to	O
represent	O
the	O
word	O
and	O
its	O
context	O
.	O

We	O
investigated	O
several	O
feature	O
selection	O
approaches	O
and	O
used	O
various	O
supervised	O
algorithms	O
.	O

Even	O
though	O
our	O
system	O
was	O
not	O
highly	O
ranked	O
,	O
we	O
believe	O
that	O
some	O
of	O
the	O
presented	O
ideas	O
can	O
be	O
useful	O
for	O
future	O
research	O
on	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

In	O
particular	O
,	O
we	O
think	O
that	O
BERT	B-MethodName
is	O
a	O
powerful	O
model	O
that	O
should	O
be	O
explored	O
.	O

Perhaps	O
,	O
fine	O
-	O
tuning	O
BERT	B-MethodName
for	O
the	O
complexity	O
prediction	O
task	O
would	O
increase	O
the	O
system	O
performance	O
.	O

We	O
investigate	O
the	O
less	O
-	O
explored	O
task	O
of	O
generating	B-TaskName
open	I-TaskName
-	I-TaskName
ended	I-TaskName
questions	I-TaskName
that	I-TaskName
are	I-TaskName
typically	I-TaskName
answered	I-TaskName
by	I-TaskName
multiple	I-TaskName
sentences	I-TaskName
.	O

We	O
first	O
define	O
a	O
new	O
question	O
type	O
ontology	O
which	O
differentiates	O
the	O
nuanced	O
nature	O
of	O
questions	O
better	O
than	O
widely	O
used	O
question	O
words	O
.	O

A	O
new	O
dataset	O
with	O
4	O
,	O
959	O
questions	O
is	O
labeled	O
based	O
on	O
the	O
new	O
ontology	O
.	O

We	O
then	O
propose	O
a	O
novel	O
question	B-TaskName
type	I-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
,	O
augmented	O
by	O
a	O
semantic	B-TaskName
graph	I-TaskName
representation	I-TaskName
,	O
to	O
jointly	O
predict	O
question	O
focuses	O
and	O
produce	O
the	O
question	O
.	O

Based	O
on	O
this	O
framework	O
,	O
we	O
further	O
use	O
both	O
exemplars	O
and	O
automatically	O
generated	O
templates	O
to	O
improve	O
controllability	O
and	O
diversity	O
.	O

Experiments	O
on	O
two	O
newly	O
collected	O
large	O
-	O
scale	O
datasets	O
show	O
that	O
our	O
model	O
improves	O
question	O
quality	O
over	O
competitive	O
comparisons	O
based	O
on	O
automatic	O
metrics	O
.	O

Human	O
judges	O
also	O
rate	O
our	O
model	O
outputs	O
highly	O
in	O
answerability	B-MetricName
,	O
coverage	B-MetricName
of	I-MetricName
scope	I-MetricName
,	O
and	O
overall	B-MetricName
quality	I-MetricName
.	O

Finally	O
,	O
our	O
model	O
variants	O
with	O
templates	O
can	O
produce	O
questions	O
with	O
enhanced	O
controllability	O
and	O
diversity	O
.	O

Question	B-TaskName
-	I-TaskName
asking	I-TaskName
has	O
long	O
served	O
as	O
an	O
effective	O
instrument	O
for	O
knowledge	B-TaskName
learning	I-TaskName
(	O
Andre	O
,	O
1979;Tobin	O
,	O
1990	O
)	O
and	O
assessing	O
learning	O
progress	O
(	O
Holme	O
,	O
2003;Downing	O
and	O
Yudkowsky	O
,	O
2009;Livingston	O
,	O
2009	O
)	O
.	O

Compared	O
to	O
the	O
widely	O
studied	O
task	O
of	O
generating	O
factoid	O
questions	O
that	O
inquire	O
about	O
"	O
one	O
bit	O
"	O
of	O
information	O
(	O
Du	O
et	O
al	O
.	O
,	O

2017;Duan	O
et	O
al	O
.	O
,	O

2017;Li	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
this	O
work	O
is	O
interested	O
in	O
generating	B-TaskName
open	I-TaskName
-	I-TaskName
ended	I-TaskName
questions	I-TaskName
that	I-TaskName
require	I-TaskName
deep	I-TaskName
comprehension	I-TaskName
and	I-TaskName
long	I-TaskName
-	I-TaskName
form	I-TaskName
answers	I-TaskName
(	O
Labutov	O
et	O
al	O
.	O
,	O

2015	O
)	O
.	O

Such	O
open	O
-	O
ended	O
questions	O
are	O
valuable	O
in	O
education	O
,	O
e.g.	O
,	O
to	O
facilitate	O
complex	O
knowledge	O
acquisition	O
(	O
Lai	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
nurture	O
reasoning	O
skills	O
(	O
Shapley	O
,	O
2000	O
)	O
,	O
as	O
well	O
as	O
in	O
other	O
applications	O
like	O
improving	O
search	O
engines	O
(	O
Han	O
Input	O
:	O
It	O
's	O
a	O
difficult	O
task	O
to	O
undertake	O
.	O

Teenagers	O
tend	O
to	O
identify	O
gangs	O
with	O
"	O
fitting	O
"	O
in	O
.	O

Peer	O
pressure	O
plays	O
a	O
large	O
part	O
in	O
it	O
and	O
sometimes	O
teenagers	O
have	O
problems	O
with	O
their	O
own	O
identity	O
being	O
part	O
of	O
a	O
gang	O
deals	O
with	O
those	O
issues	O
.	O

It	O
also	O
provides	O
a	O
little	O
bit	O
of	O
respect	O
on	O
the	O
street	O
...	O
Figure	O
1	O
:	O
Open	O
-	O
ended	O
questions	O
generated	O
by	O
different	O
models	O
after	O
reading	O
the	O
same	O
input	O
:	O
(	O
1	O
)	O
BART	B-MethodName
decoded	O
with	O
nucleus	O
sampling	O
,	O
(	O
2	O
)	O
BART	B-MethodName
that	O
considers	O
different	O
question	O
words	O
,	O
and	O
(	O
3	O
)	O
our	O
type	B-MethodName
-	I-MethodName
aware	I-MethodName
generator	I-MethodName
TPLGEN	I-MethodName
,	O
that	O
predicts	O
focuses	O
and	O
operates	O
with	O
generated	O
templates	O
(	O
to	O
the	O
left	O
of	O
the	O
arrows	O
)	O
.	O

Questions	O
generated	O
by	O
our	O
model	O
have	O
diverse	O
TYPEs	O
.	O

et	O
al	O
.	O
,	O

2019	O
)	O
and	O
building	O
open	B-MethodName
-	I-MethodName
domain	I-MethodName
dialogue	I-MethodName
systems	I-MethodName
(	O
Shum	O
et	O
al	O
.	O
,	O

2018).Significant	O
progress	O
has	O
been	O
made	O
in	O
generating	O
factoid	O
questions	O
(	O
Zhang	O
and	O
Bansal	O
,	O
2019;Zhou	O
et	O
al	O
.	O
,	O

2019b;Su	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
yet	O
new	O
challenges	O
need	O
to	O
be	O
addressed	O
for	O
open	O
-	O
ended	O
questions	O
.	O

First	O
,	O
specifying	O
the	O
question	O
type	O
is	O
crucial	O
for	O
constructing	O
meaningful	O
questions	O
(	O
Graesser	O
et	O
al	O
.	O
,	O

1992	O
)	O
.	O

Question	O
words	O
such	O
as	O
"	O
why	O
"	O
and	O
"	O
when	O
"	O
are	O
generally	O
seen	O
as	O
being	O
indicative	O
of	O
types	O
(	O
Zhou	O
et	O
al	O
.	O
,	O

2019b	O
)	O
,	O
but	O
they	O
underspecify	O
the	O
conceptual	O
content	O
of	O
questions	O
(	O
Olney	O
et	O
al	O
.	O
,	O

2012	O
)	O
.	O

Using	O
Figure	O
1	O
as	O
an	O
example	O
,	O
different	O
question	O
words	O
,	O
i.e.	O
,	O
both	O
"	O
how	O
"	O
and	O
"	O
what	O
"	O
,	O
can	O
be	O
used	O
for	O
inquiring	O
about	O
procedures	O
.	O

It	O
thus	O
calls	O
for	O
a	O
new	O
question	O
type	O
ontology	O
that	O
can	O
precisely	O
capture	O
the	O
conceptual	O
nature	O
of	O
questions	O
.	O

Second	O
,	O
constructing	O
questions	O
from	O
a	O
text	O
with	O
multiple	O
sentences	O
needs	O
to	O
focus	O
on	O
its	O
central	O
concepts	O
or	O
phenomena	O
that	O
necessitate	O
extensive	O
descriptions	O
.	O

New	O
representations	O
are	O
needed	O
to	O
capture	O
such	O
content	O
as	O
question	O
focus(es	O
)	O
,	O
to	O
go	O
beyond	O
existing	O
methods	O
that	O
rely	O
on	O
entities	O
and	O
their	O
neighboring	O
words	O
(	O
Du	O
et	O
al	O
.	O
,	O

2017;Sun	O
et	O
al	O
.	O
,	O

2018	O
)	O
even	O
though	O
they	O
are	O
effective	O
for	O
generating	O
factoid	O
questions	O
.	O

Third	O
,	O
encouraging	O
the	O
diversity	O
of	O
generated	O
questions	O
(	O
Sultan	O
et	O
al	O
.	O
,	O

2020;Wang	O
et	O
al	O
.	O
,	O

2020	O
)	O
is	O
less	O
explored	O
but	O
critical	O
for	O
real	O
world	O
applications	O
,	O
e.g.	O
,	O
various	O
questions	O
should	O
be	O
proposed	O
to	O
gauge	O
how	O
well	O
students	O
grasp	O
the	O
knowledge	O
of	O
complex	O
subjects	O
.	O

In	O
this	O
work	O
,	O
we	O
aim	O
to	O
address	O
the	O
challenges	O
of	O
generating	O
open	O
-	O
ended	O
questions	O
from	O
input	O
consisting	O
of	O
multiple	O
sentences	O
.	O

We	O
first	O
introduce	O
a	O
new	O
question	O
type	O
ontology	O
,	O
drawn	O
upon	O
researches	O
in	O
cognitive	O
science	O
and	O
psychology	O
(	O
Graesser	O
et	O
al	O
.	O
,	O

1992	O
)	O
,	O
to	O
capture	O
deeper	O
levels	O
of	O
cognition	O
,	O
such	O
as	O
causal	B-TaskName
reasoning	I-TaskName
and	I-TaskName
judgments	I-TaskName
.	O

Based	O
on	O
the	O
new	O
ontology	O
,	O
we	O
collect	O
and	O
annotate	O
a	O
dataset	O
of	O
4,959	O
questions	O
to	O
benefit	O
research	O
in	O
both	O
question	O
generation	O
and	O
answering	O
.	O

1	O
We	O
then	O
design	O
a	O
type	O
-	O
aware	O
framework	O
to	O
jointly	O
predict	O
question	O
focuses	O
(	O
what	O
to	O
ask	O
about	O
)	O
and	O
generate	O
questions	O
(	O
how	O
to	O
ask	O
it	O
)	O
.	O

Different	O
from	O
pipeline	O
-	O
based	O
approaches	O
(	O
e.g.	O
,	O
Sun	O
et	O
al	O
.	O
(	O

2018	O
)	O
)	O
,	O
our	O
framework	O
is	O
built	O
on	O
large	O
pre	O
-	O
trained	B-MethodName
BART	I-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
uses	O
shared	O
representations	O
to	O
jointly	O
conduct	O
question	O
focus	O
prediction	O
and	O
question	O
generation	O
while	O
learning	O
taskspecific	O
knowledge	O
.	O

It	O
is	O
further	O
augmented	O
by	O
a	O
semantic	B-MethodName
graph	I-MethodName
that	O
leverages	O
both	O
semantic	O
roles	O
and	O
dependency	O
relations	O
,	O
facilitating	O
long	O
text	O
comprehension	O
to	O
pinpoint	O
salient	O
concepts	O
.	O

Moreover	O
,	O
to	O
achieve	O
the	O
goal	O
of	O
producing	O
various	O
types	O
of	O
questions	O
from	O
the	O
same	O
input	O
,	O
we	O
investigate	O
two	O
model	O
variants	O
that	O
use	O
templates	O
to	O
improve	O
controllability	O
and	O
generation	O
diversity	O
:	O
one	O
using	O
pre	O
-	O
identified	O
exemplars	O
,	O
the	O
other	O
employing	O
generated	O
templates	O
to	O
guide	O
question	B-TaskName
writing	I-TaskName
,	O
with	O
sample	O
outputs	O
displayed	O
in	O
Figure	O
1.For	O
experiments	O
,	O
we	O
collect	O
two	O
new	O
large	O
-	O
scale	O
datasets	O
consisting	O
of	O
open	O
-	O
ended	O
questions	O
with	O
answers	O
from	O
(	O
1	O
)	O
Yahoo	B-DatasetName
Answers	I-DatasetName
2	I-DatasetName
L6	I-DatasetName
dataset	O
and	O
(	O
2	O
)	O
popular	O
question	O
-	O
asking	O
communities	O
on	O
Reddit	B-DatasetName
3	I-DatasetName
,	O
consisting	O
of	O
291	O
K	O
and	O
720	O
K	O
question	O
-	O
answer	O
pairs	O
,	O
respectively	O
.	O

Compared	O
to	O
existing	O
popular	O
QA	O
datasets	O
,	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
.	O
,	O

2016	O
)	O
)	O
,	O
questions	O
in	O
our	O
datasets	O
ask	O
about	O
complex	O
phenomena	O
and	O
perplexing	O
social	O
issues	O
that	O
seek	O
solutions	O
expressed	O
in	O
a	O
long	O
form	O
.	O

Automatic	O
metrics	O
show	O
that	O
our	O
type	B-MethodName
-	I-MethodName
aware	I-MethodName
question	I-MethodName
generation	I-MethodName
model	O
outperforms	O
competitive	O
comparisons	O
,	O
highlighting	O
the	O
effectiveness	O
of	O
semantic	B-MethodName
graph	I-MethodName
-	I-MethodName
augmented	I-MethodName
representation	I-MethodName
and	I-MethodName
joint	I-MethodName
modeling	I-MethodName
of	O
focus	O
prediction	O
and	O
question	O
generation	O
.	O

Human	O
judges	O
also	O
confirm	O
that	O
questions	O
generated	O
by	O
our	O
model	O
have	O
better	O
overall	O
quality	O
.	O

Adding	O
templates	B-MethodName
further	O
promotes	O
question	O
diversity	O
,	O
as	O
evaluated	O
by	O
both	O
automatic	O
evaluation	O
and	O
human	O
assessment	O
.	O

Question	B-TaskName
generation	I-TaskName
has	O
long	O
been	O
studied	O
to	O
reduce	O
human	O
efforts	O
in	O
constructing	O
questions	O
for	O
knowledge	O
learning	O
evaluation	O
(	O
Mitkov	O
and	O
Ha	O
,	O
2003;Brown	O
et	O
al	O
.	O
,	O

2005	O
)	O
.	O

Early	O
work	O
relies	O
on	O
syntactic	B-MethodName
transformation	I-MethodName
to	O
convert	O
declarative	O
sentences	O
to	O
questions	O
(	O
Heilman	O
and	O
Smith	O
,	O
2010;Chali	O
and	O
Hasan	O
,	O
2015	O
)	O
.	O

Recent	O
advancements	O
rely	O
on	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
to	O
generate	O
a	O
question	O
from	O
a	O
given	O
sentence	O
or	O
paragraph	O
by	O
considering	O
the	O
focus	O
,	O
type	O
,	O
and	O
general	O
-	O
specific	O
relations	O
of	O
questions	O
(	O
Sun	O
et	O
al	O
.	O
,	O

2018;Zhou	O
et	O
al	O
.	O
,	O

2019b;Krishna	O
and	O
Iyyer	O
,	O
2019	O
)	O
.	O

In	O
particular	O
,	O
question	O
likelihoods	O
and	O
rewards	O
are	O
designed	O
to	O
steer	O
them	O
toward	O
being	O
addressed	O
by	O
the	O
given	O
answers	O
(	O
Zhou	O
et	O
al	O
.	O
,	O

2019a;Zhang	O
and	O
Bansal	O
,	O
2019	O
)	O
.	O

Attempts	O
are	O
also	O
made	O
toward	O
creating	O
complex	O
questions	O
that	O
require	O
multi	B-TaskName
-	I-TaskName
hop	I-TaskName
reasoning	I-TaskName
over	O
the	O
given	O
text	O
,	O
and	O
graph	O
-	O
based	O
representations	O
have	O
been	O
an	O
enabling	O
tool	O
to	O
facilitate	O
the	O
access	O
to	O
both	O
entities	O
and	O
relations	O
(	O
Pan	O
et	O
al	O
.	O
,	O

2020;Su	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

While	O
our	O
model	O
also	O
enhances	O
the	O
input	O
with	O
a	O
semantic	B-MethodName
graph	I-MethodName
,	O
it	O
boasts	O
a	O
richer	O
representation	O
by	O
including	O
both	O
dependency	O
and	O
semantic	O
relations	O
,	O
with	O
predicted	O
question	O
focuses	O
highlighted	O
via	O
extra	B-MethodName
node	I-MethodName
embeddings	I-MethodName
.	O

Moreover	O
,	O
we	O
create	O
a	O
separate	O
layer	O
of	O
cross	O
attentions	O
that	O
is	O
dedicated	O
to	O
the	O
semantic	O
graph	O
,	O
while	O
prior	O
work	O
uses	O
the	O
same	O
set	O
of	O
attentions	O
to	O
attend	O
to	O
the	O
concatenated	O
text	O
and	O
graph	O
representations	O
.	O

Given	O
the	O
data	O
-	O
driven	O
nature	O
of	O
question	O
generation	O
and	O
answering	O
tasks	O
,	O
recent	O
studies	O
take	O
advantage	O
of	O
the	O
availability	O
of	O
large	O
-	O
scale	O
QA	O
datasets	O
,	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
inter	O
alia	O
.	O

These	O
corpora	O
mainly	O
contain	O
factoid	O
questions	O
,	O
while	O
our	O
newly	O
collected	O
datasets	O
are	O
not	O
only	O
larger	O
in	O
size	O
but	O
also	O
comprise	O
significantly	O
more	O
open	O
-	O
ended	O
questions	O
for	O
querying	O
reasons	O
and	O
procedures	O
.	O

A	O
dataset	O
closer	O
to	O
ours	O
is	O
ELI5	B-DatasetName
,	O
which	O
also	O
obtains	O
open	O
-	O
ended	O
questionanswer	O
pairs	O
from	O
Reddit	B-DatasetName
,	O
while	O
one	O
of	O
our	O
datasets	O
includes	O
more	O
Reddit	O
communities	O
and	O
thus	O
covers	O
a	O
wider	O
range	O
of	O
topics	O
.	O

Our	O
work	O
is	O
more	O
inline	O
with	O
generating	O
deeper	O
questions	O
with	O
responses	O
that	O
span	O
over	O
multiple	O
sentences	O
,	O
where	O
manually	O
constructed	O
templates	O
are	O
found	O
effective	O
(	O
Olney	O
et	O
al	O
.	O
,	O

2012	O
)	O
.	O

For	O
example	O
,	O
Labutov	O
et	O
al	O
.	O
(	O

2015	O
)	O
use	O
crowdsourcing	O
to	O
collect	O
question	O
templates	O
based	O
on	O
an	O
ontology	O
derived	O
from	O
Wikipedia	O
and	O
Freebase	O
topics	O
.	O

Different	O
from	O
the	O
topic	O
-	O
based	O
ontology	O
,	O
our	O
question	O
types	O
are	O
more	O
aligned	O
with	O
cognitive	O
levels	O
.	O

Moreover	O
,	O
our	O
templates	O
are	O
automatically	O
learned	O
from	O
training	O
data	O
.	O

Recent	O
work	O
Daumé	O
III	O
,	O
2018	O
,	O
2019	O
)	O
focuses	O
on	O
asking	O
clarification	O
questions	O
based	O
on	O
both	O
retrieval	O
and	O
generation	O
models	O
.	O

As	O
there	O
has	O
been	O
no	O
suitable	O
framework	O
for	O
diverse	O
types	O
of	O
questions	O
,	O
this	O
work	O
aims	O
to	O
fill	O
the	O
gap	O
by	O
introducing	O
type	O
-	O
aware	O
generation	O
models	O
which	O
optionally	O
leverage	O
question	O
templates	O
for	O
better	O
controllability	O
.	O

Generating	O
diverse	O
questions	O
is	O
much	O
less	O
studied	O
,	O
with	O
existing	O
approaches	O
mainly	O
focusing	O
on	B-MethodName
entity	I-MethodName
replacement	I-MethodName
(	O
Cho	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
sampling	B-MethodName
decoding	I-MethodName
(	O
Sultan	O
et	O
al	O
.	O
,	O

2020;Wang	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
post	B-MethodName
-	I-MethodName
filtering	I-MethodName
.	O

However	O
,	O
the	O
produced	O
diversity	O
is	O
driven	O
by	O
word	O
choice	O
and	O
syntax	O
variation	O
,	O
with	O
little	O
ability	O
to	O
control	O
on	O
question	O
types	O
,	O
which	O
is	O
the	O
focus	O
of	O
this	O
work.3	O
Data	O
Collection	O
and	O
Question	O
Type	O
Annotation	O
To	O
collect	O
open	O
-	O
ended	O
questions	O
,	O
we	O
resort	O
to	O
online	O
forums	O
with	O
active	O
question	O
-	O
asking	O
discussions	O
.	O

Concretely	O
,	O
we	O
gather	O
and	O
clean	O
question	O
-	O
answer	O
pairs	O
from	O
Reddit	O
and	O
Yahoo	O
Answers	O
,	O
to	O
train	O
generators	O
that	O
construct	O
questions	O
by	O
taking	O
the	O
corresponding	O
answer	O
as	O
input	O
.	O

Question	O
Type	O
Description	O
(	O
asking	O
for	O
...	O
)	O
VERIFICATION	O
the	O
truthfulness	O
of	O
an	O
event	O
or	O
a	O
concept	O
.	O

DISJUNCTIVE	O
the	O
true	O
one	O
given	O
multiple	O
events	O
or	O
concepts	O
,	O
where	O
comparison	O
among	O
options	O
is	O
not	O
needed	O
.	O

CONCEPT	O
a	O
definition	O
of	O
an	O
event	O
or	O
a	O
concept	O
.	O

EXTENT	O
the	O
extent	O
or	O
quantity	O
of	O
an	O
event	O
or	O
a	O
concept	O
.	O

EXAMPLE	O
example(s	O
)	O
or	O
instance(s	O
)	O
of	O
an	O
event	O
or	O
a	O
concept	O
.	O

COMPARISON	O
comparison	O
among	O
multiple	O
events	O
or	O
concepts	O
.	O

CAUSE	O
the	O
cause	O
or	O
reason	O
for	O
an	O
event	O
or	O
a	O
concept	O
.	O

CONSEQUENCE	O
the	O
consequences	O
or	O
results	O
of	O
an	O
event	O
.	O

PROCEDURAL	O
the	O
procedures	O
,	O
tools	O
,	O
or	O
methods	O
by	O
which	O
a	O
certain	O
outcome	O
is	O
achieved	O
.	O

JUDGMENTAL	O
the	O
opinions	O
of	O
the	O
answerer	O
's	O
own	O
.	O

We	O
choose	O
five	O
popular	O
Reddit	O
communities	O
:	O
r	O
/	O
AskHistorians	O
,	O
r	O
/	O
Ask	O
Politics	O
,	O
r	O
/	O
askscience	O
,	O
r	O
/	O
explainlikeimfive	O
,	O
and	O
r	O
/	O
AskReddit	O
,	O
where	O
open	O
-	O
ended	O
questions	O
are	O
actively	O
asked	O
.	O

The	O
original	O
posts	O
(	O
OPs	O
)	O
are	O
extracted	O
,	O
with	O
their	O
titles	O
becoming	O
questions	O
.	O

We	O
also	O
keep	O
the	O
best	O
answer	O
with	O
the	O
highest	O
karma	O
(	O
i.e.	O
,	O
upvotes	O
minus	O
downvotes	O
)	O
if	O
it	O
is	O
greater	O
than	O
1	O
.	O

A	O
second	O
dataset	O
with	O
question	O
-	O
answer	O
pairs	O
is	O
collected	O
from	O
the	O
Yahoo	B-DatasetName
Answers	I-DatasetName
L6	I-DatasetName
corpus	O
4	O
,	O
which	O
covers	O
a	O
broader	O
range	O
of	O
topics	O
than	O
the	O
Reddit	B-DatasetName
data	O
.	O

For	O
each	O
question	O
,	O
the	O
best	O
answer	O
is	O
rated	O
by	O
the	O
user	O
who	O
raises	O
the	O
question	O
.	O

Preprocessing	O
.	O

To	O
ensure	O
both	O
questions	O
and	O
answers	O
are	O
well	O
-	O
formed	O
,	O
human	O
inspection	O
is	O
conducted	O
in	O
multiple	O
iterations	O
to	O
design	O
rules	O
to	O
filter	O
out	O
improper	O
samples	O
.	O

For	O
instance	O
,	O
we	O
discard	O
samples	O
whose	O
answers	O
have	O
less	O
than	O
15	O
content	O
words	O
to	O
avoid	O
the	O
inclusion	O
of	O
factoid	O
question	O
.	O

More	O
details	O
are	O
provided	O
in	O
Table	O
6	O
in	O
Appendix	O
A.	O
Ultimately	O
,	O
719,988	O
question	O
-	O
answer	O
pairs	O
are	O
kept	O
for	O
Reddit	O
,	O
and	O
290,611	O
for	O
Yahoo	O
.	O

Each	O
dataset	O
is	O
then	O
divided	O
into	O
train	O
,	O
validation	O
and	O
test	O
sets	O
with	O
a	O
90%/5%/5	B-HyperparameterValue
%	I-HyperparameterValue
split	O
.	O

The	O
average	O
lengths	O
of	O
questions	O
and	O
answers	O
are	O
14.5	O
and	O
117.8	O
for	O
Reddit	O
,	O
and	O
12.2	O
and	O
123.6	O
for	O
Yahoo	O
.	O

Our	O
question	O
type	O
ontology	O
is	O
adopted	O
and	O
modified	O
from	O
Olney	O
et	O
al	O
.	O
(	O

2012	O
)	O
,	O
where	O
18	O
categories	O
are	O
originally	O
proposed	O
for	O
knowledge	O
learning	O
as	O
-	O
sessment	O
.	O

We	O
recruited	O
6	O
native	O
English	O
speakers	O
for	O
three	O
rounds	O
of	O
question	O
type	O
annotation	O
.	O

Based	O
on	O
the	O
annotators	O
'	O
feedback	O
after	O
each	O
round	O
,	O
we	O
refine	O
the	O
definitions	O
,	O
merge	O
ambiguous	O
types	O
,	O
and	O
delete	O
inapplicable	O
categories	O
.	O

For	O
example	O
,	O
an	O
initial	O
EXPECTATION	O
type	O
is	O
merged	O
into	O
CAUSE	O
due	O
to	O
their	O
similarities	O
in	O
seeking	O
causality	O
.	O

Finally	O
,	O
10	O
types	O
are	O
preserved	O
(	O
Table	O
1	O
)	O
.	O

As	O
can	O
be	O
seen	O
,	O
our	O
ontology	O
is	O
designed	O
to	O
better	O
capture	O
the	O
nature	O
of	O
questions	O
than	O
question	O
words	O
.	O

Annotating	O
Questions	O
with	O
Types	O
.	O

After	O
the	O
annotation	O
guideline	O
is	O
finalized	O
,	O
we	O
ask	O
the	O
same	O
set	O
of	O
annotators	O
to	O
label	O
5,000	O
(	O
2	O
×	O
2,500	O
)	O
randomly	O
sampled	O
questions	O
from	O
both	O
Reddit	O
and	O
Yahoo	O
's	O
training	O
sets	O
.	O

Each	O
question	O
is	O
labeled	O
by	O
two	O
annotators	O
,	O
with	O
disagreements	O
resolved	O
through	O
discussions	O
.	O

After	O
removing	O
samples	O
without	O
consensus	O
,	O
the	O
final	O
dataset	O
consists	O
of	O
4	O
,	O
959	O
questions	O
.	O

EXAMPLE	O
questions	O
are	O
most	O
prevalent	O
,	O
comprising	O
23.4	O
%	O
of	O
samples	O
,	O
while	O
only	O
2.6	O
%	O
are	O
CONSEQUENCE	O
questions	O
.	O

A	O
Krippendorff	O
's	O
α	O
of	O
0.67	O
is	O
obtained	O
for	O
all	O
samples	O
,	O
indicating	O
a	O
reasonable	O
agreement	O
level	O
.	O

The	O
annotation	O
guideline	O
and	O
examples	O
for	O
each	O
question	O
type	O
are	O
shown	O
in	O
Table	O
12	O
in	O
Appendix	O
A.Training	O
Question	O
Type	O
Classifiers	O
.	O

Since	O
our	O
type	O
-	O
aware	O
question	O
generation	O
model	O
requires	O
a	O
specified	O
type	O
as	O
input	O
,	O
here	O
we	O
describe	O
how	O
to	O
build	O
two	O
question	O
type	O
classifiers	O
:	O
(	O
1	O
)	O
γ	O
q	O
,	O
that	O
labels	O
a	O
type	O
by	O
reading	O
the	O
question	O
and	O
is	O
used	O
to	O
provide	O
question	O
type	O
labels	O
during	O
training	O
;	O
(	O
2	O
)	O
γ	O
a	O
,	O
that	O
predicts	O
a	O
type	O
for	O
use	O
by	O
taking	O
the	O
answer	O
as	O
input	O
and	O
is	O
used	O
during	O
test	O
.	O

Both	O
classifiers	O
are	O
based	O
on	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
where	O
a	O
prediction	O
layer	O
is	O
built	O
on	O
top	O
of	O
the	O
contextual	O
representation	O
of	O
the	O
[	O
BOS	O
]	O
token	O
to	O
output	O
question	O
type	O
probabilities	O
.	O

γ	O
q	O
achieves	O
a	O
macro	B-MetricName
F1	I-MetricName
score	O
of	O
0.80	B-MetricValue
on	O
a	O
reserved	O
test	O
set	O
,	O
with	O
data	O
splits	O
detailed	O
in	O
Appendix	O
B.	O
To	O
train	O
γ	O
a	O
,	O
in	O
addition	O
to	O
the	O
annotated	O
questions	O
,	O
we	O
run	O
γ	O
q	O
on	O
unlabeled	O
questions	O
in	O
Reddit	O
and	O
Yahoo	O
and	O
include	O
samples	O
whose	O
type	O
prediction	B-MetricName
confidence	I-MetricName
score	I-MetricName
is	O
above	O
0.9	B-MetricValue
.	O

We	O
train	O
one	O
γ	O
a	O
for	O
each	O
dataset	O
.	O

γ	O
a	O
obtains	O
macro	B-MetricName
F1	I-MetricName
scores	O
of	O
0.48	B-MetricValue
and	O
0.46	B-MetricValue
on	O
the	O
same	O
reserved	O
test	O
set	O
over	O
all	O
types	O
after	O
training	O
on	O
Yahoo	O
and	O
Reddit	O
,	O
respectively	O
.	O

After	O
running	O
γ	O
q	O
on	O
both	O
datasets	O
,	O
we	O
find	O
that	O
Reddit	O
has	O
significantly	O
more	O
EXAMPLE	O
questions	O
(	O
43.8	O
%	O
of	O
all	O
samples	O
)	O
.	O

Yahoo	B-DatasetName
dataset	O
is	O
more	O
balanced	O
,	O
with	O
PROCEDURAL	O
questions	O
being	O
the	O
most	O
frequent	O
type	O
(	O
19.9	O
%	O
of	O
all	O
samples	O
)	O
.	O

Distri-	O
butions	O
of	O
question	O
types	O
for	O
the	O
two	O
datasets	O
are	O
listed	O
in	O
Table	O
8	O
in	O
Appendix	O
B.4	O
Type	B-TaskName
-	I-TaskName
aware	I-TaskName
Open	I-TaskName
-	I-TaskName
ended	I-TaskName
Question	I-TaskName
GenerationIn	I-TaskName
this	O
section	O
,	O
we	O
present	O
our	O
type	B-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
.	O

As	O
shown	O
in	O
Figure	O
2	O
,	O
our	O
model	O
takes	O
in	O
a	O
multi	O
-	O
sentence	O
text	O
and	O
a	O
predicted	O
question	O
type	O
.	O

Built	O
on	O
shared	O
input	O
representations	O
,	O
it	O
first	O
detects	O
question	O
focuses	O
from	O
a	O
semantic	O
graph	O
,	O
and	O
then	O
generates	O
the	O
question	O
(	O
§	O
4.1).We	O
also	O
propose	O
two	O
model	O
variants	O
that	O
consider	O
automatically	O
extracted	O
template	O
exemplars	O
or	O
generated	O
templates	O
to	O
achieve	O
controllability	O
(	O
§	O
4.2	O
)	O
,	O
enabling	O
the	O
generation	O
of	O
diverse	O
questions	O
.	O

Our	O
generator	O
is	O
built	O
on	O
top	O
of	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

To	O
facilitate	O
the	O
detection	O
of	O
salient	O
content	O
(	O
i.e.	O
,	O
focuses	O
)	O
to	O
raise	O
questions	O
,	O
we	O
first	O
augment	O
the	O
encoder	O
with	O
a	O
semantic	O
graph	O
that	O
consists	O
of	O
both	O
dependency	O
relations	O
and	O
semantic	O
roles	O
,	O
capturing	O
semantic	O
relations	O
over	O
different	O
scopes	O
with	O
varying	O
granularities	O
.	O

Question	O
focuses	O
are	O
first	O
detected	O
based	O
on	O
the	O
semantic	B-MethodName
graph	I-MethodName
,	O
which	O
then	O
guide	O
question	O
generation	O
via	O
cross	O
-	O
attentions	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

Although	O
the	O
joint	O
modeling	O
of	O
focus	B-TaskName
prediction	I-TaskName
and	I-TaskName
question	I-TaskName
generation	I-TaskName
has	O
been	O
studied	O
before	O
,	O
our	O
design	O
differs	O
by	O
using	O
shared	O
representations	O
consisting	O
of	O
the	O
input	O
text	O
and	O
semantic	O
graph	O
,	O
and	O
the	O
prediction	O
of	O
focuses	O
are	O
included	O
through	O
gating	O
mechanisms	O
,	O
whereas	O
previous	O
work	O
,	O
e.g.	O
Pan	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
simply	O
employs	O
multi	O
-	O
task	O
learning	O
.	O

Below	O
,	O
we	O
first	O
describe	O
constructing	O
the	O
semantic	B-MethodName
graph	I-MethodName
-	I-MethodName
augmented	I-MethodName
encoder	I-MethodName
,	O
followed	O
by	O
the	O
joint	O
modeling	O
of	O
two	O
tasks	O
.	O

Improving	O
Long	O
Text	O
Comprehension	O
with	O
Semantic	B-MethodName
Graph	I-MethodName
.	O

To	O
construct	O
the	O
semantic	B-MethodName
graph	I-MethodName
,	O
for	O
each	O
sentence	O
,	O
we	O
start	O
with	O
obtaining	O
its	O
dependency	O
tree	O
using	O
Stanford	B-MethodName
CoreNLP	I-MethodName
(	O
Manning	O
et	O
al	O
.	O
,	O

2014	O
)	O
.	O

To	O
better	O
highlight	O
core	O
concepts	O
,	O
we	O
discard	O
less	O
important	O
relations	O
,	O
e.g.	O
,	O
auxiliaries	O
.	O

The	O
full	O
list	O
is	O
included	O
in	O
Appendix	O
C.	O
Since	O
our	O
goal	O
is	O
to	O
detect	O
central	O
concepts	O
that	O
are	O
well	O
connected	O
with	O
many	O
other	O
words	O
,	O
we	O
can	O
remove	O
relations	O
on	O
the	O
edges	O
to	O
minimize	O
the	O
number	O
of	O
parameters	O
to	O
learn	O
.	O

Moreover	O
,	O
as	O
semantic	O
roles	O
can	O
indicate	O
main	O
entities	O
(	O
Mannem	O
et	O
al	O
.	O
,	O

2010	O
)	O
,	O
we	O
extract	O
semantic	O
roles	O
and	O
their	O
relations	O
with	B-MethodName
AllenNLP	I-MethodName
(	O
Shi	O
and	O
Lin	O
,	O
2019	O
)	O
.	O

To	O
merge	O
the	O
two	O
sources	O
of	O
information	O
,	O
we	O
add	O
an	O
edge	O
in	O
the	O
dependency	O
tree	O
to	O
connect	O
the	O
head	O
word	O
of	O
the	O
predicate	O
and	O
the	O
head	O
word	O
of	O
each	O
semantic	O
role	O
.	O

To	O
build	O
a	O
connected	O
graph	O
from	O
the	O
multi	O
-	O
sentence	O
input	O
,	O
we	O
add	O
an	O
edge	O
between	O
each	O
sentence	O
's	O
last	O
token	O
and	O
the	O
next	O
sentence	O
's	O
first	O
token	O
.	O

Finally	O
,	O
we	O
merge	O
nodes	O
with	O
the	O
same	O
surface	O
forms	O
or	O
with	O
corefered	O
mentions	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
time	O
that	O
both	O
dependency	O
and	O
semantic	O
relations	O
are	O
encoded	O
in	O
the	O
same	O
graph	O
for	O
question	O
generation	O
,	O
and	O
with	O
enhanced	O
connectivity	O
of	O
the	O
constructed	O
graph	O
,	O
our	O
design	O
can	O
better	O
signal	O
content	O
salience	O
.	O

Joint	B-MethodName
Modeling	I-MethodName
with	O
Cross	O
-	O
attentions	O
.	O

Given	O
a	O
predicted	O
question	O
type	O
t	O
and	O
a	O
multi	O
-	O
sentence	O
textx	O
=	O
{	O
x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
n	O
}	O
,	O
the	B-MethodName
BART	I-MethodName
encoder	O
builds	O
the	O
contextual	O
representation	O
H	O
=	O
{	O
h	O
0	O
,	O
h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
n	O
}	O
at	O
the	O
last	O
layer	O
,	O
where	O
h	O
0	O
is	O
for	O
t.	O
To	O
encode	O
the	O
semantic	O
graph	O
,	O
we	O
initialize	O
the	O
node	O
representation	O
for	O
node	O
v	O
i	O
by	O
taking	O
the	O
average	O
contextual	O
representations	O
of	O
its	O
tokens	O
and	O
appending	O
four	O
bits	O
encoding	O
the	O
number	O
of	O
nodes	O
(	O
capped	O
at	O
10	O
)	O
that	O
are	O
merged	O
into	O
v	O
i	O
,	O
to	O
add	O
frequency	O
information	O
.	O

This	O
step	O
yields	O
new	O
node	O
representations	O
v	O
(	O
0	O
)	O
i	O
.	O

We	O
then	O
apply	O
graph	B-MethodName
attention	I-MethodName
networks	I-MethodName
(	O
GATs	O
)	O
(	O
Veličković	O
et	O
al	O
.	O
,	O

2018	O
)	O
of	O
L	O
layers	O
to	O
update	O
the	O
representations	O
as	O
follows	O
:	O
v	O
(	O
l	O
)	O
i	O
=	O
j∈Ni	O
a	O
i	O
,	O
j	O
W	O
(	O
l	O
)	O
v	O
(	O
l−1	O
)	O
j	O
(	O
1)where	O
W	O
(	O
l	O
)	O
is	O
a	O
learnable	O
parameter	O
for	O
the	O
l	O
-	O
th	O
layer	O
,	O
and	O
N	O
i	O
denotes	O
the	O
neighbors	O
of	O
v	O
i	O
.	O

The	O
attention	O
score	O
a	O
i	O
,	O
j	O
is	O
calculated	O
as	O
in	O
GATs	B-MethodName
.	O

We	O
use	O
L	O
=	O
2	O
for	O
experiments	O
.	O

To	O
predict	O
focuses	O
,	O
the	O
final	O
node	O
representation	O
v	O
(	O
L	O
)	O
i	O
is	O
fed	O
into	O
the	O
following	O
feedforward	B-MethodName
network	I-MethodName
,	O
yielding	O
the	O
probability	O
of	O
v	O
i	O
being	O
a	O
focus	O
as	O
:p	O
f	O
ocus	O
(	O
v	O
i	O
=	O
1	O
)	O
=	O
σ(W	O
1	O
tanh(W	O
2	O
v	O
(	O
L	O
)	O
i	O
)	O
)	O
(	O
2)where	O
W	O
1	O
and	O
W	O
2	O
are	O
learnable	O
parameters	O
.	O

Bias	O
terms	O
are	O
omitted	O
for	O
simplicity	O
.	O

We	O
construct	O
ground	O
-	O
truth	O
labels	O
by	O
treating	O
a	O
node	O
as	O
a	O
focus	O
if	O
it	O
contains	O
words	O
used	O
in	O
the	O
question	O
.	O

To	O
generate	O
the	O
question	O
,	O
we	O
use	O
the	O
gating	O
mechanism	O
to	O
inform	O
the	O
focus	O
prediction	O
results	O
,	O
where	O
new	O
node	O
representations	O
after	O
being	O
weighted	O
by	O
the	O
focus	O
probability	O
are	O
:	O
v	O
(	O
L	O
)	O
i	O
=	O
g	O
i	O
v	O
(	O
L	O
)	O
i	O
g	O
i	O
=	O
p	O
f	O
ocus	O
(	O
v	O
i	O
=	O
1)(3)Our	O
model	O
benefits	O
from	O
both	O
large	O
pre	O
-	O
training	O
and	O
hybrid	O
semantic	O
graphs	O
by	O
adding	O
a	O
separate	O
cross	O
attention	O
for	O
node	O
presentations	O
in	O
each	O
BART	B-MethodName
decoder	O
layer	O
.	O

We	O
then	O
design	O
separate	O
cross	O
attentions	O
to	O
attend	O
(	O
1	O
)	O
the	O
output	O
of	O
the	O
BART	B-MethodName
encoder	O
,	O
yielding	O
z	O
e	O
,	O
and	O
(	O
2	O
)	O
the	O
node	O
representations	O
V	O
(	O
L	O
)	O
,	O
producing	O
z	O
v	O
,	O
which	O
are	O
formulated	O
as	O
:	O
z	O
e	O
=	O
LN(z	O
s	O
+	O
Attn(z	O
s	O
,	O
H	O
)	O
)	O
(	O
4)z	O
v	O
=	O
LN(z	O
e	O
+	O
Attn(z	O
e	O
,	O
V	O
(	O
L	O
)	O
)	O
)	O
(	O
5	O
)	O
z	O
=	O
LN(z	O
v	O
+	O
FFN(z	O
v	O
)	O
)	O
(	O
6)where	O
z	O
s	O
denotes	O
the	O
output	O
of	O
self	O
attentions	O
for	O
the	O
current	O
layer	O
,	O
and	O
z	O
is	O
the	O
output	O
for	O
the	O
layer	O
.	O

Attn(•	O
,	O
•	O
)	O
denotes	O
the	O
multi	O
-	O
head	O
attention	O
operation	O
as	O
in	O
Vaswani	O
et	O
al	O
.	O
(	O

2017	O
)	O
,	O
FFN(•	O
)	O
a	O
feedforward	O
layer	O
,	O
and	O
LN(•	O
)	O
is	O
layer	O
normalization	O
.	O

Our	O
final	O
training	O
objective	O
accounts	O
for	O
both	O
focus	O
prediction	O
and	O
question	O
generation	O
objectives	O
with	O
equal	O
weights	O
.	O

An	O
important	O
goal	O
of	O
this	O
work	O
is	O
to	O
enable	O
the	O
generation	O
of	O
questions	O
of	O
diverse	O
types	O
.	O

However	O
,	O
simply	O
adding	O
question	O
type	O
as	O
input	O
is	O
insufficient	O
(	O
discussed	O
in	O
§	O
5	O
)	O
.	O

We	O
thus	O
propose	O
to	O
leverage	O
question	O
templates	O
to	O
gain	O
stronger	O
controllability	O
.	O

Below	O
we	O
first	O
present	O
how	O
to	O
automatically	O
extract	O
templates	O
from	O
the	O
training	O
set	O
,	O
and	O
then	O
introduce	O
two	O
model	O
variants	O
that	O
are	O
built	O
on	O
the	O
JOINTGEN	B-MethodName
framework	O
:	O
EXPLGEN	B-MethodName
uses	O
exemplar	O
templates	O
to	O
guide	O
the	O
model	O
to	O
generate	O
questions	O
of	O
selected	O
types	O
,	O
and	O
TPLGEN	B-MethodName
adds	O
an	O
extra	O
step	O
to	O
first	O
generate	O
type	O
-	O
specific	O
templates	O
.	O

Template	O
Extraction	O
.	O

While	O
collecting	O
templates	O
specific	O
to	O
a	O
given	O
type	O
,	O
we	O
need	O
to	O
ensure	O
they	O
remain	O
topic	O
-	O
independent	O
to	O
be	O
generalizable	O
to	O
different	O
domains	O
.	O

To	O
this	O
end	O
,	O
we	O
replace	O
a	O
word	O
in	O
the	O
question	O
with	O
a	O
template	O
token	O
that	O
indicates	O
its	O
syntax	O
function	O
,	O
e.g.	O
,	O
[	O
V	O
]	O
for	O
a	O
verb	O
,	O
if	O
it	O
appears	O
in	O
the	O
answer	O
after	O
lemmatization	O
.	O

We	O
further	O
consider	O
topically	O
related	O
words	O
in	O
the	O
questions	O
,	O
by	O
calculating	O
word	O
-	O
level	O
semantic	O
similarities	O
based	O
on	O
Numberbatch	B-MethodName
word	I-MethodName
embeddings	I-MethodName
(	O
Speer	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
which	O
are	O
found	O
to	O
perform	O
better	O
on	O
our	O
datasets	O
than	O
other	O
embeddings	O
.	O

Concretely	O
,	O
for	O
each	O
word	O
in	O
the	O
answer	O
,	O
we	O
replace	O
the	O
most	O
similar	O
word	O
in	O
the	O
question	O
with	O
the	O
template	O
token	O
.	O

This	O
process	O
is	O
repeated	O
until	O
80	O
%	O
of	O
content	O
words	O
in	O
questions	O
are	O
replaced	O
.	O

Finally	O
,	O
for	O
each	O
noun	O
phrase	O
,	O
adjective	O
phrase	O
,	O
and	O
adverb	O
phrase	O
,	O
if	O
its	O
head	O
word	O
has	O
been	O
replaced	O
,	O
the	O
whole	O
phrase	O
is	O
transformed	O
into	O
a	O
phrase	O
type	O
token	O
.	O

For	O
instance	O
,	O
a	O
question	O
"	O
What	O
are	O
the	O
differences	O
between	O
global	O
warming	O
and	O
climate	O
change	O
?	O
"	O
becomes	O
"	O
What	O
are	O
the	O
differences	O
between	O
[	O
NP	O
]	O
and	O
[	O
NP]?"Exemplars	B-MethodName
for	I-MethodName
Guidance	I-MethodName
(	I-MethodName
EXPLGEN	I-MethodName
)	I-MethodName
.	O

Our	O
first	O
model	O
variant	O
considers	O
adding	O
a	O
template	O
exemplar	O
for	O
the	O
given	O
type	O
as	O
additional	O
input	O
,	O
which	O
provide	O
more	O
specific	O
information	O
to	O
control	O
the	O
type	O
of	O
generated	O
questions	O
.	O

Figure	O
2	O
shows	O
one	O
such	O
example	O
.	O

To	O
identify	O
exemplars	O
,	O
we	O
use	O
templates	O
with	O
frequencies	O
above	O
20	O
on	O
Yahoo	O
and	O
50	O
on	O
Reddit	O
.	O

We	O
then	O
manually	O
inspect	O
these	O
templates	O
and	O
remove	O
the	O
ones	O
with	O
topic	O
-	O
specific	O
words	O
,	O
resulting	O
in	O
66	O
exemplars	O
for	O
all	O
types	O
.	O

They	O
are	O
listed	O
in	O
Table	O
10	O
in	O
Appendix	O
D.During	O
training	O
,	O
we	O
choose	O
the	O
exemplar	O
that	O
has	O
the	O
lowest	O
edit	O
distance	O
with	O
the	O
question	O
,	O
which	O
is	O
also	O
used	O
for	O
training	O
an	O
exemplar	O
selector	O
based	O
on	O
RoBERTa	B-MethodName
.	O

During	O
testing	O
,	O
the	O
exemplar	O
with	O
the	O
highest	O
selector	O
score	O
is	O
used	O
.	O

The	O
accuracy	O
of	O
the	O
exemplar	O
selector	O
for	O
each	O
question	O
type	O
on	O
the	O
test	O
set	O
is	O
reported	O
in	O
Table	O
11	O
in	O
Appendix	O
D.	O
We	O
further	O
propose	O
another	O
model	O
variant	O
where	O
we	O
generate	O
a	O
new	O
template	O
and	O
feed	O
it	O
(	O
instead	O
of	O
an	O
exemplar	O
template	O
as	O
in	O
EXPLGEN	O
)	O
as	O
part	O
of	O
the	O
question	O
generation	O
input	O
.	O

Specifically	O
,	O
we	O
reuse	O
EXPLGEN	B-MethodName
to	O
learn	O
to	O
generate	O
a	O
target	O
template	O
,	O
as	O
derived	O
from	O
the	O
template	O
extraction	O
procedure	O
.	O

During	O
question	O
realization	O
,	O
TPLGEN	B-MethodName
uses	O
a	O
BART	B-MethodName
-	I-MethodName
based	I-MethodName
generator	I-MethodName
that	O
takes	O
as	O
input	O
the	O
question	O
type	O
,	O
the	O
input	O
text	O
,	O
the	O
generated	O
template	O
,	O
and	O
the	O
words	O
that	O
are	O
predicted	O
as	O
focuses	O
.	O

We	O
use	O
separate	O
cross	O
attentions	O
to	O
attend	O
the	O
representations	O
of	O
the	O
focused	O
words	O
,	O
similar	O
to	O
how	O
node	O
representations	O
are	O
attended	O
in	O
JOINTGEN.We	B-MethodName
recognize	O
that	O
having	O
separate	O
stages	O
of	O
exemplar	O
selection	O
and	O
template	O
generation	O
introduces	O
extra	O
model	O
training	O
cost	O
and	O
potential	O
errors	O
in	O
the	O
pipeline	O
.	O

This	O
work	O
,	O
however	O
,	O
focuses	O
on	O
improving	O
the	O
controllability	O
as	O
well	O
as	O
diversity	O
of	O
question	O
generation	O
,	O
and	O
we	O
will	O
leave	O
the	O
building	O
of	O
more	O
efficient	O
models	O
in	O
the	O
future	O
work	O
.	O

Comparisons	O
and	O
Metrics	O
.	O

We	O
compare	O
with	O
DEEPQG	B-MethodName
(	O
Pan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
a	O
model	O
that	O
uses	O
dependency	O
graphs	O
for	O
multi	O
-	O
hop	O
question	O
generation	O
.	O

We	O
also	O
compare	O
with	O
BART	O
models	O
that	O
are	O
finetuned	O
on	O
the	O
same	O
datasets	O
as	O
in	O
our	O
models	O
,	O
by	O
using	O
inputs	O
of	O
(	O
1	O
)	O
the	O
answer	O
(	B-MethodName
BART	I-MethodName
)	O
,	O
(	O
2	O
)	O
the	O
answer	O
and	O
a	O
predicted	O
question	O
word	O
(	O
BART+QWORD	B-MethodName
)	O
,	O
and	O
(	O
3	O
)	O
the	O
answer	O
and	O
a	O
predicted	O
question	O
type	O
(	O
BART+QTYPE	B-MethodName
)	O
.	O

For	O
BART+QWORD	B-MethodName
,	O
the	O
question	O
word	O
is	O
predicted	O
by	O
a	O
RoBERTa	B-MethodName
classifier	O
that	O
considers	O
the	O
answer	O
and	O
is	O
trained	O
on	O
our	O
training	O
sets	O
.	O

We	O
follow	O
and	O
use	O
9	O
categories	O
of	O
question	O
words	O
.	O

For	O
both	O
our	O
models	O
and	O
BART+QTYPE	B-MethodName
,	O
the	O
most	O
confident	O
type	O
predicted	O
by	O
the	O
classifier	O
γ	O
a	O
(	O
described	O
in	O
§	O
3.2	O
)	O
,	O
which	O
reads	O
in	O
the	O
answer	O
,	O
is	O
used	O
as	O
input	O
.	O

To	O
test	O
the	O
efficacy	O
of	O
semantic	B-MethodName
graphs	I-MethodName
,	O
we	O
further	O
compare	O
with	O
a	O
variant	O
of	O
JOINTGEN	B-MethodName
that	O
only	O
uses	O
the	O
flat	O
Transformer	B-MethodName
for	O
focus	O
prediction	O
and	O
question	O
generation	O
,	O
denoted	O
as	O
JOINTGEN	B-MethodName
w/o	O
graph	O
.	O

We	O
evaluate	O
the	O
generated	O
questions	O
with	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O

2002	O
)	O
,	O
METEOR	B-MetricName
(	O
Lavie	O
and	O
Agarwal	O
,	O
2007	O
)	O
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
having	O
structured	O
representation	O
is	O
useful	O
for	O
focus	O
detection	O
and	O
the	O
final	O
question	O
generation	O
task	O
.	O

We	O
also	O
observe	O
a	O
huge	O
performance	O
gap	O
between	O
DEEPQG	B-MethodName
and	O
systems	O
based	O
on	O
BART	B-MethodName
,	O
signifying	O
the	O
importance	O
of	O
leveraging	O
pre	O
-	O
trained	O
models	O
for	O
open	O
-	O
ended	O
question	O
generation	O
.	O

Meanwhile	O
,	O
adding	O
question	O
types	O
helps	O
BART	B-MethodName
generate	O
more	O
relevant	O
questions	O
than	O
using	O
question	O
words	O
,	O
indicating	O
the	O
value	O
of	O
our	O
new	O
question	O
type	O
ontology	O
.	O

Notably	O
,	O
our	O
template	O
-	O
based	O
generators	O
,	O
EX	B-MethodName
-	I-MethodName
PLGEN	I-MethodName
and	O
TPLGEN	B-MethodName
,	O
which	O
are	O
trained	O
to	O
comply	O
with	O
the	O
given	O
templates	O
,	O
still	O
produce	O
comparable	O
scores	O
.	O

This	O
highlights	O
the	O
possibility	O
to	O
control	O
the	O
generated	O
questions	O
'	O
types	O
and	O
syntax	O
as	O
demonstrated	O
by	O
the	O
templates	O
,	O
without	O
performance	O
loss	O
.	O

Question	O
Diversity	O
Evaluation	O
.	O

Next	O
,	O
we	O
exam-	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
#	O
of	O
Given	O
Types	O
ine	O
the	O
controllability	O
of	O
models	O
by	O
specifying	O
different	O
question	O
types	O
as	O
input	O
.	O

The	O
top	O
9	O
confident	O
types	O
6	O
predicted	O
by	O
our	O
type	O
predictor	O
γ	O
a	O
are	O
used	O
as	O
input	O
to	O
our	O
models	O
,	O
producing	O
9	O
questions	O
for	O
evaluation	O
.	O

For	B-MethodName
BART	I-MethodName
,	O
we	O
use	B-HyperparameterName
nucleus	I-HyperparameterName
sampling	I-HyperparameterName
(	O
Holtzman	O
et	O
al	O
.	O
,	O

2020	O
)	O
with	B-HyperparameterName
k	I-HyperparameterName
=	O
10	B-HyperparameterValue
and	O
p	B-HyperparameterName
=	O
0.7	B-HyperparameterValue
to	O
sample	O
diverse	O
questions	O
.	O

To	O
evaluate	O
,	O
we	O
first	O
calculate	O
the	O
question	B-MetricName
type	I-MetricName
accuracy	I-MetricName
by	O
comparing	O
whether	O
the	O
types	O
of	O
the	O
generated	O
questions	O
match	O
the	O
specified	O
ones	O
,	O
with	O
types	O
labeled	O
by	O
our	O
classifier	O
γ	O
q	O
(	O
§	O
3.2	O
)	O
.	O

We	O
then	O
report	O
the	O
average	O
numbers	O
of	O
unique	O
question	O
types	O
in	O
the	O
9	O
generated	O
questions	O
per	O
sample	O
,	O
with	O
higher	O
number	O
indicating	O
better	O
controllability	O
.	O

Finally	O
,	O
we	O
consider	O
pairwise	B-MetricName
BLEU-4	I-MetricName
(	O
Cho	O
et	O
al	O
.	O
,	O

2019	O
)	O
by	O
computing	O
the	O
BLEU-4	B-MetricName
between	O
pairwise	O
generated	O
questions	O
per	O
sample	O
,	O
where	O
lower	O
values	O
suggest	O
higher	O
content	O
diversity	O
.	O

First	O
,	O
our	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
can	O
generate	O
questions	O
with	O
diverse	O
types	O
and	O
content	O
,	O
as	O
shown	O
by	O
the	O
significantly	O
higher	O
numbers	O
of	O
unique	O
types	O
than	O
all	O
comparisons	O
and	O
lower	O
pairwise	O
BLEU	B-MetricName
scores	O
than	O
comparisons	O
except	O
for	O
BART	B-MethodName
with	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
in	O
Table	O
3	O
.	O

This	O
implies	O
stronger	O
type	O
control	O
by	O
template	O
-	O
based	O
generators	O
,	O
compared	O
to	O
BART+QTYPE	B-MethodName
and	O
JOINTGEN	B-MethodName
which	O
only	O
use	O
the	O
question	O
type	O
token	O
as	O
input	O
.	O

Results	O
on	O
numbers	O
of	O
unique	O
types	O
by	O
varying	O
numbers	O
of	O
question	O
types	O
specified	O
in	O
the	O
input	O
are	O
displayed	O
in	O
Figure	O
3	O
,	O
where	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
maintain	O
steady	O
controllability	O
.	O

Second	O
,	O
our	O
question	O
type	O
ontology	O
provides	O
a	O
new	O
perspective	O
for	O
question	O
diversity	O
evaluation	O
.	O

Among	O
the	O
comparisons	O
,	O
although	O
BART	B-MethodName
with	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
and	O
BART+QWORD	B-MethodName
both	O
have	O
low	O
pairwise	B-MetricName
BLEU	I-MetricName
,	O
the	O
types	O
of	O
questions	O
they	O
can	O
generate	O
are	O
limited	O
.	O

Question	O
Diversity	O
.	O

We	O
hire	O
three	O
annotators	O
who	O
have	O
participated	O
in	O
our	O
question	O
type	O
annotation	O
study	O
to	O
evaluate	O
80	O
groups	O
of	O
questions	O
generated	O
by	O
four	O
selected	O
models	O
on	O
each	O
dataset	O
.	O

For	O
each	O
group	O
,	O
we	O
randomly	O
sample	O
an	O
answer	O
and	O
indicate	O
three	O
most	O
probably	O
question	O
types	O
to	O
each	O
model	O
,	O
to	O
generate	O
three	O
corresponding	O
questions	O
.	O

For	O
each	O
sample	O
,	O
the	O
annotators	O
are	O
asked	O
to	O
rank	O
the	O
four	O
models	O
from	O
1	O
(	O
highest	O
)	O
to	O
4	O
(	O
lowest	O
)	O
on	O
three	O
aspects	O
of	O
diversities	O
:	O
type	O
-	O
whether	O
the	O
three	O
generated	O
questions	O
have	O
different	O
types	O
,	O
syntax	O
-	O
whether	O
they	O
use	O
different	O
syntax	O
,	O
and	O
answer	O
content	O
-	O
whether	O
the	O
three	O
questions	O
need	O
to	O
be	O
addressed	O
with	O
different	O
answers	O
.	O

Ties	O
are	O
allowed	O
.	O

We	O
find	O
that	O
human	O
judges	O
rate	O
questions	O
generated	O
by	O
our	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
as	O
having	O
greater	O
diversities	O
over	O
all	O
aspects	O
,	O
except	O
for	O
syntax	O
diversity	O
on	O
Reddit	O
,	O
as	O
shown	O
in	O
Table	O
4	O
.	O

Among	O
the	O
two	O
model	O
variants	O
,	O
questions	O
by	O
TPLGEN	B-MethodName
yield	O
more	O
diverse	O
answers	O
.	O

Based	O
on	O
our	O
observation	O
,	B-MethodName
TPLGEN	I-MethodName
uses	O
automatically	O
generated	O
templates	O
to	O
produce	O
more	O
focused	O
questions	O
with	O
different	O
answers	O
,	O
compared	O
to	O
EXPLGEN	B-MethodName
which	O
employs	O
exemplars	O
.	O

This	O
shows	O
the	O
promise	O
of	O
using	O
automatically	O
generated	O
templates	O
to	O
create	O
questions	O
that	O
need	O
to	O
be	O
addressed	O
with	O
different	O
answers	O
.	O

Besides	O
Figure	O
1	O
,	O
we	O
show	O
more	O
sample	O
outputs	O
in	O
Figure	O
4	O
,	O
where	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
exhibit	O
stronger	O
controllability	O
than	O
JOINTGEN	B-MethodName
.	O

Question	O
Content	O
Quality	O
.	O

We	O
use	O
the	O
same	O
set	O
of	O
human	O
judges	O
to	O
evaluate	O
another	O
80	O
groups	O
of	O
questions	O
output	O
by	O
five	O
selected	O
models	O
and	O
the	O
reference	O
.	O

Three	O
aspects	O
are	O
rated	O
from	O
1	O
(	O
worst)Answer	O
:	O
My	O
sister	O
in	O
law	O
and	O
her	O
husband	O
"	O
genetically	O
modified	O
"	O
their	O
second	O
child	O
because	O
the	O
first	O
has	O
EB	O
.	O

They	O
eliminated	O
that	O
and	O
had	O
a	O
baby	O
that	O
gets	O
to	O
live	O
pain	O
free	O
.	O

Under	O
the	O
right	O
circumstances	O
,	O
I	O
'	O
m	O
all	O
for	O
it	O
...	O
Figure	O
4	O
:	O
Sample	O
outputs	O
of	O
our	O
models	O
given	O
different	O
question	O
types	O
.	O

Spans	O
that	O
belong	O
to	O
the	O
exemplars	O
or	O
the	O
generated	O
templates	O
are	O
colored	O
with	O
blue	O
.	O

Generated	O
questions	O
that	O
do	O
not	O
match	O
the	O
given	O
type	O
are	O
marked	O
by	O
strikethrough.to	O
5	O
(	O
best	O
):	O
appropriateness	O
-	O
whether	O
the	O
question	O
is	O
semantically	O
correct	O
,	O
without	O
considering	O
the	O
answer	O
;	O
answerability	O
-	O
whether	O
the	O
question	O
can	O
be	O
addressed	O
by	O
the	O
given	O
answer	O
;	O
and	O
scopewhether	O
the	O
question	O
is	O
related	O
to	O
a	O
longer	O
span	O
of	O
the	O
answer	O
(	O
global	O
scope	O
)	O
or	O
focuses	O
on	O
local	O
content	O
(	O
e.g.	O
,	O
one	O
phrase	O
or	O
one	O
sentence	O
)	O
.	O

We	O
further	O
ask	O
the	O
annotators	O
to	O
rank	O
questions	O
based	O
on	O
their	O
overall	O
quality	O
and	O
preferences	O
,	O
with	O
ties	O
allowed	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
our	O
JOINTGEN	B-MethodName
model	O
produces	O
questions	O
with	O
better	O
answerability	O
and	O
that	O
cover	O
broader	O
content	O
in	O
the	O
answers	O
.	O

It	O
is	O
also	O
rated	O
as	O
the	O
best	O
in	O
more	O
than	O
half	O
of	O
the	O
evaluation	O
instances	O
on	O
both	O
datasets	O
.	O

Between	O
BART+QWORD	B-MethodName
and	O
BART+QTYPE	B-MethodName
,	O
human	O
judges	O
rate	O
the	O
system	O
outputs	O
that	O
conditioned	O
on	O
our	O
question	O
types	O
to	O
have	O
better	O
overall	O
quality	O
.	O

Does	O
focus	O
prediction	O
correlate	O
with	O
question	O
quality	O
?	O
We	O
first	O
investigate	O
the	O
relationship	O
between	O
focus	O
prediction	O
and	O
question	O
generation	O
by	O
using	O
our	O
joint	O
model	O
JOINTGEN	B-MethodName
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
5	O
,	O
there	O
is	O
a	O
strong	O
correlation	O
between	O
F1	B-MetricName
scores	O
of	O
focus	O
prediction	O
and	O
BLEU-4	B-MetricName
as	O
well	O
We	O
also	O
show	O
the	O
F1	B-MetricName
scores	O
and	O
BLEU-4	B-MetricName
for	O
selected	O
question	O
types	O
on	O
the	O
right	O
of	O
Figure	O
5	O
,	O
again	O
demonstrating	O
the	O
effect	O
of	O
focus	O
detection	O
on	O
question	O
quality	O
.	O

When	O
do	O
our	O
models	O
fail	O
to	O
respect	O
the	O
given	O
types	O
?	O
Next	O
,	O
we	O
provide	O
insights	O
into	O
which	O
types	O
of	O
questions	O
are	O
challenging	O
to	O
generate	O
by	O
using	O
our	O
template	O
-	O
based	O
models	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
.	O

Both	O
variants	O
frequently	O
fail	O
to	O
respect	O
the	O
given	O
question	O
type	O
of	O
VERIFICATION	O
,	O
in	O
which	O
cases	O
they	O
often	O
produce	O
JUDGEMENTAL	O
questions	O
.	O

They	O
also	O
tend	O
to	O
confuse	O
EXAMPLE	O
and	O
EXTENT	O
with	O
CONCEPT	O
questions	O
.	O

After	O
manually	O
inspecting	O
50	O
generated	O
questions	O
for	O
the	O
aforementioned	O
three	O
types	O
,	O
we	O
find	O
that	O
many	O
of	O
them	O
can	O
be	O
labeled	O
with	O
both	O
types	O
,	O
thus	O
creating	O
confusion	O
for	O
our	O
classifier	O
.	O

For	O
instance	O
,	O
"	O
What	O
are	O
the	O
import	O
restrictions	O
in	O
the	O
US	O
?	O
"	O
can	O
be	O
considered	O
as	O
either	O
We	O
present	O
a	O
new	O
question	O
type	O
ontology	O
which	O
better	O
captures	O
the	O
nuances	O
of	O
questions	O
to	O
support	O
the	O
study	O
of	O
open	O
-	O
ended	O
question	O
generation	O
.	O

We	O
further	O
annotate	O
a	O
new	O
dataset	O
with	O
4,959	O
questions	O
based	O
on	O
the	O
proposed	O
ontology	O
.	O

We	O
describe	O
a	O
joint	O
question	O
focus	O
detection	O
and	O
question	O
generation	O
framework	O
with	O
a	O
novel	O
semantic	B-TaskName
graphaugmented	I-TaskName
representation	I-TaskName
,	O
which	O
is	O
directly	O
built	O
on	O
large	O
pre	O
-	O
trained	O
models	O
.	O

Based	O
on	O
this	O
framework	O
,	O
we	O
also	O
enhance	O
the	O
controllability	O
and	O
diversity	O
of	O
generated	O
questions	O
by	O
employing	O
template	O
exemplars	O
or	O
automatically	O
generated	O
templates	O
.	O

Experiments	O
on	O
two	O
large	O
datasets	O
show	O
that	O
questions	O
generated	O
by	O
our	O
models	O
have	O
better	O
quality	O
and	O
higher	O
diversity	O
than	O
non	O
-	O
trivial	O
comparisons	O
,	O
with	O
similar	O
results	O
rated	O
by	O
human	O
judges	O
.	O

Large	O
models	O
that	O
are	O
pre	O
-	O
trained	O
on	O
heterogeneous	O
web	O
data	O
are	O
shown	O
to	O
encode	O
biases	O
and	O
can	O
be	O
potentially	O
harmful	O
for	O
marginalized	O
populations	O
.	O

While	O
the	O
automatically	O
learned	O
templates	O
improve	O
controllability	O
in	O
question	O
generation	O
,	O
we	O
also	O
recognize	O
that	O
our	O
system	O
might	O
be	O
misused	O
to	O
create	O
questions	O
that	O
contain	O
objectionable	O
content	O
.	O

We	O
therefore	O
advocate	O
cautious	O
and	O
responsible	O
practices	O
in	O
real	O
-	O
world	O
deployment	O
.	O

Our	O
data	O
collection	O
process	O
for	O
the	O
two	O
new	O
datasets	O
involves	O
removing	O
samples	O
with	O
abusive	O
languages	O
and	O
human	O
inspection	O
on	O
random	O
samples	O
.	O

Given	O
the	O
data	O
volume	O
,	O
however	O
,	O
we	O
can	O
not	O
exhaustively	O
verify	O
that	O
all	O
records	O
are	O
free	O
of	O
potentially	O
offensive	O
content	O
.	O

Data	O
Filtering	O
.	O

After	O
collecting	O
the	O
raw	O
data	O
from	O
Yahoo	O
and	O
Reddit	O
,	O
we	O
design	O
rules	O
to	O
filter	O
out	O
ill	O
-	O
formed	O
answers	O
and	O
questions	O
.	O

These	O
rules	O
are	O
listed	O
in	O
Table	O
6	O
.	O

Finally	O
,	O
we	O
conduct	O
human	O
inspection	O
on	O
random	O
samples	O
from	O
the	O
two	O
datasets	O
and	O
confirm	O
that	O
samples	O
are	O
all	O
clean	O
and	O
contain	O
open	O
-	O
ended	O
questions	O
.	O

Rules	O
for	O
Data	O
Cleaning	O
-The	O
question	O
has	O
URL	O
links.-The	O
question	O
has	O
more	O
than	O
1	O
sentence	O
or	O
does	O
not	O
end	O
with	O
a	O
question	O
mark.-The	O
question	O
has	O
less	O
than	O
4	O
words	O
or	O
less	O
than	O
1	O
content	O
word.-The	O
question	O
does	O
not	O
start	O
with	O
wh	O
-	O
words	O
:	O
what	O
,	O
why	O
,	O
how	O
,	O
which	O
,	O
where	O
,	O
who	O
,	O
when	O
;	O
yes	O
-	O
no	O
words	O
:	O
is	O
,	O
are	O
,	O
was	O
,	O
were	O
,	O
will	O
,	O
would	O
,	O
do	O
,	O
does	O
,	O
did	O
,	O
can	O
,	O
could	O
,	O
should	O
,	O
has	O
,	O
have	O
;	O
or	O
frequent	O
words	O
for	O
conditions	O
:	O
if	O
,	O
in	O
,	O
for	O
,	O
to	O
,	O
as	O
,	O
at.-The	O
answer	O
has	O
less	O
than	O
15	O
content	O
words.-The	O
answer	O
has	O
less	O
content	O
words	O
than	O
the	O
question.-The	O
answer	O
has	O
more	O
than	O
30	O
%	O
of	O
the	O
words	O
as	O
digit	O
letters.-The	O
question	O
and	O
the	O
answer	O
have	O
less	O
than	O
2	O
overlapping	O
content	O
words.-The	O
question	O
or	O
the	O
answer	O
contains	O
abusive	O
words	O
from	O
Google	O
's	O
"	O
what	O
do	O
you	O
need	O
"	O
project	O
7	O
.-The	O
question	O
or	O
the	O
answer	O
has	O
emoticons	O
8	O
.	O

-The	O
question	O
or	O
the	O
answer	O
has	O
3	O
consecutive	O
punctuation.-The	O
question	O
or	O
the	O
answer	O
has	O
3	O
consecutive	O
fully	O
uppercased	O
words.-The	O
question	O
has	O
more	O
than	O
90	O
%	O
of	O
title	O
-	O
case	O
words	O
or	O
the	O
answer	O
has	O
more	O
than	O
30	O
%	O
of	O
title	O
-	O
case	O
words.-The	O
question	O
has	O
more	O
than	O
1	O
unique	O
word	O
not	O
in	O
the	O
English	O
dictionary	O
or	O
the	O
answer	O
has	O
more	O
than	O
2	O
unique	O
words	O
not	O
in	O
the	O
English	O
dictionary	O
9	O
.	O

Question	O
Type	O
Annotation	O
.	O

We	O
include	O
the	O
definition	O
and	O
corresponding	O
examples	O
for	O
each	O
question	O
type	O
in	O
the	O
annotation	O
guideline	O
,	O
as	O
shown	O
in	O
Table	O
12	O
.	O

We	O
allow	O
annotators	O
to	O
label	O
a	O
question	O
with	O
two	O
types	O
if	O
they	O
can	O
not	O
decide	O
between	O
the	O
two	O
.	O

All	O
recruited	O
annotators	O
are	O
U.S.	O
college	O
students	O
,	O
and	O
are	O
paid	O
$	O
15	O
per	O
hour	O
for	O
the	O
task	O
.	O

On	O
average	O
,	O
it	O
takes	O
3.5	O
hours	O
to	O
annotate	O
1000	O
questions	O
.	O

For	O
samples	O
with	O
disagreed	O
labels	O
,	O
we	O
check	O
whether	O
agreement	O
can	O
be	O
reached	O
by	O
considering	O
both	O
labeled	O
types	O
.	O

For	O
example	O
,	O
if	O
annotator	O
A	O
labels	O
VERIFICATION	O
and	O
JUDGMENTAL	O
,	O
and	O
annotator	O
B	O
labels	O
JUDGMENTAL	O
,	O
the	O
agreed	O
-	O
upon	O
type	O
is	O
JUDGMENTAL	O
.	O

We	O
then	O
resolve	O
outstanding	O
disagreements	O
by	O
discussion	O
.	O

To	O
train	O
the	O
question	O
type	O
classifier	O
γ	O
q	O
that	O
reads	O
the	O
question	O
as	O
input	O
,	O
we	O
split	O
the	O
collected	O
question	O
type	O
dataset	O
into	O
training	O
,	O
validation	O
,	O
and	O
test	O
sets	O
.	O

Sample	O
counts	O
and	O
question	O
type	O
distributions	O
for	O
different	O
data	O
splits	O
are	O
shown	O
in	O
Table	O
7	O
.	O

We	O
then	O
use	O
γ	O
q	O
to	O
identify	O
types	O
for	O
unlabeled	O
questions	O
in	O
Yahoo	O
and	O
Reddit	O
.	O

The	O
question	O
type	O
distributions	O
for	O
the	O
two	O
datasets	O
are	O
shown	O
in	O
Table	O
8	O
.	O

We	O
discard	O
secondary	O
dependency	O
relations	O
for	O
graph	O
construction	O
,	O
including	O
case	O
,	O
mark	O
,	O
cc	O
,	O
cc	O
:	O
preconj	O
,	O
aux	O
,	O
aux	O
:	O
pass	O
,	O
cop	O
,	O
det	O
,	O
discourse	O
,	O
expl	O
,	O
det	O
:	O
predet	O
,	O
punct	O
,	O
ref	O
.	O

The	O
definition	O
for	O
each	O
dependency	O
can	O
be	O
found	O
in	O
Universal	B-MethodName
Dependency	I-MethodName
.	O

10	O
Template	O
Construction	O
.	O

To	O
avoid	O
replacing	O
words	O
that	O
are	O
representative	O
of	O
question	O
types	O
during	O
template	O
construction	O
,	O
we	O
maintain	O
a	O
list	O
of	O
words	O
not	O
to	O
be	O
replaced	O
for	O
each	O
question	O
type	O
,	O
as	O
shown	O
in	O
Table	O
9	O
.	O

These	O
words	O
are	O
identified	O
by	O
frequency	O
with	O
additional	O
manual	O
inspection	O
.	O

Exemplar	O
Classifiers	O
.	O

To	O
predict	O
the	O
exemplars	O
used	O
for	O
question	O
decoding	O
,	O
we	O
train	O
one	O
exemplar	O
classifier	O
for	O
each	O
question	O
type	O
,	O
on	O
each	O
dataset	O
.	O

Accuracy	O
values	O
of	O
these	O
exemplar	O
classifiers	O
on	O
the	O
reserved	O
test	O
sets	O
are	O
listed	O
in	O
Table	O
11	O
.	O

We	O
use	O
Fairseq	O
to	O
build	O
our	O
models	O
and	O
conduct	O
training	O
and	O
decoding	O
.	O

For	O
the	O
Graph	B-MethodName
Attention	I-MethodName
Networks	I-MethodName
(	I-MethodName
GATs	I-MethodName
)	O
in	O
our	O
focus	O
predictor	O
,	O
we	O
adopt	O
the	O
implementation	O
by	O
PyTorch	O
Geometric	O
(	O
Fey	O
and	O
Lenssen	O
,	O
2019	O
)	O
.	O

All	O
our	O
experiments	O
are	O
conducted	O
on	O
a	O
Quadro	O
RTX	O
8000	O
GPU	O
with	O
48	O
GB	O
of	O
memory	O
.	O

Training	O
Settings	O
.	O

We	O
use	O
Adam	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
for	O
the	O
training	O
of	O
all	O
our	O
models	O
.	O

Our	O
question	O
type	O
classifiers	O
and	O
template	O
exemplar	O
classifiers	O
are	O
trained	O
with	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
.	O

For	O
training	O
generation	O
models	O
,	O
the	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
3	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
each	O
batch	B-HyperparameterName
contains	O
at	O
most	O
32,768	B-HyperparameterValue
models	O
except	O
for	O
models	O
with	O
GATs	B-MethodName
.	O

Decoding	O
Settings	O
.	O

We	O
use	B-HyperparameterName
beam	I-HyperparameterName
search	I-HyperparameterName
for	O
decoding	O
.	O

A	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
1.5	B-HyperparameterValue
are	O
used	O
for	O
all	O
models	O
.	O

Repeated	B-HyperparameterName
trigram	I-HyperparameterName
blocking	I-HyperparameterName
is	O
applied	O
to	O
question	O
generation	O
.	O

The	O
minimum	B-HyperparameterName
and	I-HyperparameterName
maximum	I-HyperparameterName
lengths	I-HyperparameterName
for	O
generation	O
are	O
set	O
to	O
1	B-HyperparameterValue
and	O
100	B-HyperparameterValue
,	O
respectively	O
.	O

This	O
research	O
is	O
supported	O
in	O
part	O
by	O
National	O
Science	O
Foundation	O
through	O
Grants	O
IIS-1813341	O
and	O
a	O
CAREER	O
award	O
IIS-2046016	O
.	O

We	O
thank	O
three	O
anonymous	O
reviewers	O
,	O
area	O
chair	O
,	O
and	O
senior	O
area	O
chairs	O
for	O
their	O
valuable	O
suggestions	O
for	O
improving	O
various	O
aspects	O
of	O
this	O
work	O
.	O

In	O
this	O
study	O
,	O
you	O
are	O
asked	O
to	O
annotate	O
the	O
question	O
types	O
for	O
1000	O
questions	O
.	O

The	O
question	O
type	O
reflects	O
the	O
nature	O
of	O
the	O
question	O
.	O

It	O
is	O
not	O
determined	O
by	O
the	O
interrogative	O
word	O
of	O
the	O
question	O
.	O

There	O
are	O
10	O
question	O
types	O
in	O
total	O
.	O

The	O
definition	O
for	O
each	O
type	O
is	O
shown	O
in	O
the	O
following	O
Table	O
,	O
along	O
with	O
examples	O
per	O
question	O
type	O
.	O

During	O
annotation	O
,	O
you	O
can	O
label	O
two	O
most	O
-	O
confident	O
types	O
when	O
no	O
clear	O
decision	O
can	O
be	O
made	O
for	O
the	O
most	O
probable	O
type	O
.	O

VERIFICATION	O
:	O
Asking	O
for	O
the	O
truthfulness	O
of	O
an	O
event	O
or	O
a	O
concept.-"Is	O
Michael	O
Jackson	O
an	O
African	O
American	O
?	O
"	O
-"Does	O
a	O
Mercedes	O
dealer	O
have	O
to	O
unlock	O
a	O
locked	O
radio	O
?	O
"	O
-"Could	O
stress	O
,	O
anxiety	O
,	O
or	O
worry	O
cause	O
cholesterol	O
levels	O
to	O
rise	O
?	O
"	O
DISJUNCTIVE	O
:	O
Asking	O
for	O
the	O
true	O
one	O
given	O
multiple	O
events	O
or	O
concepts	O
,	O
where	O
comparison	O
among	O
options	O
is	O
not	O
needed.-"Is	O
Michael	O
Jackson	O
an	O
African	O
American	O
or	O
Latino	O
?	O
"	O
-"Is	O
a	O
DVI	O
to	O
HDMI	O
cable	O
supposed	O
to	O
transmit	O
audio	O
and	O
video	O
or	O
just	O
video	O
?	O
"	O
-"When	O
you	O
get	O
a	O
spray	O
-	O
on	O
tan	O
does	O
someone	O
put	O
it	O
on	O
you	O
or	O
does	O
a	O
machine	O
do	O
it	O
?	O
"	O
CONCEPT	O
:	O
Asking	O
for	O
a	O
definition	O
of	O
an	O
event	O
or	O
a	O
concept.-"Who	O
said	O
the	O
sun	O
never	O
sets	O
on	O
the	O
British	O
empire	O
?	O
"	O
-"Where	O
do	O
dolphins	O
have	O
hair	O
at	O
?	O
"	O
-"What	O
is	O
the	O
origin	O
of	O
the	O
phrase	O
"	O
kicking	O
the	O
bucket	O
"	O
?	O
"	O
EXTENT	O
:	O
Asking	O
for	O
the	O
extent	O
or	O
quantity	O
of	O
an	O
event	O
or	O
a	O
concept.-"How	O
long	O
does	O
gum	O
stay	O
in	O
your	O
system	O
?	O
"	O
-"What	O
is	O
Barry	O
Larkin	O
's	O
hat	O
size	O
?	O
"	O
-"To	O
what	O
extent	O
is	O
the	O
Renewable	O
Fuel	O
Standard	O
accurate	O
nationwide	O
?	O
"	O
EXAMPLE	O
:	O
Asking	O
for	O
example(s	O
)	O
or	O
instance(s	O
)	O
of	O
an	O
event	O
or	O
a	O
concept.-"What	O
are	O
some	O
examples	O
to	O
support	O
or	O
contradict	O
this	O
?	O
"	O
-"Where	O
can	O
I	O
get	O
my	O
teeth	O
examined	O
around	O
Los	O
Angeles	O
?	O
"	O
-"What	O
countries	O
/	O
regions	O
throughout	O
the	O
world	O
do	O
not	O
celebrate	O
the	O
Christmas	O
holidays	O
?	O
"	O
-"What	O
is	O
the	O
best	O
goal	O
or	O
win	O
you	O
have	O
ever	O
made	O
in	O
a	O
sport	O
?	O
"	O
COMPARISON	O
:	O
Asking	O
for	O
comparison	O
among	O
multiple	O
events	O
or	O
concepts.-"How	O
does	O
an	O
electric	O
violin	O
"	O
play	O
"	O
differently	O
than	O
an	O
acoustic	O
violin	O
?	O
"	O
-"What	O
is	O
the	O
best	O
tinted	O
facial	O
moisturizer	O
?	O
"	O
-"In	O
what	O
hilariously	O
inaccurate	O
ways	O
is	O
your	O
job	O
/	O
career	O
portrayed	O
on	O
television	O
or	O
in	O
movies	O
?	O
"	O
-"Which	O
is	O
better	O
,	O
Nike	O
or	O
Adidas	O
?	O
"	O
CAUSE	O
:	O
Asking	O
for	O
the	O
cause	O
or	O
reason	O
for	O
an	O
event	O
or	O
a	O
concept.-"How	O
does	O
the	O
D.M.V.	O
decide	O
the	O
first	O
letter	O
of	O
the	O
California	O
driver	O
's	O
license	O
?	O
"	O
-"Why	O
are	O
parents	O
strick	O
on	O
girls	O
than	O
boys	O
?	O
"	O
-"What	O
makes	O
nerve	O
agents	O
like	O
"	O
Novichok	O
"	O
so	O
hard	O
to	O
produce	O
and	O
why	O
can	O
only	O
a	O
handful	O
of	O
laboratories	O
create	O
them	O
?	O
"	O
"	O
Why	O
is	O
the	O
sky	O
blue	O
?	O
"	O
CONSEQUENCE	O
:	O
Asking	O
for	O
the	O
consequences	O
or	O
results	O
of	O
an	O
event.-"What	O
are	O
the	O
negative	O
consequences	O
for	O
the	O
services	O
if	O
they	O
do	O
not	O
evaluate	O
their	O
programs	O
?	O
"	O
-"In	O
the	O
US	O
,	O
what	O
is	O
the	O
benefit	O
of	O
having	O
a	O
red	O
left	O
-	O
turn	O
arrow	O
?	O
"	O
-"What	O
would	O
happen	O
if	O
employers	O
violate	O
the	O
legislation	O
?	O
"	O
-"What	O
if	O
the	O
Hokey	O
Pokey	O
is	O
really	O
what	O
it	O
's	O
all	O
about	O
?	O
"	O
PROCEDURAL	O
:	O
Asking	O
for	O
the	O
procedures	O
,	O
tools	O
,	O
or	O
methods	O
by	O
which	O
a	O
certain	O
outcome	O
is	O
achieved.-"Why	O
YM	O
7.5	O
BETA	O
always	O
stupidly	O
shows	O
me	O
available	O
,	O
although	O
I	O
initially	O
set	O
it	O
to	O
invisible	O
?	O
"	O
-"How	O
did	O
the	O
Amish	O
resist	O
assimilation	O
into	O
the	O
current	O
social	O
status	O
in	O
the	O
U.S	O
?	O
"	O
-"How	O
astronomers	O
detect	O
a	O
nebula	O
when	O
there	O
are	O
no	O
stars	O
illuminating	O
it	O
?	O
"	O
JUDGMENTAL	O
:	O
Asking	O
for	O
the	O
opinions	O
of	O
the	O
answerer	O
's	O
own.-"Do	O
you	O
think	O
that	O
it	O
's	O
acceptable	O
to	O
call	O
off	O
work	O
for	O
a	O
dying	O
-	O
dead	O
pet	O
?	O
"	O
-"Should	O
I	O
date	O
a	O
guy	O
that	O
has	O
an	O
identical	O
twin	O
?	O
"	O
-"How	O
old	O
is	O
too	O
old	O
for	O
a	O
guy	O
to	O
still	O
live	O
with	O
his	O
mother	O
?	O
"	O

While	O
there	O
is	O
an	O
abundance	O
of	O
popular	O
writing	O
targeted	O
to	O
podcast	O
creators	O
on	O
how	O
to	O
speak	O
in	O
ways	O
that	O
engage	O
their	O
listeners	O
,	O
there	O
has	O
been	O
little	O
data	O
-	O
driven	O
analysis	O
of	O
podcasts	O
that	O
relates	O
linguistic	O
style	O
with	O
listener	O
engagement	O
.	O

In	O
this	O
paper	O
,	O
we	O
investigate	O
how	O
various	O
factors	O
-vocabulary	O
diversity	O
,	O
distinctiveness	O
,	O
emotion	O
,	O
and	O
syntax	O
,	O
among	O
others	O
-correlate	O
with	O
engagement	O
,	O
based	O
on	O
analysis	O
of	O
the	O
creators	O
'	O
written	O
descriptions	O
and	O
transcripts	O
of	O
the	O
audio	O
.	O

We	O
build	O
models	O
with	O
different	O
textual	O
representations	O
,	O
and	O
show	O
that	O
the	O
identified	O
features	O
are	O
highly	O
predictive	O
of	O
engagement	O
.	O

Our	O
analysis	O
tests	O
popular	O
wisdom	O
about	O
stylistic	O
elements	O
in	O
highengagement	O
podcasts	O
,	O
corroborating	O
some	O
aspects	O
,	O
and	O
adding	O
new	O
perspectives	O
on	O
others	O
.	O

What	O
makes	O
a	O
particular	O
podcast	O
broadly	O
engaging	O
?	O
As	O
a	O
media	O
form	O
,	O
podcasting	O
is	O
new	O
enough	O
that	O
such	O
questions	O
are	O
only	O
beginning	O
to	O
be	O
understood	O
.	O

Websites	O
exist	O
with	O
advice	O
on	O
podcast	O
production	O
,	O
including	O
language	O
-	O
related	O
tips	O
such	O
as	O
reducing	O
filler	O
words	O
and	O
disfluencies	O
,	O
or	O
incorporating	O
emotion	O
,	O
but	O
there	O
has	O
been	O
little	O
quantitative	O
research	O
into	O
how	O
aspects	O
of	O
language	O
usage	O
contribute	O
to	O
overall	O
listener	O
engagement	O
.	O

This	O
paper	O
investigates	O
the	O
linguistic	O
factors	O
that	O
correlate	O
with	O
engagement	O
,	O
leveraging	O
the	O
written	O
descriptions	O
of	O
the	O
parent	O
show	O
and	O
episode	O
as	O
well	O
as	O
the	O
transcript	O
of	O
the	O
audio	O
.	O

Our	O
metric	O
of	O
engagement	O
is	O
stream	B-DatasetName
rate	I-DatasetName
,	O
which	O
we	O
define	O
as	O
the	O
proportion	O
of	O
first	O
-	O
time	O
listeners	O
-of	O
those	O
who	O
have	O
begun	O
streaming	O
the	O
episode	O
-who	O
listen	O
for	O
at	O
least	O
five	O
minutes	O
.	O

Notably	O
,	O
stream	O
rate	O
is	O
different	O
from	O
the	O
metric	O
of	O
popularity	B-MetricName
as	O
given	O
by	O
the	O
raw	O
number	O
of	O
streams	O
;	O
the	O
latter	O
is	O
inevitably	O
influenced	O
by	O
factors	O
unrelated	O
to	O
the	O
content	O
,	O
such	O
as	O
the	O
host	O
or	O
publisher	O
reputation	O
,	O
publicity	O
,	O
expo	O
-	O
sure	O
in	O
recommendations	O
and	O
search	O
engines	O
,	O
and	O
time	O
of	O
publication	O
,	O
whereas	O
a	O
listener	O
's	O
decision	O
to	O
continue	O
listening	O
for	O
as	O
long	O
as	O
five	O
minutes	O
is	O
likely	O
to	O
be	O
influenced	O
by	O
the	O
content	O
.	O

We	O
perform	O
a	O
series	O
of	O
descriptive	O
tests	O
to	O
examine	O
differences	O
in	O
language	O
usage	O
between	O
high	O
and	O
low	O
engagement	O
podcasts	O
,	O
and	O
build	O
predictive	O
models	O
.	O

Our	O
tests	O
show	O
that	O
while	O
much	O
of	O
the	O
conventional	O
wisdom	O
on	O
engaging	O
podcasting	O
style	O
(	O
such	O
as	O
to	O
use	O
positive	O
language	O
)	O
bears	O
out	O
in	O
the	O
data	O
,	O
other	O
assumptions	O
(	O
such	O
as	O
to	O
speak	O
slowly	O
)	O
are	O
contradicted	O
and	O
deserve	O
a	O
closer	O
look	O
.	O

We	O
find	O
that	O
stylistic	O
features	O
tend	O
to	O
be	O
more	O
correlated	O
with	O
engagement	O
for	O
podcasts	O
with	O
low	O
absolute	O
numbers	O
of	O
streams	O
than	O
for	O
the	O
most	O
popular	O
podcasts	O
,	O
suggesting	O
that	O
listeners	O
may	O
be	O
less	O
sensitive	O
to	O
style	O
in	O
podcasts	O
made	O
by	O
well	O
-	O
known	O
creators	O
.	O

We	O
also	O
identify	O
those	O
linguistic	O
factors	O
that	O
correlate	O
with	O
our	O
engagement	O
metric	O
across	O
the	O
popularity	O
spectrum	O
,	O
and	O
those	O
that	O
are	O
limited	O
to	O
podcasts	O
within	O
a	O
certain	O
popularity	O
range	O
.	O

Our	O
predictive	O
models	O
prove	O
that	O
stylistic	O
factors	O
alone	O
play	O
a	O
significant	O
role	O
in	O
determining	O
if	O
a	O
podcast	O
has	O
high	O
or	O
low	O
engagement	O
,	O
achieving	O
an	O
accuracy	B-MetricName
of	O
72	B-MetricValue
%	I-MetricValue
in	O
distinguishing	O
between	O
very	O
high	O
engagement	O
(	O
top	O
25	B-MetricValue
%	I-MetricValue
of	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
in	O
the	O
corpus	O
)	O
and	O
very	O
low	O
engagement	O
(	O
bottom	O
25	B-MetricValue
%	I-MetricValue
)	O
examples	O
.	O

We	O
also	O
show	O
that	O
the	O
overall	O
textual	O
information	O
in	O
podcasts	O
is	O
highly	O
predictive	O
of	O
engagement	O
in	O
this	O
experiment	O
,	O
with	O
an	O
accuracy	B-MetricName
as	O
high	O
as	O
81	B-MetricValue
%	I-MetricValue
.	O

To	O
understand	O
how	O
style	O
in	O
podcasts	O
compares	O
to	O
other	O
spoken	O
media	O
,	O
we	O
apply	O
our	O
analysis	O
to	O
a	O
corpus	B-TaskName
of	I-TaskName
TED	I-TaskName
talks	I-TaskName
.	O

Finally	O
,	O
we	O
manually	O
examine	O
the	O
highest	O
engagement	O
podcasts	O
in	O
our	O
dataset	O
to	O
characterize	O
their	O
content	O
.	B-MethodName

Content	I-MethodName
-	I-MethodName
Based	I-MethodName
Podcast	I-MethodName
Recommendations	I-MethodName
Yang	O
et	O
al	O
.	O
(	O

2019	O
)	O
model	O
transcripts	O
with	O
a	O
topic	B-MethodName
model	I-MethodName
,	O
and	O
the	O
audio	O
with	O
a	O
representation	O
they	O
trained	O
to	O
predict	O
the	O
non	O
-	O
textual	O
attributes	O
of	O
seriousness	O
and	O
energy	O
.	O

They	O
find	O
that	O
combining	O
these	O
representations	O
improves	O
over	O
the	O
purely	O
topic	O
based	O
model	O
on	O
popularity	B-TaskName
prediction	I-TaskName
.	O

This	O
work	O
indicates	O
that	O
stylistic	O
attributes	O
are	O
important	O
factors	O
,	O
and	O
raises	O
the	O
question	O
of	O
whether	O
stylistic	O
features	O
derived	O
from	O
the	O
text	O
are	O
valuable	O
as	O
well	O
.	O

Tsagkias	O
et	O
al	O
.	O
(	O

2010	O
)	O
develop	O
a	O
framework	O
containing	O
a	O
set	O
of	O
attributes	O
,	O
and	O
compare	O
the	O
proportions	O
of	O
these	O
attributes	O
relative	O
to	O
engagement	O
on	O
iTunes	O
.	O

Our	O
work	O
follows	O
a	O
similar	O
spirit	O
,	O
but	O
we	O
address	O
some	O
limitations	O
of	O
their	O
study	O
,	O
namely	O
,	O
they	O
use	O
a	O
small	O
set	O
of	O
podcasts	O
(	O
250	O
)	O
,	O
and	O
manually	O
annotate	O
the	O
attributes	O
for	O
every	O
podcast	O
rather	O
than	O
deriving	O
them	O
from	O
the	O
raw	O
data	O
.	O

Since	O
we	O
derive	O
all	O
features	O
automatically	O
,	O
we	O
limit	O
ourselves	O
to	O
concrete	O
,	O
easily	O
quantifiable	O
features	O
,	O
whereas	O
the	O
above	O
paper	O
considers	O
higher	O
level	O
attributes	O
like	O
'	O
one	O
topic	O
per	O
episode	O
'	O
or	O
'	O
fluent	O
'	O
.	O

Predicting	B-TaskName
Performance	I-TaskName
from	I-TaskName
Language	I-TaskName
Previous	O
research	O
in	O
natural	O
language	O
processing	O
has	O
explored	O
the	O
connections	O
between	O
textual	O
features	O
and	O
audience	O
engagement	O
in	O
books	O
(	O
Ganjigunte	O
Ashok	O
et	O
al	O
.	O
,	O

2013;Maharjan	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
YouTube	O
(	O
Kleinberg	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
news	O
(	O
Naseri	O
and	O
Zamani	O
,	O
2019	O
)	O
,	O
TED	O
talks	O
(	O
Tanveer	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
and	O
tweets	O
(	O
Tan	O
et	O
al	O
.	O
,	O

2014;Lampos	O
et	O
al	O
.	O
,	O

2014	O
)	O
.	O

Other	O
works	O
have	O
modeled	O
the	O
relationship	O
between	O
text	O
and	O
various	O
performance	O
metrics	O
such	O
as	O
movie	O
quote	O
memorability	O
(	O
Danescu	O
-	O
Niculescu	O
-	O
Mizil	O
et	O
al	O
.	O
,	O

2012	O
)	O
,	O
forecasting	O
ability	O
(	O
Zong	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
congressional	O
bill	O
survival	O
(	O
Yano	O
et	O
al	O
.	O
,	O

2012	O
)	O
,	O
success	O
of	O
job	O
interviews	O
(	O
Naim	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
impact	O
of	O
academic	O
papers	O
(	O
Yogatama	O
et	O
al	O
.	O
,	O

2011;Li	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
in	O
addition	O
to	O
the	O
entire	O
field	O
of	O
sentiment	B-TaskName
and	I-TaskName
opinion	I-TaskName
mining	I-TaskName
of	O
data	O
such	O
as	O
user	O
reviews	O
(	O
Pang	O
et	O
al	O
.	O
,	O

2002	O
)	O
.	O

The	O
Spotify	B-DatasetName
Podcast	I-DatasetName
Dataset	O
is	O
a	O
recently	O
released	O
corpus	O
of	O
over	O
100	O
,	O
000	O
podcast	O
episodes	O
,	O
mostly	O
in	O
English	O
,	O
that	O
are	O
transcribed	O
with	O
Google	B-MethodName
's	I-MethodName
Speech	I-MethodName
to	I-MethodName
Text	I-MethodName
commercial	I-MethodName
speech	I-MethodName
recognition	I-MethodName
,	O
reported	O
in	O
the	O
paper	O
to	O
have	O
an	O
18	B-MetricValue
%	I-MetricValue
word	B-MetricName
error	I-MetricName
on	O
podcasts	O
.	O

A	O
podcast	O
,	O
also	O
known	O
as	O
a	O
'	O
show	O
'	O
in	O
the	O
dataset	O
,	O
is	O
a	O
collection	O
of	O
episodes	O
.	O

In	O
addition	O
to	O
the	O
speech	O
transcripts	O
,	O
the	O
textual	O
information	O
associated	O
with	O
each	O
podcast	O
episode	O
includes	O
the	O
title	O
and	O
description	O
of	O
the	O
episode	O
and	O
the	O
parent	O
show	O
(	O
Table	O
1	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
consider	O
descriptions	O
and	O
transcripts	O
as	O
the	O
text	O
representation	O
of	O
an	O
episode	O
.	O

All	O
textual	O
data	O
was	O
normalized	O
and	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagged	I-TaskName
with	O
spacy	O
.	O

1	O
Since	O
many	O
episode	O
descriptions	O
contain	O
promotions	O
,	O
advertisements	O
,	O
and	O
show	O
notes	O
,	O
which	O
are	O
extraneous	O
to	O
the	O
main	O
content	O
of	O
the	O
podcast	O
,	O
we	O
remove	O
such	O
material	O
before	O
analysis	O
(	O
although	O
we	O
also	O
measure	O
the	O
amount	O
of	O
ad	O
content	O
as	O
a	O
feature	O
)	O
.	O

2	O
Promotional	O
and	O
extraneous	O
material	O
was	O
detected	O
by	O
the	O
classifier	O
described	O
by	O
,	O
a	O
model	O
using	O
BERT	B-MethodName
with	O
a	O
classification	O
head	O
,	O
trained	O
on	O
a	O
manually	O
annotated	O
set	O
of	O
episode	O
descriptions	O
.	O

This	O
classifier	O
is	O
reported	O
to	O
have	O
a	O
sentence	B-MetricName
classification	I-MetricName
accuracy	I-MetricName
of	O
95	B-MetricValue
%	I-MetricValue
on	O
episode	O
descriptions	O
.	O

We	O
obtained	O
streaming	O
numbers	O
for	O
the	O
episodes	O
in	O
the	O
corpus	O
from	O
Spotify	O
,	O
a	O
music	O
and	O
podcast	O
streaming	O
platform	O
.	O

The	O
numbers	O
were	O
aggregated	O
from	O
the	O
date	O
of	O
the	O
episode	O
's	O
publication	O
on	O
the	O
platform	O
until	O
December	O
2020	O
.	O

Since	O
the	O
most	O
recently	O
published	O
episode	O
in	O
the	O
dataset	O
is	O
from	O
February	O
2020	O
,	O
all	O
episodes	O
had	O
several	O
months	O
of	O
exposure	O
by	O
the	O
time	O
of	O
collection	O
.	O

We	O
specifically	O
consider	O
streaming	O
by	O
'	O
first	O
-	O
time	O
listeners	O
'	O
who	O
are	O
not	O
already	O
familiar	O
with	O
the	O
show	O
,	O
i.e.	O
,	O
those	O
who	O
have	O
not	O
previously	O
streamed	O
any	O
other	O
episode	O
of	O
that	O
show	O
for	O
more	O
than	O
five	O
minutes	O
.	O

Listeners	O
who	O
are	O
familiar	O
with	O
the	O
show	O
through	O
other	O
episodes	O
are	O
ignored	O
since	O
they	O
may	O
be	O
habituated	O
and	O
primed	O
for	O
the	O
content	O
.	O

As	O
described	O
in	O
the	O
introduction	O
,	O
we	O
use	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
engagement	O
metric	O
,	O
defined	O
as	O
the	O
proportion	O
of	O
the	O
show	O
's	O
first	O
-	O
time	O
listeners	O
who	O
stream	O
at	O
least	O
five	O
minutes	O
of	O
the	O
episode	O
.	O

Stream	B-MetricName
rate	I-MetricName
in	O
the	O
dataset	O
shows	O
a	O
weak	O
but	O
statistically	O
significant	O
inverse	O
rank	O
correlation	O
with	O
popularity	B-MetricName
(	O
Spearman	B-HyperparameterName
's	I-HyperparameterName
ρ	I-HyperparameterName
=	O
−0.12	B-HyperparameterValue
,	O
p	B-HyperparameterName
<	O
0.001	B-HyperparameterValue
)	O
.	O

This	O
may	O
be	O
because	O
popular	O
podcasts	O
attract	O
more	O
listeners	O
who	O
may	O
realize	O
they	O
are	O
not	O
interested	O
in	O
the	O
content	O
soon	O
after	O
they	O
begin	O
streaming	O
,	O
while	O
the	O
listeners	O
of	O
less	O
popular	O
podcasts	O
may	O
have	O
actively	O
sought	O
them	O
out	O
.	O

70	B-MetricValue
%	I-MetricValue
stream	B-MetricName
rate	I-MetricName
in	O
a	O
well	O
-	O
known	O
podcast	O
which	O
A	O
weekly	O
podcast	O
covering	O
all	O
things	O
witchcraft	O
in	O
the	O
modern	O
world	O
.	O

Join	O
us	O
,	O
two	O
best	O
friends	O
and	O
Midwestern	O
witches	O
(	O
one	O
Wiccan	O
,	O
one	O
not	O
)	O
,	O
as	O
we	O
dive	O
into	O
all	O
things	O
witchy	O
.	O

We	O
're	O
starting	O
at	O
the	O
beginning	O
,	O
making	O
this	O
podcast	O
a	O
great	O
resource	O
for	O
newbies	O
...	O
would	O
have	O
attracted	O
a	O
broad	O
array	O
of	O
listeners	O
is	O
not	O
comparable	O
to	O
70	B-MetricValue
%	I-MetricValue
stream	B-MetricName
rate	I-MetricName
in	O
a	O
relatively	O
unknown	O
podcast	O
.	O

Therefore	O
,	O
we	O
bin	O
the	O
dataset	O
into	O
popularity	O
quartiles	O
for	O
analysis	O
on	O
stream	B-MetricName
rate	I-MetricName
,	O
which	O
is	O
found	O
to	O
be	O
uncorrelated	O
with	O
popularity	O
within	O
each	O
quartile	O
.	O

Stream	B-MetricName
rate	I-MetricName
is	O
uncorrelated	O
with	O
the	O
time	O
of	O
publication	O
.	O

We	O
filter	O
out	O
all	O
episodes	O
that	O
are	O
shorter	O
than	O
ten	O
minutes	O
and	O
fewer	O
than	O
a	O
threshold	O
number	O
of	O
total	O
streams	O
.	O

To	O
control	O
for	O
duration	O
effects	O
in	O
the	O
analysis	O
of	O
transcripts	O
,	O
we	O
truncate	O
transcripts	O
at	O
ten	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

The	O
original	O
podcast	O
corpus	O
contains	O
multiple	O
episodes	O
for	O
many	O
of	O
the	O
show	O
while	O
other	O
show	O
have	O
only	O
one	O
episode	O
.	O

We	O
select	O
the	O
moststreamed	O
episode	O
from	O
each	O
show	O
as	O
its	O
representative	O
,	O
thereby	O
ensuring	O
that	O
every	O
show	O
is	O
represented	O
by	O
a	O
single	O
episode	O
in	O
the	O
data	O
.	O

This	O
is	O
done	O
so	O
that	O
shows	O
with	O
several	O
episodes	O
do	O
not	O
have	O
an	O
outsize	O
influence	O
on	O
the	O
models	O
.	O

Since	O
the	O
original	O
corpus	O
is	O
an	O
English	O
-	O
language	O
collection	O
,	O
all	O
of	O
our	O
analysis	O
is	O
constrained	O
to	O
English	O
,	O
and	O
we	O
filter	O
out	O
any	O
stray	O
examples	O
in	O
the	O
corpus	O
that	O
are	O
detected	O
as	O
non	O
-	O
English	O
after	O
running	B-TaskName
language	I-TaskName
identification	I-TaskName
(	O
Lui	O
and	O
Baldwin	O
,	O
2011	O
)	O
on	O
the	O
descriptions	O
.	O

The	O
resulting	O
dataset	O
has	O
5371	B-HyperparameterValue
episodes	O
.	O

The	O
norms	O
of	O
language	O
usage	O
may	O
vary	O
depending	O
on	O
the	O
genre	O
and	O
topics	O
being	O
discussed	O
.	O

For	O
example	O
,	O
technical	O
podcasts	O
are	O
expected	O
to	O
contain	O
more	O
complex	O
language	O
compared	O
to	O
chit	O
-	O
chat	O
,	O
crime	O
podcasts	O
to	O
contain	O
words	O
with	O
negative	O
sentiments	O
as	O
opposed	O
to	O
motivational	O
podcasts	O
,	O
and	O
so	O
on	O
.	O

The	O
RSS	O
feed	O
of	O
a	O
podcast	O
show	O
contains	O
one	O
or	O
more	O
categories	O
selected	O
by	O
the	O
creators	O
from	O
the	O
Apple	O
iTunes	O
taxonomy	O
;	O
however	O
,	O
these	O
are	O
unreliable	O
,	O
since	O
many	O
of	O
the	O
categories	O
are	O
ambiguous	O
or	O
ill	O
-	O
defined	O
,	O
(	O
e.g.	O
'	O
Leisure	O
'	O
which	O
mainly	O
includes	O
gaming	O
podcasts	O
but	O
also	O
general	O
leisure	O
topics	O
,	O
'	O
Kids	O
&	O
Family	O
'	O
which	O
includes	O
podcasts	O
for	O
kids	O
as	O
well	O
as	O
about	O
parenting	O
)	O
,	O
and	O
podcast	O
creators	O
may	O
not	O
always	O
select	O
the	O
most	O
appropriate	O
categories	O
(	O
Sharpe	O
,	O
2020	O
)	O
.	O

Furthermore	O
,	O
podcasts	O
span	O
multiple	O
themes	O
and	O
structures	O
,	O
making	O
the	O
assignment	O
of	O
one	O
or	O
two	O
categories	O
per	O
podcast	O
too	O
restrictive	O
.	O

Instead	O
,	O
we	O
fit	O
an	O
LDA	B-MethodName
topic	I-MethodName
model	O
(	O
Blei	O
et	O
al	O
.	O
,	O

2003	O
)	O
with	O
100	B-HyperparameterValue
topics	B-HyperparameterName
3	O
to	O
transcripts	O
of	O
the	O
entire	O
100k	B-HyperparameterValue
podcast	O
corpus	O
as	O
in	O
previous	O
works	O
Yang	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
represent	O
each	O
episode	O
by	O
the	O
topic	O
distribution	O
,	O
and	O
measure	O
topic	O
proportions	O
relative	O
to	O
the	O
target	O
metrics	O
in	O
order	O
to	O
contextualize	O
our	O
results	O
on	O
stylistic	O
features	O
.	O

Table	O
2	O
shows	O
a	O
sample	O
of	O
the	O
inferred	O
topics	O
.	O

We	O
define	O
a	O
set	O
of	O
explainable	O
linguistic	O
features	O
that	O
are	O
hypothesized	O
to	O
affect	O
engagement	O
.	O

These	O
features	O
have	O
been	O
drawn	O
from	O
different	O
podcasting	O
advice	O
blogs	O
,	O
alongside	O
some	O
of	O
our	O
own	O
intuitions	O
.	O

Length	B-HyperparameterName
Descriptions	I-HyperparameterName
are	O
known	O
to	O
be	O
important	O
for	O
listeners	O
on	O
their	O
first	O
encounter	O
with	O
the	O
pod	O
-	O
cast	O
.	O

We	O
also	O
measure	O
audio	B-HyperparameterName
duration	I-HyperparameterName
,	O
since	O
surveys	O
show	O
it	O
is	O
a	O
consideration	O
(	O
McLean	O
,	O
2020).Proportion	O
of	O
ads	O
and	O
show	O
notes	O
Descriptions	O
of	O
well	O
-	O
known	O
podcasts	O
tend	O
to	O
contain	O
advertisements	O
of	O
other	O
podcasts	O
made	O
by	O
the	O
same	O
network	O
,	O
links	O
to	O
the	O
hosts	O
'	O
or	O
guests	O
'	O
social	O
media	O
presence	O
and	O
websites	O
,	O
or	O
show	O
notes	O
and	O
transcripts	O
,	O
and	O
podcast	O
creators	O
are	O
often	O
advised	O
to	O
include	O
such	O
information	O
(	O
Dennis	O
,	O
2020	O
)	O
,	O
and	O
surveys	O
have	O
shown	O
that	O
the	O
majority	O
of	O
podcast	O
listeners	O
do	O
not	O
mind	O
sponsor	O
ads	O
in	O
the	O
content	O
(	O
McLean	O
,	O
2020	O
)	O
.	O

We	O
measure	O
the	O
the	O
proportion	O
of	O
text	O
detected	O
on	O
episode	O
descriptions	O
by	O
the	O
extraneous	O
content	O
classifier	O
described	O
in	O
§	O
3.1	O
.	O

The	O
proportion	O
of	O
ads	O
in	O
transcripts	O
is	O
given	O
by	O
a	O
manually	O
identified	B-MethodName
LDA	I-MethodName
topic	O
that	O
corresponds	O
to	O
words	O
indicative	O
of	O
ads	O
.	O

Faithfulness	O
of	O
episode	O
descriptions	O
to	O
transcripts	O
Length	O
is	O
a	O
weak	O
signal	O
of	O
informativeness	O
.	O

Do	O
listeners	O
seem	O
to	O
prefer	O
descriptions	O
that	O
accurately	O
convey	O
the	O
topics	O
and	O
synopsis	O
of	O
the	O
episode	O
?	O
We	O
measure	O
faithfulness	O
of	O
the	O
episode	O
description	O
to	O
the	O
first	O
ten	O
minutes	O
of	O
the	O
transcript	O
as	O
the	O
cosine	B-MethodName
similarity	I-MethodName
between	O
the	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
bag	I-MethodName
of	I-MethodName
words	I-MethodName
representation	O
of	O
both	O
texts	O
.	O

While	O
we	O
do	O
not	O
have	O
ground	O
-	O
truth	O
labels	O
to	O
evaluate	O
this	O
definition	O
of	O
faithfulness	O
,	O
we	O
assessed	O
it	O
to	O
be	O
a	O
good	O
heuristic	O
by	O
anecdotally	O
reviewing	O
some	O
examples	O
.	O

4	O
Distinctiveness	O
Podcast	O
creators	O
are	O
often	O
encouraged	O
to	O
develop	O
a	O
distinctive	O
style	O
(	O
Gray	O
,	O
2021a	O
)	O
.	O

We	O
define	O
distinctiveness	O
as	O
the	O
perplexity	B-MetricName
of	O
the	O
given	O
text	O
under	O
a	O
unigram	B-MethodName
language	I-MethodName
model	I-MethodName
trained	O
over	O
all	O
the	O
episodes	O
in	O
the	O
dataset	O
.	O

To	O
control	O
for	O
length	O
,	O
we	O
follow	O
the	O
protocol	O
in	O
Zhang	O
et	O
al	O
.	O
(	O

2019	O
)	O
of	O
randomly	O
sampling	O
a	O
constant	O
number	O
of	O
words	O
from	O
each	O
text	O
and	O
taking	O
the	O
mean	B-MetricName
cross	I-MetricName
entropy	I-MetricName
over	O
a	O
few	O
samples	O
.	O

5	O
Reading	O
Grade	O
Level	O
Similarly	O
to	O
Zong	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
we	O
make	O
two	O
measurements	O
:	O
the	O
Flesch	B-MetricName
-	I-MetricName
Kincaid	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Flesch	O
,	O
1948	O
)	O
that	O
measures	O
the	O
number	O
of	O
syllables	O
per	O
word	O
and	O
the	O
number	O
of	O
words	O
per	O
sentence	O
,	O
and	O
the	O
Dale	B-MetricName
-	I-MetricName
Chall	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Chall	O
and	O
Dale	O
,	O
1948	O
)	O
which	O
measures	O
word	O
'	O
difficulty	O
'	O
using	O
a	O
lookup	O
table	O
.	O

While	O
caution	O
must	O
be	O
taken	O
on	O
interpreting	O
reading	O
grade	B-MetricName
level	I-MetricName
for	O
transcribed	O
speech	O
,	O
these	O
measures	O
have	O
been	O
explored	O
for	O
speech	O
in	O
prior	O
work	O
(	O
Schumacher	O
and	O
Eskenazi	O
,	O
2016).Vocabulary	O
Diversity	O
We	O
examine	O
whether	O
podcast	O
creators	O
of	O
high	O
engagement	O
podcasts	O
use	O
more	O
diverse	O
vocabularies	O
,	O
quantified	O
by	O
the	O
entropy	B-MetricName
of	O
the	O
unigram	O
words	O
in	O
the	O
text	O
,	O
motivated	O
by	O
advice	O
to	O
avoid	O
word	O
repetition	O
(	O
Bellis	O
,	O
2017	O
)	O
.	O

Popular	O
advice	O
often	O
encourages	O
podcast	O
creators	O
to	O
be	O
upbeat	O
and	O
positive	O
(	O
Briggman	O
,	O
2020	O
)	O
.	O

The	O
NRC	B-DatasetName
Emotion	I-DatasetName
Lexicon	I-DatasetName
(	O
Mohammad	O
and	O
Turney	O
,	O
2013	O
)	O
contains	O
positive	O
and	O
negative	O
sentiment	O
assignments	O
,	O
as	O
well	O
as	O
emotions	O
such	O
as	O
anger	O
,	O
trust	O
,	O
and	O
fear	O
,	O
for	O
14182	O
words	O
.	O

6	O
We	O
measure	O
the	O
proportion	O
of	O
words	O
associated	O
to	O
each	O
of	O
the	O
emotions	O
and	O
sentiments	O
.	O

Since	O
a	O
lexicon	O
lookup	O
for	O
sentiment	O
is	O
naturally	O
limited	O
in	O
that	O
it	O
does	O
not	O
account	O
for	O
compositionality	O
and	O
can	O
not	O
model	O
words	O
and	O
variants	O
that	O
are	O
missing	O
in	O
the	O
lexicon	O
,	O
we	O
also	O
apply	O
a	O
fullsentence	O
classifier	O
,	O
the	O
sentiment	O
model	O
from	O
the	O
Google	B-MethodName
Natural	I-MethodName
Language	I-MethodName
API	I-MethodName
7	I-MethodName
.	O

The	O
output	O
of	O
the	O
classifier	O
is	O
a	O
score	O
between	O
+1	O
and	O
−1	O
for	O
each	O
sentence	O
.	O

We	O
define	O
positive	O
and	O
negative	O
polarities	O
for	O
each	O
text	O
as	O
as	O
the	O
proportion	O
of	O
sentences	O
in	O
the	O
text	O
with	O
highly	O
positive	O
(	O
over	O
+0.5	O
)	O
or	O
highly	O
negative	O
(	O
under	O
−0.5	O
)	O
scores	O
.	O

Syntax	O
Syntactic	O
features	O
are	O
measured	O
by	O
the	O
relative	O
frequencies	O
of	O
each	O
part	O
-	O
of	O
-	O
speech	O
tag	O
.	O

While	O
previous	O
work	O
of	O
this	O
nature	O
finds	O
strong	O
effects	O
of	O
syntactic	O
patterns	O
from	O
parses	O
(	O
Ganjigunte	O
Ashok	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
we	O
find	O
that	O
the	O
noisy	O
speech	O
transcripts	O
result	O
in	O
particularly	O
noisy	O
parses	O
from	O
off	O
-	O
the	O
-	O
shelf	O
parsers	O
.	O

Swearing	O
and	O
fillers	O
We	O
conjecture	O
that	O
podcasts	O
with	O
swearing	O
and	O
adult	O
language	O
may	O
not	O
have	O
broad	O
appeal	O
.	O

Public	O
speaking	O
recommendations	O
in	O
podcasting	O
guides	O
(	O
Coips	O
and	O
Kramer	O
,	O
2020	O
)	O
emphasize	O
the	O
reduction	O
of	O
filler	O
words	O
like	O
'	O
yeah	O
'	O
or	O
'	O
okay	O
'	O
,	O
and	O
the	O
use	O
of	O
professional	O
speech	O
.	O

We	O
attempted	O
to	O
manually	O
define	O
lexicons	O
of	O
these	O
types	O
of	O
categories	O
,	O
but	O
found	O
that	O
it	O
is	O
challenging	O
and	O
prone	O
to	O
human	O
biases	O
,	O
especially	O
given	O
the	O
novel	O
domain	O
and	O
automatic	O
transcripts	O
.	O

Instead	O
,	O
we	O
take	O
advantage	O
of	O
the	O
observation	O
that	O
some	O
of	O
the	O
topics	O
inferred	O
by	O
the	B-MethodName
LDA	I-MethodName
model	O
correspond	O
to	O
swear	O
words	O
and	O
filler	O
terms	O
,	O
and	O
measure	O
the	O
proportions	O
of	O
these	O
topics	O
.	O

Speech	O
Rate	O
and	O
Non	O
-	O
Speech	O
Time	O
Podcast	O
creators	O
are	O
often	O
encouraged	O
to	O
speak	O
slowly	O
,	O
since	O
novice	O
speakers	O
tend	O
to	O
rush	O
their	O
delivery	O
(	O
Gray	O
,	O
2021b	O
)	O
.	O

Since	O
the	O
transcripts	O
in	O
the	O
dataset	O
contain	O
time	O
alignments	O
of	O
each	O
word	O
,	O
we	O
measure	O
the	O
duration	O
of	O
speech	O
segments	O
in	O
the	O
audio	O
,	O
giving	O
us	O
the	O
speech	O
rate	O
in	O
terms	O
of	O
words	O
per	O
minute	O
.	O

We	O
also	O
measure	O
the	O
amount	O
of	O
time	O
spent	O
on	O
non	O
-	O
speech	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
different	O
linguistic	O
features	O
by	O
comparing	O
group	O
means	O
between	O
the	O
top	O
and	O
bottom	O
25	O
%	O
of	O
podcasts	O
by	O
engagement	O
within	O
each	O
popularity	O
quartile	O
(	O
approximately	O
335	O
podcasts	O
per	O
group	O
)	O
with	O
bootstrapped	O
Welch	B-MethodName
's	I-MethodName
ttests	I-MethodName
.	O

We	O
report	O
the	O
group	O
mean	O
differences	O
of	B-MethodName
LDA	I-MethodName
topic	O
proportions	O
in	O
order	O
to	O
contextualize	O
results	O
on	O
the	O
other	O
features	O
.	O

For	O
LDA	B-MethodName
features	O
,	O
we	O
note	O
significance	O
after	O
a	O
Bonferroni	B-MetricName
correction	I-MetricName
of	I-MetricName
α	I-MetricName
=	O
0.05/100	B-MetricValue
,	O
and	O
for	O
the	O
other	O
linguistic	O
features	O
,	O
a	O
Bonferroni	B-HyperparameterName
correction	I-HyperparameterName
of	I-HyperparameterName
α	I-HyperparameterName
=	O
0.05/30.In	B-HyperparameterValue
the	O
results	O
,	O
'	O
description	O
'	O
refers	O
to	O
the	O
concatenation	O
of	O
the	O
show	O
description	O
and	O
the	O
representative	O
episode	O
's	O
description	O
.	O

When	O
there	O
is	O
an	O
effect	O
from	O
the	O
show	O
description	O
but	O
not	O
the	O
episode	O
's	O
or	O
vice	O
versa	O
,	O
they	O
are	O
explicitly	O
identified	O
as	O
such	O
.	O

Among	O
the	O
podcasts	O
in	O
the	O
top	O
popularity	O
quartile	O
,	O
high	O
engagement	O
is	O
associated	O
with	O
topics	O
around	O
lifestyle	O
and	O
culture	O
,	O
mental	O
health	O
,	O
spirituality	O
,	O
and	O
crime	O
,	O
while	O
in	O
the	O
lower	O
popularity	O
quartiles	O
,	O
high	O
engagement	O
podcasts	O
include	O
those	O
about	O
investing	O
,	O
working	O
out	O
,	O
careers	O
,	O
business	O
,	O
parenting	O
,	O
health	O
,	O
art	O
,	O
and	O
relationships	O
.	O

Table	O
3	O
shows	O
the	O
features	O
with	O
significant	O
differences	O
across	O
between	O
the	O
high	O
and	O
low	O
engagement	O
groups	O
.	O

We	O
review	O
the	O
main	O
takeaways	O
from	O
these	O
results	O
.	O

High	O
engagement	O
podcasts	O
are	O
longer	O
,	O
and	O
have	O
appropriate	O
descriptions	O
Across	O
all	O
quartiles	O
,	O
podcasts	O
with	O
high	O
engagement	O
tend	O
to	O
be	O
longer	O
on	O
the	O
whole	O
(	O
contrary	O
to	O
advice	O
to	O
keep	O
episodes	O
short	O
)	O
,	O
and	O
contain	O
less	O
non	O
-	O
speech	O
in	O
the	O
first	O
ten	O
minutes	O
than	O
the	O
low	O
engagement	O
group	O
.	O

They	O
also	O
have	O
descriptions	O
that	O
are	O
more	O
similar	O
to	O
the	O
first	O
ten	O
minutes	O
of	O
the	O
transcripts	O
,	O
which	O
may	O
be	O
because	O
long	O
,	O
faithful	O
descriptions	O
better	O
prepare	O
listeners	O
for	O
the	O
episode	O
.	O

The	O
correlation	O
between	O
ads	O
and	O
engagement	O
is	O
mixed	O
Large	O
amounts	O
of	O
ads	O
in	O
transcripts	O
are	O
associated	O
with	O
lower	O
engagement	O
in	O
all	O
but	O
the	O
bottom	O
popularity	O
quartile	O
.	O

While	O
this	O
may	O
be	O
explained	O
by	O
the	O
fact	O
that	O
many	O
listeners	O
skip	O
over	O
ads	O
in	O
the	O
audio	O
stream	O
,	O
the	O
effect	O
is	O
strong	O
enough	O
to	O
indicate	O
that	O
ads	O
seem	O
to	O
hurt	O
engagement	O
,	O
even	O
though	O
surveys	O
report	O
that	O
most	O
listeners	O
do	O
not	O
mind	O
ads	O
.	O

The	O
negative	O
association	O
could	O
be	O
a	O
result	O
of	O
our	O
dataset	O
being	O
constrained	O
to	O
first	O
-	O
time	O
listeners	O
;	O
further	O
analysis	O
needs	O
to	O
be	O
done	O
to	O
understand	O
if	O
it	O
holds	O
of	O
returning	O
listeners	O
.	O

Ads	O
in	O
episode	O
descriptions	O
,	O
on	O
the	O
other	O
hand	O
,	O
do	O
not	O
hurt	O
engagement	O
on	O
the	O
whole	O
,	O
and	O
in	O
fact	O
,	O
are	O
associated	O
with	O
higher	O
engagement	O
in	O
the	O
top	O
quartile	O
,	O
likely	O
because	O
much	O
of	O
the	O
detected	O
'	O
ad	O
'	O
content	O
in	O
popular	O
podcasts	O
consists	O
of	O
promotional	O
material	O
about	O
the	O
podcast	O
itself	O
,	O
which	O
often	O
includes	O
useful	O
information	O
such	O
as	O
links	O
to	O
the	O
hosts	O
'	O
websites	O
and	O
show	O
notes	O
.	O

High	O
engagement	O
podcasts	O
tend	O
to	O
use	O
diverse	O
and	O
mainstream	O
language	O
Vocabulary	O
diversity	O
in	O
descriptions	O
and	O
transcripts	O
is	O
consistently	O
larger	O
in	O
the	O
high	O
engagement	O
group	O
,	O
as	O
is	O
reading	O
grade	O
level	O
.	O

High	O
engagement	O
podcasts	O
have	O
more	O
punctuation	O
in	O
their	O
descriptions	O
and	O
more	O
conjunctions	O
(	O
arising	O
from	O
the	O
use	O
of	O
long	O
sentences	O
)	O
,	O
adverbs	O
,	O
adpositions	O
,	O
and	O
determiners	O
in	O
their	O
transcripts	O
.	O

These	O
syntactic	O
features	O
correlate	O
with	O
the	O
topics	O
such	O
as	O
culture	O
,	O
mental	O
health	O
,	O
investing	O
,	O
and	O
art	O
.	O

At	O
the	O
same	O
time	O
,	O
surprisingly	O
,	O
high	O
engagement	O
podcasts	O
use	O
less	O
distinctive	O
language	O
compared	O
to	O
the	O
rest	O
of	O
the	O
corpus	O
than	O
the	O
low	O
engagement	O
group	O
.	O

On	O
closer	O
examination	O
,	O
we	O
find	O
that	O
podcasts	O
scoring	O
low	O
on	O
reading	O
grade	O
level	O
also	O
score	O
high	O
on	O
distinctiveness	O
.	O

High	O
engagement	O
podcasts	O
tend	O
to	O
contain	O
positive	O
sentiments	O
and	O
suspense	O
On	O
the	O
whole	O
,	O
high	O
engagement	O
is	O
associated	O
with	O
more	O
positive	O
and	O
less	O
negative	O
emotions	O
and	O
sentiment	O
.	O

This	O
relationship	O
is	O
stronger	O
outside	O
of	O
the	O
top	O
popularity	O
quartile	O
.	O

A	O
notable	O
exception	O
is	O
'	O
fear	O
'	O
in	O
the	O
top	O
popularity	O
quartile	O
,	O
which	O
is	O
explained	O
by	O
the	O
high	O
engagement	O
of	O
popular	O
crime	O
-	O
related	O
podcasts	O
.	O

High	O
engagement	O
podcasts	O
are	O
less	O
likely	O
to	O
contain	O
interjections	O
and	O
swearing	O
As	O
expected	O
,	O
words	O
such	O
as	O
'	O
oh	O
'	O
,	O
'	O
right	O
'	O
,	O
and	O
'	O
cool	O
'	O
in	O
contexts	O
that	O
the	O
tagger	O
infers	O
as	O
interjections	O
are	O
significantly	O
less	O
likely	O
to	O
occur	O
in	O
high	O
engagement	O
podcasts	O
.	O

Similarly	O
,	O
swearing	O
is	O
associated	O
with	O
low	O
engagement	O
.	O

Filler	O
words	O
are	O
only	O
negatively	O
associated	O
with	O
engagement	O
in	O
the	O
lowest	O
popularity	O
quartile	O
,	O
though	O
the	O
lack	O
of	O
correlation	O
in	O
other	O
quartiles	O
could	O
be	O
because	O
the	B-MethodName
LDA	I-MethodName
topics	O
representing	O
fillers	O
do	O
n't	O
model	O
context	O
,	O
and	O
therefore	O
do	O
not	O
capture	O
their	O
discourse	O
function	O
in	O
the	O
way	O
the	O
tagger	O
does	O
for	O
interjections	O
.	O

High	O
engagement	O
podcast	O
creators	O
tend	O
to	O
speak	O
relatively	O
fast	O
While	O
popular	O
advice	O
warns	O
presenters	O
against	O
rushing	O
their	O
speech	O
,	O
the	O
data	O
indicates	O
that	O
on	O
average	O
,	O
high	O
engagement	O
is	O
associated	O
with	O
high	O
speech	O
rates	O
,	O
which	O
is	O
also	O
a	O
finding	O
in	O
previous	O
work	O
(	O
Tsagkias	O
et	O
al	O
.	O
,	O

2010	O
)	O
.	O

Next	O
,	O
we	O
build	O
classifiers	O
to	O
automatically	O
distinguish	O
high	O
and	O
low	O
engagement	O
podcasts	O
.	O

The	O
prediction	O
task	O
is	O
treated	O
as	O
a	O
balanced	O
binary	O
classification	O
problem	O
.	O

We	O
make	O
a	O
single	O
dataset	O
for	O
podcasts	O
across	O
all	O
quartiles	O
by	O
aggregating	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
within	O
each	O
quartile	O
.	O

This	O
aggregation	O
is	O
to	O
ensure	O
fair	O
comparisons	O
of	O
podcasts	O
in	O
different	O
quartiles	O
,	O
since	O
a	O
stream	B-MetricName
rate	I-MetricName
value	O
that	O
is	O
considered	O
high	O
for	O
a	O
popular	O
podcast	O
,	O
for	O
example	O
,	O
may	O
not	O
be	O
so	O
in	O
the	O
low	O
quartiles	O
.	O

Models	O
are	O
trained	O
and	O
evaluated	O
with	O
the	O
same	O
stratified	O
5	O
-	O
fold	O
cross	O
validation	O
splits	O
.	O

We	O
train	O
logistic	B-MethodName
regression	I-MethodName
classifiers	O
using	O
different	O
representations	O
of	O
the	O
content	O
:	O
the	O
linguistic	O
features	O
listed	O
previously	O
,	O
the	O
non	O
-	O
stylistic	O
LDA	B-MethodName
topic	O
proportions	O
,	O
and	O
bag	B-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
ngrams	I-MethodName
(	O
unigram	O
and	O
bigram	O
words	O
)	O
with	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
scoring	I-MethodName
.	O

In	O
addition	O
,	O
we	O
train	O
two	O
neural	O
classifiers	O
-a	O
feedforward	B-MethodName
neural	I-MethodName
network	I-MethodName
with	I-MethodName
a	I-MethodName
single	I-MethodName
hidden	I-MethodName
layer	I-MethodName
,	O
using	O
a	O
paragraph	O
vector	O
representation	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
of	O
the	O
document	O
as	O
input	O
8	O
,	O
and	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
uncased	O
English	O
model	O
9	O
with	O
a	O
classification	O
head	O
,	O
fine	O
-	O
tuned	O
on	O
this	O
task	O
.	O

With	O
the	O
linguistic	O
features	O
,	O
we	O
also	O
conduct	O
an	O
ablation	O
study	O
,	O
removing	O
one	O
group	O
of	O
features	O
at	O
a	O
time	O
,	O
to	O
estimate	O
their	O
contributions	O
to	O
predictive	O
performance	O
.	O

Prediction	O
accuracies	B-MetricName
(	O
Table	O
4	O
)	O
are	O
over	O
70	B-MetricValue
%	I-MetricValue
with	O
linguistic	O
features	O
only	O
,	O
indicating	O
that	O
the	O
features	O
that	O
we	O
have	O
identified	O
are	O
relatively	O
strong	O
predictors	O
of	O
engagement	O
.	O

The	O
reading	O
grade	O
level	O
of	O
descriptions	O
and	O
transcripts	O
makes	O
a	O
big	O
contribution	O
as	O
shown	O
in	O
the	O
ablation	O
results	O
,	O
as	O
do	O
the	O
syntactic	O
features	O
on	O
transcripts	O
.	O

Analysis	O
of	O
the	O
weights	O
of	O
the	O
bag	B-MethodName
of	I-MethodName
n	I-MethodName
-	I-MethodName
grams	I-MethodName
models	O
surface	O
patterns	O
in	O
language	O
usage	O
that	O
corroborate	O
our	O
analysis	O
on	O
linguistic	O
features	O
-swearing	O
and	O
negative	O
sentiment	O
is	O
predictive	O
of	O
low	O
engagement	O
,	O
for	O
example	O
.	O

They	O
also	O
suggest	O
subtle	O
dimensions	O
of	O
variation	O
to	O
complement	O
our	O
set	O
of	O
linguistic	O
features	O
.	O

In	O
Table	O
5	O
,	O
we	O
collect	O
some	O
of	O
the	O
most	O
predictive	O
terms	O
and	O
manually	O
group	O
them	O
into	O
classes	O
.	O

First	O
or	O
second	O
person	O
pronouns	O
are	O
predictive	O
of	O
high	O
engagement	O
in	O
contrast	O
to	O
third	O
person	O
pronouns	O
.	O

This	O
aligns	O
with	O
the	O
finding	O
by	O
Tsagkias	O
et	O
al	O
.	O
(	O

2010	O
)	O
that	O
personal	O
experiences	O
are	O
favored	O
in	O
high	O
engagement	O
podcasts	O
.	O

While	O
fillers	O
exist	O
in	O
both	O
groups	O
,	O
the	O
specific	O
terms	O
used	O
are	O
different	O
,	O
with	O
'	O
kind	O
of	O
'	O
and	O
'	O
literally	O
'	O
being	O
predictive	O
of	O
high	O
engagement	O
in	O
contrast	O
to	O
'	O
um	O
'	O
and	O
'	O
but	O
like	O
'	O
.	O

The	O
conjunction	O
'	O
and	O
'	O
is	O
preferred	O
by	O
high	O
engagement	O
podcasts	O
over	O
'	O
but	O
'	O
,	O
and	O
'	O
so	O
'	O
over	O
'	O
because	O
'	O
.	O

Interrogative	O
words	O
are	O
more	O
predictive	O
of	O
high	O
engagement	O
with	O
the	O
exception	O
of	O
'	O
which	O
'	O
,	O
as	O
are	O
open	O
-	O
ended	O
and	O
future	O
looking	O
terms	O
like	O
'	O
asking	O
'	O
,	O
'	O
explore	O
'	O
,	O
and	O
'	O
started	O
'	O
over	O
grounded	O
,	O
immediate	O
terms	O
like	O
'	O
make	O
'	O
,	O
'	O
use	O
'	O
,	O
'	O
today	O
'	O
,	O
and	O
'	O
quickly	O
'	O
.	O

We	O
emphasize	O
that	O
this	O
is	O
a	O
small	O
qualitative	O
analysis	O
of	O
the	O
most	O
predictive	O
features	O
,	O
and	O
High	O
engagement	O
he	O
,	O
she	O
,	O
they	O
,	O
his	O
,	O
her	O
,	O
him	O
,	O
it	O
me	O
,	O
you	O
,	O
us	O
,	O
we	O
,	O
my	O
,	O
our	O
,	O
their	O
,	O
myself	O
,	O
someone	O
um	O
,	O
gon	O
na	O
,	O
oh	O
,	O
like	O
like	O
,	O
because	O
like	O
,	O
but	O
like	O
,	O
such	O
as	O
,	O
okay	O
,	O
all	O
right	O
,	O
you	O
guys	O
,	O
basically	O
and	O
and	O
,	O
sort	O
of	O
,	O
kind	O
of	O
,	O
was	O
like	O
,	O
you	O
know	O
,	O
quite	O
,	O
literally	O
more	O
work	O
needs	O
to	O
done	O
to	O
establish	O
which	O
terms	O
are	O
actually	O
used	O
in	O
semantically	O
similar	O
contexts	O
in	O
the	O
data	O
.	O

We	O
leave	O
explorations	O
of	O
computable	O
features	O
that	O
encode	O
these	O
aspects	O
to	O
future	O
work	O
.	O

On	O
the	O
whole	O
,	O
models	O
with	O
lexical	O
content	O
features	O
perform	O
better	O
than	O
the	O
linguistic	O
signals	O
,	O
which	O
is	O
expected	O
since	O
these	O
models	O
encode	O
more	O
information	O
than	O
a	O
small	O
set	O
of	O
hand	O
-	O
designed	O
features	O
.	O

The	O
BERT	B-MethodName
classifiers	O
achieve	O
nearly	O
81	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
indicating	O
that	O
podcast	O
content	O
is	O
highly	O
predictive	O
of	O
engagement	O
.	O

Table	O
6	O
shows	O
how	O
classification	O
accuracies	O
change	O
when	O
the	O
task	O
is	O
to	O
distinguish	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
,	O
with	O
K	B-HyperparameterName
ranging	O
from	O
10	B-HyperparameterValue
to	I-HyperparameterValue
50	I-HyperparameterValue
(	O
all	O
reports	O
thus	O
far	O
have	O
been	O
with	O
K	B-HyperparameterName
=	O
25	B-HyperparameterValue
)	O
.	O

Performance	O
drops	O
as	O
K	B-HyperparameterName
increases	O
(	O
and	O
the	O
gap	O
between	O
the	O
two	O
sets	O
thereby	O
decreases	O
)	O
although	O
the	O
amount	O
of	O
training	O
data	O
goes	O
up	O
,	O
showing	O
that	O
the	O
differences	O
in	O
language	O
usage	O
are	O
more	O
predictable	O
at	O
the	O
extremes	O
of	O
engagement	O
.	O

To	O
understand	O
how	O
the	O
relationship	O
between	O
linguistic	O
features	O
and	O
engagement	O
in	O
podcasts	O
compares	O
to	O
other	O
spoken	O
media	O
,	O
we	O
carry	O
out	O
the	O
same	O
analysis	O
on	O
a	O
corpus	O
of	O
2480	O
talks	O
from	O
the	O
TED	O
Conferences	O
(	O
Tanveer	O
et	O
al	O
.	O
,	O

2018;Acharyya	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

While	O
we	O
do	O
n't	O
have	O
access	O
to	O
the	O
stream	B-MetricName
rate	I-MetricName
of	O
the	O
lectures	O
,	O
the	O
data	O
includes	O
the	O
total	O
view	O
count	O
and	O
ratings	O
.	O

We	O
define	O
engagement	O
as	O
the	O
proportion	O
of	O
total	O
views	O
that	O
left	O
a	O
rating	O
,	O
with	O
the	O
rationale	O
that	O
the	O
act	O
of	O
leaving	O
a	O
rating	O
is	O
roughly	O
analogous	O
to	O
the	O
podcast	O
engagement	O
metric	O
of	O
listening	O
for	O
several	O
minutes	O
.	O

Another	O
point	O
of	O
difference	O
between	O
this	O
dataset	O
and	O
the	O
podcasts	O
is	O
that	O
the	O
TED	O
lectures	O
are	O
manually	O
transcribed	O
.	O

Therefore	O
,	O
the	O
data	O
is	O
not	O
directly	O
comparable	O
to	O
the	O
podcast	O
dataset	O
,	O
but	O
we	O
carry	O
out	O
the	O
experiment	O
to	O
try	O
to	O
identify	O
which	O
features	O
of	O
high	O
-	O
engagement	O
speech	O
may	O
be	O
universal	O
,	O
and	O
which	O
are	O
podcast	O
-	O
specific	O
.	O

We	O
test	O
the	O
same	O
features	O
that	O
we	O
formulated	O
for	O
podcasts	O
,	O
except	O
for	B-MethodName
LDA	I-MethodName
topic	O
distributions	O
(	O
due	O
to	O
the	O
small	O
size	O
of	O
the	O
TED	B-DatasetName
corpus	I-DatasetName
relative	O
to	O
the	O
full	O
100k+	O
podcast	O
data	O
)	O
,	O
and	O
ads	O
and	O
swear	O
words	O
since	O
these	O
occur	O
rarely	O
if	O
at	O
all	O
in	O
TED	O
talks	O
.	O

Table	O
7	O
shows	O
the	O
group	O
means	O
differences	O
between	O
high	O
and	O
low	O
engagement	O
lectures	O
.	O

On	O
the	O
whole	O
,	O
there	O
are	O
fewer	O
significant	O
differences	O
,	O
because	O
either	O
the	O
TED	O
data	O
is	O
more	O
homogenous	O
than	O
podcasts	O
,	O
the	O
metric	O
is	O
n't	O
directly	O
indicative	O
of	O
engagement	O
,	O
or	O
the	O
features	O
that	O
we	O
designed	O
for	O
podcasts	O
do	O
n't	O
apply	O
as	O
much	O
for	O
TED	O
talks	O
.	O

Like	O
podcasts	O
,	O
higher	O
engagement	O
lectures	O
are	O
longer	O
;	O
however	O
,	O
longer	O
and	O
more	O
faithful	O
descrip-	O
tions	O
are	O
actually	O
associated	O
with	O
lower	O
engagement	O
.	O

Vocabulary	O
diversity	O
is	O
associated	O
with	O
high	O
engagement	O
,	O
but	O
unlike	O
podcasts	O
,	O
high	O
engagement	O
lectures	O
have	O
lower	O
reading	O
grade	O
levels	O
.	O

Since	O
we	O
find	O
that	O
lecture	O
transcripts	O
measure	O
over	O
one	O
grade	O
level	O
higher	O
than	O
podcasts	O
,	O
it	O
could	O
be	O
that	O
after	O
a	O
point	O
,	O
simplicity	O
is	O
rewarded	O
.	O

Positive	O
emotions	O
are	O
more	O
significantly	O
associated	O
with	O
engagement	O
compared	O
to	O
the	O
podcast	O
data	O
,	O
which	O
may	O
be	O
because	O
of	O
the	O
inspirational	O
nature	O
of	O
the	O
talks	O
and	O
the	O
relative	O
paucity	O
of	O
crime	O
-	O
related	O
content	O
(	O
and	O
in	O
fact	O
,	O
positive	O
sentiment	O
overall	O
is	O
more	O
prevalent	O
compared	O
to	O
the	O
podcast	O
data	O
)	O
.	O

There	O
is	O
less	O
variation	O
in	O
syntactic	O
features	O
,	O
possibly	O
because	O
talks	O
are	O
scripted	O
and	O
follow	O
similar	O
templates	O
.	O

The	O
syntactic	O
features	O
with	O
correlations	O
tend	O
to	O
follow	O
similar	O
patterns	O
as	O
in	O
podcasts	O
.	O

On	O
the	O
prediction	O
task	O
,	O
we	O
achieve	O
up	O
to	O
71.15	B-MetricValue
%	I-MetricValue
(	O
Table	O
8)	O
accuracy	B-MetricName
using	O
only	O
linguistic	O
features	O
,	O
similar	O
to	O
the	O
performance	O
on	O
podcasts	O
.	O

However	O
,	O
the	O
bag	B-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
ngrams	I-MethodName
features	O
are	O
less	O
predictive	O
than	O
linguistic	O
features	O
,	O
and	O
the	O
BERT	B-MethodName
model	O
only	O
matches	O
the	O
classifier	O
with	O
linguistic	O
features	O
rather	O
than	O
exceeding	O
it	O
.	O

This	O
may	O
be	O
because	O
there	O
is	O
n't	O
as	O
much	O
variation	O
in	O
topical	O
content	O
as	O
in	O
podcasts	O
.	O

Our	O
paper	O
centers	O
five	O
minute	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
target	O
metric	O
for	O
analysis	O
and	O
prediction	O
.	O

Systems	O
optimized	O
for	O
engagement	O
on	O
social	O
media	O
platforms	O
have	O
the	O
potential	O
to	O
spread	O
misinformation	O
and	O
radical	O
content	O
(	O
Ribeiro	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
or	O
be	O
manipulated	O
by	O
bad	O
actors	O
(	O
Sehgal	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

On	O
the	O
other	O
side	O
of	O
the	O
coin	O
,	O
studies	O
have	O
found	O
that	O
algorithms	O
driven	O
by	O
engagement	O
do	O
not	O
spread	O
false	O
news	O
at	O
a	O
higher	O
rate	O
than	O
true	O
news	O
(	O
Vosoughi	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
and	O
that	O
under	O
certain	O
conditions	O
,	O
engagement	O
metrics	O
may	O
actually	O
reward	O
quality	O
content	O
(	O
Ciampaglia	O
et	O
al	O
.	O
,	O

2018).Aggregate	O
stream	B-MetricName
rate	I-MetricName
in	O
podcasts	O
is	O
a	O
specific	O
engagement	O
metric	O
distinct	O
from	O
metrics	O
and	O
media	O
in	O
previous	O
studies	O
.	O

There	O
is	O
limited	O
previous	O
work	O
on	O
engagement	O
in	O
podcasts	O
.	O

Holtz	O
et	O
al	O
.	O
(	O

2020	O
)	O
find	O
that	O
algorithms	O
driven	O
by	O
engagement	O
lead	O
to	O
less	O
diverse	O
recommendations	O
;	O
however	O
,	O
that	O
work	O
does	O
not	O
study	O
the	O
relationship	O
between	O
the	O
type	O
of	O
content	O
that	O
is	O
favored	O
by	O
the	O
engagement	O
metric	O
.	O

While	O
a	O
comprehensive	O
analysis	O
of	O
podcast	O
engagement	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
work	O
,	O
we	O
manually	O
examine	O
the	O
top	O
10	O
%	O
of	O
podcast	O
episodes	O
by	O
engagement	O
in	O
our	O
collection	O
,	O
a	O
total	O
of	O
537	O
episodes	O
.	O

As	O
we	O
noted	O
in	O
§	O
5.1.1	O
,	O
the	O
LDA	B-MethodName
topics	O
associated	O
with	O
high	O
engagement	O
are	O
broad	O
:	O
lifestyle	O
,	O
mental	O
health	O
,	O
spirituality	O
,	O
crime	O
,	O
investing	O
,	O
working	O
out	O
,	O
careers	O
,	O
business	O
,	O
parenting	O
,	O
health	O
,	O
art	O
,	O
and	O
relationships	O
.	O

Our	O
manual	O
audit	O
confirms	O
that	O
high	O
engagement	O
podcast	O
do	O
primarily	O
span	O
these	O
topics	O
.	O

In	O
particular	O
,	O
we	O
do	O
not	O
find	O
any	O
episodes	O
containing	O
harmful	O
content	O
,	O
incendiary	O
language	O
,	O
or	O
politically	O
controversial	O
topics	O
in	O
this	O
set	O
.	O

We	O
conclude	O
that	O
while	O
the	O
connection	O
between	O
any	O
absolute	O
measure	O
of	O
intrinsic	O
quality	O
and	O
engagement	O
is	O
unknown	O
,	O
high	O
engagement	O
in	O
our	O
study	O
does	O
not	O
correspond	O
to	O
harmful	O
content	O
.	O

This	O
paper	O
presents	O
the	O
first	O
quantitative	O
analysis	O
of	O
how	O
linguistic	O
style	O
and	O
textual	O
attributes	O
in	O
podcasts	O
relate	O
to	O
listener	O
engagement	O
using	O
automatically	O
computed	O
features	O
.	O

We	O
test	O
several	O
hypotheses	O
,	O
and	O
identify	O
factors	O
that	O
validate	O
popular	O
advice	O
on	O
podcast	O
creation	O
,	O
as	O
well	O
as	O
those	O
with	O
unexpected	O
correlations	O
.	O

Our	O
predictive	O
models	O
perform	O
well	O
at	O
distinguishing	O
high	O
and	O
low	O
engagement	O
podcasts	O
using	O
only	O
textual	O
information	O
.	O

Our	O
comparison	O
with	O
a	O
similar	O
task	O
on	O
TED	B-DatasetName
data	O
shows	O
similarities	O
and	O
differences	O
between	O
podcasts	O
and	O
public	O
lectures	O
vis	O
a	O
vis	O
engagement	O
.	O

Opportunities	O
for	O
future	O
research	O
include	O
the	O
investigation	O
of	O
other	O
podcast	O
creation	O
advice	O
based	O
on	O
paralinguistic	O
features	O
from	O
the	O
podcast	O
audio	O
(	O
such	O
as	O
pitch	O
and	O
intonation	O
)	O
,	O
speaker	O
identities	O
and	O
shifts	O
within	O
a	O
conversation	O
,	O
trajectories	O
of	O
linguistic	O
features	O
over	O
the	O
course	O
of	O
the	O
episode	O
,	O
and	O
models	O
using	O
manual	O
transcripts	O
.	O

Since	O
our	O
dataset	O
consists	O
of	O
a	O
few	O
thousand	O
podcasts	O
,	O
uses	O
automatically	O
generated	O
transcripts	O
,	O
and	O
only	O
contains	O
podcasts	O
from	O
publishers	O
owned	O
or	O
operated	O
by	O
Spotify	O
,	O
care	O
must	O
be	O
taken	O
when	O
generalizing	O
from	O
these	O
results	O
to	O
deploying	O
automatic	O
recommendation	O
systems	O
,	O
or	O
advising	O
podcast	O
creators	O
.	O

It	O
is	O
also	O
worth	O
noting	O
that	O
aggregated	O
engagement	O
data	O
may	O
reflect	O
the	O
language	O
preferences	O
of	O
the	O
dominant	O
community	O
,	O
and	O
may	O
be	O
biased	O
against	O
minority	O
cultural	O
and	O
linguistic	O
subcommunities	O
.	O

While	O
this	O
dataset	O
lacks	O
self	O
-	O
identified	O
labels	O
on	O
demographics	O
and	O
sociolinguistic	O
identities	O
,	O
there	O
are	O
opportunities	O
for	O
future	O
work	O
(	O
in	O
either	O
podcasts	O
or	O
other	O
media	O
)	O
to	O
collect	O
these	O
selfidentifications	O
in	O
order	O
to	O
study	O
questions	O
such	O
as	O
disparities	O
in	O
automatic	O
speech	O
recognition	O
performance	O
by	O
race	O
or	O
gender	O
(	O
Koenecke	O
et	O
al	O
.	O
,	O

2020;Tatman	O
,	O
2017	O
)	O
,	O
and	O
whether	O
engagement	O
is	O
biased	O
towards	O
certain	O
dialects	O
.	O

This	O
paper	O
defined	O
a	O
specific	O
metric	O
,	O
namely	O
,	O
the	O
rate	O
of	O
streaming	O
for	O
at	O
least	O
five	O
minutes	O
;	O
results	O
related	O
to	O
this	O
metric	O
may	O
or	O
may	O
not	O
apply	O
to	O
other	O
engagement	O
metrics	O
.	O

As	O
with	O
all	O
user	O
data	O
,	O
the	O
engagement	O
metric	O
is	O
influenced	O
by	O
the	O
interface	O
and	O
recommendations	O
of	O
the	O
streaming	O
platform	O
from	O
which	O
the	O
data	O
was	O
collected	O
,	O
and	O
may	O
not	O
translate	O
to	O
other	O
platforms	O
,	O
nor	O
reflect	O
an	O
objective	O
notion	O
of	O
listener	O
engagement	O
.	O

We	O
also	O
reiterate	O
(	O
from	O
§	O
7	O
)	O
that	O
listener	O
engagement	O
must	O
not	O
be	O
used	O
as	O
a	O
proxy	O
for	O
intrinsic	O
quality	O
or	O
success	O
.	O

It	O
must	O
also	O
be	O
emphasized	O
that	O
the	O
stylistic	O
associations	O
that	O
were	O
observed	O
to	O
distinguish	O
high	O
and	O
low	O
engagement	O
podcasts	O
in	O
this	O
particular	O
dataset	O
are	O
correlations	O
with	O
no	O
causality	O
established	O
,	O
and	O
therefore	O
must	O
be	O
interpreted	O
with	O
caution	O
.	O

We	O
thank	O
Ann	O
Clifton	O
,	O
Bernd	O
Huber	O
,	O
Jussi	O
Karlgren	O
,	O
Mi	O
Tian	O
,	O
and	O
Zahra	O
Nazari	O
for	O
their	O
input	O
and	O
discussions	O
.	O

Automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
is	O
interesting	O
,	O
owing	O
to	O
the	O
various	O
linguistic	O
peculiarities	O
present	O
in	O
the	O
language	O
.	O

The	O
Sanskrit	O
language	O
is	O
lexically	O
productive	O
,	O
undergoes	O
euphonic	O
assimilation	O
of	O
phones	O
at	O
the	O
word	O
boundaries	O
and	O
exhibits	O
variations	O
in	O
spelling	O
conventions	O
and	O
in	O
pronunciations	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
the	O
first	O
large	O
scale	O
study	O
of	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
,	O
with	O
an	O
emphasis	O
on	O
the	O
impact	O
of	O
unit	O
selection	O
in	O
Sanskrit	O
ASR	B-TaskName
.	O

In	O
this	O
work	O
,	O
we	O
release	O
a	O
78	O
hour	O
ASR	B-TaskName
dataset	O
for	O
Sanskrit	O
,	O
which	O
faithfully	O
captures	O
several	O
of	O
the	O
linguistic	O
characteristics	O
expressed	O
by	O
the	O
language	O
.	O

We	O
investigate	O
the	O
role	O
of	O
different	O
acoustic	O
model	O
and	O
language	O
model	O
units	O
in	O
ASR	B-TaskName
systems	O
for	O
Sanskrit	O
.	O

We	O
also	O
propose	O
a	O
new	O
modelling	O
unit	O
,	O
inspired	O
by	O
the	O
syllable	O
level	O
unit	O
selection	O
,	O
that	O
captures	O
character	O
sequences	O
from	O
one	O
vowel	O
in	O
the	O
word	O
to	O
the	O
next	O
vowel	O
.	O

We	O
also	O
highlight	O
the	O
importance	O
of	O
choosing	O
graphemic	O
representations	O
for	O
Sanskrit	O
and	O
show	O
the	O
impact	O
of	O
this	O
choice	O
on	O
word	B-MetricName
error	I-MetricName
rates	I-MetricName
(	O
WER	B-MetricName
)	O
.	O

Finally	O
,	O
we	O
extend	O
these	O
insights	O
from	O
Sanskrit	O
ASR	B-TaskName
for	O
building	O
ASR	B-TaskName
systems	O
in	O
two	O
other	O
Indic	O
languages	O
,	O
Gujarati	O
and	O
Telugu	O
.	O

For	O
both	O
these	O
languages	O
,	O
our	O
experimental	O
results	O
show	O
that	O
the	O
use	O
of	O
phonetic	O
based	O
graphemic	O
representations	O
in	O
ASR	B-TaskName
results	O
in	O
performance	O
improvements	O
as	O
compared	O
to	O
ASR	B-TaskName
systems	O
that	O
use	O
native	O
scripts	O
.	O

1	O
*	O
Joint	O
first	O
author	O
1	O
Dataset	O
and	O
code	O
can	O
be	O
accessed	O
from	O
www.cse.iitb.ac.in/~asr	O
and	O
https://github	O
.	O

com	O
/	O
cyfer0618	O
/	O
Vaksanca.git	O
.	O

Sanskrit	O
is	O
a	O
language	O
with	O
fairly	O
advanced	O
disciplines	O
of	O
phonetics	O
(	O
Śiks	O
̣ā	O
)	O
,	O
prosody	O
(	O
Chandas	O
)	O
,	O
and	O
grammar	O
(	O
Vyākaran	O
̣a	O
)	O
.	O

The	O
language	O
has	O
a	O
rich	O
oral	O
tradition	O
and	O
it	O
tends	O
to	O
follow	O
a	O
phonemic	O
-	O
orthography	O
resulting	O
in	O
systematic	O
grapheme	O
-	O
phoneme	O
correspondences	O
.	O

Connected	O
speech	O
leads	O
to	O
phonemic	O
transformations	O
in	O
utteracnes	O
,	O
and	O
in	O
Sanskrit	O
this	O
is	O
faithfully	O
preserved	O
in	O
writing	O
as	O
well	O
.	O

This	O
is	O
called	O
as	O
Sandhi	O
and	O
is	O
defined	O
as	O
the	O
euphonic	O
assimilation	O
of	O
sounds	O
,	O
i.e.	O
,	O
modification	O
and	O
fusion	O
of	O
sounds	O
,	O
at	O
or	O
across	O
the	O
boundaries	O
of	O
grammatical	O
units	O
(	O
Matthews	O
,	O
2007	O
,	O
p.	O
353	O
)	O
.	O

Phonemic	O
orthography	O
is	O
beneficial	O
for	O
a	O
language	O
,	O
when	O
it	O
comes	O
to	O
designing	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
Systems	O
(	O
ASR	B-TaskName
)	O
,	O
specifically	O
for	O
unit	O
selection	O
at	O
both	O
the	O
Acoustic	O
Model	O
(	O
AM	O
)	O
and	O
Language	O
Model	O
(	O
LM	O
)	O
levels	O
.	O

Regardless	O
of	O
the	O
aforementioned	O
commonalities	O
preserved	O
in	O
both	O
the	O
speech	O
and	O
text	O
in	O
Sanskrit	O
,	O
designing	O
a	O
large	O
scale	O
ASR	B-TaskName
system	O
raises	O
several	O
challenges	O
.	O

The	O
Unicode	O
encoding	O
for	O
the	O
native	O
scripts	O
in	O
Sanskrit	O
,	O
both	O
in	O
Roman	O
and	O
Devanāgari	O
,	O
does	O
not	O
preserve	O
the	O
correspondence	O
with	O
the	O
phonemic	O
encoding	O
.	O

Further	O
,	O
mapping	O
the	O
graphemes	O
in	O
Unicode	O
to	O
the	O
corresponding	O
phonemes	O
either	O
leads	O
to	O
ambiguity	O
and	O
redundancy	O
or	O
often	O
requires	O
multi	O
-	O
grapheme	O
combinations	O
.	O

The	O
language	O
is	O
lexically	O
productive	O
,	O
which	O
results	O
in	O
long	O
compound	O
words	O
with	O
multiple	O
components	O
in	O
usage	O
.	O

This	O
results	O
in	O
the	O
speakers	O
segmenting	O
the	O
compounds	O
at	O
arbitrary	O
lexeme	O
boundaries	O
of	O
the	O
compound	O
,	O
as	O
it	O
need	O
not	O
always	O
be	O
possible	O
to	O
utter	O
the	O
compound	O
in	O
one	O
breath	O
and	O
also	O
to	O
convey	O
the	O
meaning	O
clearly	O
.	O

Similarly	O
,	O
such	O
arbitrary	O
segmentations	O
at	O
the	O
word	O
boundaries	O
are	O
possible	O
in	O
utterance	O
of	O
long	O
text	O
sequences	O
where	O
multiple	O
lexical	O
items	O
are	O
fused	O
together	O
via	O
Sandhi	O
.	O

These	O
segmentations	O
are	O
accompanied	O
with	O
the	O
corresponding	O
Sandhi	O
based	O
transformations	O
,	O
resulting	O
in	O
a	O
new	O
phonetic	O
sequence	O
different	O
from	O
the	O
original	O
sequence	O
.	O

Finally	O
,	O
Sanskrit	O
might	O
be	O
one	O
of	O
those	O
rare	O
natural	O
languages	O
where	O
the	O
number	O
of	O
non	O
-	O
native	O
proficient	O
speakers	O
are	O
manifold	O
in	O
comparison	O
to	O
the	O
native	O
speakers	O
(	O
Krishna	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

This	O
makes	O
the	O
ASR	B-TaskName
task	O
further	O
challenging	O
,	O
as	O
the	O
speakers	O
are	O
prone	O
to	O
carry	O
their	O
influence	O
from	O
their	O
corresponding	O
mother	O
tongues	O
into	O
the	O
Sanskrit	O
utterances	O
as	O
well	O
.	O

While	O
there	O
exist	O
several	O
computational	O
models	O
for	O
processing	O
Sanskrit	O
texts	O
(	O
Kulkarni	O
,	O
2013;Kumar	O
et	O
al	O
.	O
,	O

2010;Shukla	O
et	O
al	O
.	O
,	O

2010;Kulkarni	O
et	O
al	O
.	O
,	O

2010a;Goyal	O
et	O
al	O
.	O
,	O

2012;Kulkarni	O
et	O
al	O
.	O
,	O

2010c;Mishra	O
et	O
al	O
.	O
,	O

2013;Saluja	O
et	O
al	O
.	O
,	O

2017;Anoop	O
and	O
Ramakrishnan	O
,	O
2019;Krishna	O
et	O
al	O
.	O
,	O

2021	O
)	O
,	O
large	O
scale	O
systems	O
for	O
processing	O
of	O
speech	O
in	O
Sanskrit	O
,	O
are	O
almost	O
non	O
-	O
existent	O
.	O

First	O
,	O
we	O
present	O
a	O
new	O
dataset	O
,	O
with	O
78	O
hours	O
of	O
speech	O
covering	O
about	O
46,000	O
sentences	O
,	O
for	O
ASR	B-TaskName
in	O
Sanskrit	O
.	O

Keeping	O
the	O
rich	O
and	O
long	O
cultural	O
heritage	O
the	O
language	O
carries	O
,	O
we	O
prepare	O
our	O
dataset	O
to	O
be	O
diverse	O
both	O
chronologically	O
and	O
in	O
terms	O
of	O
the	O
domain	O
coverage	O
.	O

Further	O
,	O
the	O
dataset	O
contains	O
utterances	O
from	O
27	O
different	O
speakers	O
,	O
representing	O
6	O
different	O
native	O
languages	O
.	O

The	O
dataset	O
splits	O
have	O
disjoint	O
speakers	O
,	O
with	O
12	O
in	O
the	O
training	O
and	O
5	O
each	O
in	O
the	O
validation	O
,	O
test	O
and	O
out	O
-	O
of	O
-	O
domain	O
test	O
data	O
sets	O
.	O

Further	O
,	O
we	O
explicitly	O
mark	O
the	O
segmentation	O
decisions	O
made	O
by	O
a	O
speaker	O
to	O
segment	O
long	O
compound	O
words	O
and	O
fused	O
phrases	O
and	O
include	O
the	O
corresponding	O
transformations	O
due	O
to	O
sandhi	O
.	O

Using	O
this	O
dataset	O
,	O
we	O
propose	O
a	O
new	O
,	O
largevocabulary	O
Sanskrit	O
ASR	B-TaskName
system	O
,	O
which	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
is	O
the	O
first	O
such	O
system	O
for	O
Sanskrit	O
.	O

The	O
phonemic	O
orthography	O
followed	O
in	O
Sanskrit	O
has	O
influenced	O
our	O
design	O
choices	O
in	O
terms	O
of	O
unit	O
selection	O
at	O
the	O
level	O
of	O
the	O
acoustic	O
and	O
language	O
models	O
.	O

We	O
investigate	O
three	O
different	O
encoding	O
schemes	O
used	O
to	O
model	O
LM	O
tokens	O
,	O
namely	O
,	O
word	O
-	O
based	O
encoding	O
,	O
byte	O
pair	O
encoding	O
(	O
BPE	O
)	O
and	O
a	O
new	O
vowel	O
split	O
encoding	O
inspired	O
by	O
existing	O
linguistic	O
theories	O
of	O
syllabic	O
structure	O
popularly	O
used	O
within	O
text	O
-	O
to	O
-	O
speech	O
systems	O
(	O
Kishore	O
and	O
Black	O
,	O
2003;Mishra	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

Further	O
,	O
to	O
address	O
the	O
redundancy	O
issues	O
in	O
Unicode	O
representations	O
,	O
we	O
make	O
use	O
of	O
the	O
Sanskrit	O
Library	O
Phonetic	O
(	O
SLP1	O
)	O
encoding	O
scheme	O
proposed	O
by	O
Scharf	O
and	O
Hyman	O
(	O
2011	O
)	O
.	O

SLP1	O
is	O
designed	O
such	O
that	O
it	O
preserves	O
the	O
phonemic	O
orthography	O
.	O

Building	O
on	O
the	O
study	O
by	O
Scharf	O
and	O
Hyman	O
(	O
2011	O
)	O
,	O
we	O
focus	O
on	O
two	O
graphemic	O
representations	O
only	O
,	O
viz	O
.	O
,	O

native	O
script	O
(	O
Devanagari	O
)	O
and	O
SLP1.Finally	O
,	O
we	O
extend	O
our	O
insights	O
to	O
model	O
ASR	B-TaskName
systems	O
for	O
two	O
more	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

Telugu	O
and	O
Gujarati	O
.	O

We	O
extend	O
the	O
SLP1	O
to	O
include	O
graphemes	O
relevant	O
for	O
these	O
languages	O
which	O
are	O
missing	O
from	O
Sanskrit	O
.	O

We	O
report	O
the	O
performance	O
of	O
these	O
ASR	B-TaskName
systems	O
on	O
two	O
publicly	O
available	O
ASR	B-TaskName
datasets	O
.	O

Our	O
main	O
contributions	O
in	O
this	O
work	O
are	O
:	O
1	O
)	O
We	O
present	O
(	O
in	O
Section	O
2	O
)	O
a	O
new	O
,	O
large	O
vocabulary	O
Sanskrit	O
ASR	B-TaskName
system	O
and	O
the	O
first	O
ever	O
ASRbased	B-TaskName
study	O
for	O
Sanskrit	O
using	O
a	O
new	O
,	O
large	O
and	O
diverse	O
,	O
labeled	O
speech	O
corpus	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañ	O
cayah	O
̣/	O
)	O
.	O

2	O
)	O
We	O
investigate	O
(	O
in	O
Sections	O
3	O
and	O
4	O
)	O
different	O
modeling	O
choices	O
for	O
both	O
acoustic	O
models	O
and	O
language	O
models	O
in	O
Sanskrit	O
ASR	B-TaskName
systems	O
,	O
along	O
with	O
different	O
graphemic	O
representations	O
.	O

We	O
propose	O
a	O
new	O
word	O
segmentation	O
technique	O
based	O
on	O
splitting	O
at	O
vowels	O
that	O
can	O
be	O
used	O
with	O
both	O
the	O
acoustic	O
model	O
and	O
the	O
language	O
model	O
.	O

3	O
)	O
We	O
also	O
contextualize	O
our	O
findings	O
for	O
Sanskrit	O
by	O
providing	O
comparisons	O
on	O
ASR	B-TaskName
systems	O
built	O
for	O
two	O
other	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

Gujarati	O
and	O
Telugu	O
.	O

from	O
one	O
topical	O
domain	O
to	O
another	O
,	O
specifically	O
one	O
Śāstra	O
(	O
branch	O
of	O
learning	O
)	O
to	O
another	O
(	O
Adiga	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Our	O
corpus	O
contain	O
samples	O
from	O
diverse	O
domains	O
,	O
including	O
philosophy	O
,	O
literature	O
,	O
commentary	O
on	O
poetry	O
and	O
grammar	O
.	O

It	O
also	O
includes	O
contemporary	O
recordings	O
such	O
as	O
stories	O
,	O
live	O
lectures	O
,	O
spiritual	O
discourse	O
and	O
radio	O
program	O
/	O
podcast	O
,	O
so	O
that	O
collecting	O
a	O
wide	O
range	O
of	O
Sanskrit	O
vocabulary	O
.	O

The	O
recordings	O
were	O
primarily	O
collected	O
with	O
the	O
help	O
of	O
volunteers	O
,	O
recording	O
their	O
speech	O
by	O
using	O
the	O
Recorder	O
app	O
on	O
Android	O
phones	O
and	O
the	O
Audacity	O
platform	O
,	O
and	O
from	O
various	O
sources	O
available	O
online	O
.	O

3	O
oTranscribe	O
3	O
was	O
used	O
to	O
transcribe	O
the	O
audio	O
files	O
.	O

We	O
had	O
a	O
total	O
of	O
9	O
volunteers	O
for	O
recording	O
and	O
18	O
unique	O
speakers	O
for	O
the	O
content	O
collected	O
online	O
.	O

Each	O
of	O
these	O
speakers	O
are	O
proficient	O
Sanskrit	O
speakers	O
,	O
with	O
at	O
least	O
an	O
undergraduate	O
or	O
equivalent	O
degree	O
in	O
Sanskrit	O
.	O

These	O
speakers	O
are	O
native	O
speakers	O
of	O
one	O
of	O
the	O
6	O
following	O
Indic	O
languages	O
,	O
Hindi	O
,	O
Kannada	O
,	O
Malayalam	O
,	O
Marathi	O
,	O
Tamil	O
and	O
Telugu	O
.	O

In	O
Table	O
1	O
,	O
we	O
provide	O
the	O
details	O
of	O
the	O
training	O
/	O
validation	O
/	O
test	O
splits	O
for	O
our	O
corpus	O
,	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañcayah	O
̣/	O
)	O
.	O

The	O
speakers	O
in	O
all	O
these	O
4	O
splits	O
,	O
train	O
,	O
validation	O
,	O
test	O
,	O
and	O
out	O
-	O
of	O
-	O
domain	O
test	O
sets	O
are	O
disjoint	O
.	O

The	O
out	O
-	O
ofdomain	O
test	O
dataset	O
is	O
a	O
stratified	O
sampled	O
dataset	O
,	O
consisting	O
of	O
speech	O
samples	O
from	O
5	O
unique	O
speakers	O
.	O

Two	O
of	O
these	O
were	O
added	O
to	O
include	O
utterances	O
in	O
Sanskrit	O
from	O
speakers	O
with	O
more	O
pronounced	O
influence	O
of	O
their	O
native	O
languages	O
(	O
in	O
Hindi	O
and	O
Tamil	O
)	O
.	O

The	O
domain	O
of	O
the	O
training	O
dataset	O
primarily	O
is	O
a	O
speech	O
collection	O
of	O
readings	O
from	O
various	O
well	O
known	O
texts	O
.	O

Further	O
,	O
the	O
speech	O
in	O
the	O
training	O
data	O
is	O
in	O
accordance	O
with	O
the	O
traditional	O
phonetic	O
prescriptions	O
of	O
Sanskrit	O
(	O
Śiks	O
̣ā	O
)	O
.	O

Hence	O
,	O
the	O
remaining	O
three	O
in	O
the	O
out	O
-	O
of	O
-	O
domain	O
test	O
set	O
were	O
added	O
to	O
include	O
utterances	O
from	O
different	O
speech	O
domains	O
,	O
extempore	O
discourse	O
,	O
lecture	O
and	O
radio	O
program	O
,	O
differing	O
from	O
the	O
speech	O
domain	O
in	O
the	O
training	O
set	O
.	O

The	O
radio	O
program	O
is	O
studio	O
produced	O
,	O
while	O
the	O
other	O
two	O
are	O
live	O
recorded	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
various	O
linguistic	O
phenomena	O
that	O
are	O
important	O
to	O
consider	O
when	O
preparing	O
datasets	O
and	O
building	O
ASR	B-TaskName
systems	O
for	O
Sanskrit	O
with	O
the	O
help	O
of	O
illustrative	O
examples	O
.	O

3	O
The	O
URLs	O
of	O
the	O
tools	O
and	O
the	O
list	O
of	O
the	O
texts	O
we	O
use	O
are	O
available	O
in	O
the	O
supplementary	O
material	O
.	O

Word	O
Length	O
:	O
The	O
tokens	O
in	O
Sanskrit	O
texts	O
can	O
be	O
very	O
long	O
owing	O
to	O
"	O
Sandhi	O
"	O
and	O
the	O
lexically	O
productive	O
process	O
of	O
compounding	O
(	O
`	O
`	O
Samāsa	O
"	O
)	O
.	O

For	O
instance	O
,	O
consider	O
a	O
compound	O
word	O
,	O
वागथर्	O
प्र	O
तपत्तये	O
(	O
/vāgarthapratipattaye/	O
)	O
.	O

It	O
forms	O
a	O
19	O
letter	O
word	O
in	O
SLP1	O
(	O
vAgarTapratipattaye	O
)	O
,	O
and	O
is	O
formed	O
by	O
combining	O
the	O
three	O
Sanskrit	O
stems	O
वाक्	O
,	O
अथर्	O
,	O
प्र	O
तप	O
त्त	O
(	O
/vāk	O
,	O
artha	O
,	O
prati	O
patti/	O
)	O
,	O
as	O
per	O
the	O
rules	O
of	O
Sandhi	O
and	O
Samāsa	O
.	O

In	O
Figure	O
1	O
,	O
we	O
present	O
the	O
distribution	O
of	O
the	O
number	O
of	O
characters	O
(	O
in	O
SLP1	O
format	O
)	O
per	O
word	O
across	O
the	O
three	O
languages	O
that	O
we	O
experimentally	O
analyse	O
in	O
this	O
work	O
,	O
viz	O
.	O
,	O

Sanskrit	O
,	O
Telugu	O
and	O
Gujarati	O
.	O

The	O
plots	O
are	O
normalized	O
with	O
respect	O
to	O
the	O
size	O
of	O
the	O
vocabulary	O
.	O

The	O
average	O
word	O
length	O
is	O
much	O
higher	O
in	O
Sanskrit	O
(	O
10.75	O
)	O
compared	O
to	O
Gujarati	O
(	O
7.79	O
)	O
and	O
Telugu	O
(	O
9.35	O
)	O
.	O

Table	O
2	O
same	O
word	O
.	O

While	O
recognising	O
longer	O
sequences	O
due	O
to	O
sandhi	O
and	O
compounding	O
is	O
a	O
challenge	O
in	O
itself	O
,	O
the	O
external	O
sandhi	O
gives	O
rise	O
to	O
the	O
issue	O
of	O
arbitrary	O
points	O
of	O
segmentation	O
performed	O
by	O
speakers	O
at	O
the	O
time	O
of	O
utterance	O
.	O

Figure	O
2a	O
shows	O
text	O
-	O
sequence	O
where	O
the	O
sequence	O
contains	O
a	O
word	O
nityah	O
̣	O
and	O
a	O
compound	O
śabdārthasambandhah	O
̣	O
fused	O
together	O
via	O
Sandhi	O
.	O

Further	O
,	O
śabdārthasam	O
bandhah	O
̣	O
is	O
a	O
compound	O
with	O
śabda	O
,	O
artha	O
and	O
sambandha	O
as	O
its	O
components	O
.	O

While	O
it	O
is	O
expected	O
to	O
be	O
uttered	O
without	O
any	O
pause	O
after	O
considering	O
Samhitā	O
(	O
As	O
̣t	O
̣ādhyāyī-1	O
-	O
4	O
-	O
109	O
)	O
,	O
a	O
speaker	O
may	O
choose	O
to	O
segment	O
at	O
the	O
lexical	O
boundaries	O
as	O
shown	O
in	O
Figure	O
2c	O
and	O
Figure	O
2d	O
.	O

However	O
in	O
doing	O
so	O
,	O
a	O
proficient	O
speaker	O
would	O
prefer	O
a	O
sequence	O
similar	O
to	O
Figure	O
2a	O
or	O
2b	O
,	O
rather	O
than	O
Figure	O
2c	O
or	O
2d	O
.	O

This	O
is	O
because	O
,	O
the	O
former	O
two	O
,	O
though	O
result	O
in	O
phonetic	O
transformations	O
,	O
preserve	O
the	O
syntactic	O
and	O
semantic	O
validity	O
of	O
the	O
sequence	O
.	O

However	O
,	O
the	O
latter	O
do	O
not	O
preserve	O
the	O
syntactic	O
and	O
semantic	O
validity	O
of	O
the	O
sequence	O
.	O

Similarly	O
,	O
there	O
are	O
cases	O
where	O
there	O
can	O
be	O
phonetic	O
transformations	O
between	O
the	O
bound	O
morphemes	O
and	O
the	O
free	O
morpheme	O
of	O
a	O
word	O
.	O

These	O
transformations	O
do	O
not	O
result	O
in	O
any	O
modification	O
to	O
the	O
word	O
,	O
other	O
than	O
phonetic	O
variations	O
.	O

However	O
,	O
this	O
makes	O
it	O
challenging	O
for	O
an	O
ASR	B-TaskName
system	O
.	O

The	O
case	O
of	O
Diphthongs	O
is	O
the	O
quite	O
prevalent	O
under	O
these	O
cases	O
.	O

In	O
Diphthongs	O
,	O
which	O
can	O
occur	O
both	O
at	O
internal	O
or	O
external	O
sandhi	O
,	O
the	O
independent	O
vowel	O
can	O
only	O
occur	O
at	O
the	O
start	O
of	O
a	O
word	O
.	O

Any	O
vowel	O
appearing	O
in	O
the	O
middle	O
of	O
a	O
word	O
either	O
gets	O
converted	O
to	O
a	O
dependent	O
vowel	O
or	O
a	O
diphthong	O
.	O

When	O
a	O
word	O
ending	O
with	O
"	O
ए(/ē/	O
)	O
or	O
ओ(/ō/	O
)	O
"	O
and	O
followed	O
by	O
any	O
vowel	O
,	O
then	O
ending	O
will	O
be	O
changed	O
to	O
either	O
"	O
अय्	O
(	O
/ay/	O
)	O
or	O
अव्	O
(	O
/av/	O
)	O
respectively	O
"	O
or	O
"	O
अ(/a/	O
)	O
"	O
.	O

For	O
example	O
िवष्णो+इह(/vis	O
̣n	O
̣o+iha/	O
)	O
will	O
get	O
converted	O
to	O
िवष्णइह(/vis	O
̣n	O
̣aiha/	O
)	O
or	O
िवष्णिवह(/vis	O
̣n	O
̣aviha/	O
)	O
.	O

The	O
Unicode	O
encoding	O
for	O
the	O
native	O
scripts	O
in	O
Sanskrit	O
,	O
similar	O
to	O
several	O
indian	O
languages	O
,	O
does	O
not	O
preserve	O
the	O
correspondence	O
with	O
the	O
phonemic	O
encoding	O
.	O

Further	O
,	O
mapping	O
the	O
graphemes	O
in	O
Unicode	O
to	O
the	O
corresponding	O
phonemes	O
either	O
suffers	O
from	O
ambiguity	O
and	O
redundancy	O
or	O
often	O
requires	O
multigrapheme	O
combinations	O
.	O

For	O
instance	O
,	O
consider	O
the	O
word	O
वागथार्	O
िवव(/vāgarthāviva/	O
)	O
in	O
Sanskrit	O
.	O

Here	O
the	O
graphemes	O
in	O
Devanagari	O
'	O
व	O
◌	O
ा	O
ग	O
र	O
◌	O
्	O
थ	O
◌	O
ा	O
व	O
ि	O
◌	O
व'and	O
Roman	O
(	O
v	O
ā	O
g	O
a	O
r	O
t	O
h	O
ā	O
v	O
i	O
v	O
a	O
)	O
do	O
not	O
exhibit	O
a	O
one	O
-	O
to	O
-	O
one	O
mapping	O
with	O
the	O
phonemes	O
.	O

For	O
instance	O
,	O
a	O
single	O
grapheme	O
(	O
e.g.	O
,	O
ग	O
)	O
may	O
correspond	O
to	O
2	O
phonemes	O
while	O
two	O
graphemes	O
(	O
e.g.	O
,	O
र	O
◌	O
्	O
in	O
devanagari	O
,	O
'	O
t	O
h	O
'	O
in	O
roman	O
)	O
may	O
correspond	O
to	O
a	O
single	O
phoneme	O
.	O

In	O
this	O
section	O
,	O
we	O
discuss	O
different	O
alternatives	O
for	O
identifying	O
units	O
of	O
the	O
language	O
model	O
and	O
the	O
acoustic	O
model	O
that	O
we	O
subsequently	O
employ	O
in	O
our	O
experimental	O
evaluation	O
and	O
analysis	O
.	O

The	O
Unicode	O
standard	O
for	O
native	O
scripts	O
of	O
Sanskrit	O
:	O
Devanagari	O
,	O
Gujarati	O
and	O
Telugu	O
faces	O
challenge	O
for	O
computational	O
language	O
processing	O
due	O
to	O
redundancy	O
in	O
mappings	O
between	O
phonemes	O
and	O
graphemes	O
as	O
previously	O
discussed	O
.	O

So	O
for	O
Sanskrit	O
,	O
we	O
use	O
Sanskrit	O
Library	O
Phonetic	O
encodings	O
(	O
SLP1	O
)	O
designed	O
by	O
Scharf	O
and	O
Hyman	O
(	O
2011).This	O
encoding	O
gives	O
unique	O
one	O
-	O
to	O
-	O
one	O
mapping	O
to	O
the	O
phoneme	O
.	O

However	O
Gujarati	O
possesses	O
extra	O
native	O
characters	O
such	O
as	O
ઍ(/e/	O
)	O
,	O
ઑ(/o/	O
)	O
.	O

Telugu	O
also	O
possesses	O
extra	O
characters	O
such	O
as	O
ఎ(/e/	O
)	O
,	O
ఒ(/o/	O
)	O
,	O
and	O
ఱ(/r/	O
)	O
.	O

So	O
we	O
extend	O
SLP1	O
to	O
fit	O
to	O
the	O
character	O
set	O
of	O
Gujarati	O
and	O
Telugu	O
in	O
our	O
experiments	O
.	O

One	O
possibility	O
for	O
deriving	O
subword	O
units	O
for	O
the	O
language	O
modeling	O
is	O
to	O
segment	O
words	O
in	O
Sanskrit	O
based	O
on	O
Sandhi	O
rules	O
.	O

However	O
,	O
Sandhi	O
splitting	O
can	O
change	O
some	O
phonemes	O
corresponding	O
to	O
the	O
words	O
in	O
almost	O
all	O
cases	O
.	O

Consider	O
the	O
word	O
रामाये	O
दम्	O
=	O
रामाय+इदम्	O
(	O
/rāmāyedam/	O
=	O
/rāmāya/+/idam/	O
)	O
,	O
wherein	O
the	O
vowel	O
ए	O
(	O
/e/	O
)	O
is	O
changed	O
into	O
अ+इ	O
(	O
/a/+/i/	O
)	O
after	O
performing	O
Sandhi	O
-	O
based	O
splitting	O
.	O

This	O
leads	O
to	O
a	O
mismatch	O
between	O
the	O
speech	O
transcript	O
and	O
the	O
speech	O
audio	O
,	O
potentially	O
creating	O
further	O
complications	O
for	O
ASR	B-TaskName
.	O

Byte	O
pair	O
encoding	O
(	O
BPE	O
)	O
is	O
a	O
simple	O
data	O
compression	O
algorithm	O
that	O
iteratively	O
replaces	O
the	O
frequently	O
occurring	O
subword	O
units	O
with	O
a	O
single	O
unused	O
byte	O
(	O
Gage	O
,	O
1994	O
)	O
.	O

This	O
technique	O
was	O
first	O
adopted	O
to	O
model	O
rare	O
words	O
using	O
subword	O
units	O
in	O
neural	O
machine	O
translation	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O

2016	O
)	O
.	O

Interestingly	O
,	O
BPE	O
has	O
been	O
explored	O
for	O
learning	O
new	O
vocabulary	O
for	O
poetry	O
to	O
prose	O
conversion	O
in	O
Sanskrit	O
(	O
Krishna	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

We	O
consider	O
the	O
benefits	O
of	O
using	O
BPE	O
as	O
a	O
subword	O
unit	O
for	O
Sanskrit	O
ASR.While	B-TaskName
BPE	O
is	O
a	O
purely	O
data	O
-	O
driven	O
segmentation	O
strategy	O
,	O
we	O
next	O
present	O
a	O
linguistically	O
motivated	O
segmentation	O
approach	O
that	O
might	O
be	O
aligned	O
with	O
finding	O
syllable	O
units	O
for	O
ASR	B-TaskName
that	O
are	O
more	O
phonetically	O
compliant	O
.	O

We	O
refer	O
to	O
this	O
technique	O
as	O
vowel	O
segmentation	O
.	O

Splitting	O
the	O
tokens	O
based	O
on	O
vowels	O
and	O
adjacent	O
consonants	O
is	O
inspired	O
by	O
the	O
identification	O
of	O
metres	O
in	O
Sanskrit	O
prosody	O
,	O
where	O
the	O
metre	O
of	O
a	O
verse	O
is	O
identified	O
by	O
using	O
syllable	O
segmentation	O
,	O
followed	O
by	O
identification	O
of	O
syllable	O
weights	O
and	O
it	O
's	O
combinations	O
(	O
Melnad	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

The	O
syllable	O
weight	O
of	O
a	O
syllable	O
can	O
either	O
be	O
laghu	O
(	O
light)(represented	O
by	O
the	O
symbol	O
।	O
)	O
or	O
guru	O
(	O
heavy)(represented	O
by	O
the	O
symbol	O
ऽ	O
)	O
.	O

Syllables	O
with	O
short	O
vowels	O
generally	O
form	O
Laghu	O
and	O
those	O
with	O
loing	O
vowels	O
form	O
a	O
Guru	O
.	O

Also	O
,	O
when	O
a	O
short	O
vowel	O
is	O
followed	O
by	O
a	O
conjunct	O
consonant	O
or	O
Anusvāra	O
(	O
nasal	O
sound	O
/ṃ	O
/	O
)	O
or	O
Visarga	O
(	O
voiceless	O
glottal	O
fricative	O
/h	O
̣/	O
)	O
,	O
the	O
short	O
vowel	O
now	O
becomes	O
Guru	O
.	O

E.g.	O
,	O
the	O
Laghu	O
-	O
Guru	O
mapping	O
of	O
"	O
अन्यािन	O
सं	O
या	O
त(/anyāni	O
saṃ	O
yāti/	O
)	O
"	O
is	O
"	O
ऽऽ	O
।	O
ऽऽ	O
।	O
"	O
.	O

In	O
prior	O
work	O
involving	O
Indian	O
languages	O
for	O
TTS	O
,	O
Kishore	O
et	O
al	O
.	O
(	O

2002	O
)	O
proposed	O
various	O
syllabification	O
rules	O
for	O
words	O
.	O

Herein	O
(	O
with	O
a	O
few	O
exceptions	O
)	O
,	O
if	O
a	O
vowel	O
is	O
followed	O
by	O
3	O
or	O
more	O
consonants	O
,	O
only	O
the	O
first	O
following	O
vowel	O
is	O
grouped	O
with	O
the	O
preceding	O
vowel	O
to	O
form	O
the	O
subword	O
unit	O
.	O

Our	O
proposed	O
algorithm	O
for	O
vowel	O
segmentation	O
(	O
VS	O
)	O
is	O
outlined	O
in	O
Algorithm	O
1	O
.	O

We	O
propose	O
segmenting	O
words	O
at	O
vowel	O
boundaries	O
to	O
extract	O
the	O
units	O
for	O
which	O
alignment	O
with	O
speech	O
is	O
learnt	O
within	O
the	O
ASR	B-TaskName
system	O
.	O

For	O
acoustic	O
models	O
,	O
an	O
effective	O
unit	O
of	O
a	O
word	O
for	O
ASR	B-TaskName
would	O
arguably	O
be	O
the	O
syllable	O
(	O
Lee	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

Representing	O
a	O
word	O
in	O
terms	O
of	O
syllables	O
demands	O
the	O
mapping	O
ALGORITHM	O
1	O
:	O
Vowel	O
segmentation	O
algorithm	O
for	O
Indian	O
languages	O
Input	O
:	O
word	O
in	O
Indian	O
language	O
Output	O
:	O
Vowel	O
segments	O
in	O
output	O
output	O
=	O
"	O
"	O
;	O
for	O
each	O
graphemic	O
unit	O
c	O
i	O
in	O
word	O
doif	O
c	O
i	O
is	O
V	O
then	O
if	O
c	O
i+1	O
is	O
V	O
then	O
output	O
+	O
=	O
c	O
i	O
+	O
"	O
"	O
;	O
else	O
if	O
c	O
i+1	O
is	O
C	O
and	O
c	O
i+2	O
is	O
C	O
then	O
output	O
+	O
=	O
c	O
i	O
;	O
else	O
if	O
c	O
i+1	O
is	O
C	O
and	O
c	O
i+2	O
is	O
V	O
then	O
output	O
+	O
=	O
c	O
i	O
+	O
"	O
"	O
;	O
else	O
if	O
c	O
i+1	O
is	O
V	O
then	O
output	O
+	O
=	O
c	O
i	O
;	O
else	O
if	O
c	O
i+1	O
is	O
C	O
and	O
c	O
i+2	O
is	O
C	O
then	O
output	O
+	O
=	O
c	O
i	O
;	O
else	O
if	O
c	O
i+1	O
is	O
C	O
and	O
c	O
i+2	O
is	O
V	O
and	O
c	O
i+2	O
is	O
first	O
vowel	O
of	O
the	O
word	O
then	O
output	O
+	O
=	O
c	O
i	O
;	O
else	O
if	O
c	O
i+1	O
is	O
C	O
and	O
c	O
i+2	O
is	O
V	O
then	O
output	O
+	O
=	O
c	O
i	O
+	O
"	O
"	O
;	O
of	O
a	O
word	O
from	O
graphemes	O
to	O
phonemes	O
.	O

To	O
create	O
syllable	O
units	O
,	O
phonemes	O
are	O
then	O
combined	O
together	O
based	O
on	O
the	O
sonority	O
sequencing	O
principle	O
(	O
Clements	O
,	O
1990	O
)	O
.	O

Absence	O
of	O
accurate	O
syllabifiers	O
for	O
Indian	O
languages	O
restricts	O
the	O
use	O
of	O
syllables	O
as	O
units	O
for	O
learning	O
alignment	O
.	O

Our	O
approach	O
produces	O
units	O
which	O
can	O
be	O
viewed	O
as	O
a	O
rough	O
approximation	O
to	O
a	O
syllable	O
.	O

A	O
syllable	O
is	O
composed	O
of	O
three	O
parts	O
viz	O
.	O
,	O

onset	O
,	O
nucleus	O
and	O
coda	O
,	O
where	O
nucleus	O
has	O
the	O
highest	O
sonority	O
and	O
is	O
always	O
a	O
vowel	O
.	O

In	O
our	O
approach	O
,	O
the	O
onset	O
is	O
always	O
one	O
or	O
zero	O
consonants	O
and	O
the	O
coda	O
is	O
zero	O
or	O
n-1	O
consonants	O
if	O
the	O
nucleus	O
is	O
followed	O
by	O
n	O
consonants	O
.	O

It	O
is	O
also	O
observed	O
in	O
the	O
pronunciation	O
of	O
conjunct	O
consonant	O
by	O
professional	O
speakers	O
that	O
the	O
beginning	O
part	O
of	O
conjunct	O
consonant	O
gets	O
associated	O
more	O
with	O
the	O
preceding	O
vowel	O
than	O
the	O
following	O
.	O

We	O
consider	O
nasal	O
Anusvāra	O
(	O
◌	O
ं	O
)	O
,	O
Chandrabindu	O
(	O
◌	O
ॅ	O
or	O
◌	O
ँ	O
)	O
,	O
and	O
Visarga	O
(	O
◌	O
ः	O
)	O
to	O
be	O
part	O
of	O
the	O
consonant	O
set	O
.	O

For	O
example	O
,	O
in	O
Sanskrit	O
,	O
the	O
units	O
for	O
a	O
word	O
उद्यान	O
:	O
(	O
udyāna	O
,	O
park	O
)	O
will	O
be	O
'	O
उद्	O
या	O
न:(/ud	O
yā	O
na	O
/	O
)	O
'	O
and	O
subword	O
units	O
of	O
the	O
Telugu	O
word	O
తలిల్	O
తండు	O
ర్	O
లు(/tallitaṃ	O
d	O
̣rulu/	O
)	O
will	O
be	O
'	O
తల్	O
లి	O
తండ్	O
రు	O
లు(/tal	O
li	O
taṃ	O
d	O
̣	O
ru	O
lu/	O
)	O
'	O
.	O

For	O
each	O
choice	O
of	O
graphemic	O
unit	O
(	O
viz	O
.	O

native	O
script	O
and	O
SLP1	O
)	O
described	O
in	O
Section	O
3.1	O
,	O
we	O
study	O
three	O
different	O
units	O
for	O
the	O
acoustic	O
modeling	O
(	O
AM	O
)	O
in	O
ASR	B-TaskName
,	O
viz	O
.	O
,	O

graphemic	O
unit	O
and	O
vowel	O
segmentation	O
for	O
Sanskrit	O
and	O
also	O
phonemic	O
unit	O
across	O
the	O
two	O
other	O
representative	O
Indian	O
languages	O
viz	O
.	O
,	O

Gujarati	O
and	O
Telugu	O
.	O

Whereas	O
,	O
for	O
language	O
modeling	O
(	O
LM	O
)	O
,	O
we	O
study	O
word	O
,	O
BPE	O
and	O
VS	O
based	O
units	O
.	O

In	O
Figure	O
3	O
,	O
we	O
report	O
the	O
vocabulary	O
size	O
based	O
on	O
each	O
of	O
these	O
three	O
different	O
unit	O
selections	O
and	O
contrast	O
the	O
sizes	O
with	O
that	O
of	O
two	O
extreme	O
hypothetical	O
systems	O
-one	O
that	O
considers	O
the	O
entire	O
word	O
as	O
a	O
single	O
unit	O
for	O
AM	O
and	O
the	O
other	O
that	O
treats	O
the	O
phoneme	O
as	O
a	O
single	O
unit	O
for	O
AM	O
.	O

Note	O
that	O
while	O
phonetic	O
dictionaries	O
are	O
available	O
for	O
Telugu	O
and	O
Gujarati	O
,	O
our	O
dataset	O
for	O
Sanskrit	O
does	O
not	O
have	O
an	O
accompanying	O
phonetic	O
dictionary	O
.	O

We	O
present	O
the	O
variation	O
in	O
vocabulary	O
size	O
as	O
a	O
function	O
of	O
the	O
graphemic	O
unit	O
(	O
native	O
script	O
vs.	O
SLP1	O
)	O
.	O

In	O
both	O
Gujarati	O
and	O
Telugu	O
,	O
we	O
point	O
out	O
that	O
the	O
number	O
of	O
SLP1	O
graphemic	O
units	O
almost	O
coincide	O
with	O
the	O
number	O
of	O
phonemes	O
,	O
while	O
the	O
native	O
script	O
-	O
based	O
graphemes	O
are	O
much	O
larger	O
in	O
number	O
compared	O
to	O
phonemes	O
.	O

We	O
can	O
also	O
roughly	O
estimate	O
the	O
extent	O
of	O
data	O
sparsity	O
in	O
terms	O
of	O
vocabulary	O
size	O
in	O
each	O
setting	O
-larger	O
the	O
vocabulary	O
size	O
,	O
higher	O
is	O
the	O
chance	O
of	O
data	O
sparsity	O
.	O

We	O
note	O
that	O
data	O
sparsity	O
is	O
minimal	O
for	O
graphemes	O
and	O
highest	O
for	O
a	O
hypothetical	O
system	O
where	O
whole	O
words	O
are	O
the	O
unit	O
of	O
selection	O
.	O

Description	O
of	O
Datasets	O
:	O
In	O
addition	O
to	O
reporting	O
ASR	B-TaskName
results	O
on	O
the	O
carefully	O
created	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañcayah	O
̣/	O
)	O
dataset	O
(	O
described	O
in	O
Section	O
2	O
)	O
,	O
we	O
also	O
contrast	O
through	O
experimental	O
analysis	O
on	O
two	O
other	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

Telugu	O
and	O
Gujarati	O
.	O

For	O
Telugu	O
and	O
Gujarati	O
,	O
we	O
used	O
the	O
publicly	O
available	O
speech	O
corpora	O
released	O
by	O
Microsoft	O
(	O
Srivastava	O
et	O
al	O
.	O
,	O

2018	O
)	O
that	O
contains	O
36.2/8.7	O
hours	O
and	O
33.2/5.8	O
hours	O
of	O
training	O
/	O
test	O
speech	O
in	O
Telugu	O
and	O
Gujarati	O
,	O
respectively	O
.	O

We	O
use	O
a	O
new	O
train	O
-	O
test	O
split	O
for	O
the	O
Gujarati	O
and	O
Telugu	O
datasets	O
because	O
the	O
original	O
split	O
had	O
overlapping	O
spekaers	O
in	O
their	O
train	O
and	O
test	O
.	O

Our	O
new	O
split	O
ensures	O
that	O
the	O
train	O
-	O
test	O
split	O
have	O
disjoint	O
speakers	O
.	O

Transcript	O
of	O
this	O
corpora	O
was	O
cleaned	O
for	O
orthographic	O
errors	O
.	O

Corpora	O
in	O
these	O
two	O
languages	O
were	O
accompanied	O
by	O
pronunciation	O
lexicons	O
,	O
which	O
we	O
used	O
to	O
build	O
phoneme	O
-	O
based	O
ASR	B-TaskName
systems	O
to	O
compare	O
against	O
our	O
grapheme	O
-	O
based	O
systems	O
.	O

Experimental	O
Setup	O
:	O
We	O
use	O
the	O
Kaldi	O
toolkit	O
(	O
Povey	O
et	O
al	O
.	O
,	O

2011	O
)	O
for	O
all	O
our	O
ASR	B-TaskName
experiments	O
.	O

Our	O
acoustic	O
model	O
is	O
implemented	O
using	O
Time	B-MethodName
Delay	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
TDNNs	B-MethodName
)	O
(	O
Peddinti	O
et	O
al	O
.	O
,	O

2015	O
)	O
containing	O
14	B-HyperparameterValue
layers	B-HyperparameterName
.	O

We	O
use	O
40dimensional	O
MFCCs	O
as	O
our	O
input	O
features	O
along	O
with	O
100	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
i	B-HyperparameterName
-	I-HyperparameterName
vector	I-HyperparameterName
based	I-HyperparameterName
speaker	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Saon	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

We	O
used	O
ngram	O
language	O
models	O
with	O
Kneser	O
-	O
Ney	O
smoothing	O
implemented	O
using	O
the	O
SRILM	O
toolkit	O
(	O
Stolcke	O
,	O
2002	O
)	O
.	O

The	O
language	O
models	O
were	O
trained	O
using	O
both	O
training	O
transcripts	O
from	O
the	O
speech	O
data	O
,	O
as	O
well	O
as	O
additional	O
textual	O
data	O
derived	O
from	O
the	O
Leipzig	B-DatasetName
Corpora	I-DatasetName
Collection	I-DatasetName
for	O
Gujarati	O
and	O
Telugu	O
(	O
Goldhahn	O
et	O
al	O
.	O
,	O

2012	O
)	O
and	O
the	O
Digital	B-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Sanskrit	I-DatasetName
(	O
Hellwig	O
,	O
2010	O
)	O
for	O
Sanskrit	O
.	O

The	O
word	O
vocabulary	O
sizes	O
in	O
the	O
lexicons	O
for	O
Sanskrit	O
,	O
Telugu	O
and	O
Gujarati	O
are	O
76	O
K	O
,	O
43	O
K	O
and	O
48	O
K	O
,	O
respectively	O
.	O

Results	O
:	O
Tables	O
3	O
,	O
4	O
and	O
5	O
,	O
present	O
the	O
WERs	B-MetricName
from	O
ASR	B-TaskName
systems	O
built	O
using	O
different	O
choices	O
of	O
AM	O
and	O
LM	O
units	O
using	O
both	O
the	O
graphemic	O
representations	O
(	O
Native	O
and	O
SLP1	O
)	O
for	O
Sanskrit	O
,	O
Gujarati	O
and	O
Telugu	O
,	O
respectively	O
.	O

From	O
Table	O
3	O
see	O
that	O
BPE	O
units	O
4	O
and	O
vowel	O
segment	O
units	O
are	O
far	O
superior	O
compared	O
to	O
words	O
as	O
an	O
LM	O
unit	O
for	O
Sanskrit	O
.	O

This	O
is	O
unsurprising	O
given	O
that	O
Sanskrit	O
has	O
a	O
high	O
rates	O
of	O
OOV	O
(	O
44.16	O
%	O
)	O
.	O

However	O
,	O
as	O
shown	O
in	O
Tables	O
4	O
and	O
5	O
the	O
configurations	O
with	O
word	O
based	O
LMs	O
performs	O
the	O
best	O
for	O
Gujarati	O
and	O
Telugu	O
respectively	O
.	O

Gujarati	O
and	O
Telugu	O
have	O
lower	O
OOV	O
rates	O
of	O
18.63	O
%	O
and	O
15.26	O
%	O
.Table	O
6	O
shows	O
the	O
distribution	O
of	O
words	O
with	O
1	O
-	O
4	O
continuous	O
consonants	O
in	O
all	O
three	O
languages	O
.	O

For	O
Telugu	O
,	O
even	O
though	O
the	O
number	O
of	O
conjunct	O
consonants	O
with	O
N	O
=	O
2	O
is	O
higher	O
than	O
in	O
Sanskrit	O
,	O
we	O
found	O
on	O
inspecting	O
the	O
audio	O
data	O
that	O
such	O
conjunct	O
consonants	O
are	O
often	O
not	O
enunciated	O
(	O
Kulkarni	O
et	O
al	O
.	O
,	O

2015	O
)	O
,	O
inflections	O
and	O
compounds	O
,	O
Sanskrit	O
always	O
has	O
the	O
highest	O
number	O
of	O
rare	O
words	O
.	O

In	O
the	O
training	O
dataset	O
used	O
in	O
the	O
Sanskrit	O
ASR	B-TaskName
experiments	O
with	O
the	O
vocab	O
size	O
of	O
70.5	O
K	O
,	O
more	O
than	O
87.25	O
%	O
words	O
have	O
a	O
frequency	O
less	O
than	O
3	O
,	O
where	O
as	O
in	O
Telugu	O
and	O
Gujarati	O
training	O
dataset	O
,	O
this	O
is	O
76.76	O
%	O
and	O
77.26	O
%	O
,	O
respectively	O
.	O

Clear	O
articulation	O
of	O
conjunct	O
consonants	O
and	O
higher	O
rare	O
word	O
rates	O
makes	O
the	O
BPE	O
and	O
VS	O
based	O
models	O
performs	O
better	O
in	O
Sanskrit	O
than	O
other	O
two	O
languages	O
along	O
with	O
the	O
impact	O
of	O
OOVs	O
.	O

We	O
observe	O
that	O
use	O
of	O
SLP1	O
as	O
a	O
graphemic	O
representation	O
schemes	O
performs	O
best	O
for	O
all	O
the	O
three	O
languages	O
.	O

SLP1	O
is	O
designed	O
to	O
capture	O
the	O
phonemic	O
-	O
graphemic	O
correspondences	O
present	O
in	O
Indic	O
languages	O
.	O

We	O
also	O
find	O
that	O
ASR	B-TaskName
performance	O
using	O
phonemes	O
is	O
comparable	O
to	O
graphemes	O
for	O
Gujarati	O
and	O
Telugu	O
.	O

In	O
Sanskrit	O
,	O
we	O
observe	O
that	O
purely	O
grapheme	O
-	O
based	O
acous-	O
tic	O
models	O
outperform	O
grapheme+vowel	O
segmentbased	O
acoustic	O
models	O
.	O

With	O
the	O
consistent	O
mapping	O
between	O
graphemes	O
and	O
phonemes	O
and	O
the	O
absence	O
of	O
schwa	O
deletion	O
,	O
it	O
is	O
intuitive	O
that	O
grapheme	O
-	O
based	O
models	O
would	O
be	O
most	O
appropriate	O
for	O
Sanskrit	O
.	O

Even	O
though	O
for	O
Sanskrit	O
in	O
some	O
cases	O
Devanagari	O
as	O
a	O
graphemic	O
representation	O
outperforms	O
the	O
SLP1	O
(	O
Sr	O
.	O

1,3,9,11	O
in	O
Table	O
3	O
)	O
,	O
the	O
model	O
that	O
uses	O
SLP1	O
script	O
always	O
outperforms	O
the	O
other	O
in	O
terms	O
of	O
character	O
error	O
rate	O
.	O

In	O
Sanskrit	O
the	O
pause	O
given	O
between	O
the	O
subwords	O
of	O
a	O
compound	O
word	O
and	O
in	O
between	O
two	O
words	O
varies	O
depending	O
on	O
the	O
fluency	O
of	O
the	O
speaker	O
and	O
the	O
complexity	O
of	O
the	O
text	O
,	O
which	O
can	O
deteriorate	O
the	O
WER	B-MetricName
.	O

The	O
utterance	O
for	O
'	O
महान्	O
प्राकारः	O
'	O
/mahān	O
prākārah	O
̣/	O
may	O
get	O
recognised	O
as	O
'	O
महान्प्राकारः	O
'	O
/mahānprākārah	O
̣/	O
,	O
where	O
two	O
correctly	O
recognised	O
words	O
will	O
be	O
evaluated	O
as	O
one	O
deletion	O
and	O
one	O
substituion	O
by	O
the	O
evaluation	O
model	O
.	O

Similarly	O
if	O
the	O
audio	O
of	O
'	O
शोभमानमासीत्	O
'	O
/śobhamā	O
namāsīt/	O
gets	O
recognised	O
as	O
'	O
शोभमानम्	O
आसीत्	O
'	O
/śob	O
hamānam	O
āsīt/	O
,	O
then	O
it	O
will	O
be	O
considered	O
as	O
one	O
insertion	O
followed	O
by	O
one	O
substituion	O
.	O

After	O
negating	O
these	O
two	O
particular	O
errors	O
,	O
we	O
will	O
get	O
17.79	B-MetricValue
%	I-MetricValue
as	O
the	O
modulo	B-MetricName
substitution	I-MetricName
deletion	I-MetricName
WER	I-MetricName
for	O
our	O
best	O
model	O
of	O
Sanskrit	O
(	O
Sr	O
.	O

6	O
of	O
Table	O
3	O
)	O
.	O

The	O
character	B-MetricName
error	I-MetricName
rate	I-MetricName
3.10	B-MetricValue
%	I-MetricValue
for	O
the	O
best	O
model	O
in	O
Sanskrit	O
also	O
ensures	O
the	O
performance	O
of	O
the	O
model	O
and	O
the	O
quality	O
of	O
the	O
dataset	O
,	O
where	O
as	O
the	O
CER	B-MetricName
for	O
the	O
best	O
model	O
of	O
Gujarati	O
and	O
Telugu	O
are	O
5.49	B-MetricValue
%	I-MetricValue
and	O
5.60	B-MetricValue
%	I-MetricValue
respectively	O
,	O
much	O
higher	O
than	O
Sanskrit	O
.	O

Out	O
-	O
of	O
-	O
domain	O
test	O
set	O
.	O

Table	O
7	O
presents	O
results	O
on	O
the	O
out	O
-	O
of	O
-	O
domain	O
test	O
set	O
described	O
in	O
Section	O
2	O
.	O

It	O
shows	O
the	O
WERs	B-MetricName
we	O
can	O
expect	O
from	O
our	O
models	O
when	O
the	O
speakers	O
and	O
content	O
largely	O
vary	O
in	O
domain	O
from	O
our	O
dataset	O
.	O

This	O
test	O
set	O
was	O
sampled	O
for	O
specific	O
speakers	O
and	O
content	O
that	O
qualify	O
as	O
being	O
out	O
-	O
of	O
-	O
domain	O
.	O

These	O
test	O
utterances	O
were	O
evaluated	O
using	O
our	O
best	O
performing	O
Sanskrit	O
ASR	B-TaskName
models	O
.	O

Speakers	O
#	O
1	O
and	O
#	O
2	O
were	O
included	O
,	O
as	O
their	O
utterances	O
show	O
more	O
pronounced	O
influence	O
of	O
their	O
native	O
languages	O
,	O
Tamil	O
and	O
Hindi	O
respectively	O
.	O

It	O
is	O
observed	O
that	O
speaker	O
#	O
1	O
,	O
does	O
not	O
often	O
attempt	O
to	O
distinguish	O
between	O
the	O
pronunciation	O
of	O
the	O
phoneme	O
pairs	O
such	O
as	O
/ta/	O
and	O
/da/	O
,	O
/ka/	O
and	O
/ga/	O
,	O
etc	O
.	O

This	O
is	O
in	O
congruence	O
with	O
the	O
orthography	O
followed	O
in	O
Tamil	O
,	O
the	O
speaker	O
's	O
native	O
language	O
.	O

Speaker	O
#	O
2	O
's	O
reading	O
was	O
influenced	O
by	O
schwa	O
deletion	O
,	O
i.e.	O
,	O
the	O
phenomena	O
of	O
deleting	O
vowel	O
markers	O
accompanying	O
consonants	O
at	O
certain	O
contexts	O
(	O
elaborated	O
in	O
the	O
supplementary	O
material	O
)	O
which	O
is	O
dominant	O
in	O
Hindi	O
.	O

The	O
inclusion	O
of	O
poetry	O
data	O
would	O
require	O
substantial	O
changes	O
to	O
the	O
system	O
,	O
which	O
we	O
plan	O
to	O
address	O
in	O
the	O
near	O
future	O
.	O

For	O
instance	O
,	O
the	O
poetry	O
data	O
would	O
greatly	O
benefit	O
from	O
insights	O
from	O
Sanskrit	O
prosody	O
.	O

More	O
importantly	O
,	O
the	O
degree	O
of	O
free	O
word	O
orderness	O
in	O
prose	O
and	O
poetry	O
greatly	O
varies	O
in	O
Sanskrit	O
,	O
so	O
much	O
so	O
that	O
an	O
n	O
-	O
gram	O
LM	O
will	O
not	O
be	O
effective	O
.	O

Sr	O
.	O

We	O
would	O
like	O
to	O
thank	O
Prof.	O
K.	O
Ramasubramanian	O
,	O
IIT	O
Bombay	O
,	O
for	O
supporting	O
the	O
creation	O
of	O
Sanskrit	O
speech	O
corpus	O
.	O

We	O
express	O
our	O
gratitude	O
to	O
the	O
volunteers	O
who	O
have	O
participated	O
in	O
recording	O
readings	O
of	O
classical	O
Sanskrit	O
texts	O
and	O
helping	O
make	O
this	O
resource	O
available	O
for	O
the	O
purpose	O
of	O
research	O
.	O

A	O
Differences	O
between	O
Sanskrit	O
and	O
other	O
Indic	O
languages	O
for	O
ASR	B-TaskName
Many	O
Indian	O
languages	O
are	O
known	O
to	O
be	O
derived	O
from	O
Sanskrit	O
(	O
Kulkarni	O
et	O
al	O
.	O
,	O

2010b	O
)	O
and	O
their	O
scripts	O
derived	O
from	O
the	O
Brahmi	O
script	O
(	O
Salomon	O
,	O
1996;Sproat	O
,	O
2003	O
)	O
,	O
which	O
leads	O
to	O
graphemebased	O
similarites	O
amongst	O
them	O
.	O

In	O
Figure	O
5	O
,	O
we	O
illustrate	O
through	O
an	O
example	O
,	O
the	O
spectrum	O
of	O
mapping	O
the	O
native	O
character	O
/	O
grapheme	O
(	O
units	O
)	O
in	O
words	O
across	O
languages	O
;	O
at	O
one	O
end	O
of	O
the	O
spectrum	O
is	O
राम(/rām/	O
)	O
in	O
Hindi	O
mapped	O
to	O
రామ(/rāma/	O
)	O
in	O
Telugu	O
as	O
an	O
example	O
where	O
direct	O
correspondence	O
with	O
the	O
native	O
character	O
exists	O
.	O

Going	O
further	O
in	O
the	O
spectrum	O
are	O
examples	O
for	O
which	O
direct	O
character	O
correspondence	O
does	O
not	O
exist	O
.	O

सीता(/sītā/	O
)	O
in	O
Hindi	O
going	O
to	O
ಸೀತೆ(/sīte/	O
)	O
in	O
Kannada	O
is	O
an	O
instance	O
where	O
there	O
is	O
a	O
change	O
in	O
the	O
ending	O
vowel	O
.	O

The	O
schwa	O
deletion	O
phenomenon	O
plays	O
a	O
crucial	O
role	O
in	O
the	O
north	O
Indian	O
languages	O
.	O

Every	O
consonant	O
by	O
itself	O
includes	O
a	O
short	O
/a/	O
vowel	O
sound	O
(	O
referred	O
to	O
as	O
"	O
schwa	O
"	O
)	O
unless	O
otherwise	O
specified	O
.	O

For	O
example	O
,	O
the	O
letter	O
'	O
त	O
'	O
in	O
Hindi	O
is	O
pronounced	O
as	O
/ta/.	O
This	O
sound	O
can	O
be	O
associated	O
with	O
any	O
other	O
vowel	O
sound	O
by	O
the	O
use	O
of	O
"	O
Mātras	O
"	O
.	O

Mātras	O
are	O
dependent	O
forms	O
of	O
vowels	O
.	O

Schwa	O
is	O
the	O
default	O
vowel	O
for	O
a	O
consonant	O
and	O
hence	O
does	O
not	O
require	O
any	O
explicit	O
Mātra	O
to	O
represent	O
it	O
.	O

Schwa	O
deletion	O
is	O
a	O
phenomenon	O
where	O
implicit	O
schwas	O
of	O
a	O
word	O
are	O
deleted	O
during	O
pronunciation	O
.	O

For	O
example	O
,	O
in	O
Hindi	O
,	O
the	O
proper	O
noun	O
,	O
'	O
अजु	O
र्	O
न	O
(	O
/arjun/	O
,	O
the	O
name	O
of	O
a	O
person	O
)	O
has	O
schwa	O
deletion	O
after	O
the	O
consonant	O
'	O
न	O
'	O
and	O
is	O
pronounced	O
as	O
Arjun	O
.	O

This	O
phenomenon	O
is	O
not	O
observed	O
in	O
the	O
South	O
Indian	O
languages	O
.	O

For	O
instance	O
,	O
in	O
Kannada	O
it	O
is	O
pronounced	O
as	O
'	O
Arjuna	O
'	O
.	O

There	O
is	O
no	O
implicit	O
schwa	O
deletion	O
in	O
Sanskrit	O
as	O
well	O
as	O
in	O
the	O
traditional	O
use	O
of	O
South	O
Indian	O
languages	O
such	O
as	O
Kannada	O
.	O

North	O
Indian	O
languages	O
observe	O
schwa	O
deletion	O
not	O
only	O
at	O
the	O
end	O
of	O
the	O
word	O
,	O
but	O
also	O
in	O
the	O
middle	O
of	O
a	O
word	O
in	O
some	O
cases	O
.	O

For	O
example	O
,	O
the	O
word	O
'	O
गलती	O
'	O
(	O
/galtī/	O
meaning	O
mistake	O
)	O
in	O
Hindi	O
observes	O
implicit	O
schwa	O
deletion	O
after	O
the	O
consonant	O
'	O
ल'(/la/	O
)	O
.	O

ASR	B-TaskName
becomes	O
challenging	O
because	O
of	O
this	O
phenomenon	O
since	O
the	O
occurrence	O
of	O
schwa	O
deletion	O
is	O
not	O
always	O
explicitly	O
specified	O
in	O
the	O
orthography	O
.	O

For	O
example	O
,	O
the	O
name	O
रामबाबु	O
(	O
/rāmbābu/	O
)	O
has	O
two	O
basic	O
words	O
concatenated	O
to	O
form	O
a	O
name	O
.	O

In	O
Hindi	O
,	O
this	O
name	O
has	O
an	O
implicit	O
schwa	O
deleted	O
at	O
म	O
(	O
consonant	O
sounding	O
'	O
ma	O
'	O
)	O
of	O
राम	O
(	O
/rām/	O
)	O
.	O

While	O
constructing	O
phonetic	O
representations	O
for	O
ASR	B-TaskName
,	O
such	O
deletions	O
introduce	O
ambiguities	O
in	O
pronunciation	O
which	O
could	O
be	O
alleviated	O
by	O
enforcing	O
more	O
consistency	O
between	O
graphemes	O
and	O
phonemes	O
.	O

This	O
same	O
word	O
रामबाबु	O
written	O
in	O
Telugu	O
would	O
be	O
phonetically	O
represented	O
as	O
రామాబ్బు	O
(	O
/rāmbābu/	O
)	O
instead	O
of	O
రామబాబు	O
(	O
/rāmabābu/	O
)	O
which	O
is	O
intuitive	O
.	O

Note	O
that	O
in	O
the	O
former	O
case	O
,	O
there	O
is	O
an	O
addition	O
of	O
'	O
◌	O
్	O
'	O
(	O
halant	O
:	O
an	O
explicit	O
schwa	O
deletion	O
marker	O
)	O
at	O
మ(/ma/	O
)	O
.	O

This	O
forces	O
the	O
consonants	O
మ(/ma/	O
)	O
and	O
బ(/ba/	O
)	O
to	O
combine	O
and	O
form	O
a	O
conjunct	O
.	O

In	O
the	O
latter	O
case	O
there	O
is	O
a	O
grapheme	O
consistency	O
across	O
both	O
Hindi	O
and	O
Telugu	O
languages	O
but	O
there	O
is	O
a	O
variation	O
in	O
their	O
pronunciation	O
due	O
to	O
the	O
schwa	O
deletion	O
phenomenon	O
.	O

In	O
contrast	O
,	O
in	O
the	O
case	O
of	O
Sanskrit	O
,	O
since	O
pronunciation	O
is	O
strictly	O
governed	O
by	O
the	O
शक्षा(/śiks	O
̣ā/	O
)	O
(	O
Manomohan	O
and	O
Pān	O
̣ini	O
,	O
1938	O
)	O
,	O
a	O
treatise	O
on	O
phonetics	O
,	O
schwa	O
deletion	O
is	O
not	O
observed	O
.	O

In	O
Sanskrit	O
a	O
noun	O
can	O
have	O
24	O
to	O
92	O
inflections	O
(	O
depending	O
on	O
base	O
word	O
's	O
gender	O
and	O
alternate	O
forms	O
)	O
and	O
a	O
verb	O
can	O
have	O
90	O
to	O
180	O
inflections	O
.	O

Derivative	O
nouns	O
(	O
Taddhitas	O
)	O
and	O
verbs	O
(	O
passive	O
(	O
Karman	O
̣i	O
)	O
,	O
san	O
,	O
n	O
̣ic	O
,	O
yaṅ	O
,	O
etc	O
)	O
are	O
also	O
used	O
often	O
in	O
Sanskrit	O
literature	O
.	O

Due	O
to	O
this	O
morphological	O
richness	O
and	O
frequently	O
occurring	O
compound	O
words	O
,	O
vocab	O
size	O
can	O
be	O
reduced	O
by	O
properly	O
selecting	O
repetitive	O
stems	O
and	O
suffices	O
using	O
BPE	O
by	O
specifying	O
the	O
number	O
of	O
merge	O
operations	O
.	O

Therefore	O
we	O
experimented	O
varying	O
number	O
of	O
subword	O
unit	O
with	O
vocabulary	O
sizes	O
of	O
2	O
K	O
,	O
4	O
K	O
,	O
8	O
K	O
,	O
16	O
K	O
,	O
32	O
K	O
and	O
64	O
K	O
(	O
K=1000	O
)	O
.	O

Table	O
8	O
shows	O
the	O
varying	O
BPE	O
configuration	O
on	O
our	O
best	O
configuration	O
,	O
i.e	O
graphemes	O
as	O
AM	O
unit	O
and	O
BPE	O
as	O
LM	O
unit	O
.	O

However	O
,	O
the	O
performance	O
of	O
these	O
configurations	O
are	O
comparable	O
irrespective	O
of	O
their	O
BPE	O
vocabulary	O
size	O
.	O

BPE	O
with	O
vocabulary	O
size	O
of	O
32,000	O
stands	O
closes	O
to	O
that	O
of	O
VS	O
,	O
with	O
a	O
vocabulary	O
size	O
of	O
29,147	O
.	O

Even	O
in	O
this	O
configuration	O
,	O
BPE	O
outperforms	O
VS	O
,	O
as	O
BPE	O
reports	O
a	O
WER	B-MetricName
of	O
21.94	B-MetricValue
as	O

Pre	O
-	O
trained	O
multilingual	O
language	O
models	O
(	O
LMs	O
)	O
have	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
cross	O
-	O
lingual	O
transfer	O
,	O
but	O
they	O
often	O
lead	O
to	O
an	O
inequitable	O
representation	O
of	O
languages	O
due	O
to	O
limited	O
capacity	O
,	O
skewed	O
pre	O
-	O
training	O
data	O
,	O
and	O
sub	O
-	O
optimal	O
vocabularies	O
.	O

This	O
has	O
prompted	O
the	O
creation	O
of	O
an	O
ever	O
-	O
growing	O
pretrained	O
model	O
universe	O
,	O
where	O
each	O
model	O
is	O
trained	O
on	O
large	O
amounts	O
of	O
language	O
or	O
domain	O
specific	O
data	O
with	O
a	O
carefully	O
curated	O
,	O
linguistically	O
informed	O
vocabulary	O
.	O

However	O
,	O
doing	O
so	O
brings	O
us	O
back	O
full	O
circle	O
and	O
prevents	O
one	O
from	O
leveraging	O
the	O
benefits	O
of	O
multilinguality	O
.	O

To	O
address	O
the	O
gaps	O
at	O
both	O
ends	O
of	O
the	O
spectrum	O
,	O
we	O
propose	O
MERGEDISTILL	B-MethodName
,	O
a	O
framework	O
to	O
merge	O
pre	O
-	O
trained	O
LMs	O
in	O
a	O
way	O
that	O
can	O
best	O
leverage	O
their	O
assets	O
with	O
minimal	O
dependencies	O
,	O
using	O
task	O
-	O
agnostic	O
knowledge	O
distillation	O
.	O

We	O
demonstrate	O
the	O
applicability	O
of	O
our	O
framework	O
in	O
a	O
practical	O
setting	O
by	O
leveraging	O
pre	O
-	O
existing	O
teacher	O
LMs	O
and	O
training	O
student	O
LMs	O
that	O
perform	O
competitively	O
with	O
or	O
even	O
outperform	O
teacher	O
LMs	O
trained	O
on	O
several	O
orders	O
of	O
magnitude	O
more	O
data	O
and	O
with	O
a	O
fixed	O
model	O
capacity	O
.	O

We	O
also	O
highlight	O
the	O
importance	O
of	O
teacher	O
selection	O
and	O
its	O
impact	O
on	O
student	O
model	O
performance	O
.	O

While	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
multilingual	O
language	O
models	O
(	O
LMs	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019;Conneau	O
et	O
al	O
.	O
,	O

2020	O
)	O
aim	O
to	O
represent	O
100	O
+	O
languages	O
in	O
a	O
single	O
model	O
,	O
efforts	O
towards	O
building	O
monolingual	O
(	O
Martin	O
et	O
al	O
.	O
,	O

2019;Kuratov	O
and	O
Arkhipov	O
,	O
2019	O
)	O
or	O
language	O
-	O
family	O
based	O
(	O
Khanuja	O
et	O
al	O
.	O
,	O

2021	O
)	O
models	O
are	O
only	O
increasing	O
with	O
time	O
(	O
Rust	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

A	O
single	O
model	O
is	O
often	O
incapable	O
of	O
effectively	O
representing	O
a	O
diverse	O
set	O
of	O
languages	O
,	O
evidence	O
of	O
which	O
has	O
been	O
provided	O
by	O
works	O
highlighting	O
the	O
importance	O
of	O
vocabulary	O
curation	O
and	O
size	O
(	O
Chung	O
et	O
al	O
.	O
,	O

2020;Artetxe	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
pre	O
-	O
training	O
data	O
volume	O
(	O
Liu	O
et	O
al	O
.	O
,	O

2019a;Conneau	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
the	O
curse	O
of	O
multilinguality	O
(	O
Conneau	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Language	O
specific	O
models	O
alleviate	O
these	O
issues	O
with	O
a	O
custom	O
vocabulary	O
which	O
captures	O
language	O
subtleties	O
1	O
and	O
large	O
magnitudes	O
of	O
pre	O
-	O
training	O
data	O
scraped	O
from	O
several	O
domains	O
(	O
Virtanen	O
et	O
al	O
.	O
,	O

2019;Antoun	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

However	O
,	O
building	O
language	O
specific	O
LMs	O
brings	O
us	O
back	O
to	O
where	O
we	O
started	O
,	O
preventing	O
us	O
from	O
leveraging	O
the	O
benefits	O
of	O
multilinguality	O
like	O
zeroshot	O
task	O
transfer	O
(	O
Hu	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
positive	O
transfer	O
between	O
related	O
languages	O
(	O
Pires	O
et	O
al	O
.	O
,	O

2019;Lauscher	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
an	O
ability	O
to	O
handle	O
codemixed	O
text	O
(	O
Pires	O
et	O
al	O
.	O
,	O

2019;Tsai	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

We	O
need	O
an	O
approach	O
that	O
encompasses	O
the	O
best	O
of	O
both	O
worlds	O
,	O
i.e.	O
,	O
leverage	O
the	O
capabilities	O
of	O
the	O
powerful	O
language	O
-	O
specific	O
LMs	O
while	O
still	O
being	O
multilingual	O
and	O
enabling	O
positive	O
language	O
trans-1	O
For	O
example	O
,	O
in	O
Arabic	O
,	O
(	O
Antoun	O
et	O
al	O
.	O
,	O

2020	O
)	O
argue	O
that	O
while	O
the	O
definite	O
article	O
"	O
Al	O
"	O
,	O
which	O
is	O
equivalent	O
to	O
"	O
the	O
"	O
in	O
English	O
,	O
is	O
always	O
prefixed	O
to	O
other	O
words	O
,	O
it	O
is	O
not	O
an	O
intrinsic	O
part	O
of	O
that	O
word	O
.	O

While	O
with	O
a	O
BERT	B-MethodName
-	O
compatible	O
tokenization	O
tokens	O
will	O
appear	O
twice	O
,	O
once	O
with	O
"	O
Al-	O
"	O
and	O
once	O
without	O
it	O
,	O
AraBERT	B-MethodName
first	O
segments	O
the	O
words	O
using	O
Farasa	O
(	O
Abdelali	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
then	O
learns	O
the	O
vocabulary	O
,	O
thereby	O
alleviating	O
the	O
problem	O
.	O

Figure	O
2	O
:	O
Overview	O
of	O
MERGEDISTILL	B-MethodName
:	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pretraining	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

Here	O
,	O
we	O
combine	O
four	O
teacher	O
LMs	O
comprising	O
of	O
three	O
monolingual	O
(	O
trained	O
on	O
English	O
,	O
Spanish	O
and	O
Korean	O
respectively	O
)	O
and	O
one	O
multilingual	O
LM	O
(	O
trained	O
on	O
English	O
and	O
Hindi	O
)	O
.	O

The	O
student	O
LM	O
is	O
trained	O
on	O
English	O
,	O
Spanish	O
,	O
Hindi	O
and	O
Korean	O
.	O

Pre	O
-	O
training	O
transfer	O
corpora	O
for	O
each	O
language	O
is	O
tokenized	O
and	O
masked	O
using	O
their	O
respective	O
teacher	O
LMs	O
vocabulary	O
.	O

We	O
then	O
obtain	O
predictions	O
for	O
each	O
masked	O
word	O
in	O
each	O
language	O
,	O
by	O
evaluating	O
all	O
of	O
their	O
respective	O
teacher	O
LMs	O
.	O

For	O
example	O
,	O
we	O
evaluate	O
English	O
masked	O
examples	O
on	O
both	O
the	O
monolingual	O
and	O
multilingual	O
LM	O
as	O
shown	O
.	O

The	O
student	O
's	O
vocabulary	O
is	O
a	O
union	O
of	O
all	O
teacher	O
vocabularies	O
.	O

Hence	O
,	O
the	O
input	O
,	O
prediction	O
and	O
label	O
indices	O
obtained	O
from	O
teacher	O
evaluation	O
are	O
now	O
mapped	O
to	O
the	O
student	O
vocabulary	O
,	O
and	O
input	O
to	O
the	O
student	O
LM	O
for	O
training	O
.	O

Please	O
refer	O
to	O
Section	O
3.1	O
for	O
details	O
.	O

In	O
this	O
paper	O
,	O
we	O
use	O
knowledge	O
distillation	O
(	O
KD	O
)	O
(	O
Hinton	O
et	O
al	O
.	O
,	O

2015	O
)	O
to	O
achieve	O
this	O
.	O

In	O
the	O
context	O
of	O
language	O
modeling	O
,	O
KD	O
methods	O
can	O
be	O
broadly	O
classified	O
into	O
two	O
categories	O
:	O
task	O
-	O
specific	O
and	O
task	O
-	O
agnostic	O
.	O

In	O
task	O
-	O
specific	O
distillation	O
,	O
the	O
teacher	O
LM	O
is	O
first	O
fine	O
-	O
tuned	O
for	O
a	O
specific	O
task	O
and	O
is	O
then	O
distilled	O
into	O
a	O
student	O
model	O
which	O
can	O
solve	O
that	O
task	O
.	O

Task	O
-	O
agnostic	O
methods	O
perform	O
distillation	O
on	O
the	O
pre	O
-	O
training	O
objective	O
like	O
masked	O
language	O
modeling	O
(	O
MLM	O
)	O
in	O
order	O
to	O
obtain	O
a	O
task	O
-	O
agnostic	O
student	O
model	O
.	O

Prior	O
work	O
has	O
either	O
used	O
task	O
-	O
agnostic	O
distillation	O
to	O
compress	O
singlelanguage	O
teachers	O
(	O
Sanh	O
et	O
al	O
.	O
,	O

2019	O
;	O
or	O
used	O
task	O
-	O
specific	O
distillation	O
to	O
combine	O
multiple	O
fine	O
-	O
tuned	O
teachers	O
into	O
a	O
multi	O
-	O
task	O
student	O
(	O
Liu	O
et	O
al	O
.	O
,	O

2019b;Clark	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

The	O
former	O
prevents	O
positive	O
language	O
transfer	O
while	O
the	O
latter	O
restricts	O
the	O
student	O
's	O
capabilities	O
to	O
the	O
tasks	O
and	O
languages	O
in	O
the	O
fine	O
-	O
tuned	O
teacher	O
LMs	O
(	O
as	O
shown	O
in	O
Figure	O
1).We	O
focus	O
on	O
the	O
problem	O
of	O
merging	O
multiple	O
pre	O
-	O
trained	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
in	O
the	O
task	O
-	O
agnostic	O
setting	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
effort	O
of	O
its	O
kind	O
,	O
and	O
makes	O
the	O
following	O
contributions:•	O
We	O
propose	O
MERGEDISTILL	B-MethodName
,	O
a	O
task	O
-	O
agnostic	O
distillation	O
approach	O
to	O
merge	O
multiple	O
teacher	O
LMs	O
at	O
the	O
pre	O
-	O
training	O
stage	O
,	O
to	O
train	O
a	O
strong	O
multilingual	O
student	O
LM	O
that	O
can	O
then	O
be	O
finetuned	O
for	O
any	O
task	O
on	O
all	O
languages	O
in	O
the	O
student	O
LM	O
.	O

Our	O
approach	O
is	O
more	O
maintainable	O
(	O
fewer	O
models	O
)	O
,	O
compute	O
efficient	O
and	O
teacherarchitecture	O
agnostic	O
(	O
since	O
we	O
obtain	O
offline	O
predictions).•	O
We	O
use	O
MERGEDISTILL	B-MethodName
to	O
i	O
)	O
combine	O
monolingual	O
teacher	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
that	O
is	O
competitive	O
with	O
or	O
outperforms	O
individual	O
teachers	O
,	O
ii	O
)	O
combine	O
multilingual	O
teacher	O
LMs	O
,	O
such	O
that	O
the	O
overlapping	O
languages	O
can	O
learn	O
from	O
multiple	O
teachers.•	O
Through	O
extensive	O
experiments	O
and	O
analysis	O
,	O
we	O
study	O
the	O
importance	O
of	O
typological	O
similarity	O
in	O
building	O
multilingual	O
models	O
,	O
and	O
the	O
impact	O
of	O
strong	O
teacher	O
LM	O
vocabularies	O
and	O
predictions	O
in	O
our	O
framework	O
.	O

Language	O
Model	O
pre	O
-	O
training	O
has	O
evolved	O
from	O
learning	O
pre	O
-	O
trained	O
word	O
embeddings	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

2013	O
)	O
to	O
contextualized	O
word	O
representations	O
(	O
McCann	O
et	O
al	O
.	O
,	O

2017;Peters	O
et	O
al	O
.	O
,	O

2018;Eriguchi	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
to	O
the	O
most	O
recent	O
Transformer	O
-	O
based	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
LMs	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019;Liu	O
et	O
al	O
.	O
,	O

2019a	O
)	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
various	O
downstream	O
NLP	O
tasks	O
.	O

Most	O
commonly	O
,	O
these	O
LMs	O
are	O
pre	O
-	O
trained	O
with	O
the	O
MLM	O
objective	O
(	O
Taylor	O
,	O
1953	O
)	O
on	O
large	O
unsupervised	O
corpora	O
and	O
then	O
fine	O
-	O
tuned	O
on	O
labeled	O
data	O
for	O
the	O
task	O
at	O
hand	O
.	O

Concurrently	O
,	O
multilingual	O
LMs	O
(	O
Lample	O
and	O
Conneau	O
,	O
2019;Conneau	O
et	O
al	O
.	O
,	O

2020;Chung	O
et	O
al	O
.	O
,	O

2021	O
)	O
,	O
trained	O
on	O
massive	O
amounts	O
of	O
multilingual	O
data	O
,	O
have	O
surpassed	O
cross	O
-	O
lingual	O
word	O
embedding	O
spaces	O
(	O
Glavaš	O
et	O
al	O
.	O
,	O

2019	O
;	O
to	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
cross	O
-	O
lingual	O
transfer	O
.	O

While	O
Pires	O
et	O
al	O
.	O
(	O

2019	O
)	O
;	O
Wu	O
and	O
Dredze	O
(	O
2019	O
)	O
highlight	O
their	O
cross	O
-	O
lingual	O
ability	O
,	O
several	O
limitations	O
have	O
been	O
studied	O
.	O

Conneau	O
et	O
al	O
.	O
(	O

2020	O
)	O
highlight	O
the	O
curse	O
of	O
multilinguality	O
.	O

Hu	O
et	O
al	O
.	O
(	O

2020	O
)	O
highlight	O
that	O
even	O
the	O
best	O
multilingual	O
models	O
do	O
not	O
yield	O
satisfactory	O
transfer	O
performance	O
on	O
the	O
XTREME	B-DatasetName
bechmark	O
covering	O
9	O
tasks	O
and	O
40	O
languages	O
.	O

Importantly	O
,	O
Wu	O
and	O
Dredze	O
(	O
2020	O
)	O
and	O
Lauscher	O
et	O
al	O
.	O
(	O

2020	O
)	O
observe	O
that	O
these	O
models	O
significantly	O
under	O
-	O
perform	O
for	O
low	O
-	O
resource	O
languages	O
as	O
representation	O
of	O
these	O
languages	O
in	O
the	O
vocabulary	O
and	O
pre	O
-	O
training	O
corpora	O
are	O
severely	O
limited	O
.	O

Language	O
-	O
specific	O
LMs	O
are	O
becoming	O
increasingly	O
popular	O
as	O
issues	O
with	O
multilingual	O
language	O
models	O
persist	O
.	O

As	O
language	O
identification	O
systems	O
are	O
extended	O
to	O
1000	O
+	O
languages	O
(	O
Caswell	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
increasing	O
capacity	O
for	O
a	O
single	O
model	O
to	O
uniformly	O
represent	O
all	O
languages	O
is	O
prohibitive	O
.	O

Often	O
,	O
practitioners	O
prefer	O
to	O
have	O
a	O
model	O
performing	O
well	O
on	O
a	O
subset	O
of	O
languages	O
that	O
their	O
application	O
calls	O
for	O
.	O

To	O
address	O
this	O
,	O
the	O
community	O
continues	O
its	O
efforts	O
in	O
building	O
strong	O
multi	O
-	O
domain	O
language	O
models	O
using	O
linguistic	O
expertise	O
.	O

A	O
few	O
examples	O
of	O
these	O
are	O
AraBERT	B-MethodName
(	O
Antoun	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
CamemBERT	B-MethodName
(	O
Martin	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
FinBERT	B-MethodName
(	O
Virtanen	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

2	O
Knowledge	O
Distillation	O
in	O
pre	O
-	O
trained	O
LMs	O
has	O
2	O
(	O
Nozza	O
et	O
al	O
.	O
,	O

2020	O
)	O
maintain	O
an	O
ever	O
-	O
growing	O
list	O
of	O
BERT	B-MethodName
models	O
here	O
most	O
commonly	O
been	O
used	O
for	O
task	O
-	O
specific	O
model	O
compression	O
of	O
a	O
teacher	O
into	O
a	O
single	O
-	O
task	O
student	O
(	O
Tang	O
et	O
al	O
.	O
,	O

2019;Kaliamoorthi	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

This	O
has	O
been	O
extended	O
to	O
perform	O
task	O
-	O
specific	O
distillation	O
of	O
multiple	O
single	O
-	O
task	O
teachers	O
into	O
one	O
multi	O
-	O
task	O
student	O
(	O
Clark	O
et	O
al	O
.	O
,	O

2019;Turc	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

In	O
the	O
task	O
-	O
agnostic	O
scenario	O
,	O
prior	O
work	O
has	O
focused	O
on	O
distilling	O
a	O
single	O
large	O
teacher	O
model	O
into	O
a	O
student	O
model	O
leveraging	O
teacher	O
predictions	O
(	O
Sanh	O
et	O
al	O
.	O
,	O

2019	O
)	O
or	O
internal	O
teacher	O
representations	O
(	O
Sun	O
et	O
al	O
.	O
,	O
,	O

2019	O
with	O
the	O
goal	O
of	O
model	O
compression	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
perform	O
task	O
-	O
agnostic	O
distillation	O
from	O
multiple	O
teachers	O
into	O
a	O
single	O
task	O
-	O
agnostic	O
student	O
.	O

In	O
the	O
context	O
of	O
neural	O
machine	O
translation	O
,	O
Tan	O
et	O
al	O
.	O
(	O

2019	O
)	O
come	O
close	O
to	O
our	O
work	O
where	O
they	O
attempt	O
to	O
combine	O
multiple	O
single	O
language	O
-	O
pair	O
teacher	O
models	O
to	O
train	O
a	O
multilingual	O
student	O
.	O

However	O
,	O
our	O
work	O
differs	O
from	O
theirs	O
in	O
three	O
key	O
aspects	O
:	O
1	O
)	O
our	O
students	O
are	O
task	O
-	O
agnostic	O
while	O
theirs	O
are	O
task	O
-	O
specific	O
,	O
2	O
)	O
we	O
can	O
leverage	O
pre	O
-	O
existing	O
teachers	O
while	O
they	O
can	O
not	O
,	O
and	O
3	O
)	O
we	O
support	O
teachers	O
with	O
overlapping	O
sets	O
of	O
languages	O
while	O
they	O
only	O
consider	O
single	O
language	O
-	O
pairs	O
teachers	O
.	O

Notations	O
:	O
Let	O
K	O
denote	O
the	O
set	O
of	O
languages	O
we	O
train	O
our	O
student	O
LM	O
on	O
and	O
T	O
denote	O
the	O
set	O
of	O
teacher	O
LMs	O
input	O
to	O
MERGEDISTILL	B-MethodName
3	O
.	O

Consequently	O
,	O
T	O
k	O
denotes	O
the	O
set	O
of	O
teacher	O
LMs	O
trained	O
on	O
language	O
k	O
,	O
where|T	O
k	O
|	O
≥	O
1	O
∀	O
k	O
∈	O
K.	O
An	O
overview	O
of	O
MERGEDISTILL	B-MethodName
is	O
presented	O
in	O
Figure	O
2	O
.	O

Here	O
we	O
detail	O
each	O
step	O
involved	O
in	O
training	O
the	O
student	O
LM	O
from	O
multiple	O
teacher	O
LMs	O
.	O

Step	O
1	O
:	O
Input	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pre	O
-	O
training	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

With	O
reference	O
to	O
Figure	O
2	O
,	O
the	O
student	O
LM	O
is	O
trained	O
on	O
K	O
=	O
{	O
English	O
(	O
en	O
)	O
,	O
Spanish	O
(	O
es	O
)	O
,	O
Hindi	O
(	O
hi	O
)	O
,	O
Korean	O
(	O
ko	O
)	O
}	O
.	O

We	O
combine	O
four	O
teacher	O
LMs	O
comprising	O
of	O
three	O
monolingual	O
and	O
one	O
multilingual	O
LM	O
.	O

The	O
monolingual	O
LMs	O
are	O
trained	O
on	O
English	O
(	O
M	O
en	O
)	O
,	O
Spanish	O
(	O
M	O
es	O
)	O
,	O
and	O
Korean	O
(	O
M	O
ko	O
)	O
while	O
the	O
multilingual	O
LM	O
is	O
trained	O
on	O
English	O
and	O
Hindi	O
(	O
M	O
en	O
,	O
hi	O
)	O
.	O

Therefore	O
,	O
for	O
each	O
language	O
,	O
the	O
corresponding	O
set	O
of	O
teacher	O
LMs	O
(	O
T	O
k	O
)	O
can	O
be	O
defined	O
as	O
:	O
[	O
T	O
en	O
=	O
{	O
M	O
en	O
,	O
M	O
en	O
,	O
hi	O
}	O
,	O
T	O
es	O
=	O
{	O
M	O
es	O
}	O
,	O
T	O
hi	O
=	O
{	O
M	O
en	O
,	O
hi	O
}	O
,	O
T	O
ko	O
=	O
{	O
M	O
ko	O
}	O
]	O
.	O

First	O
,	O
the	O
pretraining	O
transfer	O
corpora	O
is	O
tokenized	O
and	O
masked	O
for	O
each	O
language	O
using	O
their	O
respective	O
teacher	O
LM	O
's	O
tokenizer	O
.	O

For	O
the	O
language	O
with	O
two	O
teachers	O
,	O
English	O
,	O
we	O
tokenize	O
each	O
example	O
using	O
both	O
the	O
teacher	O
LMs	O
.	O

Step	O
2	O
:	O
Offline	O
Teacher	O
LM	O
Evaluation	O
We	O
now	O
obtain	O
predictions	O
and	O
logits	O
for	O
each	O
masked	O
,	O
tokenized	O
example	O
in	O
each	O
language	O
,	O
by	O
evaluating	O
their	O
respective	O
teacher	O
LMs	O
.	O

For	O
English	O
,	O
we	O
obtain	O
predictions	O
from	O
both	O
M	O
en	O
and	O
M	O
en	O
,	O
hi	O
on	O
their	O
respective	O
copies	O
of	O
each	O
training	O
example	O
.	O

In	O
an	O
ideal	O
situation	O
,	O
we	O
believe	O
that	O
multiple	O
strong	O
teachers	O
can	O
present	O
a	O
multi	O
-	O
view	O
generalisation	O
to	O
the	O
student	O
as	O
each	O
teacher	O
learns	O
different	O
features	O
in	O
training	O
.	O

Let	O
x	O
denote	O
a	O
sequence	O
of	O
tokens	O
where	O
x	O
m	O
=	O
{	O
x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
...	O
x	O
n	O
}	O
denote	O
the	O
masked	O
tokens	O
,	O
and	O
x	O
−m	O
denote	O
the	O
non	O
-	O
masked	O
tokens	O
.	O

Let	O
v	O
be	O
the	O
vocabulary	O
of	O
student	O
LM	O
θ	O
s	O
.	O

In	O
the	O
conventional	O
case	O
of	O
learning	O
from	O
gold	O
labels	O
,	O
we	O
minimize	O
the	O
cross	O
-	O
entropy	O
of	O
student	O
logit	O
distribution	O
for	O
a	O
masked	O
word	O
x	O
m	O
i	O
,	O
with	O
the	O
one	O
-	O
hot	O
label	O
v	O
j	O
,	O
given	O
by	O
:	O
P(x	O
m	O
i	O
,	O
v	O
j	O
)	O
=	O
1(x	O
m	O
i	O
=	O
v	O
j	O
)	O
×	O
log	O
p(x	O
m	O
i	O
=	O
v	O
j	O
|x	O
−m	O
;	O
θ	O
s	O
)	O
(	O
1)With	O
the	O
teacher	O
evaluations	O
,	O
we	O
obtain	O
predictions	O
(	O
and	O
corresponding	O
logits	O
)	O
of	O
the	O
teacher	O
for	O
the	O
masked	O
tokens	O
.	O

Let	O
us	O
denote	O
the	O
teacher	O
output	O
probability	O
distribution	O
(	O
softmax	O
over	O
logits	O
)	O
for	O
token	O
x	O
m	O
i	O
by	O
Q(x	O
m	O
i	O
|x	O
−m	O
;	O
θ	O
t	O
)	O
.	O

Therefore	O
,	O
in	O
addition	O
to	O
the	O
loss	O
from	O
gold	O
labels	O
,	O
we	O
minimize	O
the	O
entropy	O
between	O
the	O
student	O
logits	O
and	O
the	O
teacher	O
distribution	O
,	O
given	O
by	O
:	O
P(x	O
m	O
i	O
,	O
v	O
j	O
)	O
=	O
Q(x	O
m	O
i	O
=	O
v	O
j	O
|x	O
−m	O
;	O
θ	O
t	O
)	O
×	O
log	O
p(x	O
m	O
i	O
=	O
v	O
j	O
|x	O
−m	O
;	O
θ	O
s	O
)	O
(	O
2)It	O
is	O
extremely	O
burdensome	O
(	O
both	O
memory	O
and	O
time	O
)	O
to	O
load	O
multiple	O
teacher	O
LMs	O
and	O
obtain	O
predictions	O
during	O
training	O
.	O

Hence	O
,	O
we	O
first	O
store	O
the	O
top	O
-	O
k	B-HyperparameterName
logits	O
for	O
each	O
masked	O
word	O
offline	O
,	O
loading	O
and	O
normalizing	O
them	O
during	O
student	O
LM	O
training	O
,	O
similar	O
to	O
(	O
Tan	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Additionally	O
,	O
obtaining	O
offline	O
predictions	O
gives	O
one	O
the	O
freedom	O
to	O
use	O
expensive	O
teacher	O
LMs	O
without	O
increasing	O
the	O
student	O
model	O
training	O
costs	O
and	O
makes	O
our	O
framework	O
teacher	O
-	O
architecture	O
agnostic	O
.	O

Step	O
3	O
:	O
Vocab	O
Mapping	O
A	O
deterrent	O
in	O
attempting	O
to	O
distill	O
from	O
multiple	O
pre	O
-	O
trained	O
teacher	O
LMs	O
is	O
that	O
each	O
LM	O
has	O
its	O
own	O
vocabulary	O
.	O

This	O
makes	O
it	O
non	O
-	O
trivial	O
to	O
uniformly	O
process	O
an	O
input	O
example	O
for	O
consumption	O
by	O
both	O
the	O
teacher	O
and	O
student	O
LMs	O
.	O

Our	O
student	O
model	O
's	O
vocabulary	O
is	O
the	O
union	O
of	O
all	O
teacher	O
LM	O
vocabularies	O
.	O

In	O
the	O
vocab	O
mapping	O
step	O
,	O
the	O
input	O
indices	O
,	O
prediction	O
indices	O
,	O
and	O
the	O
gold	O
label	O
indices	O
,	O
obtained	O
after	O
evaluation	O
from	O
each	O
teacher	O
LM	O
are	O
processed	O
using	O
a	O
teacher→student	O
vocab	O
map	O
.	O

This	O
converts	O
each	O
teacher	O
token	O
index	O
to	O
its	O
corresponding	O
student	O
token	O
index	O
,	O
ready	O
for	O
consumption	O
by	O
the	O
student	O
model	O
.	O

For	O
simplicity	O
,	O
each	O
teacher	O
and	O
student	O
LM	O
uses	O
WordPiece	O
tokenization	O
(	O
Schuster	O
and	O
Nakajima	O
,	O
2012;Wu	O
et	O
al	O
.	O
,	O

2016	O
)	O
in	O
all	O
our	O
experiments	O
.	O

Step	O
L	O
MLM	O
(	O
x	O
m	O
|x	O
−m	O
)	O
=	O
−	O
1	O
n	O
n	O
i=1	O
|v|	O
j=1	O
P(x	O
m	O
i	O
,	O
v	O
j	O
)	O
In	O
addition	O
to	O
learning	O
from	O
gold	O
labels	O
,	O
we	O
use	O
teacher	O
predictions	O
as	O
soft	O
labels	O
and	O
minimize	O
the	O
cross	O
entropy	O
between	O
student	O
and	O
teacher	O
distributions	O
.	O

Let	O
L	O
KD	O
denote	O
the	O
KD	O
loss	O
from	O
a	O
single	O
teacher	O
LM	O
.	O

With	O
reference	O
to	O
Equation	O
2	O
:	O
L	O
KD	O
(	O
x	O
m	O
|x	O
−m	O
)	O
=	O
−	O
1	O
n	O
n	O
i=1	O
|v|	O
j=1P	O
(	O
x	O
m	O
i	O
,	O
v	O
j	O
)	O
;	O
The	O
total	O
loss	O
across	O
all	O
languages	O
is	O
minimized	O
,	O
as	O
shown	O
below	O
:	O
L	O
ALL	O
=	O
K	O
k=1	O
λ(L	O
T	O
k	O
KD	O
)	O
+	O
(	O
1	O
−	O
λ)L	O
k	O
MLMIn	O
the	O
case	O
of	O
multiple	O
teacher	O
LMs	O
,	O
we	O
have	O
n	O
tokenized	O
instances	O
for	O
a	O
given	O
example	O
(	O
where	O
n	O
denotes	O
the	O
number	O
of	O
teachers	O
for	O
a	O
particular	O
language	O
)	O
.	O

In	O
this	O
case	O
,	O
each	O
example	O
in	O
English	O
has	O
two	O
copies	O
-one	O
tokenized	O
using	O
M	O
en	O
and	O
another	O
using	O
M	O
en	O
,	O
hi	O
.	O

Thus	O
,	O
we	O
explore	O
two	O
possibilities	O
of	O
training	O
in	O
this	O
multi	O
-	O
teacher	O
scenario	O
:	O
•	O
Include	O
all	O
the	O
copies	O
in	O
training	O
.	O

Here	O
the	O
model	O
is	O
exposed	O
to	O
n	O
different	O
teacher	O
LM	O
predictions	O
,	O
each	O
presenting	O
a	O
multi	O
-	O
view	O
generalisation	O
to	O
the	O
student	O
LM.•	O
Include	O
the	O
best	O
copy	O
in	O
training	O
.	O

The	O
best	O
copy	O
is	O
the	O
one	O
having	O
minimum	O
teacher	O
LM	O
loss	O
for	O
a	O
given	O
example	O
.	O

Here	O
the	O
model	O
is	O
only	O
exposed	O
to	O
the	O
best	O
teacher	O
LM	O
predictions	O
for	O
each	O
example	O
.	O

In	O
this	O
section	O
,	O
we	O
aim	O
to	O
answer	O
the	O
following	O
questions	O
:	O
Distillation	O
Parameters	O
:	O
We	O
have	O
two	O
hyperparameter	O
choices	O
here	O
:	O
1	O
)	O
k	B-HyperparameterName
in	O
top	O
-	O
k	B-HyperparameterName
logits	O
-as	O
it	O
increases	O
,	O
we	O
observe	O
that	O
while	O
performances	O
remain	O
similar	O
,	O
storing	O
k>8	B-HyperparameterName
number	O
of	O
predictions	O
for	O
each	O
masked	O
word	O
offline	O
significantly	O
increases	O
resource	O
requirements	O
4	O
.	O

Hence	O
,	O
we	O
set	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

2	O
)	O
the	O
value	O
of	O
λ	B-HyperparameterName
in	O
the	O
loss	O
function	O
,	O
which	O
decides	O
the	O
proportion	O
of	O
teacher	O
loss	O
,	O
is	O
annealed	O
through	O
training	O
similar	O
to	O
Clark	O
et	O
al	O
.	O
(	O

2019).Evaluation	O
Metrics	O
:	O
We	O
report	O
F1	B-MetricName
scores	O
for	O
structured	O
prediction	O
tasks	O
(	O
NER	O
,	O
POS	O
)	O
,	O
accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O

scores	O
for	O
sentence	O
classification	O
tasks	O
(	O
XNLI	O
,	O
PAWS	O
-	O
X	O
)	O
,	O
and	O
F1	B-MetricName
/	O
Exact	B-MetricName
Match	I-MetricName
(	O
F1	B-MetricName
/	O
EM	B-MetricName
)	O
scores	O
for	O
question	O
answering	O
tasks	O
(	O
XQuAD	O
,	O
MLQA	O
,	O
TyDiQA	O
)	O
.	O

We	O
also	O
report	O
a	O
task	O
-	O
specific	O
relative	B-MetricName
deviation	I-MetricName
from	O
teachers	O
(	O
RDT	B-MetricName
)	O
(	O
in	O
%	O
)	O
averaged	O
across	O
all	O
languages	O
(	O
n	O
)	O
.	O

For	O
each	O
task	O
,	O
RDT	B-MetricName
is	O
calculated	O
as	O
:	O
RDT(S	B-MetricName
,	O
{	O
T	O
1	O
,	O
...	O
,	O
T	O
n	O
}	O
)	O
=	O
100	O
n	O
n	O
i=1	O
(	O
P	O
T	O
i	O
−	O
P	O
S	O
)	O
P	O
T	O
i(3)where	O
P	O
T	O
i	O
and	O
P	O
S	O
are	O
performances	O
of	O
the	O
i	O
th	O
teacher	O
and	O
student	O
LMs	O
,	O
respectively	O
.	O

Pre	O
-	O
training	O
:	O
In	O
this	O
experiment	O
,	O
we	O
use	O
preexisting	O
monolingual	O
teacher	O
LMs	O
,	O
as	O
shown	O
in	O
Table	O
1	O
,	O
to	O
train	O
a	O
multilingual	O
student	O
LM	O
on	O
the	O
union	O
of	O
all	O
teacher	O
languages	O
.	O

In	O
this	O
setup,|T	O
k	O
|	O
=	O
1	O
∀	O
k	O
∈	O
K	O
,	O
i.e.	O
,	O
each	O
language	O
can	O
learn	O
from	O
its	O
respective	O
monolingual	O
teacher	O
LM	O
only	O
.	O

Our	O
teacher	O
selection	O
and	O
setup	O
follows	O
a	O
two	O
-	O
step	O
process	O
.	O

First	O
,	O
we	O
aim	O
to	O
select	O
languages	O
having	O
pre	O
-	O
trained	O
monolingual	O
LMs	O
available	O
,	O
and	O
evaluation	O
sets	O
across	O
a	O
number	O
of	O
downstream	O
tasks	O
.	O

This	O
makes	O
us	O
choose	O
teacher	O
LMs	O
for	O
:	O
Arabic	O
(	O
ar	O
)	O
,	O
Chinese	O
(	O
zh	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Finnish	O
(	O
fi	O
)	O
,	O
Table	O
4	O
:	O
Results	O
for	O
multilingual	O
teacher	O
and	O
student	O
LMs	O
on	O
the	O
XTREME	B-MetricName
benchmark	O
.	O

We	O
compare	O
performances	O
of	O
three	O
student	O
LM	O
variants	O
as	O
described	O
in	O
Section	O
4.3	O
to	O
the	O
two	O
teachers	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

Relative	B-MetricName
deviations	I-MetricName
of	O
5	B-MetricValue
%	I-MetricValue
or	O
less	O
from	O
teacher	O
(	O
i.e.	O
,	O
RDT	B-MetricName
≥	O
−5	B-MetricValue
%	I-MetricValue
)	O
are	O
marked	O
in	O
bold	O
.	O

Overall	O
,	O
we	O
find	O
that	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
among	O
all	O
student	O
variants	O
and	O
report	O
its	O
RDT	B-MetricName
(	O
in	O
%	O
)	O
(	O
Equation	O
3	O
)	O
from	O
the	O
two	O
teachers	O
.	O

Please	O
refer	O
to	O
Section	O
4.3	O
for	O
a	O
detailed	O
analysis.we	O
can	O
attribute	O
this	O
gain	O
to	O
the	O
fact	O
that	O
English	O
is	O
trained	O
with	O
linguistically	O
and	O
typologically	O
similar	O
languages	O
in	O
Student	O
similar	O
.	O

Second	O
,	O
Student	O
similar	O
outperforms	O
its	O
teacher	O
LMs	O
while	O
Student	O
dissimilar	O
is	O
competitive	O
for	O
all	O
languages	O
.	O

These	O
two	O
results	O
across	O
all	O
languages	O
point	O
towards	O
Student	O
similar	O
benefiting	O
from	O
a	O
positive	O
transfer	O
across	O
similar	O
languages	O
.	O

In	O
Table	O
3	O
,	O
we	O
observe	O
that	O
Student	O
similar	O
is	O
trained	O
on	O
9.9	O
%	O
of	O
the	O
total	O
unique	O
tokens	O
seen	O
by	O
its	O
respective	O
teacher	O
LMs	O
and	O
Student	O
dissimilar	O
lies	O
close	O
with	O
13.6	O
%	O
.	O

Despite	O
this	O
huge	O
disparity	O
in	O
pre	O
-	O
training	O
corpora	O
,	O
student	O
LMs	O
are	O
competitive	O
with	O
their	O
teachers	O
.	O

This	O
encouraging	O
result	O
proves	O
that	O
even	O
with	O
very	O
limited	O
data	O
,	O
MERGEDISTILL	B-MethodName
enables	O
one	O
to	O
combine	O
strong	O
monolingual	O
teacher	O
LMs	O
to	O
train	O
competitive	O
student	O
LMs	O
that	O
can	O
leverage	O
the	O
benefits	O
of	O
multilinguality	O
.	O

Pre	O
-	O
training	O
:	O
In	O
this	O
experiment	O
,	O
we	O
make	O
use	O
of	O
pre	O
-	O
existing	O
multilingual	O
models	O
:	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

mBERT	B-MethodName
is	O
trained	O
on	O
104	O
languages	O
and	O
MuRIL	B-MethodName
covers	O
12	O
of	O
these	O
(	O
11	O
Indian	O
languages	O
+	O
English	O
):	O
Bengali	O
(	O
bn	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Gujarati	O
(	O
gu	O
)	O
,	O
Hindi	O
(	O
hi	O
)	O
,	O
Kannada	O
(	O
kn	O
)	O
,	O
Malayalam	O
(	O
ml	O
)	O
,	O
Marathi	O
(	O
mr	O
)	O
,	O
Nepali	O
(	O
ne	O
)	O
,	O
Punjabi	O
(	O
pa	O
)	O
,	O
Tamil	O
(	O
ta	O
)	O
,	O
Telugu	O
(	O
te	O
)	O
,	O
and	O
Urdu	O
(	O
ur	O
)	O
,	O
with	O
higher	O
performance	O
for	O
these	O
languages	O
on	O
the	O
XTREME	B-DatasetName
benchmark	O
.	O

We	O
train	O
the	O
student	O
model	O
on	O
all	O
104	O
languages	O
.	O

In	O
this	O
case	O
,	O
the	O
MuRIL	B-MethodName
Languages	O
(	O
MuL	O
)	O
have	O
two	O
as	O
shown	O
in	O
Table	O
3	O
teachers	O
(	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
)	O
and	O
the	O
Non	O
-	O
MuRIL	B-MethodName
Languages	O
(	O
Non	O
-	O
MuL	O
)	O
can	O
learn	O
from	O
mBERT	B-MethodName
only	O
.	O

Therefore	O
,	O
while	O
we	O
only	O
use	O
mBERT	B-MethodName
as	O
the	O
teacher	O
LM	O
for	O
Non	O
-	O
MuL	O
across	O
all	O
experiments	O
,	O
we	O
consider	O
three	O
possibilities	O
for	O
MuL	O
:	O
•	O
Student	O
Both	O
all	O
:	O
Tokenize	O
each	O
input	O
example	O
using	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
separately	O
and	O
include	O
both	O
copies	O
in	O
training.•	O
Student	O
Both	O
best	O
:	O
Tokenize	O
each	O
input	O
example	O
using	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
separately	O
and	O
include	O
only	O
the	O
best	O
copy	O
in	O
training	O
.	O

The	O
best	O
copy	O
is	O
the	O
one	O
having	O
minimum	O
teacher	O
LM	O
loss	O
for	O
the	O
example	O
.	O

Note	O
,	O
it	O
is	O
non	O
-	O
trivial	O
to	O
tokenize	O
each	O
example	O
in	O
a	O
way	O
that	O
is	O
compatible	O
with	O
all	O
teacher	O
LMs	O
.	O

One	O
must	O
resort	O
to	O
tokenization	O
using	O
an	O
intersection	O
of	O
vocabularies	O
which	O
is	O
sub	O
-	O
optimal	O
.	O

All	O
the	O
student	O
LMs	O
use	O
a	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
architecture	O
and	O
have	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
288,973	B-HyperparameterValue
.	O

We	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512.Finetuning	B-HyperparameterValue
:	O
We	O
report	O
zero	O
-	O
shot	O
performance	O
for	O
all	O
languages	O
in	O
the	O
XTREME	B-DatasetName
(	O
Hu	O
et	O
al	O
.	O
,	O

2020	O
)	O
benchmark	O
8	O
.Results	O
:	O
We	O
report	O
results	O
of	O
our	O
teacher	O
and	O
student	O
LMs	O
in	O
Table	O
4	O
.	O

Overall	O
,	O
we	O
find	O
that	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
among	O
all	O
student	O
variants	O
.	O

For	O
Non	O
-	O
MuL	O
,	O
Student	O
MuRIL	B-MethodName
beats	O
the	O
teacher	O
(	O
mBERT	B-MethodName
)	O
by	O
an	O
average	B-MetricName
relative	I-MetricName
score	I-MetricName
of	O
3.8	B-MetricValue
%	I-MetricValue
.	O

For	O
MuL	O
,	O
Student	O
MuRIL	B-MethodName
beats	O
one	O
teacher	O
(	O
mBERT	B-MethodName
)	O
by	O
8.8	B-MetricValue
%	I-MetricValue
,	O
but	O
underperforms	O
the	O
other	O
teacher	O
(	O
MuRIL	B-MethodName
)	O
by	O
3.8	B-MetricValue
%	I-MetricValue
.	O

There	O
can	O
be	O
two	O
factors	O
at	O
play	O
here	O
.	O

MuRIL	B-MethodName
is	O
trained	O
on	O
monolingual	O
and	O
parallel	O
data	O
9	O
while	O
the	O
student	O
LMs	O
only	O
see	O
∼22	O
%	O
of	O
unique	O
tokens	O
in	O
comparison	O
.	O

MuRIL	B-MethodName
also	O
has	O
different	O
language	O
sampling	O
strategies	O
(	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
as	O
opposed	O
to	O
0.7	B-HyperparameterValue
in	O
our	O
setting	O
,	O
where	O
a	O
lower	O
α	B-HyperparameterName
value	O
upsamples	O
more	O
rigorously	O
from	O
the	O
tail	O
languages	O
)	O
,	O
which	O
have	O
a	O
significant	O
role	O
to	O
play	O
in	O
multilingual	O
model	O
performances	O
(	O
Conneau	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

We	O
also	O
observe	O
a	O
significant	O
drop	O
in	O
Student	O
mBERT	B-MethodName
's	O
performance	O
for	O
MuL	O
when	O
compared	O
to	O
the	O
other	O
student	O
LM	O
variants	O
.	O

This	O
might	O
be	O
because	O
the	O
input	O
is	O
tokenized	O
using	O
the	O
mBERT	B-MethodName
tokenizer	O
which	O
prevents	O
learning	O
from	O
MuRIL	B-MethodName
tokens	O
in	O
the	O
student	O
vocabulary	O
.	O

For	O
Student	O
Both	O
,	O
we	O
do	O
not	O
observe	O
much	O
of	O
a	O
difference	O
between	O
Student	O
Both	O
all	O
and	O
Student	O
Both	O
best	O
.	O

This	O
observation	O
may	O
differ	O
with	O
one	O
's	O
choice	O
of	O
teacher	O
LMs	O
depending	O
on	O
how	O
well	O
it	O
performs	O
for	O
a	O
particular	O
language	O
.	O

In	O
our	O
case	O
,	O
we	O
do	O
n't	O
observe	O
much	O
of	O
a	O
difference	O
in	O
incorporating	O
mBERT	B-MethodName
predictions	O
for	O
MuL.8	O
More	O
details	O
in	O
Appendix	O
A.3	O
9	O
More	O
details	O
in	O
Appendix	O
A.2	O
The	O
importance	O
of	O
vocabulary	O
and	O
teacher	O
LM	O
preditions	O
:	O
In	O
Furthermore	O
,	O
we	O
also	O
observe	O
that	O
SM2	B-MethodName
and	O
SM3	B-MethodName
achieve	O
competitive	O
performances	O
despite	O
SM3	B-MethodName
being	O
additionally	O
trained	O
on	O
teacher	O
LM	O
labels	O
.	O

To	O
motivate	O
the	O
need	O
for	O
teacher	O
predictions	O
,	O
Hinton	O
et	O
al	O
.	O
(	O

2015	O
)	O
argue	O
that	O
when	O
soft	O
targets	O
have	O
high	O
entropy	O
,	O
they	O
provide	O
much	O
more	O
information	O
per	O
training	O
case	O
than	O
hard	O
targets	O
and	O
can	O
be	O
trained	O
on	O
much	O
less	O
data	O
than	O
the	O
original	O
cumbersome	O
model	O
.	O

In	O
our	O
case	O
,	O
we	O
hypothesize	O
that	O
training	O
on	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
exposes	O
the	O
model	O
to	O
sufficient	O
data	O
for	O
it	O
to	O
generalize	O
well	O
enough	O
and	O
mask	O
the	O
benefits	O
of	O
teacher	O
LM	O
predictions	O
.	O

To	O
validate	O
this	O
,	O
we	O
evaluate	O
the	O
performances	O
of	O
SM2	B-MethodName
and	O
SM3	B-MethodName
,	O
20	O
%	O
into	O
training	O
(	O
i.e.	O
100,000	O
steps	O
/	O
500,000	O
total	O
steps	O
)	O
as	O
shown	O
in	O
Table	O
5	O
.	O

We	O
observe	O
a	O
∼2.9	B-MetricValue
%	I-MetricValue
gain	B-MetricName
in	O
average	O
performance	O
for	O
SM3	B-MethodName
over	O
SM2	B-MethodName
,	O
clearly	O
highlighting	O
the	O
importance	O
of	O
teacher	O
LM	O
predictions	O
in	O
a	O
limited	O
data	O
scenario	O
.	O

This	O
is	O
especially	O
important	O
when	O
one	O
has	O
access	O
to	O
very	O
limited	O
monolingual	O
data	O
and	O
a	O
strong	O
teacher	O
LM	O
for	O
a	O
particular	O
language	O
.	O

Pre	O
-	O
trained	O
zero	O
-	O
shot	O
transfer	O
:	O
Interestingly	O
,	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
on	O
almost	O
all	O
tasks	O
for	O
Non	O
-	O
MuL.	O
This	O
hints	O
at	O
positive	O
transfer	O
from	O
strong	O
teachers	O
to	O
languages	O
that	O
the	O
teacher	O
does	O
not	O
cover	O
at	O
all	O
,	O
due	O
to	O
the	O
shared	O
multilingual	O
representations	O
.	O

10	O
This	O
would	O
mean	O
that	O
learning	O
from	O
strong	O
teachers	O
can	O
improve	O
the	O
student	O
model	O
's	O
performance	O
in	O
a	O
zero	O
-	O
shot	O
manner	O
on	O
related	O
languages	O
not	O
covered	O
by	O
the	O
teacher	O
.	O

This	O
would	O
make	O
MERGEDISTILL	B-MethodName
highly	O
beneficial	O
for	O
low	O
-	O
resource	O
languages	O
that	O
do	O
not	O
have	O
a	O
strong	O
teacher	O
or	O
limited	O
gold	O
data	O
.	O

We	O
leave	O
this	O
exploration	O
to	O
future	O
work	O
.	O

In	O
this	O
paper	O
we	O
address	O
the	O
problem	O
of	O
merging	O
multiple	O
pre	O
-	O
trained	O
teacher	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
by	O
proposing	O
MERGEDIS	B-MethodName
-	I-MethodName
TILL	I-MethodName
,	O
a	O
task	O
-	O
agnostic	O
distillation	O
method	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
attempt	O
of	O
its	O
kind	O
.	O

The	O
student	O
LM	O
learned	O
by	O
MERGEDISTILL	B-MethodName
may	O
be	O
further	O
fine	O
-	O
tuned	O
for	O
any	O
task	O
across	O
all	O
of	O
the	O
languages	O
covered	O
by	O
the	O
teacher	O
LMs	O
.	O

Our	O
approach	O
results	O
in	O
better	O
maintainability	O
(	O
fewer	O
models	O
)	O
and	O
is	O
compute	O
efficient	O
(	O
due	O
to	O
offline	O
predictions	O
)	O
.	O

We	O
use	O
MERGEDISTILL	B-MethodName
to	O
i	O
)	O
combine	O
monolingual	O
teacher	O
LMs	O
into	O
one	O
student	O
multilingual	O
LM	O
which	O
is	O
competitive	O
with	O
the	O
teachers	O
,	O
thereby	O
demonstrating	O
positive	O
crosslingual	O
transfer	O
,	O
and	O
ii	O
)	O
combine	O
multilingual	O
LMs	O
to	O
train	O
student	O
LMs	O
that	O
learn	O
from	O
multiple	O
teachers	O
.	O

Through	O
experiments	O
on	O
multiple	O
benchmark	O
datasets	O
,	O
we	O
show	O
that	O
student	O
LMs	O
learned	O
by	O
MERGEDISTILL	B-MethodName
perform	O
competitively	O
or	O
even	O
outperform	O
teacher	O
LMs	O
trained	O
on	O
orders	O
of	O
magnitude	O
more	O
data	O
.	O

We	O
disentangle	O
the	O
positive	O
impact	O
of	O
incorporating	O
strong	O
teacher	O
LM	O
vocabu	O
-	O
laries	O
and	O
learning	O
from	O
teacher	O
LM	O
predictions	O
,	O
highlighting	O
the	O
importance	O
of	O
the	O
latter	O
in	O
a	O
limited	O
data	O
scenario	O
.	O

We	O
also	O
find	O
that	O
MERGEDIS	B-MethodName
-	I-MethodName
TILL	I-MethodName
enables	O
positive	O
transfer	O
from	O
strong	O
teachers	O
to	O
languages	O
not	O
covered	O
by	O
them	O
(	O
i.e.	O
zero	O
-	O
shot	O
transfer	O
)	O
.	O

Our	O
work	O
bridges	O
the	O
gap	O
between	O
the	O
universe	O
of	O
language	O
-	O
specific	O
models	O
and	O
massively	O
multilingual	O
LMs	O
,	O
incorporating	O
benefits	O
of	O
both	O
into	O
one	O
framework	O
.	O

We	O
pre	O
-	O
train	O
our	O
student	O
models	O
using	O
the	O
BERT	B-MethodName
base	I-MethodName
architecture	O
.	O

Student	O
similar	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
99112	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
162	O
M	O
parameters	O
.	O

Student	O
different	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
180996	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
225	O
M	O
parameters	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
250k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

We	O
use	O
TPUs	O
,	O
and	O
it	O
takes	O
around	O
1.5	O
days	O
to	O
pre	O
-	O
train	O
each	O
student	O
LM	O
.	O

We	O
pre	O
-	O
train	O
our	O
student	O
models	O
using	O
the	O
BERT	B-MethodName
base	I-MethodName
architecture	O
.	O

All	O
student	O
LMs	O
have	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
288973	B-HyperparameterName
.	O

Hence	O
,	O
we	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

We	O
use	O
TPUs	O
,	O
and	O
it	O
takes	O
around	O
3	O
days	O
to	O
pre	O
-	O
train	O
each	O
student	O
LM.We	O
present	O
pre	O
-	O
training	O
data	O
statistics	O
for	O
MuRIL	B-MethodName
and	O
the	O
student	O
LMs	O
in	O
Table	O
6	O
.	O

Here	O
we	O
only	O
include	O
the	O
monolingual	O
data	O
statistics	O
,	O
but	O
MuRIL	B-MethodName
is	O
additionally	O
trained	O
on	O
parallel	O
translated	O
and	O
transliterated	O
data	O
.	O

(	O
Fang	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

All	O
results	O
are	O
computed	O
in	O
a	O
zero	O
-	O
shot	O
setting	O
.	O

Hyperparameter	O
Details	O
We	O
use	O
the	O
same	O
hyperparameters	O
for	O
fine	O
-	O
tuning	O
all	O
teacher	O
and	O
student	O
LMs	O
,	O
as	O
shown	O
in	O
Table	O
11	O
.	O

We	O
report	O
results	O
on	O
the	O
best	O
-	O
performing	O
checkpoint	O
for	O
the	O
We	O
present	O
results	O
for	O
Student	O
MuRIL	B-MethodName
trained	O
with	O
different	O
top	O
-	O
k	B-HyperparameterName
values	O
from	O
teacher	O
predictions	O
in	O
Table	O
8	O
.	O

We	O
observe	O
that	O
while	O
performances	O
remain	O
similar	O
for	O
higher	O
values	O
of	O
k	O
,	O
storage	O
becomes	O
increasingly	O
expensive	O
.	O

Hence	O
,	O
we	O
stick	O
to	O
a	O
value	O
of	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
and	O
constructive	O
feedback	O
.	O

We	O
thank	O
Iulia	O
Turc	O
,	O
Ming	O
-	O
Wei	O
Chang	O
,	O
and	O
Slav	O
Petrov	O
for	O
valuable	O
comments	O
on	O
an	O
earlier	O
version	O
of	O
this	O
paper	O
.	O

We	O
train	O
our	O
LMs	O
with	O
the	O
MLM	O
objective	O
.	O

Let	O
x	O
denote	O
a	O
sequence	O
of	O
tokens	O
where	O
x	O
m	O
=	O
{	O
x	O
1	O
,	O
x	O
2	O
,	O
x	O
3	O
...	O
x	O
n	O
}	O
denote	O
the	O
masked	O
tokens	O
,	O
and	O
x	O
−m	O
denote	O
the	O
non	O
-	O
masked	O
tokens	O
.	O

Let	O
v	O
be	O
the	O
vocabulary	O
of	O
LM	O
θ	O
.	O

The	O
log	O
-	O
likelihood	O
loss	O
(	O
crossentropy	O
with	O
one	O
-	O
hot	O
label	O
)	O
can	O
be	O
formulated	O
as	O
follows	O
:	O
In	O
a	O
distillation	O
setup	O
,	O
the	O
student	O
is	O
trained	O
to	O
not	O
only	O
match	O
the	O
one	O
-	O
hot	O
labels	O
for	O
masked	O
words	O
,	O
but	O
also	O
the	O
probability	O
output	O
distribution	O
of	O
the	O
teacher	O
t.	O
Let	O
us	O
denote	O
the	O
teacher	O
output	O
probability	O
distribution	O
for	O
token	O
x	O
m	O
i	O
by	O
Q(x	O
m	O
i	O
|x	O
−m	O
;	O
θ	O
t	O
)	O
.The	O
cross	O
entropy	O
between	O
the	O
teacher	O
and	O
student	O
distributions	O
then	O
serves	O
as	O
the	O
distillation	O
loss	O
:	O
The	O
total	O
loss	O
is	O
then	O
defined	O
as	O
:	O
With	O
the	O
addition	O
of	O
the	O
teacher	O
,	O
the	O
target	O
distribution	O
is	O
no	O
longer	O
a	O
single	O
one	O
-	O
hot	O
label	O
,	O
but	O
a	O
smoother	O
distribution	O
with	O
multiple	O
words	O
having	O
non	O
-	O
zero	O
probabilities	O
which	O
yields	O
in	O
a	O
smaller	O
variance	O
in	O
gradients	O
(	O
Hinton	O
et	O
al	O
.	O
,	O

2015	O
)	O
.	O

Intuitively	O
,	O
a	O
single	O
masked	O
word	O
can	O
have	O
several	O
valid	O
predictions	O
,	O
which	O
appropriately	O
fit	O
the	O
context	O
.	O

Warning	O
:	O
This	O
paper	O
contains	O
comments	O
that	O
may	O
be	O
offensive	O
or	O
upsetting	O
.	O

On	O
social	O
media	O
platforms	O
,	O
hateful	O
and	O
offensive	O
language	O
negatively	O
impact	O
the	O
mental	O
well	O
-	O
being	O
of	O
users	O
and	O
the	O
participation	O
of	O
people	O
from	O
diverse	O
backgrounds	O
.	O

Automatic	O
methods	O
to	O
detect	O
offensive	O
language	O
have	O
largely	O
relied	O
on	O
datasets	O
with	O
categorical	O
labels	O
.	O

However	O
,	O
comments	O
can	O
vary	O
in	O
their	O
degree	O
of	O
offensiveness	O
.	O

We	O
create	O
the	O
first	O
dataset	O
of	O
English	O
language	O
Reddit	O
comments	O
that	O
has	O
finegrained	O
,	O
real	O
-	O
valued	O
scores	O
between	O
-1	O
(	O
maximally	O
supportive	O
)	O
and	O
1	O
(	O
maximally	O
offensive	O
)	O
.	O

The	O
dataset	O
was	O
annotated	O
using	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
a	O
form	O
of	O
comparative	O
annotation	O
that	O
has	O
been	O
shown	O
to	O
alleviate	O
known	O
biases	O
of	O
using	O
rating	O
scales	O
.	O

We	O
show	O
that	O
the	O
method	O
produces	O
highly	O
reliable	O
offensiveness	O
scores	O
.	O

Finally	O
,	O
we	O
evaluate	O
the	O
ability	O
of	O
widely	O
-	O
used	O
neural	O
models	O
to	O
predict	O
offensiveness	O
scores	O
on	O
this	O
new	O
dataset	O
.	O

Social	O
media	O
platforms	O
serve	O
as	O
a	O
medium	O
for	O
exchange	O
of	O
ideas	O
on	O
a	O
range	O
of	O
topics	O
,	O
from	O
the	O
personal	O
to	O
the	O
political	O
.	O

This	O
exchange	O
can	O
,	O
however	O
,	O
be	O
disrupted	O
by	O
offensive	O
or	O
hateful	O
language	O
.	O

Such	O
language	O
is	O
pervasive	O
online	O
(	O
Statista	O
,	O
2020b	O
)	O
,	O
and	O
exposure	O
to	O
it	O
may	O
have	O
numerous	O
negative	O
consequences	O
for	O
the	O
victim	O
's	O
mental	O
health	O
(	O
Munro	O
,	O
2011	O
)	O
.	O

Automated	B-TaskName
offensive	I-TaskName
language	I-TaskName
detection	I-TaskName
has	O
thus	O
been	O
gaining	O
interest	O
in	O
the	O
NLP	O
community	O
,	O
as	O
a	O
promising	O
direction	O
to	O
better	O
understand	O
the	O
nature	O
and	O
spread	O
of	O
such	O
content	O
.	O

There	O
are	O
several	O
challenges	O
in	O
the	O
automatic	B-TaskName
detection	I-TaskName
of	I-TaskName
offensive	I-TaskName
language	I-TaskName
(	O
Wiedemann	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

The	O
NLP	O
community	O
has	O
adopted	O
various	O
definitions	O
for	O
offensive	O
language	O
,	O
classifying	O
it	O
into	O
specific	O
categories	O
.	O

For	O
example	O
,	O
Waseem	O
and	O
*	O
Both	O
authors	O
contributed	O
equally	O
.	O

Hovy	O
(	O
2016	O
)	O
classified	O
comments	O
as	O
racist	O
,	O
sexist	O
,	O
neither	O
;	O
as	O
hate	O
-	O
speech	O
,	O
offensive	O
but	O
not	O
hate	O
-	O
speech	O
,	O
neither	O
offensive	O
nor	O
hate	O
-	O
speech	O
and	O
Founta	O
et	O
al	O
.	O
(	O

2018	O
)	O
as	O
abusive	O
,	O
hateful	O
,	O
normal	O
,	O
spam	O
.	O

Schmidt	O
and	O
Wiegand	O
(	O
2017	O
)	O
;	O
Fortuna	O
and	O
Nunes	O
(	O
2018	O
)	O
;	O
Mishra	O
et	O
al	O
.	O
(	O

2019	O
)	O
;	O
Kiritchenko	O
and	O
Nejadgholi	O
(	O
2020	O
)	O
summarize	O
the	O
different	O
definitions	O
.	O

However	O
,	O
these	O
categories	O
have	O
significant	O
overlaps	O
with	O
each	O
other	O
,	O
creating	O
ill	O
-	O
defined	O
boundaries	O
,	O
thus	O
introducing	O
ambiguity	O
and	O
annotation	O
inconsistency	O
(	O
Founta	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

A	O
further	O
challenge	O
is	O
that	O
after	O
encountering	O
several	O
highly	O
offensive	O
comments	O
,	O
an	O
annotator	O
might	O
find	O
subsequent	O
moderately	O
offensive	O
comments	O
to	O
not	O
be	O
offensive	O
(	O
de	O
-	O
sensitization	O
)	O
(	O
Kurrek	O
et	O
al	O
.	O
,	O

2020;Soral	O
et	O
al	O
.	O
,	O

2018).At	O
the	O
same	O
time	O
,	O
existing	O
approaches	O
do	O
not	O
take	O
into	O
account	O
that	O
comments	O
can	O
be	O
offensive	O
to	O
a	O
different	O
degree	O
.	O

Knowing	O
the	O
degree	O
of	O
offensiveness	O
of	O
a	O
comment	O
has	O
practical	O
implications	O
,	O
when	O
taking	O
action	O
against	O
inappropriate	O
behaviour	O
online	O
,	O
as	O
it	O
allows	O
for	O
a	O
more	O
fine	O
-	O
grained	O
analysis	O
and	O
prioritization	O
in	O
moderation	O
.	O

The	O
representation	O
of	O
the	O
offensive	O
class	O
in	O
a	O
dataset	O
is	O
often	O
boosted	O
using	O
different	O
strategies	O
.	O

The	O
most	O
common	O
strategy	O
used	O
is	O
key	B-MethodName
-	I-MethodName
word	I-MethodName
based	I-MethodName
sampling	I-MethodName
.	O

This	O
results	O
in	O
datasets	O
that	O
are	O
rich	O
in	O
explicit	O
offensive	O
language	O
(	O
language	O
that	O
is	O
unambiguous	O
in	O
its	O
potential	O
to	O
be	O
offensive	O
,	O
such	O
as	O
those	O
using	O
slurs	O
or	O
swear	O
words	O
(	O
Waseem	O
et	O
al	O
.	O
,	O

2017	O
)	O
)	O
but	O
lack	O
cases	O
of	O
implicit	O
offensive	O
language	O
(	O
language	O
with	O
its	O
true	O
offensive	O
nature	O
obscured	O
due	O
to	O
lack	O
of	O
unambiguous	O
swear	O
words	O
,	O
usage	O
of	O
sarcasm	O
or	O
offensive	O
analogies	O
,	O
and	O
others	O
(	O
Waseem	O
et	O
al	O
.	O
,	O

2017;Wiegand	O
et	O
al	O
.	O
,	O

2021	O
)	O
)	O
(	O
Waseem	O
,	O
2016;Wiegand	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

key	B-MethodName
-	I-MethodName
word	I-MethodName
based	I-MethodName
sampling	I-MethodName
often	O
results	O
in	O
spurious	O
correlations	O
(	O
e.g.	O
,	O
sports	O
-	O
related	O
expressions	O
such	O
as	O
announcer	O
and	O
sport	O
occur	O
very	O
frequently	O
in	O
offensive	O
tweets	O
)	O
.	O

Lastly	O
,	O
existing	O
datasets	O
consider	O
of	O
-	O
fensive	O
comments	O
in	O
isolation	O
from	O
the	O
wider	O
conversation	O
of	O
which	O
they	O
are	O
a	O
part	O
.	O

Offensive	O
language	O
is	O
,	O
however	O
,	O
inherently	O
a	O
social	O
phenomenon	O
and	O
its	O
analysis	O
has	O
much	O
to	O
gain	O
from	O
taking	O
the	O
conversational	O
context	O
into	O
account	O
(	O
Gao	O
and	O
Huang	O
,	O
2017).In	O
this	O
paper	O
,	O
we	O
present	O
the	O
first	O
dataset	O
of	O
6000	O
English	O
language	O
Reddit	O
comments	O
that	O
has	O
finegrained	O
,	O
real	O
-	O
valued	O
scores	O
between	O
-1	O
(	O
maximally	O
supportive	O
)	O
and	O
1	O
(	O
maximally	O
offensive	O
)	O
-normative	O
offensiveness	O
ratings	O
for	O
the	O
comments	O
.	O

For	O
the	O
first	O
time	O
,	O
we	O
use	O
comparative	O
annotations	O
to	O
detect	O
offensive	O
language	O
.	O

In	O
its	O
simplest	O
form	O
,	O
comparative	O
annotations	O
involve	O
giving	O
the	O
annotators	O
two	O
instances	O
at	O
a	O
time	O
,	O
and	O
asking	O
which	O
exhibits	O
the	O
property	O
of	O
interest	O
to	O
a	O
greater	O
extent	O
.	O

This	O
alleviates	O
several	O
annotation	O
biases	O
present	O
in	O
standard	O
rating	O
scales	O
,	O
such	O
as	O
scale	O
-	O
region	O
bias	O
(	O
Presser	O
and	O
Schuman	O
,	O
1996;Asaadi	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
and	O
improves	O
annotation	O
consistency	O
(	O
Kiritchenko	O
and	O
Mohammad	O
,	O
2017	O
)	O
.	O

However	O
,	O
instead	O
of	O
needing	O
to	O
annotate	O
N	O
instances	O
,	O
one	O
now	O
needs	O
to	O
annotate	O
N	O
2	O
instance	O
pairs	O
-	O
which	O
can	O
be	O
prohibitive	O
.	O

Thus	O
,	O
we	O
annotate	O
our	O
dataset	O
using	O
an	O
efficient	O
form	O
of	O
comparative	O
annotation	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
(	I-MethodName
BWS	I-MethodName
)	I-MethodName
(	O
Louviere	O
,	O
1991;Louviere	O
et	O
al	O
.	O
,	O

2015;Mohammad	O
,	O
2016	O
,	O
2017).By	O
eliminating	O
different	O
offensiveness	O
categories	O
,	O
treating	O
offensiveness	O
as	O
a	O
continuous	O
dimension	O
,	O
and	O
eliciting	O
comparative	O
judgments	O
from	O
the	O
annotators	O
(	O
based	O
on	O
their	O
understanding	O
of	O
what	O
is	O
offensive	O
)	O
,	O
we	O
alleviate	O
the	O
issues	O
regarding	O
category	O
definitions	O
and	O
arbitrary	O
category	O
boundaries	O
discussed	O
earlier	O
.	O

By	O
obtaining	O
real	O
-	O
valued	O
offensiveness	O
scores	O
,	O
different	O
thresholds	O
can	O
be	O
used	O
in	O
downstream	O
applications	O
to	O
handle	O
varying	O
degrees	O
of	O
offensiveness	O
appropriately	O
.	O

By	O
framing	O
the	O
task	O
as	O
a	O
comparative	O
annotation	O
task	O
,	O
we	O
obtain	O
consistent	O
and	O
reliable	O
annotations	O
.	O

We	O
also	O
greatly	O
mitigate	O
issues	O
of	O
annotator	O
de	O
-	O
sensitization	O
as	O
one	O
will	O
still	O
be	O
able	O
to	O
recognize	O
if	O
one	O
comment	O
is	O
more	O
offensive	O
than	O
another	O
,	O
even	O
if	O
they	O
think	O
both	O
comments	O
are	O
not	O
that	O
offensive	O
.	O

In	O
contrast	O
to	O
existing	O
resources	O
,	O
which	O
provide	O
annotations	O
for	O
individual	O
comments	O
,	O
our	O
dataset	O
includes	O
conversational	O
context	O
for	O
each	O
comment	O
(	O
i.e.	O
the	O
Reddit	O
thread	O
in	O
which	O
the	O
comment	O
occurred	O
)	O
.	O

We	O
conduct	O
quantitative	O
and	O
qualitative	O
analyses	O
of	O
the	O
dataset	O
to	O
obtain	O
insights	O
into	O
how	O
emotions	O
,	O
identity	O
terms	O
,	O
swear	O
words	O
,	O
are	O
related	O
to	O
offensiveness	O
.	O

Finally	O
,	O
we	O
benchmark	O
several	O
widely	O
-	O
used	O
neural	O
models	O
in	O
their	O
ability	O
to	O
predict	O
offensiveness	O
scores	O
on	O
this	O
new	O
dataset	O
.	O

1	O
2	O
Related	O
Work	O
Surveys	O
by	O
Schmidt	O
and	O
Wiegand	O
(	O
2017	O
)	O
;	O
Fortuna	O
and	O
Nunes	O
(	O
2018	O
)	O
;	O
Mishra	O
et	O
al	O
.	O
(	O

2019	O
)	O
;	O
Vidgen	O
and	O
Derczynski	O
(	O
2020	O
)	O
discuss	O
various	O
existing	O
datasets	O
and	O
their	O
compositions	O
in	O
detail	O
.	O

Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
;	O
;	O
Founta	O
et	O
al	O
.	O
(	O

2018	O
)	O
created	O
datasets	O
based	O
on	O
Twitter	O
data	O
.	O

Due	O
to	O
prevalence	O
of	O
the	O
non	O
-	O
offensive	O
class	O
in	O
naturallyoccurring	O
data	O
(	O
Waseem	O
,	O
2016;Founta	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
the	O
authors	O
devised	O
techniques	O
to	O
boost	O
the	O
presence	O
of	O
the	O
offensive	O
class	O
in	O
the	O
dataset	O
.	O

Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
used	O
terms	O
frequently	O
occurring	O
in	O
offensive	O
tweets	O
,	O
while	O
used	O
a	O
list	O
of	O
hate	O
-	O
related	O
terms	O
to	O
extract	O
offensive	O
tweets	O
from	O
the	O
Twitter	O
search	O
API	O
.	O

Park	O
et	O
al	O
.	O
(	O

2018	O
)	O
,	O
Wiegand	O
et	O
al	O
.	O
(	O

2019	O
)	O
,	O
and	O
Davidson	O
et	O
al	O
.	O
(	O

2019	O
)	O
show	O
that	O
the	O
Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
dataset	O
exhibits	O
topic	O
bias	O
and	O
author	O
bias	O
due	O
to	O
the	O
employed	O
sampling	O
strategy	O
.	O

Founta	O
et	O
al	O
.	O
(	O

2018	O
)	O
boosted	O
the	O
representation	O
of	O
offensive	O
class	O
in	O
their	O
dataset	O
by	O
analysing	O
the	O
sentiment	O
of	O
the	O
tweets	O
and	O
checking	O
for	O
the	O
presence	O
of	O
offensive	O
terms	O
.	O

In	O
our	O
work	O
,	O
we	O
employ	O
a	O
hybrid	O
approach	O
,	O
selecting	O
our	O
data	O
in	O
three	O
ways	O
:	O
specific	O
topics	O
,	O
emotion	O
-	O
related	O
key	O
-	O
words	O
,	O
and	O
random	O
sampling	O
.	O

Past	O
work	O
has	O
partitioned	O
offensive	O
comments	O
into	O
explicitly	O
offensive	O
(	O
those	O
that	O
include	O
profanity	O
-	O
swear	O
words	O
,	O
taboo	O
words	O
,	O
or	O
hate	O
terms	O
)	O
and	O
implicitly	O
offensive	O
(	O
those	O
that	O
do	O
not	O
include	O
profanity	O
)	O
(	O
Waseem	O
et	O
al	O
.	O
,	O

2017;Caselli	O
et	O
al	O
.	O
,	O

2020a;Wiegand	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

Some	O
other	O
past	O
work	O
has	O
defined	O
explicitly	O
and	O
implicitly	O
offensive	O
instances	O
a	O
little	O
differently	O
:	O
Sap	O
et	O
al	O
.	O
(	O

2020	O
)	O
considered	O
factors	O
such	O
as	O
obviousness	O
,	O
intent	O
to	O
offend	O
and	O
biased	O
implications	O
,	O
Breitfeller	O
et	O
al	O
.	O
(	O

2019	O
)	O
considered	O
factors	O
such	O
as	O
the	O
context	O
and	O
the	O
person	O
annotating	O
the	O
instance	O
,	O
and	O
Razo	O
and	O
Kübler	O
(	O
2020	O
)	O
considered	O
the	O
kind	O
of	O
lexicon	O
used	O
.	O

Regardless	O
of	O
the	O
exact	O
definition	O
,	O
implicit	O
offensive	O
language	O
,	O
due	O
to	O
a	O
lack	O
of	O
lexical	O
cues	O
,	O
is	O
harder	O
to	O
classify	O
not	O
only	O
for	O
computational	O
models	O
,	O
but	O
also	O
for	O
humans	O
.	O

In	O
our	O
work	O
,	O
we	O
consider	O
implicitly	O
offensive	O
comments	O
as	O
those	O
offensive	O
comments	O
that	O
do	O
not	O
contain	O
any	O
swear	O
words	O
.	O

Wulczyn	O
et	O
al	O
.	O
(	O

2016Wulczyn	O
et	O
al	O
.	O
(	O
,	O

2017	O
created	O
three	O
different	O
datasets	O
from	O
Wikipedia	O
Talk	O
pages	O
,	O
focusing	O
on	O
aggression	O
,	O
personal	O
attacks	O
and	O
toxicity	O
.	O

The	O
comments	O
were	O
sampled	O
at	O
random	O
from	O
a	O
large	O
dump	O
of	O
English	O
Wikipedia	O
,	O
and	O
boosted	O
by	O
including	O
comments	O
from	O
blocked	O
users	O
.	O

For	O
the	O
personal	O
attacks	O
dataset	O
,	O
Wulczyn	O
et	O
al	O
.	O
(	O

2016	O
)	O
used	O
two	O
different	O
kinds	O
of	O
labels	O
:	O
ED	O
(	O
empirical	O
distribution	O
)	O
,	O
OH	O
(	O
one	O
hot	O
)	O
.	O

In	O
case	O
of	O
ED	O
,	O
the	O
comments	O
were	O
assigned	O
real	O
-	O
valued	O
scores	O
between	O
0	O
and	O
1	O
representing	O
the	O
fraction	O
of	O
annotators	O
who	O
considered	O
the	O
comment	O
a	O
personal	O
attack	O
.	O

While	O
these	O
labels	O
were	O
introduced	O
to	O
create	O
a	O
separation	O
between	O
the	O
nature	O
of	O
comments	O
with	O
a	O
score	O
of	O
1.0	O
and	O
those	O
with	O
a	O
score	O
of	O
0.6	O
(	O
which	O
would	O
otherwise	O
be	O
classified	O
as	O
attacks	O
)	O
,	O
they	O
are	O
discrete	O
.	O

In	O
our	O
work	O
,	O
using	O
the	O
BWS	B-MethodName
comparative	O
annotation	O
setup	O
,	O
we	O
assign	O
fine	O
-	O
grained	O
continuous	O
scores	O
to	O
comments	O
to	O
denote	O
their	O
degree	O
of	O
offensiveness	O
.	O

BWS	B-MethodName
was	O
proposed	O
by	O
Louviere	O
(	O
1991	O
)	O
.	O

Kiritchenko	O
and	O
Mohammad	O
(	O
2017	O
)	O
have	O
experimentally	O
shown	O
that	O
BWS	B-MethodName
produces	O
more	O
reliable	O
finegrained	O
scores	O
than	O
the	O
scores	O
acquired	O
utilizing	O
rating	O
scales	O
.	O

In	O
the	O
BWS	B-MethodName
annotation	O
setup	O
,	O
the	O
annotators	O
are	O
given	O
an	O
n	O
-	O
tuple	O
(	O
where	O
n	O
>	O
1	O
,	O
and	O
commonly	O
n	O
=	O
4	O
)	O
,	O
and	O
asked	O
which	O
item	O
is	O
the	O
best	O
and	O
which	O
is	O
the	O
worst	O
(	O
best	O
and	O
worst	O
correspond	O
to	O
the	O
highest	O
and	O
the	O
lowest	O
with	O
respect	O
to	O
a	O
property	O
of	O
interest	O
)	O
.	O

Best	O
-	O
worst	O
annotations	O
are	O
particularly	O
efficient	O
when	O
using	O
4	O
-	O
tuples	O
,	O
as	O
each	O
annotation	O
results	O
in	O
inequalities	O
for	O
5	O
of	O
the	O
6	O
item	O
pairs	O
.	O

For	O
example	O
,	O
a	O
4	O
-	O
tuple	O
with	O
items	O
A	O
,	O
B	O
,	O
C	O
,	O
and	O
D	O
,	O
where	O
A	O
is	O
the	O
best	O
,	O
and	O
D	O
is	O
the	O
worst	O
,	O
results	O
in	O
inequalities	O
:	O
A	O
>	O
B	O
,	O
A	O
>	O
C	O
,	O
A	O
>	O
D	O
,	O
B	O
>	O
D	O
,	O
and	O
C	O
>	O
D.	O
Real	O
-	O
valued	O
scores	O
of	O
associations	O
are	O
calculated	O
between	O
the	O
items	O
and	O
the	O
property	O
of	O
interest	O
from	O
the	O
best	O
-	O
worst	O
annotations	O
for	O
a	O
set	O
of	O
4	O
-	O
tuples	O
(	O
Orme	O
,	O
2009;Flynn	O
and	O
Marley	O
,	O
2014	O
)	O
.	O

The	O
scores	O
can	O
be	O
used	O
to	O
rank	O
items	O
by	O
the	O
degree	O
of	O
association	O
with	O
the	O
property	O
of	O
interest	O
.	O

Within	O
the	O
NLP	O
community	O
,	O
BWS	B-MethodName
has	O
thus	O
far	O
been	O
used	O
only	O
for	O
creating	O
datasets	O
for	O
relational	O
similarity	O
(	O
Jurgens	O
et	O
al	O
.	O
,	O

2012	O
)	O
,	O
word	O
-	O
sense	O
disambiguation	O
(	O
Jurgens	O
,	O
2013	O
)	O
,	O
word	O
-	O
sentiment	O
intensity	O
(	O
Kiritchenko	O
et	O
al	O
.	O
,	O

2014	O
)	O
,	O
phrase	O
sentiment	O
composition	O
(	O
Kiritchenko	O
and	O
Mohammad	O
,	O
2016	O
)	O
,	O
and	O
tweet	O
-	O
emotion	O
intensity	O
(	O
Mohammad	O
and	O
Bravo	O
-	O
Marquez	O
,	O
2017;Mohammad	O
and	O
Kiritchenko	O
,	O
2018	O
)	O
.	O

Using	O
BWS	B-MethodName
,	O
we	O
create	O
the	O
first	O
dataset	O
with	O
degree	O
of	O
offensiveness	O
scores	O
for	O
social	O
media	O
comments	O
.	O

We	O
extracted	O
Reddit	O
data	O
from	O
the	O
Pushshift	O
repository	O
(	O
Baumgartner	O
et	O
al	O
.	O
,	O

2020	O
)	O
using	O
Google	O
BigQuery	O
.	O

Reddit	O
is	O
a	O
social	O
news	O
aggregation	O
,	O
web	O
content	O
rating	O
,	O
and	O
discussion	O
website	O
.	O

It	O
contains	O
forums	O
called	O
subreddits	O
dedicated	O
to	O
specific	O
topics	O
.	O

Users	O
can	O
make	O
a	O
post	O
on	O
the	O
subreddit	O
to	O
start	O
a	O
discussion	O
.	O

Users	O
can	O
comment	O
on	O
existing	O
posts	O
or	O
comments	O
to	O
participate	O
in	O
the	O
discussion	O
.	O

As	O
users	O
can	O
also	O
reply	O
to	O
a	O
comment	O
,	O
the	O
entire	O
discussion	O
has	O
a	O
hierarchical	O
structure	O
called	O
the	O
comment	O
thread	O
.	O

We	O
divided	O
the	O
extracted	O
comments	O
into	O
3	O
categories	O
based	O
on	O
their	O
subreddit	O
source	O
:	O
1	O
.	O

Topics	O
(	O
50	O
%	O
):	O
Contains	O
comments	O
from	O
topic	O
-	O
focused	O
subreddits	O
:	O
AskMen	O
,	O
AskReddit	O
,	O
TwoXChromosomes	O
,	O
vaxxhappened	O
,	O
worldnews	O
,	O
worldpolitics	O
.	O

These	O
subreddits	O
were	O
chosen	O
to	O
cover	O
a	O
diverse	O
range	O
of	O
topics	O
.	O

AskReddit	O
,	O
vaxxhappened	O
,	O
worldnews	O
,	O
worldpolitics	O
discuss	O
generic	O
themes	O
.	O

TwoXChromosomes	O
contains	O
women	O
's	O
perspectives	O
on	O
various	O
topics	O
and	O
AskMen	O
contains	O
men	O
's	O
perspectives	O
.	O

The	O
CMV	O
subreddit	O
(	O
with	O
over	O
a	O
million	O
users	O
)	O
has	O
posts	O
and	O
comments	O
on	O
controversial	O
topics.3	O
.	O

Random	O
(	O
25	O
%	O
):	O
Contains	O
comments	O
from	O
random	O
subreddits	O
.	O

We	O
selected	O
808	O
posts	O
from	O
the	O
subreddits	O
based	O
on	O
criteria	O
such	O
as	O
date	O
,	O
thread	O
length	O
,	O
and	O
post	O
length	O
.	O
(	O

Further	O
details	O
in	O
the	O
Appendix	O
A.1	O
.	O
)	O

We	O
took	O
the	O
first	O
25	O
and	O
the	O
last	O
25	O
comments	O
per	O
post	O
(	O
skipping	O
comments	O
that	O
had	O
[	O
DELETED	O
]	O
or	O
[	O
REMOVED	O
]	O
as	O
comment	O
body	O
)	O
.	O

The	O
first	O
responses	O
are	O
likely	O
to	O
be	O
most	O
relevant	O
to	O
the	O
post	O
.	O

The	O
final	O
comments	O
indicate	O
how	O
the	O
discussion	O
ended	O
.	O

We	O
sampled	O
6000	O
comments	O
from	O
this	O
set	O
for	O
annotation	O
.	O

The	O
goal	O
of	O
the	O
sampling	O
was	O
to	O
increase	O
the	O
proportion	O
of	O
offensive	O
and	O
emotional	O
comments	O
.	O

Emotions	O
are	O
highly	O
representative	O
of	O
one	O
's	O
mental	O
state	O
,	O
which	O
in	O
turn	O
are	O
associated	O
with	O
their	O
behaviour	O
(	O
Poria	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

For	O
example	O
,	O
Jay	O
and	O
Janschewitz	O
(	O
2008	O
)	O
show	O
that	O
people	O
tend	O
to	O
swear	O
when	O
they	O
are	O
angry	O
,	O
frustrated	O
or	O
anxious	O
.	O

Studies	O
have	O
shown	O
that	O
the	O
primary	O
dimensions	O
of	O
emotion	O
are	O
valence	O
,	O
arousal	O
,	O
and	O
dominance	O
(	O
VAD	O
)	O
(	O
Osgood	O
et	O
al	O
.	O
,	O

1957;Russell	O
,	O
1980Russell	O
,	O
,	O
2003.Valence	O
is	O
the	O
positive	O
-negative	O
or	O
pleasuredispleasure	O
dimension	O
.	O

Arousal	O
is	O
the	O
excitedcalm	O
or	O
active	O
-	O
passive	O
dimension	O
.	O

Dominance	O
is	O
powerful	O
-	O
weak	O
or	O
'	O
have	O
full	O
control'-'have	O
no	O
control	O
'	O
dimension	O
(	O
Mohammad	O
,	O
2018	O
)	O
.	O

To	O
boost	O
the	O
representation	O
of	O
offensive	O
and	O
emotional	O
comments	O
in	O
our	O
dataset	O
,	O
we	O
up	O
-	O
sampled	O
comments	O
that	O
included	O
low	O
-	O
valence	O
(	O
highly	O
negative	O
)	O
words	O
and	O
those	O
that	O
included	O
high	O
-	O
arousal	O
words	O
(	O
as	O
per	O
the	O
NRC	O
VAD	O
lexicon	O
(	O
Mohammad	O
,	O
2018	O
)	O
)	O
.	O

2	O
The	O
manually	O
constructed	O
NRC	O
VAD	O
lexicon	O
includes	O
20,000	O
English	O
words	O
,	O
each	O
with	O
a	O
real	O
-	O
valued	O
score	O
between	O
0	O
and	O
1	O
in	O
the	O
V	O
,	O
A	O
,	O
D	O
dimensions	O
.	O

In	O
order	O
to	O
do	O
this	O
upsampling	O
,	O
we	O
first	O
defined	O
the	O
valence	O
score	O
of	O
each	O
comment	O
as	O
the	O
average	O
valence	O
score	O
of	O
the	O
negative	O
words	O
within	O
the	O
comment	O
(	O
A	O
negative	O
word	O
is	O
defined	O
as	O
a	O
word	O
with	O
a	O
valence	O
score	O
≤	O
0.25	O
in	O
the	O
VAD	O
lexicon	O
.	O
)	O

Similarly	O
,	O
we	O
defined	O
the	O
arousal	O
score	O
for	O
a	O
comment	O
as	O
the	O
average	O
arousal	O
score	O
of	O
high	O
-	O
arousal	O
words	O
in	O
each	O
comment	O
.	O
(	O

A	O
high	O
-	O
arousal	O
word	O
is	O
defined	O
as	O
a	O
word	O
with	O
an	O
arousal	O
score	O
≥	O
0.75.)We	O
selected	O
comments	O
from	O
the	O
comment	O
pool	O
such	O
that	O
50	O
%	O
were	O
from	O
the	O
Topics	O
category	O
,	O
25	O
%	O
from	O
the	O
CMV	O
category	O
,	O
and	O
25	O
%	O
from	O
the	O
Random	O
category	O
.	O

Within	O
each	O
category	O
,	O
33	O
%	O
of	O
the	O
comments	O
were	O
those	O
that	O
had	O
the	O
lowest	O
valence	O
scores	O
,	O
33	O
%	O
of	O
the	O
comments	O
were	O
those	O
that	O
had	O
the	O
highest	O
arousal	O
scores	O
,	O
and	O
the	O
remaining	O
were	O
chosen	O
at	O
random	O
.	O

The	O
perception	O
of	O
'	O
offensiveness	O
'	O
of	O
a	O
comment	O
can	O
vary	O
from	O
person	O
to	O
person	O
.	O

Therefore	O
,	O
we	O
used	O
crowdsourcing	O
to	O
annotate	O
our	O
data	O
.	O

Crowdsourcing	O
helps	O
us	O
get	O
an	O
aggregation	O
of	O
varied	O
perspectives	O
rather	O
than	O
expert	O
opinions	O
which	O
can	O
leave	O
out	O
offensiveness	O
in	O
a	O
comment	O
that	O
lies	O
outside	O
the	O
'	O
typical	O
'	O
offensiveness	O
norms	O
(	O
Blackwell	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

We	O
carried	O
out	O
all	O
the	O
annotation	O
tasks	O
on	O
Amazon	O
Mechanical	O
Turk	O
(	O
AMT	O
)	O
.	O

Due	O
to	O
the	O
strong	O
language	O
,	O
an	O
adult	O
content	O
warning	O
was	O
issued	O
for	O
the	O
task	O
.	O

Reddit	O
is	O
most	O
popular	O
in	O
the	O
US	O
,	O
which	O
accounts	O
for	O
50	O
%	O
of	O
its	O
desktop	O
traffic	O
(	O
Statista	O
,	O
2020a	O
)	O
.	O

Therefore	O
,	O
we	O
restricted	O
annotators	O
to	O
those	O
residing	O
in	O
the	O
US	O
.	O

To	O
maintain	O
the	O
quality	O
of	O
annotations	O
,	O
only	O
annotators	O
with	O
high	O
approval	O
rate	O
were	O
allowed	O
to	O
participate	O
.	O

We	O
followed	O
the	O
procedure	O
described	O
in	O
Kiritchenko	O
and	O
Mohammad	O
(	O
2016	O
)	O
to	O
obtain	O
BWS	B-MethodName
annotations	O
.	O

Annotators	O
were	O
presented	O
with	O
4	O
comments	O
(	O
4	O
-	O
tuple	O
)	O
at	O
a	O
time	O
and	O
asked	O
to	O
select	O
the	O
comment	O
that	O
is	O
most	O
offensive	O
(	O
least	O
supportive	O
)	O
and	O
the	O
comment	O
that	O
is	O
least	O
offensive	O
(	O
most	O
supportive	O
)	O
.	O

We	O
randomly	O
generated	O
2N	O
distinct	O
4	O
-	O
tuples	O
(	O
where	O
N	O
is	O
the	O
number	O
of	O
comments	O
in	O
the	O
dataset	O
)	O
,	O
such	O
that	O
each	O
comment	O
was	O
seen	O
in	O
eight	O
different	O
4	O
-	O
tuples	O
and	O
no	O
two	O
4	O
-	O
tuples	O
had	O
more	O
than	O
2	O
items	O
in	O
common	O
.	O

We	O
used	O
the	O
script	O
provided	O
by	O
Kiritchenko	O
and	O
Mohammad	O
(	O
2016	O
)	O
to	O
obtain	O
the	O
4	O
-	O
tuples	O
to	O
be	O
annotated	O
.	O

3	O
Kiritchenko	O
and	O
Mohammad	O
(	O
2016	O
)	O
show	O
that	O
in	O
a	O
word	O
-	O
level	O
sentiment	O
task	O
,	O
using	O
just	O
three	O
annotations	O
per	O
4	O
-	O
tuple	O
produces	O
highly	O
reliable	O
results	O
.	O

However	O
,	O
since	O
we	O
work	O
with	O
long	O
comments	O
and	O
a	O
relatively	O
more	O
difficult	O
task	O
,	O
we	O
got	O
each	O
tuple	O
annotated	O
by	O
6	O
annotators	O
.	O

Since	O
each	O
comment	O
is	O
seen	O
in	O
8	O
different	O
4	O
-	O
tuples	O
,	O
we	O
obtain	O
8	O
X	O
6	O
=	O
48	O
judgements	O
per	O
comment	O
.	O

In	O
our	O
instructions	O
to	O
the	O
annotators	O
,	O
we	O
defined	O
offensive	O
language	O
as	O
comments	O
that	O
include	O
but	O
are	O
not	O
limited	O
to	O
[	O
being	O
hurtful	O
(	O
with	O
or	O
without	O
the	O
usage	O
of	O
abusive	O
words)/	O
being	O
intentionally	O
harmful/	O
treating	O
someone	O
improperly/	O
harming	O
the	O
'	O
self	O
-	O
concept	O
'	O
of	O
another	O
person/	O
aggressive	O
outbursts/	O
name	O
calling/	O
showing	O
anger	O
and	O
hostility/	O
bullying/	O
hurtful	O
sarcasm	O
]	O
.	O

We	O
also	O
encouraged	O
the	O
annotators	O
to	O
follow	O
their	O
instincts	O
.	O

By	O
framing	O
the	O
task	O
in	O
terms	O
of	O
comparisons	O
and	O
providing	O
a	O
broad	O
definition	O
of	O
offensiveness	O
,	O
we	O
avoided	O
introducing	O
artificial	O
categories	O
and	O
elicit	O
responses	O
guided	O
by	O
their	O
intuition	O
of	O
the	O
language	O
.	O

Detailed	O
annotation	O
instructions	O
are	O
made	O
publicly	O
available	O
(	O
Figure	O
5	O
in	O
Appendix	O
A.2	O
)	O
.	O

4	O
A	O
sample	O
questionnaire	O
is	O
shown	O
in	O
Figure	O
6	O
in	O
Appendix	O
A.2	O
.	O

For	O
quality	O
control	O
purposes	O
,	O
we	O
manually	O
annotated	O
around	O
5	O
%	O
of	O
the	O
data	O
ourselves	O
beforehand	O
.	O

We	O
will	O
refer	O
to	O
these	O
instances	O
as	O
gold	O
questions	O
.	O

The	O
gold	O
questions	O
were	O
interspersed	O
with	O
the	O
other	O
questions	O
.	O

If	O
a	O
worker	O
's	O
accuracy	O
on	O
the	O
gold	O
questions	O
fell	O
below	O
70	O
%	O
,	O
they	O
were	O
refused	O
further	O
annotation	O
and	O
all	O
of	O
their	O
annotations	O
were	O
discarded	O
.	O

The	O
discarded	O
annotations	O
were	O
published	O
again	O
for	O
re	O
-	O
annotation	O
.	O

We	O
received	O
a	O
total	O
of	O
95,255	O
annotations	O
by	O
725	O
crowd	O
workers	O
.	O

The	O
BWS	B-MethodName
responses	O
were	O
converted	O
to	O
scores	O
using	O
a	O
simple	O
counting	O
procedure	O
(	O
Orme	O
,	O
2009;Flynn	O
and	O
Marley	O
,	O
2014	O
)	O
.	O

For	O
each	O
item	O
,	O
the	O
score	O
is	O
the	O
proportion	O
of	O
times	O
the	O
item	O
is	O
chosen	O
as	O
the	O
most	O
offensive	O
minus	O
the	O
proportion	O
of	O
times	O
the	O
item	O
is	O
chosen	O
as	O
the	O
least	O
offensive	O
.	O

We	O
release	O
the	O
aggregated	O
annotations	O
as	O
well	O
as	O
the	O
individual	O
annotations	O
of	O
Ruddit	B-DatasetName
,	O
to	O
allow	O
further	O
work	O
on	O
examining	O
and	O
understanding	O
the	O
variability	O
.	O

5	O
We	O
can	O
not	O
use	O
standard	O
inter	O
-	O
annotator	O
agreement	O
measures	O
to	O
ascertain	O
the	O
quality	O
of	O
comparative	O
annotations	O
.	O

The	O
disagreement	O
that	O
arises	O
in	O
tuples	O
having	O
two	O
items	O
that	O
are	O
close	O
together	O
in	O
their	O
degree	O
of	O
offensiveness	O
is	O
a	O
useful	O
signal	O
for	O
BWS	B-MethodName
(	O
helping	O
it	O
give	O
similar	O
scores	O
to	O
the	O
two	O
items	O
)	O
.	O

The	O
quality	O
of	O
annotations	O
can	O
be	O
measured	O
by	O
measuring	O
the	O
reproducibility	O
of	O
the	O
end	O
resultif	O
repeated	O
manual	O
annotations	O
from	O
multiple	O
annotators	O
can	O
produce	O
similar	O
rankings	O
and	O
scores	O
,	O
then	O
,	O
one	O
can	O
be	O
confident	O
about	O
the	O
quality	O
of	O
annotations	O
received	O
.	O

To	O
assess	O
this	O
reproducibility	O
,	O
we	O
computed	O
average	O
split	B-MetricName
-	I-MetricName
half	I-MetricName
reliability	I-MetricName
(	I-MetricName
SHR	I-MetricName
)	I-MetricName
values	O
over	O
100	O
trials	O
.	O

SHR	B-MetricName
is	O
a	O
commonly	O
used	O
approach	O
to	O
determine	O
consistency	O
in	O
psychological	O
studies	O
.	O

For	O
computing	O
SHR	B-MetricName
values	O
,	O
the	O
annotations	O
for	O
each	O
4	O
-	O
tuple	O
were	O
randomly	O
split	O
in	O
two	O
halves	O
.	O

Using	O
these	O
two	O
splits	O
,	O
two	O
sets	O
of	O
rankings	O
were	O
determined	O
.	O

We	O
then	O
calculated	O
the	O
correlation	O
values	O
between	O
these	O
two	O
sets	O
.	O

This	O
procedure	O
was	O
repeated	O
100	O
times	O
and	O
the	O
correlations	O
were	O
averaged	O
.	O

A	O
high	O
correlation	O
value	O
indicates	O
that	O
the	O
annotations	O
are	O
of	O
good	O
quality	O
.	O

Table	O
1	O
shows	O
the	O
SHR	B-MetricName
for	O
our	O
annotations	O
.	O

SHR	B-MetricName
scores	O
of	O
over	O
0.8	B-MetricValue
indicate	O
substantial	O
reliability	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
various	O
aspects	O
of	O
the	O
data	O
,	O
including	O
:	O
the	O
distribution	O
of	O
scores	O
,	O
the	O
as-	O
sociation	O
with	O
identity	O
terms	O
,	O
the	O
relationship	O
with	O
emotion	O
dimensions	O
,	O
the	O
relationship	O
with	O
data	O
source	O
,	O
and	O
the	O
role	O
of	O
swear	O
words	O
.	O

Figure	O
1	O
shows	O
a	O
histogram	O
of	O
frequency	O
of	O
comments	O
vs.	O
degree	O
of	O
offensiveness	O
,	O
over	O
40	O
equi	O
-	O
spaced	O
score	O
bins	O
of	O
size	O
0.05	O
.	O

We	O
observe	O
a	O
normal	O
distribution	O
.	O

To	O
analyze	O
the	O
data	O
,	O
we	O
placed	O
the	O
comments	O
in	O
5	O
equi	O
-	O
spaced	O
score	O
bins	O
of	O
size	O
0.4	O
(	O
bin	O
1	O
:	O
−1.0	O
to	O
−0.6	O
,	O
bin	O
2	O
:	O
−0.6	O
to	O
−0.2	O
,	O
and	O
so	O
on	O
)	O
.	O

Table	O
2	O
shows	O
some	O
comments	O
from	O
the	O
dataset	O
(	O
more	O
examples	O
can	O
be	O
found	O
in	O
Appendix	O
A.3	O
Table	O
6	O
)	O
.	O

We	O
observed	O
that	O
bin	O
1	O
primarily	O
contains	O
supportive	O
comments	O
while	O
bin	O
2	O
shows	O
a	O
transition	O
from	O
supportive	O
to	O
neutral	O
comments	O
.	O

Bin	O
3	O
is	O
dominated	O
by	O
neutral	O
comments	O
but	O
as	O
the	O
score	O
increases	O
the	O
comments	O
become	O
potentially	O
offensive	O
and	O
bins	O
4	O
&	O
5	O
predominantly	O
contain	O
offensive	O
comments	O
.	O

It	O
is	O
interesting	O
to	O
note	O
that	O
bin	O
4	O
contains	O
some	O
instances	O
of	O
implicit	O
offensive	O
language	O
such	O
as	O
'	O
You	O
look	O
like	O
a	O
lesbian	O
mechanic	O
who	O
has	O
a	O
shell	O
collection	O
'	O
.	O

In	O
their	O
paper	O
,	O
Wiegand	O
et	O
al	O
.	O
(	O

2021	O
)	O
explore	O
the	O
category	O
of	O
such	O
"	O
implicity	O
abusive	O
comparisons	O
"	O
,	O
in	O
depth	O
.	O

More	O
examples	O
of	O
implicitly	O
offensive	O
comments	O
present	O
in	O
our	O
dataset	O
can	O
be	O
found	O
in	O
table	O
2	O
and	O
table	O
6	O
(	O
in	O
Appendix	O
A.3).To	O
explore	O
whether	O
specific	O
bins	O
capture	O
specific	O
topics	O
or	O
key	O
-	O
words	O
,	O
we	O
calculated	O
Pointwise	O
Mutual	O
Information	O
(	O
PMI	O
)	O
scores	O
of	O
all	O
the	O
unique	O
words	O
in	O
the	O
comments	O
(	O
excluding	O
stop	O
words	O
)	O
with	O
the	O
five	O
score	O
bins	O
.	O

Table	O
3	O
shows	O
the	O
top	O
scoring	O
words	O
for	O
each	O
bin	O
.	O

We	O
observed	O
that	O
bins	O
1	O
,	O
2	O
,	O
and	O
3	O
exhibit	O
a	O
strong	O
association	O
with	O
supportive	O
or	O
neutral	O
words	O
,	O
while	O
bins	O
4	O
and	O
5	O
show	O
a	O
strong	O
association	O
with	O
swear	O
words	O
and	O
identity	O
terms	O
commonly	O
found	O
in	O
offensive	O
contexts	O
.	O

Identity	O
terms	O
A	O
common	O
criticism	O
of	O
the	O
existing	O
offensive	O
language	O
datasets	O
is	O
that	O
in	O
those	O
datasets	O
,	O
certain	O
identity	O
terms	O
(	O
particularly	O
those	O
referring	O
to	O
minority	O
groups	O
)	O
occur	O
mainly	O
in	O
texts	O
that	O
are	O
offensive	O
(	O
Sap	O
et	O
al	O
.	O
,	O

2019;Davidson	O
et	O
al	O
.	O
,	O

2019;Wiegand	O
et	O
al	O
.	O
,	O

2019;Park	O
et	O
al	O
.	O
,	O

2018;Dixon	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

This	O
leads	O
to	O
high	O
association	O
of	O
targeted	O
minority	O
groups	O
(	O
such	O
as	O
Muslims	O
,	O
females	O
,	O
black	O
people	O
and	O
others	O
)	O
with	O
the	O
offensive	O
class(es	O
)	O
.	O

This	O
bias	O
,	O
in	O
turn	O
,	O
is	O
captured	O
by	O
the	O
computational	O
models	O
trained	O
on	O
such	O
datasets	O
.	O

As	O
mentioned	O
earlier	O
,	O
in	O
Ruddit	B-DatasetName
,	O
certain	O
words	O
such	O
as	O
gay	O
,	O
trans	O
,	O
male	O
,	O
female	O
,	O
black	O
,	O
white	O
were	O
found	O
to	O
exhibit	O
a	O
relatively	O
higher	O
association	O
with	O
the	O
offensive	O
bins	O
than	O
with	O
the	O
supportive	O
bins	O
.	O

In	O
order	O
to	O
probe	O
the	O
effect	O
of	O
this	O
on	O
the	O
computational	O
models	O
,	O
we	O
created	O
a	O
variant	O
of	O
Ruddit	B-DatasetName
by	O
replacing	O
all	O
the	O
identity	O
terms	O
(	O
from	O
the	O
list	O
given	O
in	O
Appendix	O
A.4	O
)	O
in	O
the	O
comments	O
with	O
the	O
[	O
group	O
]	O
token	O
and	O
observed	O
the	O
effect	O
on	O
the	O
models	O
'	O
per	O
-	O
formance	O
.	O

We	O
refer	O
to	O
this	O
variant	O
of	O
the	O
dataset	O
as	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

We	O
analyse	O
the	O
models	O
'	O
performance	O
in	O
the	O
next	O
section	O
.	O

Offensiveness	O
vs.	O
emotion	O
As	O
discussed	O
earlier	O
,	O
our	O
emotions	O
impact	O
the	O
words	O
we	O
use	O
in	O
text	O
.	O

We	O
examined	O
this	O
relationship	O
quantitatively	O
using	O
Ruddit	B-DatasetName
and	O
the	O
NRC	B-DatasetName
VAD	I-DatasetName
Lexicon	I-DatasetName
(	O
which	O
has	O
intensity	O
scores	O
along	O
the	O
valence	O
,	O
arousal	O
,	O
and	O
dominance	O
dimensions	O
)	O
.	O

For	O
each	O
comment	O
in	O
Ruddit	B-DatasetName
,	O
we	O
calculated	O
three	O
scores	O
that	O
captured	O
the	O
intensities	O
of	O
the	O
V	O
,	O
A	O
,	O
D	O
words	O
(	O
the	O
averages	O
of	O
the	O
intensities	O
of	O
the	O
V	O
/	O
A	O
/	O
D	O
words	O
in	O
the	O
comment	O
)	O
,	O
using	O
the	O
entire	O
lexicon	O
.	O

We	O
then	O
determined	O
the	O
correlation	O
between	O
each	O
of	O
the	O
three	O
scores	O
and	O
the	O
degree	O
of	O
offensiveness	O
.	O

Only	O
comments	O
containing	O
at	O
least	O
4	O
words	O
from	O
the	O
VAD	O
lexicon	O
were	O
considered	O
for	O
the	O
score	O
and	O
correlation	O
calculation	O
.	O

A	O
total	O
of	O
4831	O
comments	O
qualified	O
the	O
criteria	O
.	O

See	O
Table	O
4	O
.	O

From	O
the	O
table	O
,	O
we	O
can	O
observe	O
that	O
valence	O
is	O
weakly	O
inversely	O
correlated	O
,	O
arousal	O
is	O
weakly	O
correlated	O
,	O
and	O
dominance	O
does	O
not	O
exhibit	O
notable	O
correlation	O
with	O
offensiveness	O
.	O

This	O
behaviour	O
can	O
also	O
be	O
observed	O
in	O
Figure	O
2	O
that	O
shows	O
a	O
plot	O
of	O
the	O
average	O
V	O
,	O
A	O
,	O
and	O
D	O
scores	O
of	O
comments	O
in	O
the	O
five	O
equi	O
-	O
spaced	O
offensivenessscore	O
bins	O
.	O

Note	O
the	O
clear	O
trend	O
that	O
as	O
we	O
look	O
at	O
bins	O
with	O
more	O
offensive	O
comments	O
,	O
the	O
average	O
valence	O
of	O
the	O
comments	O
decreases	O
and	O
the	O
average	O
arousal	O
increases	O
.	O

two	O
sources	O
,	O
comments	O
are	O
more	O
prevalent	O
in	O
the	O
supportive	O
bins	O
.	O

The	O
higher	O
representation	O
of	O
comments	O
from	O
Topics	O
than	O
the	O
other	O
two	O
sources	O
in	O
the	O
offensive	O
bins	O
,	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
the	O
Topics	O
category	O
includes	O
subreddits	O
such	O
as	O
worldnews	O
and	O
worldpolitics	O
.	O

Discussions	O
on	O
these	O
subreddits	O
covers	O
controversial	O
topics	O
and	O
lead	O
to	O
the	O
usage	O
of	O
offensive	O
language	O
.	O

We	O
observed	O
that	O
worldnews	O
and	O
worldpolitics	O
indeed	O
have	O
high	O
representation	O
in	O
the	O
offensive	O
bins	O
(	O
Figure	O
8	O
in	O
Appendix	O
A.4	O
)	O
.	O

We	O
identified	O
868	O
comments	O
in	O
our	O
dataset	O
that	O
contain	O
at	O
least	O
one	O
swear	O
word	O
from	O
the	O
cursing	O
lexicon	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2014	O
)	O
.	O

Comments	O
containing	O
swear	O
words	O
can	O
have	O
a	O
wide	O
range	O
of	O
offensiveness	O
scores	O
.	O

To	O
visualize	O
the	O
distribution	O
,	O
we	O
plot	O
a	O
histogram	O
of	O
the	O
comments	O
containing	O
swear	O
words	O
vs.	O
degree	O
of	O
offensiveness	O
(	O
see	O
Figure	O
7	O
in	O
Appendix	O
A.4	O
)	O
.	O

The	O
distribution	O
is	O
skewed	O
towards	O
the	O
offensive	O
end	O
of	O
the	O
scale	O
.	O

An	O
interesting	O
observation	O
is	O
that	O
some	O
comments	O
with	O
low	O
offensiveness	O
scores	O
contain	O
phrases	O
using	O
swear	O
words	O
to	O
express	O
enthusiasm	O
or	O
to	O
lay	O
more	O
emphasis	O
,	O
for	O
example	O
'	O
Hell	O
yes	O
'	O
,	O
'	O
sure	O
as	O
hell	O
love	O
it	O
'	O
,	O
'	O
uncomfortable	O
as	O
shit	O
'	O
and	O
others	O
.	O

To	O
study	O
the	O
impact	O
of	O
comments	O
containing	O
swear	O
words	O
on	O
computational	O
models	O
,	O
we	O
created	O
another	O
variant	O
of	O
Ruddit	B-DatasetName
in	O
which	O
we	O
removed	O
all	O
the	O
comments	O
containing	O
at	O
least	O
one	O
swear	O
word	O
.	O

We	O
refer	O
to	O
this	O
variant	O
as	O
the	O
no	O
-	O
swearing	O
dataset	O
.	O

This	O
dataset	O
contains	O
5132	O
comments	O
.	O

We	O
analyse	O
the	O
models	O
'	O
performance	O
on	O
this	O
dataset	O
in	O
the	O
next	O
section	O
.	O

Offensiveness	O
in	O
different	O
score	O
ranges	O
It	O
is	O
possible	O
that	O
comments	O
in	O
the	O
middle	O
region	O
of	O
the	O
scale	O
may	O
be	O
more	O
difficult	O
for	O
the	O
computational	O
models	O
.	O

Thus	O
,	O
we	O
created	O
a	O
subset	O
of	O
Ruddit	B-DatasetName
containing	O
comments	O
with	O
scores	O
from	O
−0.5	O
to	O
0.5	O
.	O

We	O
call	O
this	O
subset	O
(	O
of	O
5151	O
comments	O
)	O
,	O
the	O
reduced	O
-	O
range	O
dataset	O
.	O

We	O
discuss	O
the	O
models	O
'	O
performance	O
on	O
this	O
dataset	O
in	O
the	O
next	O
section	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
benchmark	O
experiments	O
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
by	O
implementing	O
some	O
commonly	O
used	O
model	O
architectures	O
.	O

The	O
task	O
of	O
the	O
models	O
was	O
to	O
predict	O
the	O
offensiveness	O
score	O
of	O
a	O
given	O
comment	O
.	O

We	O
performed	O
5	B-HyperparameterValue
-	O
fold	O
crossvalidation	B-MetricName
for	O
each	O
of	O
the	O
models	O
.	O

6	O
Bidirectional	O
LSTM	O
We	O
fed	O
pre	O
-	O
trained	O
300	B-HyperparameterValue
dimensional	O
GloVe	B-HyperparameterName
word	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Pennington	O
et	O
al	O
.	O
,	O

2014	O
)	O
to	O
a	O
2	B-HyperparameterValue
-	O
layered	O
BiLSTM	B-MethodName
to	O
obtain	O
a	O
sentence	O
representation	O
(	O
using	O
a	O
concatenation	O
of	O
the	O
last	O
hidden	O
state	O
from	O
the	O
forward	O
and	O
backward	O
direction	O
)	O
.	O

This	O
sentence	O
representation	O
was	O
then	O
passed	O
to	O
a	O
linear	O
layer	O
with	O
a	O
tanh	O
activation	O
to	O
produce	O
a	O
score	O
between	O
−1	O
and	O
1	O
.	O

We	O
used	O
Mean	O
Squared	O
Error	O
(	O
MSE	O
)	O
loss	O
as	O
the	O
objective	O
function	O
,	O
Adam	B-HyperparameterValue
with	O
0.001	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
the	O
optimizer	B-HyperparameterName
,	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
,	O
and	O
a	O
dropout	B-HyperparameterName
of	O
0.5	B-HyperparameterValue
.	O

The	O
model	O
was	O
trained	O
for	O
7	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

Dataset	O
HateBERT	B-MethodName
BERT	B-MethodName
BiLSTM	B-MethodName
r	B-MetricName
MSE	B-MetricName
r	B-MetricName
MSE	B-MetricName
r	B-MetricName
MSEa	B-MetricName
.	O

Ruddit	B-DatasetName
0.886	O
±	O
0.003	O
0.025	O
±	O
0.001	O
0.873	O
±	O
0.005	O
0.027	O
±	O
0.001	O
0.831	O
±	O
0.005	O
0.035	O
±	O
0.001	O
b.	O
Identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
0.883	O
±	O
0.006	O
0.025	O
±	O
0.001	O
0.869	O
±	O
0.007	O
0.027	O
±	O
0.001	O
0.824	O
±	O
0.007	O
0.036	O
±	O
0.001	O
c.	O
No	O
-	O
swearing	O
0.808	O
±	O
0.013	O
0.023	O
±	O
0.001	O
0.783	O
±	O
0.012	O
0.027	O
±	O
0.001	O
0.704	O
±	O
0.014	O
0.036	O
±	O
0.002	O
d.	O
Reduced	O
-	O
range	O
0.781	O
±	O
0.014	O
0.022	O
±	O
0.001	O
0.757	O
±	O
0.011	O
0.025	O
±	O
0.001	O
0.659	O
±	O
0.008	O
0.033	O
±	O
0.001	O
BERT	B-MethodName
We	O
fine	O
-	O
tuned	O
BERT	B-MethodName
base	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

We	O
added	O
a	O
regression	O
head	O
containing	O
a	O
linear	O
layer	O
to	O
the	O
pre	O
-	O
trained	O
model	O
.	O

We	O
used	O
MSE	O
loss	O
as	O
the	O
objective	O
function	O
,	O
batch	O
size	O
of	O
16	O
,	O
and	O
learning	O
rate	O
of	O
2e	O
−	O
5	O
(	O
other	O
hyperparameters	O
same	O
as	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
)	O
.	O

We	O
used	O
the	O
AdamW	O
optimizer	O
with	O
a	O
linear	O
learning	O
rate	O
scheduler	O
with	O
no	O
warm	O
up	O
steps	O
.	O

The	O
model	O
was	O
trained	O
for	O
3	O
epochs	O
.	O
(	O

More	O
details	O
in	O
Appendix	O
A.5.)HateBERT	B-MethodName
HateBERT	B-MethodName
(	O
Caselli	O
et	O
al	O
.	O
,	O

2020b	O
)	O
is	O
a	O
version	O
of	O
BERT	B-MethodName
pretrained	O
for	O
abusive	O
language	O
detection	O
in	O
English	O
.	O

HateBERT	B-MethodName
was	O
trained	O
on	O
RAL	O
-	O
E	O
,	O
a	O
large	O
dataset	O
of	O
English	O
language	O
Reddit	O
comments	O
from	O
communities	O
banned	O
for	O
being	O
offensive	O
or	O
hateful	O
.	O

HateBERT	O
has	O
been	O
shown	O
to	O
outperform	O
the	O
general	O
purpose	O
BERT	O
model	O
on	O
the	O
offensive	O
language	O
detection	O
task	O
when	O
finetuned	O
on	O
popular	O
datasets	O
such	O
as	O
OffensEval	O
2019	O
(	O
Zampieri	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
AbusEval	O
(	O
Caselli	O
et	O
al	O
.	O
,	O

2020a	O
)	O
,	O
and	O
HatEval	O
(	O
Basile	O
et	O
al	O
.	O
,	O

2019).We	O
fine	O
-	O
tuned	O
HateBERT	B-MethodName
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
.	O

The	O
experimental	O
setup	O
for	O
this	O
model	O
is	O
the	O
same	O
as	O
that	O
described	O
for	O
the	O
BERT	B-MethodName
model	O
.	O

We	O
report	O
Pearson	B-MetricName
correlation	I-MetricName
(	I-MetricName
r	I-MetricName
)	I-MetricName
and	O
MSE	B-MetricName
,	O
averaged	O
over	O
all	O
folds	O
.	O

The	O
performance	O
of	O
the	O
models	O
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
is	O
shown	O
in	O
the	O
Table	O
5	O
.	O

Note	O
that	O
the	O
performance	O
values	O
on	O
the	O
noswearing	O
and	O
the	O
reduced	O
-	O
range	O
datasets	O
are	O
not	O
directly	O
comparable	O
to	O
the	O
performance	O
values	O
on	O
the	O
full	O
Ruddit	B-DatasetName
as	O
their	O
score	O
range	O
is	O
different	O
.	O

We	O
can	O
see	O
that	O
on	O
all	O
the	O
datasets	O
,	O
the	O
HateBERT	B-MethodName
model	O
performs	O
the	O
best	O
,	O
followed	O
by	O
the	O
BERT	B-MethodName
model	O
.	O

Interestingly	O
,	O
the	O
model	O
performance	O
(	O
for	O
all	O
models	O
)	O
does	O
not	O
change	O
substantially	O
when	O
trained	O
on	O
Ruddit	O
or	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

This	O
indicates	O
that	O
the	O
computational	O
models	O
are	O
not	O
learning	O
to	O
benefit	O
from	O
the	O
association	O
of	O
certain	O
identity	O
terms	O
with	O
a	O
specific	O
range	O
of	O
scores	O
on	O
the	O
offensiveness	O
scale	O
.	O

7	O
The	O
models	O
show	O
a	O
performance	O
drop	O
on	O
the	O
no	O
-	O
swearing	O
dataset	O
,	O
which	O
suggests	O
that	O
swear	O
words	O
are	O
useful	O
indicators	O
of	O
offensiveness	O
and	O
that	O
the	O
comments	O
containing	O
them	O
are	O
easier	O
to	O
classify	O
.	O

Yet	O
,	O
the	O
fact	O
that	O
the	O
models	O
still	O
obtain	O
performance	O
of	O
up	O
to	O
0.8	B-MetricValue
(	O
r	B-MetricName
)	O
demonstrates	O
that	O
they	O
necessitate	O
and	O
are	O
able	O
to	O
learn	O
other	O
types	O
of	O
offensiveness	O
features	O
.	O

It	O
is	O
also	O
worth	O
mentioning	O
that	O
even	O
if	O
they	O
encounter	O
swear	O
words	O
in	O
a	O
comment	O
,	O
the	O
task	O
is	O
not	O
simply	O
to	O
label	O
the	O
comment	O
as	O
offensive	O
but	O
to	O
provide	O
a	O
suitable	O
score	O
.	O

Finally	O
,	O
the	O
models	O
obtained	O
the	O
performance	O
of	O
up	O
to	O
0.78	B-MetricValue
(	O
r	B-MetricName
)	O
on	O
the	O
reduced	O
-	O
range	O
dataset	O
,	O
which	O
shows	O
that	O
even	O
if	O
the	O
comments	O
from	O
the	O
extreme	O
ends	O
of	O
the	O
offensiveness	O
scale	O
are	O
removed	O
,	O
Ruddit	O
still	O
presents	O
an	O
interesting	O
and	O
feasible	O
offensiveness	O
scoring	O
task	O
.	O

Error	O
Analysis	O
Figure	O
4	O
shows	O
the	O
squared	O
error	O
values	O
of	O
the	O
3	O
models	O
over	O
the	O
offensiveness	O
score	O
range	O
in	O
Ruddit	B-DatasetName
.	O

As	O
expected	O
,	O
for	O
all	O
the	O
models	O
,	O
the	O
error	O
in	O
predictions	O
is	O
lower	O
on	O
both	O
the	O
extreme	O
ends	O
of	O
the	O
scale	O
than	O
in	O
the	O
middle	O
region	O
.	O

Comments	O
with	O
very	O
high	O
or	O
very	O
low	O
offensiveness	O
scores	O
are	O
rich	O
in	O
obvious	O
linguistic	O
cues	O
,	O
making	O
it	O
easier	O
for	O
the	O
computational	O
models	O
to	O
predict	O
scores	O
.	O

Most	O
of	O
the	O
not	O
-	O
obvious	O
,	O
indirect	O
implicitly	O
offensive	O
,	O
and	O
neutral	O
comments	O
should	O
be	O
present	O
in	O
the	O
middle	O
region	O
of	O
the	O
offensiveness	O
scale	O
,	O
making	O
them	O
more	O
difficult	O
for	O
the	O
models	O
.	O

It	O
is	O
interesting	O
to	O
observe	O
that	O
HateBERT	B-MethodName
,	O
unlike	O
the	O
other	O
two	O
models	O
,	O
does	O
not	O
have	O
high	O
error	O
values	O
for	O
samples	O
within	O
the	O
score	O
range	O
0.25	O
-	O
0.75	O
.	O

This	O
indicates	O
that	O
HateBERT	B-MethodName
is	O
efficient	O
in	O
dealing	O
with	O
offensive	O
language	O
that	O
does	O
not	O
lie	O
in	O
the	O
extreme	O
offensive	O
end	O
.	O

BiLSTM	B-MethodName
seems	O
relatively	O
less	O
accurate	O
for	O
samples	O
in	O
the	O
supportive	O
range	O
(	O
−0.75	O
to	O
−0.25	O
)	O
.	O

This	O
could	O
be	O
attributed	O
to	O
the	O
less	O
complex	O
model	O
architecture	O
and	O
the	O
usage	O
of	O
GloVe	O
word	O
embeddings	O
.	O

We	O
presented	O
the	O
first	O
dataset	O
of	O
online	O
comments	O
annotated	O
for	O
their	O
degree	O
of	O
offensiveness	O
.	O

We	O
used	O
a	O
comparative	O
annotation	O
technique	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
which	O
addresses	O
the	O
limitations	O
of	O
traditional	O
rating	O
scales	O
.	O

We	O
showed	O
that	O
the	O
ratings	O
obtained	O
are	O
highly	O
reliable	O
(	O
SHR	O
Pearson	O
r	O
≈	O
0.88	O
)	O
.	O

We	O
performed	O
data	O
analysis	O
to	O
gain	O
insight	O
into	O
the	O
relation	O
of	O
emotions	O
,	O
data	O
sources	O
,	O
identity	O
terms	O
,	O
and	O
swear	O
words	O
with	O
the	O
offensiveness	O
scores	O
.	O

We	O
showed	O
that	O
valence	O
is	O
inversely	O
correlated	O
with	O
offensiveness	O
and	O
arousal	O
is	O
directly	O
correlated	O
with	O
offensiveness	O
.	O

Finally	O
,	O
we	O
presented	O
benchmark	O
experiments	O
to	O
predict	O
the	O
offensiveness	O
score	O
of	O
a	O
comment	O
,	O
on	O
our	O
dataset	O
.	O

We	O
found	O
that	O
computational	O
models	O
are	O
not	O
benefiting	O
from	O
the	O
association	O
of	O
identity	O
terms	O
with	O
specific	O
range	O
of	O
scores	O
on	O
the	O
offensiveness	O
scale	O
.	O

In	O
future	O
work	O
,	O
it	O
would	O
be	O
interesting	O
to	O
explore	O
the	O
use	O
of	O
conversational	O
context	O
in	O
computational	O
modeling	O
of	O
offensiveness	O
,	O
as	O
well	O
as	O
studying	O
the	O
interaction	O
between	O
offensiveness	O
and	O
emotions	O
in	O
more	O
depth	O
.	O

We	O
make	O
our	O
dataset	O
freely	O
available	O
to	O
the	O
research	O
community	O
.	O

We	O
create	O
Ruddit	B-DatasetName
to	O
study	O
,	O
understand	O
and	O
explore	O
the	O
nature	O
of	O
offensive	O
language	O
.	O

Any	O
such	O
dataset	O
might	O
also	O
be	O
used	O
to	O
create	O
automatic	O
offensive	O
language	O
detection	O
systems	O
.	O

While	O
we	O
realise	O
the	O
importance	O
of	O
such	O
systems	O
,	O
we	O
also	O
accept	O
that	O
any	O
moderation	O
of	O
online	O
content	O
is	O
a	O
threat	O
to	O
free	O
speech	O
.	O

Offensive	O
language	O
datasets	O
or	O
automatic	O
systems	O
can	O
be	O
misused	O
to	O
stifle	O
disagreeing	O
voices	O
.	O

Our	O
intent	O
is	O
solely	O
to	O
learn	O
more	O
about	O
the	O
use	O
of	O
offensive	O
language	O
,	O
learn	O
about	O
the	O
various	O
degrees	O
of	O
offensive	O
language	O
,	O
explore	O
how	O
computational	O
models	O
can	O
be	O
enabled	O
to	O
watch	O
and	O
contain	O
offensive	O
language	O
,	O
and	O
encourage	O
others	O
to	O
do	O
so	O
.	O

We	O
follow	O
the	O
format	O
provided	O
by	O
Bender	O
and	O
Friedman	O
(	O
2018	O
)	O
to	O
discuss	O
the	O
ethical	O
considerations	O
for	O
our	O
dataset	O
.	O

Institutional	O
Review	O
:	O
This	O
research	O
was	O
funded	O
by	O
the	O
Facebook	O
Online	O
Safety	O
Benchmark	O
Research	O
award	O
.	O

The	O
primary	O
objective	O
of	O
this	O
research	O
award	O
is	O
the	O
creation	O
of	O
publicly	O
available	O
benchmarks	O
to	O
improve	O
online	O
safety	O
.	O

This	O
award	O
does	O
not	O
directly	O
benefit	O
Facebook	O
in	O
any	O
way	O
.	O

This	O
research	O
was	O
reviewed	O
by	O
Facebook	O
for	O
various	O
aspects	O
,	O
in	O
particular	O
:	O
•	O
Legal	O
Review	O
:	O
Evaluates	O
whether	O
the	O
research	O
to	O
be	O
undertaken	O
or	O
the	O
research	O
performed	O
can	O
violate	O
intellectual	O
property	O
rights.•	O
Policy	O
and	O
Ethics	O
Review	O
:	O
Evaluates	O
whether	O
the	O
research	O
to	O
be	O
undertaken	O
aligns	O
with	O
the	O
best	O
ethics	O
practices	O
.	O

This	O
includes	O
several	O
aspects	O
such	O
as	O
mitigating	O
harm	O
to	O
people	O
involved	O
,	O
improving	O
data	O
privacy	O
,	O
and	O
informed	O
consent	O
.	O

Data	O
Redistribution	O
/	O
User	O
Privacy	O
:	O
We	O
extracted	O
our	O
data	O
from	O
the	O
Pushshift	O
Reddit	O
dataset	O
made	O
publicly	O
available	O
by	O
Baumgartner	O
et	O
al	O
.	O
(	O

2020	O
)	O
for	O
research	O
purposes	O
.	O

The	O
creators	O
of	O
the	O
Pushshift	O
Reddit	O
dataset	O
have	O
provisions	O
to	O
delete	O
comments	O
from	O
their	O
dataset	O
upon	O
user	O
's	O
request	O
.	O

We	O
release	O
data	O
in	O
a	O
manner	O
that	O
is	O
GDPR	O
compliant	O
.	O

We	O
do	O
not	O
provide	O
any	O
user	O
-	O
specific	O
information	O
.	O

We	O
release	O
only	O
the	O
comment	O
IDs	O
and	O
post	O
IDs	O
.	O

Reddit	O
's	O
Terms	O
of	O
Service	O
do	O
not	O
prohibit	O
the	O
distribution	O
of	O
ids	O
.	O

8	O
The	O
researchers	O
using	O
the	O
dataset	O
need	O
to	O
retrieve	O
the	O
data	O
using	O
the	O
Reddit	O
API.Speaker	O
and	O
Annotator	O
Demographic	O
:	O
No	O
specific	O
speaker	O
demographic	O
information	O
is	O
available	O
for	O
the	O
comments	O
included	O
in	O
Ruddit	B-DatasetName
.	O

According	O
to	O
the	O
October	O
2020	O
survey	O
published	O
by	O
Statista	O
(	O
Statista	O
,	O
2020a	O
)	O
,	O
50	O
%	O
of	O
the	O
Reddit	O
's	O
desktop	O
traffic	O
is	O
from	O
the	O
United	O
States	O
.	O

They	O
also	O
state	O
that	O
from	O
the	O
internet	O
users	O
in	O
the	O
US	O
,	O
21	O
%	O
from	O
ages	O
18	O
-	O
24	O
,	O
23	O
%	O
from	O
ages	O
25	O
-	O
29	O
and	O
14	O
%	O
from	O
ages	O
30	O
-	O
49	O
use	O
Reddit	O
.	O

We	O
restricted	O
annotators	O
to	O
those	O
residing	O
in	O
the	O
US	O
.	O

A	O
total	O
of	O
725	O
crowd	O
-	O
workers	O
participated	O
in	O
the	O
task	O
.	O

Apart	O
from	O
the	O
country	O
of	O
residence	O
,	O
no	O
other	O
information	O
is	O
known	O
about	O
the	O
annotators	O
.	O

The	O
annotators	O
are	O
governed	O
by	O
AMT	O
's	O
privacy	O
policy	O
.	O

9	O
Pew	O
Research	O
Center	O
conducted	O
a	O
demographic	O
survey	O
of	O
AMT	O
workers	O
in	O
2016	O
.	O

In	O
this	O
survey	O
,	O
3370	O
workers	O
participated	O
.	O

They	O
found	O
out	O
that	O
80	O
%	O
of	O
the	O
crowd	O
-	O
workers	O
on	O
AMT	O
are	O
from	O
the	O
US	O
(	O
PRC	O
,	O
2020	O
)	O
.	O

More	O
information	O
about	O
the	O
workers	O
who	O
participated	O
in	O
their	O
survey	O
can	O
be	O
found	O
in	O
their	O
article	O
.	O

It	O
is	O
important	O
to	O
include	O
the	O
opinions	O
of	O
targeted	O
minorities	O
and	O
marginalized	O
groups	O
when	O
dealing	O
with	O
the	O
annotation	O
of	O
offensive	O
language	O
(	O
Kiritchenko	O
and	O
Nejadgholi	O
,	O
2020;Blackwell	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

However	O
,	O
we	O
did	O
not	O
have	O
our	O
data	O
annotated	O
by	O
the	O
specific	O
target	O
demographic	O
because	O
it	O
poses	O
certain	O
challenges	O
.	O

For	O
example	O
:	O
identification	O
of	O
the	O
target	O
of	O
offensive	O
language	O
;	O
finding	O
people	O
of	O
the	O
target	O
demographic	O
group	O
who	O
are	O
willing	O
to	O
annotate	O
offensive	O
language	O
;	O
and	O
others	O
.	O

Annotating	O
such	O
offensive	O
data	O
can	O
be	O
even	O
more	O
traumatizing	O
for	O
the	O
members	O
of	O
the	O
targeted	O
minorities	O
.	O

Finally	O
,	O
Ruddit	B-DatasetName
was	O
created	O
with	O
the	O
intention	O
to	O
look	O
at	O
wide	O
ranging	O
offensive	O
language	O
of	O
various	O
degrees	O
as	O
opposed	O
to	O
detecting	O
offensive	O
language	O
towards	O
specific	O
target	O
groups	O
.	O

Annotation	O
Guidelines	O
:	O
We	O
created	O
our	O
annotation	O
guidelines	O
drawing	O
inspiration	O
from	O
the	O
community	O
standards	O
set	O
for	O
offensive	O
language	O
on	O
several	O
social	O
media	O
platforms	O
.	O

These	O
standards	O
are	O
made	O
after	O
thorough	O
research	O
and	O
feedback	O
from	O
the	O
community	O
.	O

However	O
,	O
we	O
are	O
aware	O
that	O
the	O
definitions	O
in	O
our	O
guidelines	O
are	O
not	O
representative	O
of	O
all	O
possible	O
perspectives	O
.	O

The	O
degree	O
of	O
offensiveness	O
scores	O
that	O
we	O
provide	O
in	O
Ruddit	O
are	O
a	O
representation	O
of	O
what	O
the	O
majority	O
of	O
our	O
annotators	O
think	O
.	O

We	O
would	O
like	O
to	O
emphasize	O
that	O
the	O
scores	O
provided	O
are	O
not	O
the	O
"	O
correct	O
"	O
or	O
the	O
only	O
appropriate	O
value	O
of	O
offensiveness	O
.	O

Different	O
individuals	O
and	O
demographic	O
groups	O
may	O
find	O
the	O
same	O
comment	O
to	O
be	O
more	O
or	O
less	O
offensive	O
than	O
the	O
scores	O
provided	O
.	O

Impact	O
on	O
Annotators	O
:	O
Annotation	O
of	O
harsh	O
and	O
offensive	O
language	O
might	O
impact	O
the	O
mental	O
health	O
of	O
the	O
annotators	O
negatively	O
(	O
Vidgen	O
et	O
al	O
.	O
,	O

2019;Roberts	O
,	O
2016Roberts	O
,	O
,	O
2019Kiritchenko	O
and	O
Nejadgholi	O
,	O
2020	O
)	O
.	O

The	O
following	O
minimized	O
negative	O
mental	O
impact	O
on	O
the	O
annotators	O
participating	O
in	O
our	O
task	O
:	O
•	O
The	O
comments	O
that	O
we	O
included	O
in	O
our	O
dataset	O
are	O
pre	O
-	O
moderated	O
by	O
Reddit	O
's	O
admins	O
and	O
subreddit	O
specific	O
moderators	O
.	O

Any	O
comments	O
that	O
do	O
not	O
comply	O
with	O
Reddit	O
's	O
content	O
policy	O
are	O
not	O
included	O
.	O

10	O
•	O
Our	O
goal	O
was	O
to	O
annotate	O
posts	O
one	O
sees	O
on	O
social	O
media	O
(	O
after	O
content	O
moderation	O
)	O
.	O

Unlike	O
some	O
past	O
work	O
,	O
we	O
do	O
not	O
limit	O
the	O
data	O
to	O
include	O
only	O
negative	O
comments	O
.	O

We	O
included	O
a	O
large	O
sample	O
of	O
posts	O
that	O
one	O
normally	O
sees	O
on	O
social	O
media	O
,	O
and	O
annotated	O
it	O
for	O
degree	O
of	O
supportiveness	O
or	O
degree	O
of	O
offensiveness	O
.	O
•	O

AMT	O
provides	O
a	O
checkbox	O
where	O
requesters	O
can	O
indicate	O
that	O
some	O
content	O
in	O
the	O
task	O
may	O
be	O
offensive	O
.	O

These	O
tasks	O
are	O
not	O
shown	O
to	O
annotators	O
who	O
have	O
specified	O
so	O
in	O
their	O
profile	O
.	O

We	O
used	O
the	O
checkbox	O
to	O
indicate	O
that	O
this	O
task	O
has	O
offensive	O
content	O
.	O
•	O

We	O
explicitly	O
warned	O
the	O
annotators	O
about	O
the	O
content	O
of	O
annotation	O
,	O
and	O
advised	O
worker	O
discretion	O
.	O
•	O

We	O
provided	O
detailed	O
annotation	O
instructions	O
and	O
informed	O
the	O
annotators	O
about	O
how	O
the	O
annotations	O
for	O
offensive	O
language	O
will	O
be	O
used	O
for	O
studying	O
and	O
understanding	O
offensive	O
language	O
.	O
•	O

The	O
annotation	O
of	O
our	O
data	O
was	O
crowdsourced	O
,	O
allowing	O
for	O
a	O
large	O
number	O
of	O
raters	O
(	O
725	O
)	O
.	O

This	O
reduces	O
the	O
number	O
of	O
comments	O
seen	O
per	O
rater	O
.	O

We	O
also	O
placed	O
a	O
limit	O
on	O
how	O
many	O
posts	O
one	O
may	O
annotate	O
.	O

Annotators	O
were	O
not	O
allowed	O
to	O
submit	O
more	O
than	O
∼	O
5	O
%	O
of	O
the	O
total	O
assignments	O
.	O
•	O

There	O
are	O
just	O
25	O
comments	O
in	O
the	O
top	O
10	O
%	O
of	O
the	O
offensiveness	O
score	O
range	O
.	O

Thus	O
,	O
most	O
annotators	O
(	O
>	O
99.95	O
%	O
)	O
do	O
not	O
see	O
even	O
one	O
such	O
comment	O
.	O

Identity	O
Terms	O
:	O
As	O
discussed	O
in	O
section	O
5	O
,	O
in	O
Ruddit	O
,	O
certain	O
identity	O
terms	O
show	O
a	O
higher	O
association	O
with	O
offensive	O
comments	O
than	O
with	O
the	O
supportive	O
comments	O
.	O

In	O
order	O
to	O
address	O
this	O
,	O
we	O
created	O
a	O
variant	O
of	O
Ruddit	O
,	O
in	O
which	O
we	O
replaced	O
all	O
the	O
identity	O
terms	O
(	O
from	O
the	O
list	O
given	O
in	O
Appendix	O
A.4	O
)	O
with	O
the	O
[	O
group	O
]	O
token	O
.	O

We	O
call	O
this	O
variant	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

We	O
release	O
the	O
code	O
for	O
creating	O
this	O
variant	O
from	O
the	O
original	O
dataset	O
.	O

We	O
evaluated	O
our	O
computational	O
models	O
on	O
this	O
variant	O
and	O
observed	O
that	O
the	O
models	O
did	O
not	O
learn	O
to	O
benefit	O
from	O
the	O
association	O
of	O
the	O
identity	O
terms	O
with	O
the	O
offensive	O
comments	O
.	O

Computational	O
Models	O
:	O
The	O
models	O
reported	O
in	O
this	O
paper	O
are	O
not	O
intended	O
to	O
fully	O
automate	O
offensive	O
content	O
moderation	O
or	O
to	O
make	O
judgements	O
about	O
specific	O
individuals	O
.	O

Owing	O
to	O
privacy	O
concerns	O
,	O
we	O
do	O
not	O
model	O
user	O
history	O
to	O
predict	O
offensiveness	O
scores	O
(	O
Mitchell	O
et	O
al	O
.	O
,	O

2018).Feedback	O
:	O
We	O
are	O
aware	O
that	O
our	O
dataset	O
is	O
subject	O
to	O
the	O
inherent	O
bias	O
of	O
the	O
data	O
,	O
the	O
sampling	O
procedure	O
and	O
the	O
opinion	O
of	O
the	O
annotators	O
who	O
annotated	O
it	O
.	O

Finally	O
,	O
we	O
acknowledge	O
that	O
this	O
is	O
not	O
a	O
comprehensive	O
listing	O
of	O
all	O
the	O
ethical	O
considerations	O
and	O
limitations	O
.	O

We	O
welcome	O
feedback	O
from	O
the	O
research	O
community	O
and	O
anyone	O
using	O
our	O
dataset	O
.	O

A	O
Supplemental	O
Material	O
We	O
selected	O
the	O
posts	O
from	O
the	O
subreddits	O
based	O
on	O
the	O
following	O
criteria:1	O
.	O

Date	O
:	O
To	O
extract	O
comments	O
from	O
posts	O
that	O
discuss	O
current	O
matters	O
,	O
we	O
took	O
comments	O
from	O
the	O
time	O
period	O
of	O
January	O
,	O
2015	O
to	O
September	O
,	O
2019	O
(	O
last	O
available	O
month	O
at	O
the	O
time	O
of	O
extraction).2	O
.	O

Thread	O
length	O
:	O
We	O
chose	O
posts	O
with	O
more	O
than	O
150	O
comments	O
and	O
less	O
than	O
5000	O
comments	O
.	O

This	O
criteria	O
ensured	O
that	O
the	O
posts	O
contained	O
enough	O
comments	O
to	O
capture	O
meaningful	O
discussion.3	O
.	O

Post	O
length	O
:	O
We	O
chose	O
posts	O
containing	O
more	O
than	O
5	O
words	O
and	O
less	O
than	O
60	O
words	O
in	O
the	O
post	O
body	O
.	O

This	O
was	O
done	O
to	O
avoid	O
posts	O
that	O
are	O
too	O
short	O
to	O
provide	O
enough	O
information	O
or	O
are	O
too	O
long	O
and	O
have	O
a	O
possibility	O
of	O
being	O
spam.4	O
.	O

URL	O
:	O
Often	O
,	O
posts	O
on	O
Reddit	O
contain	O
URLs	O
redirecting	O
to	O
images	O
,	O
videos	O
,	O
news	O
articles	O
and	O
others	O
.	O

We	O
limited	O
our	O
posts	O
to	O
those	O
containing	O
at	O
most	O
one	O
URL	O
to	O
avoid	O
issues	O
arising	O
due	O
to	O
missing	O
context	O
.	O

For	O
each	O
post	O
,	O
the	O
hierarchical	O
threads	O
were	O
reconstructed	O
using	O
the	O
Anytree	O
python	O
library	O
.	O

We	O
filtered	O
comments	O
from	O
these	O
posts	O
based	O
on	O
the	O
following	O
criteria:1	O
.	O

Comment	O
length	O
:	O
We	O
chose	O
comments	O
containing	O
more	O
than	O
5	O
words	O
and	O
less	O
than	O
150	O
words	O
in	O
the	O
comment	O
body	O
.	O

We	O
did	O
this	O
to	O
include	O
comments	O
that	O
are	O
neither	O
too	O
long	O
(	O
can	O
be	O
difficult	O
to	O
annotate	O
)	O
nor	O
too	O
short	O
(	O
not	O
very	O
valuable	O
)	O
.	O

No	O
.	O

of	O
users	O
:	O
In	O
the	O
first	O
and	O
last	O
25	O
comments	O
of	O
the	O
thread	O
,	O
we	O
ensured	O
participation	O
of	O
at	O
least	O
4	O
users	O
.	O

This	O
was	O
done	O
to	O
ensure	O
that	O
the	O
comments	O
in	O
our	O
dataset	O
are	O
from	O
a	O
diverse	O
set	O
of	O
users	O
.	O

We	O
chose	O
comments	O
with	O
no	O
URL	O
in	O
them	O
.	O

Comments	O
with	O
URL	O
can	O
be	O
difficult	O
to	O
annotate	O
as	O
the	O
URLs	O
provide	O
extra	O
context	O
for	O
the	O
comment	O
.	O

Figure	O
5	O
shows	O
the	O
detailed	O
annotation	O
instructions	O
given	O
to	O
the	O
crowd	O
-	O
workers	O
for	O
the	O
task	O
.	O

A	O
sample	O
questionnaire	O
for	O
the	O
final	O
annotation	O
task	O
is	O
shown	O
in	O
Figure	O
6.The	O
hourly	O
compensation	O
rate	O
for	O
annotators	O
on	O
Amazon	O
Mechanical	O
Turk	O
was	O
US$	O
7.50	O
/	O
hr	O
.	O

The	O
task	O
received	O
considerable	O
attention	O
with	O
725	O
participants	O
in	O
total	O
.	O

Table	O
6	O
contains	O
comments	O
from	O
Ruddit	O
grouped	O
according	O
to	O
the	O
5	O
score	O
bins	O
.	O

We	O
used	O
the	O
list	O
of	O
identity	O
terms	O
used	O
by	O
Dixon	O
et	O
al	O
.	O
(	O

2018	O
)	O
with	O
a	O
few	O
of	O
our	O
own	O
additions	O
.	O

The	O
terms	O
used	O
are	O
lesbian	O
,	O
gay	O
,	O
bisexual	O
,	O
transgender	O
,	O
trans	O
,	O
queer	O
,	O
lgbt	O
,	O
lgbtq	O
,	O
homosexual	O
,	O
straight	O
,	O
heterosexual	O
,	O
male	O
,	O
female	O
,	O
nonbinary	O
,	O
african	O
,	O
africanamerican	O
,	O
black	O
,	O
white	O
,	O
european	O
,	O
hispanic	O
,	O
latino	O
,	O
latina	O
,	O
latinx	O
,	O
mexican	O
,	O
canadian	O
,	O
american	O
,	O
asian	O
,	O
indian	O
,	O
middle	O
eastern	O
,	O
chinese	O
,	O
japanese	O
,	O
christian	O
,	O
muslim	O
,	O
jewish	O
,	O
buddhist	O
,	O
catholic	O
,	O
protestant	O
,	O
sikh	O
,	O
taoist	O
,	O
old	O
,	O
older	O
,	O
young	O
,	O
younger	O
,	O
teenage	O
,	O
millenial	O
,	O
middle	O
aged	O
,	O
elderly	O
,	O
blind	O
,	O
deaf	O
,	O
paralyzed	O
,	O
atheist	O
,	O
feminist	O
,	O
islam	O
,	O
muslim	O
,	O
man	O
,	O
woman	O
,	O
boy	O
,	O
girl	O
.	O

Figure	O
7	O
shows	O
a	O
histogram	O
of	O
the	O
comments	O
containing	O
swear	O
words	O
-	O
degree	O
of	O
offensiveness	O
,	O
over	O
40	O
equi	O
-	O
spaced	O
score	O
bins	O
of	O
size	O
0.05.Figure	O
8	O
shows	O
a	O
distribution	O
of	O
comments	O
within	O
each	O
of	O
the	O
5	O
score	O
bins	O
over	O
the	O
subreddits	O
that	O
were	O
included	O
in	O
the	O
Topics	O
category	O
.	O

Hyperparameter	O
Tuning	O
We	O
tuned	O
hyperparameters	O
for	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
.	O

We	O
performed	O
grid	O
search	O
cross	O
-	O
validation	O
on	O
Ruddit	O
and	O
used	O
Pearson	O
's	O
r	O
to	O
select	O
the	O
best	O
hyperparameter	O
setting	O
.	O

All	O
experiments	O
were	O
performed	O
on	O
a	O
fixed	O
seed	O
value	O
of	O
12.For	O
the	O
BiLSTM	B-MethodName
model	O
,	O
the	O
batch	O
size	O
was	O
fixed	O
at	O
32	O
and	O
the	O
number	O
of	O
epochs	O
was	O
set	O
to	O
7	O
.	O

The	O
hyperparameter	O
search	O
space	O
is	O
as	O
follows	O
:	O
For	O
the	O
BERT	B-MethodName
model	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
fixed	O
at	O
16	B-HyperparameterValue
and	O
BERT	B-MethodName
tokenizer	B-HyperparameterName
's	I-HyperparameterName
maximum	I-HyperparameterName
length	I-HyperparameterName
was	O
set	O
to	O
200	B-HyperparameterValue
.	O

We	O
tune	O
hyperparameters	O
on	O
the	O
settings	O
that	O
Devlin	O
et	O
al	O
.	O
(	O

2019	O
)	O
found	O
to	O
work	O
best	O
on	O
all	O
tasks	O
.	O

The	O
search	O
space	O
is	O
as	O
follows:2714•	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
:	O
2e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
,	O
3e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
,	O
5e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
We	O
reported	O
the	O
best	O
setting	O
for	O
the	O
models	O
in	O
section	O
6.1	O
.	O

The	O
average	B-MetricName
r	I-MetricName
of	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
across	O
all	O
hyperparameter	O
search	O
This	O
research	O
was	O
funded	O
by	O
the	O
Facebook	O
Online	O
Safety	O
Benchmark	O
Research	O
award	O
for	O
the	O
project	O
"	O
A	O
Benchmark	O
and	O
Evaluation	O
Framework	O
for	O
Abusive	O
Language	O
Detection	O
.	O
"	O

Bin	O
Comment	O
Score	O
1Truly	O
gave	O
me	O
a	O
smile	O
−0.812	O
Awwwwwe	O
That	O
's	O
was	O
the	O
best	O
reveal	O
I	O
'	O
ve	O
seen.−0.688	O
I	O
'	O
m	O
literally	O
doing	O
the	O
same	O
tonight	O
!	O
Have	O
an	O
awesome	O
time	O
:)	O
−0.625	O
My	O
apologies	O
for	O
missing	O
that	O
in	O
your	O
post	O
.	O

Carry	O
on	O
.	O

−0.583	O
I	O
'	O
m	O
assuming	O
this	O
is	O
a	O
great	O
response	O
for	O
i	O
have	O
n't	O
seen	O
the	O
video	O
in	O
question	O
,	O
but	O
i	O
do	O
believe	O
that	O
the	O
woman	O
in	O
the	O
video	O
carried	O
herself	O
very	O
elegantly	O
!	O
−0.375	O
Could	O
you	O
not	O
love	O
a	O
"	O
purrfectly	O
"	O
healthy	O
moggy	O
?	O
One	O
whose	O
life	O
you	O
could	O
save	O
from	O
a	O
shelter	O
and	O
wo	O
n't	O
break	O
the	O
bank	O
.	O

−0.25	O
Handwritten	O
letters	O
are	O
pure	O
gold	O
.	O

It	O
is	O
a	O
simple	O
but	O
immensely	O
pleasant	O
gift	O
to	O
receive	O
.	O

Still	O
have	O
the	O
ones	O
my	O
ex	O
gf	O
would	O
sent	O
me	O
when	O
we	O
were	O
apart.−0.196	O
It	O
would	O
take	O
way	O
more	O
time	O
to	O
get	O
a	O
screaming	O
baby	O
to	O
stay	O
still	O
long	O
enough	O
to	O
inject	O
them	O
.	O

I	O
remember	O
my	O
little	O
sister	O
throwing	O
off	O
doctors	O
and	O
nurses	O
like	O
a	O
tiny	O
she	O
-	O
hulk	O
when	O
she	O
was	O
a	O
toddler	O
.	O

She	O
also	O
punched	O
my	O
dad	O
in	O
the	O
face	O
.	O


With	O
the	O
increasing	O
interest	O
in	O
low	O
-	O
resource	O
languages	O
,	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
has	O
become	O
an	O
active	O
area	O
of	O
research	O
,	O
where	O
approaches	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

We	O
demonstrate	O
the	O
power	O
of	O
harnessing	O
linguistic	O
knowledge	O
as	O
priors	O
within	O
Adaptor	B-MethodName
Grammars	I-MethodName
in	O
a	O
minimally	O
-	O
supervised	O
learning	O
fashion	O
.	O

We	O
introduce	O
two	O
types	O
of	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
language	O
-	O
specific	O
grammars	O
;	O
and	O
2	O
)	O
linguistprovided	B-MethodName
affixes	I-MethodName
,	O
collected	O
by	O
an	O
expert	O
in	O
the	O
language	O
and	O
seeded	O
into	O
the	O
grammars	O
.	O

We	O
use	O
Japanese	O
and	O
Georgian	O
as	O
respective	O
case	O
studies	O
for	O
the	O
two	O
types	O
of	O
priors	O
and	O
introduce	O
new	O
datasets	O
for	O
these	O
languages	O
,	O
with	O
gold	O
morphological	B-TaskName
segmentation	I-TaskName
for	O
evaluation	O
.	O

We	O
show	O
that	O
the	O
use	O
of	O
priors	O
results	O
in	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
and	O
34.2	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
over	O
the	O
equivalent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
system	O
.	O

Morphological	B-TaskName
segmentation	I-TaskName
is	O
an	O
essential	O
subtask	O
in	O
many	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
applications	O
,	O
especially	O
in	O
the	O
case	O
of	O
morphologically	O
complex	O
languages	O
.	O

With	O
the	O
need	O
to	O
develop	O
NLP	O
tools	O
for	O
low	O
-	O
resource	O
languages	O
,	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
has	O
been	O
receiving	O
increasing	O
interest	O
over	O
the	O
last	O
two	O
decades	O
(	O
Goldsmith	O
,	O
2001;Creutz	O
and	O
Lagus	O
,	O
2007a;Poon	O
et	O
al	O
.	O
,	O

2009;Sirts	O
and	O
Goldwater	O
,	O
2013;Botha	O
and	O
Blunsom	O
,	O
2013;Narasimhan	O
et	O
al	O
.	O
,	O

2014;Eskander	O
et	O
al	O
.	O
,	O

2016Eskander	O
et	O
al	O
.	O
,	O
,	O

2018Eskander	O
et	O
al	O
.	O
,	O
,	O

2019.In	O
this	O
work	O
,	O
we	O
show	O
how	O
linguistic	O
priors	O
effectively	O
boost	O
morphological	O
-	O
segmentation	O
performance	O
in	O
a	O
minimally	O
-	O
supervised	O
manner	O
that	O
does	O
not	O
require	O
segmented	O
words	O
for	O
training	O
.	O

We	O
integrate	O
our	O
priors	O
within	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
Johnson	O
et	O
al	O
.	O
,	O

2007	O
)	O
,	O
a	O
type	O
of	O
nonparametric	O
Bayesian	O
models	O
that	O
generalize	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammars	I-MethodName
(	O
PCFGs	O
)	O
.	O

Adaptor	B-MethodName
Grammars	I-MethodName
have	O
proved	O
successful	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
a	O
variety	O
of	O
typologically	O
diverse	O
languages	O
(	O
Eskander	O
et	O
al	O
.	O
,	O

2020).We	O
introduce	O
two	O
types	O
of	O
linguistic	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
a	O
languagespecific	O
grammar	O
that	O
is	O
tailored	O
for	O
the	O
language	O
of	O
interest	O
by	O
modeling	O
specific	O
morphological	O
phenomena	O
,	O
and	O
2	O
)	O
linguist	B-MethodName
-	I-MethodName
provided	I-MethodName
affixes	I-MethodName
,	O
where	O
an	O
expert	O
in	O
the	O
underlying	O
language	O
compiles	O
a	O
list	O
of	O
carefully	O
selected	O
affixes	O
and	O
seeds	O
it	O
into	O
the	O
grammars	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
.	O

We	O
use	O
Japanese	O
and	O
Georgian	O
as	O
case	O
studies	O
for	O
priors	O
1	O
and	O
2	O
,	O
respectively	O
.	O

As	O
our	O
goal	O
is	O
to	O
develop	O
a	O
robust	O
approach	O
that	O
benefits	O
low	O
-	O
resource	O
and/or	O
endangered	O
languages	O
of	O
high	O
morphological	O
complexity	O
,	O
we	O
use	O
Japanese	O
and	O
Georgian	O
in	O
a	O
low	O
-	O
resource	O
setting	O
where	O
we	O
do	O
not	O
have	O
access	O
to	O
morphologically	O
segmented	O
data	O
for	O
training	O
but	O
have	O
access	O
to	O
linguistic	O
information	O
such	O
as	O
word	O
structure	O
and	O
affixes	O
.	O

We	O
show	O
that	O
using	O
linguistic	O
priors	O
in	O
a	O
minimally	O
-	O
supervised	O
setting	O
leads	O
to	O
a	O
significant	O
improvement	O
in	O
performance	O
over	O
the	O
equivalent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
system	O
.	O

We	O
also	O
present	O
two	O
morphologically	O
segmented	O
datasets	O
for	O
Japanese	O
and	O
Georgian	O
that	O
we	O
use	O
as	O
our	O
gold	O
standard	O
and	O
that	O
can	O
be	O
utilized	O
in	O
other	O
morphology	O
tasks	O
.	O

1	O
We	O
utilize	O
MorphAGram	B-MethodName
(	O
Eskander	O
et	O
al	O
.	O
,	O

2020	O
)	O
2	O
,	O
an	O
open	O
-	O
source	O
morphologicalsegmentation	B-TaskName
framework	O
that	O
is	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
AGs	O
)	O
(	O
Johnson	O
et	O
al	O
.	O
,	O

2007	O
)	O
.	O

AGs	O
have	O
proved	O
successful	O
for	O
unsupervised	O
and	O
1	O
The	O
training	O
and	O
evaluation	O
datasets	O
,	O
linguistic	O
priors	O
and	O
models	O
for	O
both	O
Japanese	O
and	O
Georgian	O
are	O
available	O
at	O
https://github.com/rnd2110/MorphAGram/data	O
.	O

minimally	O
-	O
supervised	O
morphological	O
segmentation	O
,	O
outperforming	O
the	O
competing	O
discriminative	O
models	O
(	O
Sirts	O
and	O
Goldwater	O
,	O
2013;Eskander	O
et	O
al	O
.	O
,	O

2019Eskander	O
et	O
al	O
.	O
,	O
,	O

2020.Adaptor	O
Grammars	O
are	O
non	O
-	O
parametric	O
Bayesian	O
models	O
that	O
are	O
composed	O
of	O
two	O
main	O
components	O
:	O
1	O
)	O
a	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammar	I-MethodName
(	I-MethodName
PCFG	I-MethodName
)	I-MethodName
whose	O
definition	O
relies	O
on	O
the	O
underlying	O
task	O
(	O
in	O
the	O
case	O
of	O
morphological	O
segmentation	O
,	O
a	O
PCFG	O
models	O
word	O
structure	O
)	O
;	O
and	O
2	O
)	O
an	O
adaptor	B-MethodName
that	O
is	O
based	O
on	O
the	O
Pitman	B-MethodName
-	I-MethodName
Yor	I-MethodName
process	I-MethodName
(	O
Pitman	O
,	O
1995	O
)	O
.	O

The	O
adaptor	B-MethodName
keeps	O
the	O
posterior	O
probability	O
of	O
a	O
subtree	O
proportional	O
to	O
the	O
number	O
of	O
times	O
that	O
subtree	O
is	O
utilized	O
to	O
parse	O
the	O
input	O
data	O
and	O
manages	O
the	O
caching	O
of	O
the	O
subtrees	O
.	O

The	O
learning	O
process	O
is	B-MethodName
Markov	I-MethodName
Chain	I-MethodName
Monte	I-MethodName
Carlo	I-MethodName
sampling	I-MethodName
(	I-MethodName
MCMC	I-MethodName
)	I-MethodName
(	O
Andrieu	O
et	O
al	O
.	O
,	O

2003	O
)	O
that	O
does	O
the	O
inference	O
of	O
the	O
PCFG	O
probabilities	O
and	O
the	O
hyperparameters	O
of	O
the	O
model	O
.	O

Eskander	O
et	O
al	O
.	O
(	O

2016	O
)	O
define	O
a	O
set	O
of	O
languageindependent	O
grammars	O
and	O
three	O
learning	O
settings	O
for	B-MethodName
Adaptor	I-MethodName
Grammars	I-MethodName
:	O
1	O
)	O
Standard	O
,	O
fully	O
unsupervised	O
;	O
2	O
)	O
Scholar	O
-	O
Seeded	O
,	O
minimally	O
-	O
supervised	O
by	O
manually	O
seeding	O
affixes	O
into	O
the	O
grammar	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
,	O
and	O
3	O
)	O
Cascaded	O
,	O
fully	O
unsupervised	O
by	O
approximating	O
the	O
Scholar	O
-	O
Seeded	O
setting	O
using	O
automatically	O
generated	O
af	O
-	O
fixes	O
from	O
an	O
initial	O
round	O
of	O
learning	O
.	O

We	O
next	O
present	O
two	O
ways	O
of	O
including	O
linguistic	O
priors	O
in	O
Adaptor	O
Grammars	O
:	O
1	O
)	O
defining	O
a	O
language	O
-	O
specific	O
grammar	O
;	O
and	O
2	O
)	O
using	O
linguist	O
-	O
provided	O
affixes	O
in	O
the	O
Scholar	O
-	O
Seeded	O
learning	O
setup	O
.	O

Eskander	O
et	O
al	O
.	O
(	O

2016	O
)	O
define	O
languageindependent	O
grammars	O
that	O
model	O
the	O
word	O
as	O
a	O
sequence	O
of	O
generic	O
morphemes	O
or	O
as	O
a	O
sequence	O
of	O
prefixes	O
,	O
stem	O
and	O
suffixes	O
.	O

We	O
consider	O
their	O
PrStSu+SM	B-MethodName
grammar	I-MethodName
in	O
the	O
current	O
study	O
as	O
it	O
is	O
the	O
grammar	O
that	O
performed	O
best	O
on	O
average	O
across	O
different	O
languages	O
.	O

This	O
language	O
-	O
independent	O
definition	O
of	O
the	O
grammar	O
is	O
depicted	O
on	O
the	O
left	O
side	O
of	O
Figure	O
1	O
,	O
where	O
the	O
word	O
is	O
modeled	O
as	O
a	O
prefix	O
Pr	O
,	O
a	O
stem	O
St	O
and	O
a	O
suffix	O
Su	O
,	O
and	O
both	O
the	O
prefix	O
and	O
suffix	O
are	O
recursively	O
defined	O
in	O
order	O
to	O
model	O
compounding	O
in	O
affixes	O
,	O
while	O
a	O
morpheme	O
is	O
composed	O
of	O
smaller	O
units	O
,	O
submorphemes	O
SM	O
,	O
representing	O
sequences	O
of	O
characters	O
.	O

While	O
this	O
grammar	O
is	O
intended	O
to	O
be	O
generic	O
and	O
to	O
describe	O
word	O
structure	O
in	O
any	O
language	O
,	O
we	O
hypothesize	O
that	O
a	O
definition	O
that	O
imposes	O
languagespecific	O
constraints	O
would	O
be	O
more	O
efficient	O
.	O

Therefore	O
,	O
we	O
define	O
a	O
grammar	O
for	O
Japanese	O
,	O
where	O
we	O
use	O
characteristics	O
that	O
are	O
specific	O
to	O
Japane	O
-	O
se	O
word	O
structure	O
as	O
language	O
priors	O
.	O

Our	O
tailored	O
grammar	O
definition	O
for	O
Japanese	O
is	O
shown	O
on	O
the	O
right	O
side	O
of	O
Figure	O
1	O
,	O
where	O
we	O
impose	O
the	O
following	O
specifications	O
:	O
A	O
word	O
has	O
a	O
maximum	O
of	O
one	O
one	O
-	O
character	O
or	O
two	O
-	O
character	O
prefix	O
morphemes	O
.	O

A	O
stem	O
is	O
recursively	O
defined	O
as	O
a	O
sequence	O
of	O
morphemes	O
in	O
order	O
to	O
allow	O
for	O
stem	O
compounding	O
.	O

Characters	O
are	O
separated	O
into	O
two	O
groups	O
,	O
Kana	O
(	O
Japanese	O
syllabaries	O
)	O
and	O
Kanji	O
(	O
adapted	O
Chinese	O
characters).A	O
submorpheme	O
represents	O
a	O
sequence	O
of	O
characters	O
that	O
is	O
either	O
in	O
Kana	O
or	O
Kanji	O
.	O

Similar	O
to	O
the	O
Scholar	O
-	O
Seeded	O
setting	O
,	O
we	O
compile	O
a	O
list	O
of	O
affixes	O
and	O
seed	O
it	O
into	O
the	O
grammar	O
trees	O
before	O
learning	O
the	O
segmentation	O
model	O
.	O

However	O
,	O
unlike	O
Eskander	O
et	O
al	O
.	O
(	O

2016	O
)	O
,	O
where	O
the	O
affixes	O
are	O
collected	O
from	O
online	O
resources	O
by	O
someone	O
who	O
may	O
have	O
never	O
studied	O
the	O
language	O
of	O
interest	O
,	O
in	O
this	O
study	O
we	O
use	O
affixes	O
that	O
are	O
carefully	O
compiled	O
by	O
an	O
expert	O
linguist	O
who	O
specializes	O
in	O
Georgian	O
,	O
resulting	O
in	O
more	O
accurate	O
linguistic	O
priors	O
.	O

With	O
that	O
goal	O
in	O
mind	O
,	O
a	O
total	O
of	O
119	O
affixes	O
are	O
collected	O
from	O
the	O
leading	O
reference	O
grammar	O
book	O
(	O
Aronson	O
,	O
1990	O
)	O
.	O

We	O
annotate	O
two	O
datasets	O
with	O
morphological	B-TaskName
segmentation	I-TaskName
that	O
we	O
use	O
as	O
the	O
gold	O
standard	O
to	O
evaluate	O
our	O
segmentation	B-TaskName
models	O
for	O
Japanese	O
and	O
Georgian	O
.	O

Both	O
datasets	O
are	O
composed	O
of	O
1,000	O
words	O
that	O
are	O
randomly	O
sampled	O
from	O
the	O
most	O
frequent	O
50,000	O
words	O
in	O
Wikipedia	O
and	O
segmented	O
into	O
their	O
basic	O
morphemes	O
3	O
,	O
similar	O
to	O
the	O
data	O
of	O
the	O
Morpho	O
Challenge	O
shared	O
task	O
4	O
.	O

Table	O
1	O
lists	O
segmentation	O
examples	O
for	O
both	O
languages	O
.	O

The	O
Japanese	O
gold	O
segmentation	O
was	O
created	O
by	O
a	O
native	O
-	O
speaker	O
linguist	O
.	O

For	O
Georgian	O
,	O
which	O
has	O
highly	O
complex	O
morphology	O
,	O
we	O
started	O
with	O
the	O
gold	O
-	O
standard	O
dataset	O
of	O
1000	O
words	O
introduced	O
by	O
Eskander	O
We	O
evaluate	O
our	O
morphological	O
-	O
segmentation	O
models	O
for	O
Japanese	O
in	O
the	O
Standard	O
(	O
STD	O
)	O
and	O
Cascaded	O
(	O
CAS	O
)	O
5	O
settings	O
,	O
both	O
with	O
generic	O
and	O
language	O
-	O
specific	O
(	O
LS	O
)	O
grammar	O
definitions	O
.	O

For	O
Georgian	O
,	O
we	O
evaluate	O
our	O
morphologicalsegmentation	O
models	O
in	O
the	O
Standard	O
(	O
STD	O
)	O
,	O
Cascaded	O
(	O
CAS	O
)	O
and	O
Scholar	O
-	O
Seeded	O
(	O
SS	O
)	O
settings	O
,	O
in	O
addition	O
to	O
the	O
proposed	O
Scholar	O
-	O
Seeded	O
setting	O
with	O
linguist	O
-	O
provided	O
affixes	O
(	O
SS	O
-	O
Ling).We	O
perform	O
the	O
evaluation	O
in	O
a	O
transductive	O
manner	O
,	O
where	O
the	O
unsegmented	O
words	O
in	O
the	O
gold	O
standard	O
are	O
part	O
of	O
the	O
training	O
sets	O
;	O
this	O
is	O
common	O
in	O
evaluating	O
unsupervised	O
and	O
minimally	B-TaskName
-	I-TaskName
supervised	I-TaskName
morphological	I-TaskName
segmentation	I-TaskName
(	O
Poon	O
et	O
al	O
.	O
,	O

2009;Sirts	O
and	O
Goldwater	O
,	O
2013;Narasimhan	O
et	O
al	O
.	O
,	O

2014;Eskander	O
et	O
al	O
.	O
,	O

2016Eskander	O
et	O
al	O
.	O
,	O
,	O

2019Eskander	O
et	O
al	O
.	O
,	O
,	O

2020	O
.	O

For	O
the	O
metrics	O
,	O
we	O
use	O
Boundary	O
Precision	O
and	O
Recall	O
(	O
BPR	O
)	O
and	O
EMMA-2	O
(	O
Virpioja	O
et	O
al	O
.	O
,	O

2011	O
)	O
.	O

BPR	B-MetricName
is	O
the	O
classical	O
metric	O
for	O
evaluating	O
morphological	B-TaskName
segmentation	I-TaskName
;	O
it	O
compares	O
the	O
boundaries	O
in	O
the	O
proposed	O
segmentation	O
to	O
those	O
in	O
the	O
reference	O
.	O

EMMA-2	O
Table	O
3	O
:	O
Category	O
-	O
wise	O
morphological	O
-	O
segmentation	O
performance	O
for	O
Georgian	O
using	O
the	O
BPR	O
and	O
EMMA-2	O
metrics	O
.	O

AG	O
=	O
Adaptor	O
Grammars	O
.	O

SS	O
=	O
Scholar	O
-	O
Seeded	O
.	O

SS	O
-	O
Ling	O
=	O
Scholar	O
-	O
Seeded	O
with	O
linguist	O
-	O
provided	O
affixes.is	O
based	O
on	O
matching	O
the	O
morphemes	O
in	O
the	O
proposed	O
segmentation	O
to	O
those	O
in	O
the	O
reference	O
in	O
a	O
many	O
-	O
to	O
-	O
one	O
assignment	O
setup	O
.	O

We	O
evaluate	O
our	O
system	O
versus	O
two	O
state	O
-	O
of	O
-	O
theart	O
unsupervised	O
baselines	O
:	O
MorphAGram	B-MethodName
without	O
the	O
use	O
of	O
linguistic	O
priors	O
and	O
Morfessor	B-MethodName
(	O
Virpioja	O
et	O
al	O
.	O
,	O

2013	O
)	O
6	O
.	O

Morfessor	B-MethodName
is	O
a	O
commonly	O
-	O
used	O
framework	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
.	O

It	O
is	O
based	O
on	O
an	O
HMM	B-MethodName
model	O
that	O
relies	O
on	O
the	O
Minimum	B-MethodName
Description	I-MethodName
Length	I-MethodName
(	O
MDL	O
)	O
concept	O
for	O
deriving	O
the	O
optimal	O
segmentation	O
(	O
Creutz	O
and	O
Lagus	O
,	O
2007b	O
)	O
.	O

Since	O
our	O
approach	O
does	O
not	O
assume	O
access	O
to	O
manually	O
annotated	O
segmentation	O
,	O
it	O
is	O
not	O
directly	O
comparable	O
to	O
semi	O
-	O
supervised	O
approaches	O
that	O
rely	O
on	O
such	O
annotations	O
(	O
Ruokolainen	O
et	O
al	O
.	O
,	O

2014;Kann	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Finally	O
,	O
we	O
report	O
all	O
the	O
Adaptor	B-MethodName
-	I-MethodName
Grammar	I-MethodName
results	O
as	O
the	O
average	O
over	O
three	O
runs	O
of	O
different	O
randomization	O
parameters	O
.	O

Table	O
2	O
reports	O
the	O
overall	O
performance	O
of	O
our	O
models	O
for	O
both	O
Japanese	O
and	O
Georgian	O
,	O
while	O
Table	O
3	O
shows	O
the	O
results	O
per	O
part	O
-	O
of	O
-	O
speech	O
category	O
for	O
Georgian	O
.	O

For	O
Japanese	O
,	O
the	O
use	O
of	O
a	O
language	O
-	O
specific	O
grammar	O
definition	O
improves	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
resulting	O
in	O
BPR	B-MetricName
F1	I-MetricName
-	O
score	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
and	O
7.1	B-MetricValue
%	I-MetricValue
over	O
the	O
generic	O
Standard	O
and	O
Cascaded	O
settings	O
,	O
respectively	O
,	O
and	O
a	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reduction	O
of	B-MetricValue
9.8	I-MetricValue
%	I-MetricValue
over	O
Morfessor	B-MethodName
.	O

For	O
Georgian	O
,	O
the	O
use	O
of	O
linguist	O
-	O
provided	O
seeded	O
affixes	O
improves	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
where	O
the	O
recall	B-MetricName
significantly	O
increases	O
by	O
absolute	O
13.3	B-MetricValue
%	I-MetricValue
over	O
using	O
an	O
affix	O
list	O
of	O
lower	O
quality	O
.	O

In	O
addition	O
,	O
the	O
proposed	O
linguistic	O
priors	O
result	O
in	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reductions	O
of	O
34.2	B-MetricValue
%	I-MetricValue
,	O
30.0	B-MetricValue
%	I-MetricValue
and	O
31.1	B-MetricValue
%	I-MetricValue
over	O
the	O
Standard	O
,	O
Cascaded	O
and	O
regular	O
Scholar	O
-	O
Seeded	O
settings	O
,	O
respectively	O
,	O
and	O
a	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reduction	O
of	O
53.3	B-MetricValue
%	I-MetricValue
over	O
Morfessor	B-MethodName
.	O

Analysing	O
results	O
per	O
category	O
,	O
verbs	O
and	O
nouns	O
receive	O
the	O
biggest	O
F1	B-MetricName
-	O
score	O
improvements	O
of	O
absolute	O
14.3	B-MetricValue
%	I-MetricValue
and	O
4.9	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
with	O
the	O
use	O
of	O
linguist	O
-	O
provided	O
affixes	O
.	O

A	O
similar	O
pattern	O
of	O
results	O
is	O
found	O
with	O
EMMA-2	B-MethodName
.	O

Finally	O
,	O
all	O
the	O
improvements	O
due	O
to	O
the	O
use	O
of	O
linguistic	O
priors	O
are	O
statistically	O
significant	O
(	O
P	B-HyperparameterName
<	O
0.01	B-HyperparameterValue
)	O
on	O
both	O
metrics	O
.	O

Georgian	B-MethodName
segmentation	I-MethodName
models	I-MethodName
.	O

We	O
discuss	O
the	O
most	O
prominent	O
observations	O
below	O
.	O

Japanese	O
:	O
Both	O
the	O
STD	B-MethodName
and	B-MethodName
STD	I-MethodName
-	I-MethodName
LS	I-MethodName
models	O
perform	O
well	O
on	B-MethodName
prefix	I-MethodName
segmentation	I-MethodName
,	O
achieving	B-MetricName
F1	I-MetricName
-	O
scores	O
of	O
more	O
than	O
90	B-MetricValue
%	I-MetricValue
in	O
the	O
detection	O
of	O
several	O
one	O
-	O
character	O
prefixes	O
,	O
such	O
as	O
お	O
and	O
ご	O
.	O

However	O
,	O
STD	B-MethodName
-	I-MethodName
LS	I-MethodName
outperforms	O
its	O
languageindependent	O
counterpart	O
in	O
the	O
detection	O
of	O
stems	O
,	O
where	O
compounding	O
is	O
explicitly	O
modeled	O
.	O

For	O
instance	O
,	O
STD	B-MethodName
and	O
STD	B-MethodName
-	I-MethodName
LS	I-MethodName
achieve	O
F1	B-MetricName
-	O
scores	O
of	O
15.8	B-MetricValue
%	I-MetricValue
and	O
98.6	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
in	O
the	O
detection	O
of	O
the	O
common	O
stem	O
られ	O
(	O
be	O
)	O
.	O

On	O
the	O
other	O
hand	O
,	O
when	O
either	O
model	O
consistently	O
fails	O
to	O
detect	O
a	O
specific	O
morpheme	O
,	O
the	O
other	O
model	O
fails	O
as	O
well	O
.	O

For	O
example	O
,	O
neither	O
model	O
can	O
detect	O
the	O
morphemes	O
せん	O
and	O
かった	O
.	O

Georgian	O
:	O
SS	O
-	O
Ling	O
outperforms	O
both	O
STD	B-MethodName
and	O
SS	B-MethodName
at	O
discovering	O
the	O
top	O
most	O
frequent	O
one	O
-	O
letter	O
morphemes	O
,	O
such	O
as	O
i	O
,	O
a	O
,	O
s	O
,	O
e	O
,	O
m	O
,	O
o	O
and	O
v	O
,	O
achieving	O
an	O
average	B-MetricName
F1	I-MetricName
-	O
score	O
of	O
76.0	B-MetricValue
%	I-MetricValue
,	O
compared	O
to	O
57.7	B-MetricValue
%	I-MetricValue
and	O
57.3	B-MetricValue
%	I-MetricValue
by	O
STD	B-MethodName
and	O
SS	B-MethodName
,	O
respectively	O
.	O

In	O
addition	O
,	O
SS	B-MethodName
and	O
STD	B-MethodName
suffer	O
lower	O
precision	B-MetricName
as	O
they	O
tend	O
to	O
oversegment	O
the	O
morphemes	O
represented	O
by	O
a	O
single	O
letter	O
.	O

Similarly	O
,	B-MethodName
SS	I-MethodName
-	I-MethodName
Ling	I-MethodName
can	O
recognize	O
the	O
most	O
frequent	O
two	O
-	O
letter	O
morphemes	O
,	O
namely	O
eb	O
and	O
da	O
,	O
with	O
absolute	O
increases	O
in	O
precision	B-MetricName
of	B-MetricValue
59.0	I-MetricValue
%	I-MetricValue
and	O
62.0	B-MetricValue
%	I-MetricValue
over	O
STD	B-MethodName
and	O
SS	B-MethodName
,	O
respectively	O
;	O
both	O
morphemes	O
are	O
explicitly	O
seeded	O
into	O
the	O
SS	O
-	O
Ling	O
grammar	O
prior	O
to	O
training	O
the	O
model	O
.	O

We	O
proposed	O
two	O
types	O
of	O
linguistic	O
priors	O
for	O
minimally	B-TaskName
-	I-TaskName
supervised	I-TaskName
morphological	I-TaskName
segmentation	I-TaskName
using	O
Adaptor	B-MethodName
Grammars	I-MethodName
.	O

The	O
first	O
prior	O
is	O
in	O
the	O
form	O
of	O
defining	O
a	O
language	O
-	O
specific	O
grammar	O
,	O
whi	O
-	O
le	O
the	O
second	O
relies	O
on	O
compiling	O
a	O
list	O
of	O
linguistprovided	O
affixes	O
and	O
seeding	O
it	O
into	O
the	O
grammars	O
.	O

Our	O
approaches	O
result	O
in	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
,	O
for	O
Japanese	O
,	O
and	O
34.2	B-MetricValue
%	I-MetricValue
,	O
for	O
Georgian	O
,	O
as	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
.	O

In	O
future	O
work	O
,	O
we	O
plan	O
to	O
explore	O
the	O
use	O
of	O
linguistic	O
priors	O
that	O
apply	O
to	O
a	O
group	O
of	O
morphologically	O
similar	O
lowresource	O
languages	O
.	O

This	O
research	O
is	O
based	O
upon	O
work	O
supported	O
by	O
the	O
Intelligence	O
Advanced	O
Research	O
Projects	O
Activity	O
(	O
IARPA	O
)	O
,	O
(	O
contract	O
#	O
FA8650	O
-	O
17	O
-	O
C-9117	O
)	O
and	O
the	O
National	O
Science	O
Foundation	O
(	O
awards	O
#	O
1941742	O
and	O
#	O
1941733	O
)	O
.	O

The	O
views	O
and	O
conclusions	O
herein	O
are	O
those	O
of	O
the	O
authors	O
and	O
should	O
not	O
be	O
interpreted	O
as	O
necessarily	O
representing	O
official	O
policies	O
,	O
expressed	O
or	O
implied	O
,	O
of	O
ODNI	O
,	O
IARPA	O
,	O
NSF	O
or	O
the	O
U.S.	O
Government	O
.	O

The	O
U.S.	O
Government	O
is	O
authorized	O
to	O
reproduce	O
and	O
distribute	O
reprints	O
for	O
governmental	O
purposes	O
notwithstanding	O
any	O
copyright	O
annotation	O
therein	O
.	O

The	O
Japanese	O
annotations	O
were	O
done	O
by	O
a	O
linguist	O
with	O
appropriate	O
compensation	O
;	O
we	O
thus	O
have	O
ownership	O
of	O
the	O
Japanese	O
dataset	O
for	O
open	O
distribution	O
.	O

We	O
have	O
been	O
granted	O
the	O
rights	O
to	O
modify	O
and	O
distribute	O
the	O
dataset	O
for	O
Georgian	O
by	O
Eskander	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
where	O
the	O
annotations	O
were	O
done	O
in	O
-	O
house	O
by	O
a	O
paid	O
linguist	O
.	O

Finally	O
,	O
the	O
quality	O
of	O
the	O
annotations	O
was	O
examined	O
,	O
both	O
manually	O
and	O
empirically	O
.	O

In	O
this	O
article	O
,	O
we	O
present	O
our	O
methodologies	O
for	O
SemEval-2021	B-DatasetName
Task-4	I-DatasetName
:	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
.	O

Given	O
a	O
fill	O
-	O
inthe	O
-	O
blank	O
-	O
type	O
question	O
and	O
a	O
corresponding	O
context	O
,	O
the	O
task	O
is	O
to	O
predict	O
the	O
most	O
suitable	O
word	O
from	O
a	O
list	O
of	O
5	O
options	O
.	O

There	O
are	O
three	O
sub	O
-	O
tasks	O
within	O
this	O
task	O
:	O
Imperceptibility	B-TaskName
(	O
subtask	O
-	O
I	O
)	O
,	O
Non	B-TaskName
-	I-TaskName
Specificity	I-TaskName
(	O
subtask	O
-	O
II	O
)	O
,	O
and	O
Intersection	B-TaskName
(	O
subtask	O
-	O
III	O
)	O
.	O

We	O
use	O
encoders	B-MethodName
of	I-MethodName
transformers	I-MethodName
-	I-MethodName
based	I-MethodName
models	I-MethodName
pre	O
-	O
trained	O
on	O
the	O
masked	O
language	O
modelling	O
(	O
MLM	O
)	O
task	O
to	O
build	O
our	O
Fill	B-MethodName
-	I-MethodName
in	I-MethodName
-	I-MethodName
the	I-MethodName
-	I-MethodName
blank	I-MethodName
(	I-MethodName
FitB	I-MethodName
)	I-MethodName
models	O
.	O

Moreover	O
,	O
to	O
model	O
imperceptibility	B-MetricValue
,	O
we	O
define	O
certain	O
linguistic	O
features	O
,	O
and	O
to	O
model	O
non	B-MetricValue
-	I-MetricValue
specificity	I-MetricValue
,	O
we	O
leverage	O
information	O
from	O
hypernyms	O
and	O
hyponyms	O
provided	O
by	O
a	O
lexical	O
database	O
.	O

Specifically	O
,	O
for	B-MetricValue
non	I-MetricValue
-	I-MetricValue
specificity	I-MetricValue
,	O
we	O
try	O
out	O
augmentation	O
techniques	O
,	O
and	O
other	O
statistical	O
techniques	O
.	O

We	O
also	O
propose	O
variants	O
,	O
namely	O
Chunk	B-MethodName
Voting	I-MethodName
and	O
Max	B-MethodName
Context	I-MethodName
,	O
to	O
take	O
care	O
of	O
input	O
length	O
restrictions	O
for	O
BERT	B-MethodName
,	O
etc	O
.	O

Additionally	O
,	O
we	O
perform	O
a	O
thorough	O
ablation	O
study	O
,	O
and	O
use	O
Integrated	B-MethodName
Gradients	I-MethodName
to	O
explain	O
our	O
predictions	O
on	O
a	O
few	O
samples	O
.	O

Our	O
best	O
submissions	O
achieve	O
accuracies	B-MetricName
of	O
75.31	B-MetricValue
%	I-MetricValue
and	O
77.84	B-MetricValue
%	I-MetricValue
,	O
on	O
the	O
test	O
sets	O
for	O
subtask	O
-	O
I	O
and	O
subtask	O
-	O
II	O
,	O
respectively	O
.	O

For	O
subtask	O
-	O
III	O
,	O
we	O
achieve	O
accuracies	B-MetricName
of	O
65.64	B-MetricValue
%	I-MetricValue
and	O
62.27	B-MetricValue
%	I-MetricValue
.	O

The	O
code	O
is	O
available	O
here	O
.	O

A	O
very	O
common	O
assessment	O
in	O
schools	O
is	O
questionanswering	O
based	O
on	O
a	O
given	O
"	O
comprehension	O
passage	O
"	O
.	O

Students	O
are	O
given	O
a	O
comprehension	O
passage	O
,	O
from	O
which	O
they	O
are	O
supposed	O
to	O
glean	O
necessary	O
information	O
,	O
and	O
answer	O
short	O
questions	O
(	O
such	O
as	O
fill	O
-	O
in	O
-	O
the	O
-	O
blanks	O
-	O
type	O
question	O
)	O
based	O
on	O
what	O
they	O
have	O
garnered	O
from	O
the	O
given	O
passage	O
.	O

While	O
trying	O
to	O
find	O
the	O
most	O
appropriate	O
word	O
for	O
the	O
blank	O
,	O
the	O
children	O
look	O
at	O
the	O
words	O
surrounding	O
the	O
blank	O
*	O
Equal	O
contribution	O
.	O

Author	O
ordering	O
determined	O
by	O
coin	O
flip	O
.	O
(	O
"	O

context	O
"	O
)	O
.	O

The	O
word	O
should	O
be	O
such	O
that	O
when	O
the	O
word	O
fills	O
the	O
blank	O
,	O
the	O
sentence	O
makes	O
sense	O
and	O
it	O
is	O
grammatically	O
correct	O
.	O

Inspired	O
by	O
this	O
,	O
and	O
perhaps	O
,	O
after	O
the	O
enormous	O
success	O
of	O
Transformers	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
researchers	O
at	O
Google	O
came	O
up	O
with	O
a	O
large	O
number	O
of	O
"	O
pretraining	O
tasks	O
"	O
and	O
built	O
knowledge	O
-	O
heavy	O
language	O
models	O
which	O
could	O
be	O
fine	O
-	O
tuned	O
on	O
various	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
downstream	O
tasks	O
.	O

One	O
of	O
the	O
earlier	O
pretraining	O
tasks	O
was	O
"	O
Masked	O
Language	O
Modelling	O
(	O
MLM	O
)	O
"	O
,	O
one	O
of	O
the	O
two	O
pretraining	O
tasks	O
of	O
the	O
breakthrough	O
model	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

The	O
approach	O
here	O
was	O
similar	O
to	O
how	O
kids	O
are	O
taught	O
language	O
at	O
school	O
:	O
some	O
tokens	O
in	O
the	O
text	O
were	O
randomly	O
"	O
masked	O
"	O
and	O
the	O
model	O
was	O
trained	O
to	O
predict	O
these	O
masked	O
tokens	O
.	O

SemEval-2021	B-DatasetName
Task-4	I-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021	O
)	O
focuses	O
on	O
a	O
similar	O
idea	O
.	O

Every	O
sample	O
has	O
an	O
article	O
,	O
and	O
a	O
corresponding	O
question	O
.	O

The	O
question	O
has	O
a	O
blank	O
which	O
the	O
model	O
is	O
supposed	O
to	O
predict	O
from	O
a	O
set	O
of	O
5	O
options	O
.	O

The	O
novelty	O
in	O
the	O
task	O
lies	O
in	O
its	O
3	O
subtasks	O
:	O
Imperceptibility	B-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
I	I-TaskName
)	I-TaskName
,	I-TaskName
Non	I-TaskName
-	I-TaskName
Specificity	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
II	I-TaskName
)	I-TaskName
,	I-TaskName
and	I-TaskName
Intersection	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
III	I-TaskName
)	O
.	O

A	O
description	O
of	O
these	O
subtasks	O
is	O
given	O
in	O
Section	O
3	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
using	O
BERT	B-MethodName
and	O
its	O
derivative	O
models	O
such	O
as	O
DistilBERT	B-MethodName
,	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Further	O
,	O
we	O
propose	O
2	O
BERT	B-MethodName
variants	O
:	O
(	O
1	O
)	O
BERT	B-MethodName
Voting	I-MethodName
;	O
(	O
2	O
)	O
BERT	B-MethodName
Max	I-MethodName
.	O

Context	O
.	O

Most	O
importantly	O
,	O
we	O
also	O
model	O
the	O
concepts	O
of	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

For	O
imperceptibility	B-TaskName
,	O
we	O
create	O
statistical	O
embeddings	O
using	O
features	O
that	O
have	O
a	O
high	O
correlation	O
with	O
concreteness	O
.	O

For	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
,	O
we	O
propose	O
two	O
approaches	O
:	O
(	O
1	O
)	O
we	O
augment	O
the	O
dataset	O
by	O
replacing	O
some	O
nouns	O
in	O
the	O
article	O
by	O
their	O
hypernyms	O
;	O
and	O
(	O
2	O
)	O
we	O
use	O
the	O
options	O
'	O
hyponyms	O
to	O
decide	O
the	O
most	O
appropriate	O
option	O
.	O

We	O
also	O
experiement	O
with	O
GA	O
-	O
Reader	O
(	O
Dhingra	O
et	O
al	O
.	O
,	O

2017b	O
)	O
and	O
GSAMNbased	O
approaches	O
(	O
Lai	O
et	O
al	O
.	O
,	O

2019	O
)	O
by	O
trying	O
out	O
their	O
various	O
combinations	O
with	O
BERT.In	B-MethodName
Section	O
2	O
,	O
we	O
perform	O
a	O
succinct	O
literature	O
survey	O
.	O

Section	O
3	O
elucidates	O
our	O
approach	O
,	O
including	O
the	O
modelling	O
aspect	O
,	O
the	O
various	O
variants	O
of	O
the	O
base	O
model	O
,	O
and	O
the	O
different	O
ways	O
we	O
model	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

In	O
Section	O
4	O
,	O
we	O
perform	O
an	O
extensive	O
ablation	O
and	O
comparative	O
study	O
.	O

The	O
advent	O
of	O
large	O
-	O
scale	O
question	O
answering	O
systems	O
began	O
with	O
straightforward	O
tasks	O
,	O
like	O
the	O
one	O
introduced	O
by	O
the	O
SimpleQuestions	B-DatasetName
Dataset	O
(	O
Bordes	O
et	O
al	O
.	O
,	O

2015	O
)	O
,	O
which	O
consisted	O
of	O
knowledgebase	O
fact	O
triples	O
which	O
were	O
later	O
used	O
to	O
answer	O
questions	O
.	O

However	O
,	O
this	O
dataset	O
would	O
only	O
judge	O
a	O
model	O
based	O
on	O
the	O
ability	O
to	O
relate	O
the	O
facts	O
to	O
the	O
question	O
at	O
hand	O
.	O

The	O
purpose	O
of	O
NLP	O
research	O
is	O
to	O
be	O
able	O
to	O
create	O
a	O
generalised	O
model	O
that	O
may	O
answer	O
questions	O
based	O
on	O
any	O
context	O
,	O
thus	O
datasets	O
like	O
the	O
CNN	B-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
(	O
Hermann	O
et	O
al	O
.	O
,	O

2015	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
were	O
created	O
.	O

In	O
a	O
typical	O
question	O
-	O
answering	O
dataset	O
,	O
an	O
original	O
and	O
anonymised	O
context	O
is	O
provided	O
before	O
each	O
question	O
.	O

Before	O
transformers	O
,	O
methods	O
consisting	O
of	O
LSTM	B-MethodName
/	I-MethodName
GRUs	I-MethodName
were	O
used	O
to	O
achieve	O
good	O
results	O
on	O
the	O
aforementioned	O
tasks	O
.	O

These	O
datasets	O
however	O
,	O
always	O
had	O
answers	O
in	O
the	O
passage	O
.	O

The	O
CLOTH	B-DatasetName
(	O
Xie	O
et	O
al	O
.	O
,	O

2018	O
)	O
dataset	O
focuses	O
on	O
passages	O
from	O
middle	O
-	O
school	O
and	O
high	O
-	O
school	O
text	O
,	O
with	O
multiple	O
fill	O
-	O
in	O
-	O
the	O
-	O
blanks	O
in	O
the	O
passage	O
.	O

The	O
ReCAM	B-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021	O
)	O
dataset	O
puts	O
a	O
twist	O
to	O
archetypal	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
datasets	O
by	O
providing	O
answer	O
choices	O
that	O
are	O
abstract	O
in	O
some	O
form	O
and	O
which	O
are	O
not	O
available	O
in	O
the	O
passage	O
itself	O
.	O

The	O
models	O
created	O
for	O
the	O
QA	O
task	O
have	O
to	O
take	O
into	O
account	O
semantic	O
relations	O
between	O
the	O
options	O
and	O
the	O
context	O
.	B-MethodName

GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

2017b	O
)	O
,	O
is	O
one	O
such	O
model	O
,	O
which	O
utilises	O
a	O
multi	O
-	O
hop	O
architecture	O
with	O
a	O
novel	O
attention	O
mechanism	O
,	O
that	O
serves	O
as	O
a	O
baseline	O
to	O
this	O
task	O
.	O

Cloze	O
-	O
Style	O
QAThe	O
first	O
model	O
we	O
employ	O
follows	O
a	O
cloze	B-MethodName
-	I-MethodName
style	I-MethodName
question	I-MethodName
answering	I-MethodName
approach	O
,	O
in	O
which	O
we	O
use	O
various	O
pretrained	O
transformer	O
models	O
as	O
encoders	O
,	O
followed	O
by	O
a	O
decoder	O
layer	O
,	O
which	O
helps	O
us	O
to	O
select	O
the	O
correct	O
answer	O
.	O

Specifically	O
,	O
we	O
leverage	O
BERT	B-MethodName
along	O
with	O
some	O
of	O
its	O
popular	O
and	O
successful	O
variants	O
such	O
as	O
:	B-MethodName
Dis	I-MethodName
-	I-MethodName
tilBERT	I-MethodName
,	I-MethodName
ALBERT	I-MethodName
,	I-MethodName
and	I-MethodName
RoBERTa	I-MethodName
.	O

In	O
the	O
MLM	O
task	O
,	O
tokens	O
in	O
the	O
text	O
are	O
randomly	O
masked	O
,	O
and	O
the	O
model	O
is	O
trained	O
in	O
a	O
self	O
-	O
supervised	O
way	O
to	O
predict	O
these	O
masked	O
tokens	O
.	O

Conceptually	O
,	O
these	O
transformers	O
-	O
based	O
models	O
are	O
expected	O
to	O
take	O
care	O
of	O
bidirectional	O
context	O
while	O
predicting	O
the	O
masked	O
token	O
.	O

In	O
our	O
method	O
,	O
firstly	O
,	O
the	O
transformer	O
model	O
learn	O
the	O
contextual	O
embeddings	O
of	O
the	O
article	O
and	O
the	O
question	O
.	O

For	O
the	O
next	O
block	O
,	O
the	O
embedding	O
of	O
the	O
masked	O
token	O
(	O
i.e.	O
,	O
the	O
blank	O
)	O
is	O
passed	O
through	O
a	O
fully	O
-	O
connected	O
layer	O
,	O
of	O
which	O
,	O
the	O
number	O
of	O
outputs	O
corresponds	O
to	O
the	O
size	O
of	O
the	O
vocabulary	O
space	O
for	O
the	O
pretrained	O
model	O
.	O

Each	O
candidate	O
option	O
is	O
first	O
tokenised	O
using	O
WordPiece	B-MethodName
tokeniser	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
mapped	O
to	O
the	O
vector	O
in	O
the	O
output	O
vocabulary	O
space	O
.	O

If	O
the	O
candidate	O
option	O
generates	O
multiple	O
tokens	O
,	O
we	O
average	O
the	O
mapped	O
scores	O
.	O

The	O
model	O
chooses	O
the	O
option	O
with	O
the	O
highest	O
logit	O
value	O
.	O

An	O
overview	O
of	O
the	O
model	O
is	O
given	O
in	O
Figure	O
2	O
.	O

Nouns	O
can	O
be	O
clearly	O
demarcated	O
into	O
two	O
broad	O
categories	O
:	O
Concrete	O
Nouns	O
,	O
and	O
Abstract	O
Nouns	O
.	O

Concrete	O
Nouns	O
are	O
words	O
that	O
represent	O
tangible	O
concepts	O
,	O
i.e.	O
,	O
any	O
noun	O
referring	O
to	O
a	O
name	O
,	O
place	O
,	O
object	O
,	O
material	O
,	O
etc	O
.	O

is	O
considered	O
a	O
concrete	O
word	O
.	O

Concrete	O
words	O
refer	O
to	O
concepts	O
that	O
can	O
be	O
felt	O
by	O
5	O
human	O
senses	O
:	O
Sight	O
,	O
Sound	O
,	O
Smell	O
,	O
Taste	O
,	O
and	O
Touch	O
.	O

In	O
contrast	O
,	O
any	O
noun	O
alluding	O
to	O
an	O
abstract	O
concept	O
that	O
can	O
not	O
be	O
experienced	O
by	O
our	O
senses	O
is	O
an	O
abstract	O
word	O
(	O
Spreen	O
and	O
Schulz	O
,	O
1966	O
)	O
.	O

In	O
subtask	O
-	O
I	O
,	O
the	O
model	O
has	O
to	O
predict	O
the	O
most	O
accurate	O
and	O
the	O
most	O
imperceptible	O
word	O
from	O
the	O
given	O
options	O
.	O

Length	O
and	O
Frequency	O
of	O
the	O
Word	O
In	O
existing	O
literature	O
,	O
authors	O
have	O
claimed	O
that	O
there	O
exists	O
strong	O
evidence	O
that	O
concrete	O
words	O
are	O
,	O
in	O
general	O
,	O
shorter	O
than	O
abstract	O
words	O
(	O
Tanaka	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

A	O
reasonable	O
justification	O
provided	O
is	O
that	O
more	O
frequently	O
used	O
words	O
tend	O
to	O
be	O
short	O
(	O
Feng	O
et	O
al	O
.	O
,	O

2011	O
)	O
and	O
since	O
humans	O
have	O
a	O
penchant	O
for	O
describing	O
objects	O
,	O
places	O
,	O
or	O
things	O
near	O
them	O
,	O
these	O
frequently	O
used	O
words	O
are	O
generally	O
concrete	O
nouns	O
.	O

It	O
is	O
rather	O
intuitive	O
that	O
humans	O
would	O
prefer	O
ease	O
in	O
the	O
pronunciation	O
of	O
oft	O
-	O
used	O
words	O
.	O

Moreover	O
,	O
many	O
abstract	O
words	O
in	O
the	O
English	O
language	O
are	O
formed	O
by	O
adding	O
suffixes	O
to	O
the	O
root	O
word	O
,	O
such	O
as	O
"	O
coarse	O
"	O
becomes	O
"	O
coarseness	O
"	O
,	O
"	O
forget	O
"	O
becomes	O
"	O
forgetfulness	O
"	O
and	O
so	O
on	O
(	O
Tanaka	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

In	O
Linguistics	O
,	O
polysemy	O
refers	O
to	O
the	O
capacity	O
for	O
a	O
word	O
to	O
have	O
multiple	O
meanings	O
or	O
senses	O
.	O

Abstract	O
nouns	O
are	O
observed	O
to	O
be	O
more	O
"	O
polysemous	O
"	O
than	O
concrete	O
nouns	O
(	O
Tanaka	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

For	O
example	O
,	O
in	O
Word	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
the	O
word	O
"	O
dog	O
"	O
has	O
8	O
senses	O
,	O
while	O
the	O
word	O
"	O
love	O
"	O
has	O
10	O
senses	O
.	O

Tanaka	O
et	O
al	O
.	O

2013	O
find	O
a	O
direct	O
correlation	O
between	O
the	O
abstractness	O
of	O
a	O
noun	O
and	O
the	O
number	O
of	O
hyponyms	O
the	O
word	O
has	O
.	O

We	O
consider	O
the	O
number	O
of	O
hyponyms	O
of	O
the	O
most	O
commonly	O
occurring	O
sense	O
of	O
the	O
word	O
,	O
and	O
the	O
average	O
number	O
of	O
hyponyms	O
of	O
all	O
the	O
senses	O
of	O
the	O
word	O
.	O

Score	O
-	O
based	O
Features	O
Abstract	O
nouns	O
evoke	O
emotions	O
in	O
humans	O
.	O

SentiWordNet	B-MethodName
(	O
Baccianella	O
et	O
al	O
.	O
,	O

2010	O
)	O
,	O
another	O
lexical	O
database	O
like	O
Word	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
,	O
gives	O
scores	O
based	O
on	O
the	O
how	O
positive	O
,	O
negative	O
or	O
objective	O
they	O
are	O
.	O

Abstract	O
words	O
have	O
a	O
higher	O
positive	O
/	O
negative	O
score	O
,	O
while	O
concrete	O
words	O
have	O
a	O
higher	O
objective	O
score	O
.	O

Again	O
,	O
here	O
,	O
we	O
consider	O
these	O
scores	O
for	O
the	O
most	O
commonly	O
occurring	O
sense	O
,	O
and	O
the	O
average	O
scores	O
of	O
all	O
the	O
senses	O
of	O
the	O
word	O
.	O

From	O
the	O
features	O
above	O
,	O
we	O
have	O
a	O
13dimensional	O
vector	O
for	O
every	O
word	O
in	O
the	O
lexicon	O
.	O

The	O
embedding	O
is	O
created	O
so	O
that	O
every	O
dimension	O
is	O
directly	O
proportional	O
to	O
the	O
concreteness	O
of	O
the	O
word	O
.	O

For	O
example	O
,	O
the	O
length	O
of	O
a	O
word	O
is	O
in	O
general	O
,	O
indirectly	O
proportional	O
to	O
the	O
concreteness	O
of	O
the	O
word	O
,	O
so	O
we	O
take	O
the	O
length	O
dimension	O
of	O
the	O
vector	O
as	O
large	O
value	O
−	O
length	O
of	O
word	O
,	O
where	O
we	O
take	O
10	O
,	O
000	O
as	O
the	O
large	O
value	O
.	O

The	O
large	O
value	O
chosen	O
was	O
the	O
same	O
for	O
all	O
features	O
which	O
are	O
indirectly	O
proportional	O
to	O
concreteness	O
.	O

Towards	O
improving	O
the	O
trained	O
model	O
,	O
we	O
use	O
a	O
method	O
which	O
we	O
term	O
as	O
the	O
Difference	B-MethodName
Method	I-MethodName
.	O

If	O
the	O
difference	O
of	O
the	O
top-2	O
probabilities	O
predicted	O
by	O
the	O
model	O
is	O
greater	O
than	O
a	O
certain	O
threshold	O
,	O
this	O
implies	O
that	O
the	O
model	O
is	O
sure	O
of	O
the	O
prediction	O
it	O
has	O
made	O
.	O

However	O
,	O
if	O
the	O
difference	O
is	O
less	O
than	O
the	O
tunable	O
threshold	O
,	O
the	O
model	O
is	O
ambivalent	O
about	O
whether	O
the	O
option	O
with	O
the	O
highest	O
probability	O
or	O
the	O
option	O
with	O
the	O
second	O
highest	O
probability	O
is	O
correct	O
.	O

In	O
this	O
case	O
,	O
we	O
compute	O
for	O
how	O
many	O
dimensions	O
the	O
value	O
of	O
the	O
linguistic	O
embedding	O
of	O
the	O
second	O
word	O
is	O
less	O
than	O
the	O
value	O
of	O
the	O
linguistic	O
embedding	O
of	O
the	O
first	O
word	O
.	O

If	O
the	O
majority	O
of	O
the	O
values	O
(	O
i.e.	O
,	O
7	O
)	O
are	O
less	O
,	O
we	O
change	O
the	O
prediction	O
of	O
the	O
model	O
to	O
the	O
second	O
-	O
most	O
probable	O
option	O
.	O

The	O
threshold	O
is	O
tuned	O
on	O
the	O
dev	O
set	O
.	O

Furthermore	O
,	O
we	O
use	O
a	O
Threshold	B-MethodName
Method	I-MethodName
towards	O
improving	O
the	O
model	O
performance	O
.	O

If	O
the	O
highest	O
probability	O
is	O
less	O
than	O
a	O
tunable	O
threshold	O
,	O
the	O
model	O
is	O
unsure	O
of	O
its	O
predictions	O
and	O
we	O
consider	O
the	O
improvement	O
approaches	O
on	O
the	O
option	O
with	O
the	O
second	O
-	O
highest	O
probability	O
.	O

According	O
to	O
Spreen	O
and	O
Schulz	O
,	O
1966	O
,	O
a	O
highly	O
specific	O
word	O
refers	O
to	O
a	O
very	O
particular	O
instance	O
,	O
while	O
a	O
non	O
-	O
specific	O
word	O
refers	O
to	O
a	O
generic	O
concept	O
,	O
i.e.	O
,	O
it	O
encompasses	O
many	O
classes	O
/	O
instances	O
.	O

For	O
example	O
,	O
consider	O
the	O
words	O
"	O
animal	O
"	O
,	O
"	O
bird	O
"	O
and	O
"	O
eagle	O
"	O
.	O

The	O
words	O
are	O
listed	O
in	O
increasing	O
order	O
of	O
specificity	O
.	O

We	O
find	O
parallels	O
between	O
the	O
definition	O
of	O
specificity	O
/	O
non	O
-	O
specificity	O
and	O
the	O
linguistic	O
phenomenon	O
of	O
hypernymy	O
.	O

Schreuder	O
and	O
Baayen	O
,	O
1995	O
define	O
a	O
hypernym	O
as	O
"	O
a	O
word	O
with	O
a	O
general	O
meaning	O
that	O
has	O
basically	O
the	O
same	O
meaning	O
of	O
a	O
more	O
specific	O
word	O
"	O
.	O

The	O
more	O
specific	O
word	O
is	O
the	O
corresponding	O
hyponym	O
.	O

In	O
simpler	O
terms	O
,	O
each	O
word	O
is	O
related	O
to	O
some	O
super	O
-	O
types	O
and	O
sub	O
-	O
types	O
,	O
called	O
as	O
hypernyms	O
and	O
hyponyms	O
,	O
respectively	O
.	O

In	O
linguistics	O
,	O
hyponymy	O
is	O
a	O
semantic	O
relation	O
be	O
-	O
tween	O
a	O
hyponym	O
denoting	O
a	O
subtype	O
and	O
a	O
hypernym	O
denoting	O
a	O
supertype	O
.	O

For	O
example	O
,	O
in	O
figure	O
1	O
,	O
as	O
we	O
traverse	O
up	O
the	O
hypernymy	O
tree	O
,	O
assuming	O
we	O
consider	O
the	O
word	O
"	O
dog	O
"	O
,	O
we	O
find	O
that	O
its	O
hypernym	O
is	O
"	O
animal	O
"	O
,	O
which	O
is	O
much	O
broader	O
than	O
"	O
dog	O
"	O
.	O

On	O
the	O
other	O
hand	O
,	O
as	O
we	O
go	O
down	O
the	O
hypernymy	O
tree	O
,	O
we	O
find	O
more	O
specific	O
terms	O
for	O
the	O
word	O
"	O
dog	O
"	O
such	O
as	O
"	O
terrier	O
"	O
.	O

Essentially	O
,	O
hyponyms	O
represent	O
"	O
IS	O
-	O
A	O
"	O
relationships	O
.	O

For	O
example	O
,	O
"	O
terrier	O
"	O
is	O
a	O
"	O
dog	O
"	O
.	O

We	O
leverage	O
the	O
hypernymy	O
property	O
of	O
words	O
to	O
help	O
the	O
model	O
in	O
deciding	O
the	O
most	O
non	O
-	O
specific	O
option	O
.	O

The	O
two	O
methods	O
which	O
we	O
implement	O
are	O
:	O
Hypernym	O
Augmentation	O
Method	O
In	O
order	O
to	O
infuse	O
a	O
sense	O
of	O
non	O
-	O
specificity	O
(	O
other	O
than	O
training	O
on	O
the	O
given	O
dataset	O
for	O
non	O
-	O
specificity	O
)	O
,	O
we	O
augment	O
the	O
dataset	O
for	O
subtask	O
-	O
I.	O
We	O
randomly	O
select	O
n	O
nouns	O
from	O
the	O
article	O
by	O
using	O
a	O
basic	O
POS	O
Tagging	O
pipeline	O
.	O

For	O
each	O
noun	O
,	O
we	O
use	O
the	O
Lesk	B-MethodName
algorithm	I-MethodName
(	O
Lesk	O
,	O
1986	O
)	O
to	O
find	O
the	O
most	O
appropriate	O
sense	O
of	O
the	O
word	O
based	O
on	O
its	O
context	O
.	O

For	O
this	O
sense	O
of	O
the	O
word	O
,	O
we	O
find	O
its	O
hypernyms	O
,	O
pick	O
a	O
hypernym	O
uniformly	O
at	O
random	O
from	O
this	O
list	O
of	O
hypernyms	O
and	O
replace	O
the	O
noun	O
in	O
the	O
article	O
with	O
the	O
hypernym	O
.	O

We	O
do	O
this	O
for	O
all	O
2	O
n	O
combinations	O
,	O
i.e.	O
,	O
corresponding	O
to	O
every	O
sample	O
,	O
we	O
have	O
2	O
n	O
augmented	O
samples	O
.	O

Furthermore	O
,	O
we	O
randomly	O
mask	O
tokens	O
in	O
this	O
dataset	O
and	O
train	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
on	O
this	O
dataset	O
.	O

This	O
serves	O
a	O
dual	O
purpose	O
.	O

Firstly	O
,	O
it	O
serves	O
as	O
a	O
sort	O
of	O
domain	B-MethodName
adaptation	I-MethodName
,	O
and	O
secondly	O
,	O
it	O
infuses	O
a	O
sense	O
of	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
in	O
the	O
model	O
.	O

While	O
finetuning	O
BERT	B-MethodName
MLM	O
on	O
the	O
augmented	O
dataset	O
,	O
we	O
freeze	O
two	O
layers	O
,	O
due	O
to	O
time	O
and	O
computational	O
constraints	O
.	O

We	O
replace	O
the	O
normal	O
BERT	B-MethodName
Encoder	O
in	O
our	O
BERT	B-MethodName
FitB	I-MethodName
model	O
with	O
the	O
BERT	B-MethodName
Encoder	I-MethodName
fine	O
-	O
tuned	O
on	O
the	O
augmented	O
dataset	O
.	O

Hyponyms	B-MethodName
Options	I-MethodName
Method	I-MethodName
Here	O
,	O
we	O
use	O
the	O
Difference	B-MethodName
Method	I-MethodName
/	I-MethodName
Threshold	I-MethodName
Method	I-MethodName
.	O

If	O
the	O
model	O
is	O
sure	O
of	O
its	O
prediction	O
,	O
we	O
keep	O
the	O
prediction	O
of	O
the	O
model	O
.	O

Otherwise	O
,	O
we	O
generate	O
hyponyms	O
for	O
each	O
option	O
using	O
WordNet	O
.	O

After	O
the	O
hyponyms	O
are	O
tokenised	O
,	O
we	O
use	O
the	O
trained	O
model	O
's	O
output	O
and	O
map	O
each	O
hyponym	O
token	O
to	O
the	O
output	O
vocabulary	O
space	O
and	O
get	O
the	O
corresponding	O
scores	O
.	O

We	O
then	O
take	O
the	O
maximum	O
score	O
amongst	O
all	O
of	O
the	O
hyponyms	O
as	O
the	O
predicted	O
probability	O
for	O
that	O
option	O
.	O

The	O
reason	O
for	O
incorporating	O
this	O
approach	O
pertains	O
to	O
how	O
the	O
transformer	O
models	O
were	O
pre	O
-	O
trained	O
.	O

Consider	O
the	O
following	O
sentence	O
:	O
"	O
He	O
had	O
a	O
[	O
MASK	O
]	O
and	O
it	O
was	O
bitter	O
"	O
.	O

Now	O
,	O
suppose	O
that	O
we	O
have	O
two	O
options	O
:	O
"	O
beer	O
"	O
and	O
"	O
drink	O
"	O
.	O

Generally	O
,	O
our	O
transformer	O
-	O
based	O
model	O
would	O
look	O
at	O
the	O
word	O
"	O
bitter	O
"	O
and	O
predict	O
"	O
beer	O
"	O
.	O

However	O
,	O
"	O
drink	O
"	O
is	O
more	O
non	O
-	O
specific	O
than	O
"	O
beer	O
"	O
.	O

To	O
address	O
the	O
limitations	O
of	O
the	O
vanilla	O
transformer	O
-	O
based	O
models	O
,	O
we	O
attempt	O
multiple	O
modifications	O
to	O
the	O
proposed	O
baseline	O
transformer	O
models	O
,	O
specifically	O
for	O
BERT	B-MethodName
.	O

The	O
major	O
limitation	O
of	O
the	O
pretrained	O
BERT	B-MethodName
model	O
that	O
we	O
'	O
ve	O
used	O
,	O
is	O
the	O
restriction	O
on	O
the	O
length	O
of	O
the	O
tokenised	O
inputs	O
.	O

Only	O
512	B-HyperparameterValue
tokens	B-HyperparameterName
from	O
a	O
sample	O
can	O
be	O
processed	O
by	O
BERT	B-MethodName
in	O
one	O
parse	O
and	O
hence	O
,	O
some	O
articles	O
end	O
up	O
getting	O
truncated	O
and	O
context	O
is	O
lost	O
.	O

The	O
following	O
are	O
some	O
of	O
the	O
modifications	O
we	O
'	O
ve	O
made	O
to	O
improve	O
the	O
performance	O
of	O
our	O
models	O
:	O
Voting	O
We	O
tokenise	O
the	O
question	O
and	O
the	O
article	O
.	O

We	O
split	O
the	O
article	O
into	O
chunks	O
and	O
pair	O
each	O
chunk	O
with	O
the	O
question	O
such	O
that	O
the	O
length	O
of	O
the	O
tokenised	O
(	O
chunk	O
,	O
question	O
)	O
pair	O
is	O
512	B-HyperparameterValue
.	O

While	O
splitting	O
the	O
article	O
into	O
chunks	O
,	O
we	O
keep	O
a	O
maxoverlap	B-HyperparameterName
stride	I-HyperparameterName
of	O
128	B-HyperparameterValue
so	O
that	O
the	O
context	O
of	O
the	O
previous	O
chunk	O
is	O
not	O
lost	O
.	O

We	O
train	O
the	O
model	O
on	O
these	O
newly	O
formed	O
(	O
chunk	O
,	O
question	O
)	O
pairs	O
.	O

During	O
inference	O
,	O
we	O
take	O
the	O
weighted	O
sum	O
of	O
the	O
logits	O
.	O

For	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Similarity	I-MethodName
)	I-MethodName
,	O
the	O
weights	O
are	O
calculated	O
as	O
:	O
weight	O
ij	O
=	O
u	O
i	O
.v	O
j	O
||u	O
i	O
||||v	O
j	O
||	O
(	O
1)where	O
u	O
i	O
is	O
the	O
embedding	O
of	O
the	O
question	O
in	O
the	O
i	O
th	O
sample	O
,	O
and	O
v	O
j	O
is	O
the	O
embedding	O
of	O
the	O
j	O
th	O
chunk	O
of	O
the	O
sample	O
's	O
article	O
.	O

To	O
find	O
the	O
embeddings	O
,	O
we	O
extract	O
the	O
[	O
CLS	O
]	O
embedding	O
from	O
a	O
pretrained	O
BERT	B-MethodName
encoder	O
.	O

We	O
also	O
try	O
out	O
an	O
alternate	O
way	O
of	O
defining	O
the	O
weights	O
:	O
weight	O
ij	O
=	O
|{q	O
i	O
toks	O
.	O
}	O

∩	O
{	O
chunk	O
j	O
toks.}|	O
|{chunk	O
j	O
toks.}|(2	O
)	O
where	O
{	O
q	O
i	O
toks	O
.	O
}	O

is	O
the	O
set	O
of	O
tokens	O
in	O
the	O
i	O
th	O
sample	O
's	O
question	O
,	O
and	O
{	O
chunk	O
j	O
toks	O
.	O
}	O

is	O
the	O
set	O
of	O
tokens	O
in	O
the	O
j	O
th	O
chunk	O
of	O
the	O
sample	O
.	O

|.|	O
represents	O
the	O
cardinality	O
of	O
a	O
set	O
.	O

We	O
call	O
the	O
method	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching).We	I-MethodName
normalise	O
the	O
computed	O
weights	O
:	O
norm	O
weight	O
ij	O
=	O
weight	O
ij	O
n	O
i	O
j=1	O
weight	O
ij	O
(	O
3)where	O
n	O
i	O
is	O
the	O
number	O
of	O
chunks	O
in	O
the	O
i	O
th	O
sample	O
.	O

The	O
idea	O
behind	O
this	O
is	O
that	O
higher	O
the	O
similarity	O
between	O
the	O
question	O
and	O
the	O
article	O
's	O
chunk	O
,	O
higher	O
is	O
the	O
weight	O
assigned	O
to	O
the	O
logits	O
returned	O
by	O
the	O
trained	O
model	O
with	O
the	O
question	O
-	O
chunk	O
pair	O
as	O
input	O
.	O

In	O
Equation	O
2	O
,	O
we	O
find	O
the	O
fraction	O
of	O
tokens	O
common	O
between	O
the	O
question	O
and	O
chunk	O
.	O

Max	B-MethodName
Context	I-MethodName
This	O
method	O
is	O
a	O
slight	O
modification	O
of	O
the	O
Voting	B-MethodName
Method	I-MethodName
.	O

Instead	O
of	O
training	O
the	O
model	O
on	O
all	O
(	O
chunk	O
,	O
question	O
)	O
pairs	O
for	O
a	O
particular	O
sample	O
,	O
we	O
train	O
the	O
model	O
on	O
the	O
pair	O
with	O
the	O
highest	O
weight	O
.	O

The	O
weights	O
are	O
calculated	O
as	O
described	O
in	O
Equation	O
2	O
.	O

We	O
propose	O
a	O
few	O
modifications	O
to	O
the	O
baseline	O
,	O
namely	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

2017a	O
)	O
provided	O
by	O
the	O
organisers	O
.	O

GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
BERT	I-MethodName
We	O
use	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
on	O
top	O
of	O
BERT	B-MethodName
embeddings	O
.	O

This	O
could	O
lead	O
to	O
potential	O
improvement	O
in	O
performance	O
for	O
subtask	O
-	O
I	O
as	O
BERT	B-MethodName
embeddings	O
are	O
more	O
feature	O
-	O
rich	O
than	O
GloVe	B-MethodName
embeddings	O
.	O

Reader	O
,	O
we	O
came	O
up	O
with	O
an	O
approach	O
that	O
uses	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
across	O
two	O
-	O
BERT	B-MethodName
streams	O
.	O

The	O
first	O
stream	O
takes	O
in	O
the	O
question	O
input	O
,	O
and	O
works	O
like	O
the	O
regular	O
BERT	O
model	O
.	O

The	O
second	O
stream	O
takes	O
the	O
article	O
input	O
.	O

Assume	O
the	O
layer	O
outputs	O
for	O
layer	O
L	O
are	O
Q	O
L	O
and	O
A	O
L	O
,	O
respectively	O
,	O
for	O
question	O
and	O
article	O
streams	O
.	O

Then	O
,	O
to	O
the	O
layer	O
L	O
+	O
1	O
for	O
question	O
stream	O
,	O
Q	O
L	O
is	O
passed	O
as	O
input	O
,	O
while	O
to	O
layer	O
L	O
+	O
1	O
for	O
article	O
stream	O
,	O
GA(Q	O
L	O
,	O
A	O
L	O
)	O
is	O
passed	O
,	O
where	O
GA	O
is	O
the	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
function	O
.	O

This	O
is	O
done	O
for	O
all	O
12	O
layers	O
of	O
BERT	B-MethodName
-	I-MethodName
BASE	I-MethodName
.	O

Finally	O
,	O
on	O
this	O
model	O
,	O
two	O
types	O
of	O
heads	O
are	O
attached	O
-Selection	B-MethodName
and	I-MethodName
Pooling	I-MethodName
(	I-MethodName
similar	I-MethodName
to	I-MethodName
BERT	I-MethodName
FitB	I-MethodName
)	I-MethodName
,	O
and	O
Attention	B-MethodName
Classification	I-MethodName
(	I-MethodName
similar	I-MethodName
to	I-MethodName
GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
)	I-MethodName
.	O

The	O
logits	O
for	O
each	O
head	O
are	O
concatenated	O
and	O
a	O
fully	O
-	O
connected	O
layer	O
is	O
added	O
on	O
top	O
.	O

Since	O
this	O
is	O
a	O
major	O
change	O
in	O
the	O
architecture	O
of	O
BERT	B-MethodName
,	O
this	O
model	O
needs	O
a	O
significant	O
amount	O
of	O
pretraining	O
.	O

Answer	O
-	O
Attention	O
Since	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
also	O
attends	O
to	O
the	O
candidate	O
answer	O
embeddings	O
,	O
we	O
also	O
attempt	O
an	O
approach	O
where	O
we	O
pass	O
the	O
options	O
to	O
the	O
BERT	B-MethodName
model	O
.	O

On	O
the	O
option	O
embeddings	O
and	O
the	O
[	O
MASK	O
]	O
token	O
embeddings	O
,	O
we	O
apply	O
multiplicative	O
attention	O
(	O
dot	O
product	O
)	O
to	O
get	O
attention	O
scores	O
.	O

These	O
scores	O
are	O
directly	O
used	O
as	O
logits	O
for	O
the	O
prediction	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
Lai	O
et	O
al	O
.	O
(	O

2019	O
)	O
propose	O
a	O
combination	O
of	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
and	I-MethodName
Self	I-MethodName
-	I-MethodName
Attention	I-MethodName
-Gated	I-MethodName
Self	I-MethodName
-	I-MethodName
Attention	I-MethodName
(	I-MethodName
GSA	I-MethodName
)	I-MethodName
.	O

They	O
show	O
improvements	O
on	O
smaller	O
datasets	O
compared	O
to	O
Compare	B-MethodName
-	I-MethodName
Aggregate	I-MethodName
Approaches	O
.	O

We	O
use	O
two	O
GSA	B-MethodName
layers	O
on	O
top	O
of	O
BERT	B-MethodName
Embeddings	O
,	O
and	O
use	O
the	O
same	O
decoder	O
and	O
selection	O
method	O
as	O
BERT	B-MethodName
FitB.	I-MethodName
In	O
all	O
our	O
experiments	O
,	O
we	O
use	O
the	O
PyTorch	O
implementations	O
of	O
the	O
transformers	O
-	O
based	O
models	O
provided	O
by	O
the	O
HuggingFace	O
.The	O
metric	O
for	O
all	O
the	O
3	O
subtasks	O
is	O
accuracy	B-MetricName
.	O

For	O
subtask	O
-	O
I	O
,	O
to	O
obtain	O
the	O
linguistic	O
features	O
mentioned	O
in	O
3.2	O
,	O
and	O
to	O
obtain	O
the	O
hypernyms	O
and	O
hyponyms	O
for	O
subtask	O
-	O
II	O
,	O
we	O
use	O
the	O
lexical	O
database	O
,	O
WordNet	B-DatasetName
provided	O
by	O
NLTK	O
(	O
Bird	O
and	O
Loper	O
,	O
2004	O
)	O
,	O
a	O
library	O
in	O
Python	O
.	O

For	O
both	O
subtasks	O
,	O
we	O
train	O
our	O
models	O
on	O
train	O
+	O
trial	O
dataset	O
,	O
and	O
evaluate	O
them	O
on	O
the	O
dev	O
set	O
.	O

The	O
training	O
and	O
the	O
evaluation	O
of	O
systems	O
was	O
on	O
Google	O
Colaboratory	O
's	O
free	O
GPU	O
(	O
NVIDIA	O
K80	O
/	O
P100	O
)	O
.	O

The	O
training	O
time	O
varies	O
with	O
the	O
models	O
.	O

It	O
is	O
around	O
1	O
-	O
2	O
hours	O
for	O
the	O
base	O
variants	O
and	O
2	O
-	O
4	O
hours	O
for	O
the	O
large	O
models	O
,	O
which	O
is	O
well	O
within	O
the	O
12	O
hour	O
limit	O
of	O
Colab	O
.	O

DistilBERT	B-MethodName
took	O
about	O
half	O
an	O
hour	O
for	O
training	O
.	O

For	O
finetuning	O
the	O
BERT	B-MethodName
FitB	I-MethodName
Hypr	I-MethodName
Aug	I-MethodName
Model	O
on	O
the	O
augmented	O
dataset	O
on	O
the	O
MLM	O
task	O
,	O
we	O
use	O
Nvidia	O
-	O
DGX	O
Station	O
with	O
the	O
following	O
specifications	O
:	O
four	O
32	O
GB	O
Tesla	O
V100	O
GPUs	O
,	O
256	O
GB	O
RAM	O
and	O
forty	O
Intel	O
Xeon	O
2.20GHz	O
processors	O
since	O
it	O
is	O
a	O
computationally	O
intensive	O
task	O
.	O

For	O
all	O
our	O
experiments	O
,	O
we	O
use	O
Adam	B-HyperparameterName
Optimiser	I-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2017	O
)	O
and	O
Cross	B-HyperparameterName
Entropy	I-HyperparameterName
Loss	I-HyperparameterName
.	O

For	O
choosing	O
the	O
optimal	O
set	O
of	O
hyperparameters	O
,	O
we	O
run	O
a	O
Grid	B-MethodName
Search	I-MethodName
on	O
our	O
models	O
.	O

We	O
zero	O
in	O
on	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e-5	B-MetricValue
.	O

Schedulers	O
such	O
as	O
Linear	B-MetricName
Scheduler	I-MetricName
,	O
Cosine	B-MetricName
Annealing	I-MetricName
Scheduler	I-MetricName
,	O
etc	O
.	O

seem	O
to	O
have	O
a	O
negative	O
impact	O
on	O
the	O
results	O
.	O

For	O
the	O
FitB	B-HyperparameterName
models	O
,	O
we	O
keep	O
all	O
the	O
layers	O
unfrozen	O
.	O

Additionally	O
,	O
the	O
maximum	B-HyperparameterName
input	I-HyperparameterName
length	I-HyperparameterName
is	O
kept	O
as	O
512	B-HyperparameterValue
.	O

We	O
train	O
our	O
models	O
for	O
4	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
keeping	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
2	B-HyperparameterValue
.	O

Among	O
the	O
vanilla	O
models	O
,	O
BERT	B-MethodName
FitB	I-MethodName
Large	I-MethodName
performs	O
the	O
best	O
.	O

This	O
is	O
understandable	O
when	O
it	O
comes	O
to	O
DistilBERT	B-MethodName
and	O
ALBERT	B-MethodName
,	O
since	O
these	O
models	O
are	O
pruned	O
and	O
distilled	O
for	O
faster	O
computation	O
.	O

Notably	O
,	O
DistilBERT	B-MethodName
gives	O
comparable	O
performance	O
to	O
BERT	B-MethodName
FitB	I-MethodName
Base	I-MethodName
.	O

A	O
slightly	O
surprising	O
observation	O
was	O
that	O
there	O
is	O
a	O
degradation	O
in	O
accuracy	O
on	O
using	O
RoBERTa	B-MethodName
.	O

This	O
could	O
be	O
because	O
even	O
though	O
it	O
was	O
pretrained	O
more	O
robustly	O
than	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
it	O
was	O
not	O
pretrained	O
on	O
the	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
Task	I-TaskName
,	O
and	O
hence	O
,	O
might	O
perform	O
worse	O
on	O
Textual	B-TaskName
Entailment	I-TaskName
tasks	I-TaskName
.	O

A	O
peculiar	O
observation	O
is	O
that	O
the	O
large	O
variants	O
of	O
ALBERT	B-MethodName
FitB	I-MethodName
and	O
RoBERTa	B-MethodName
FitB	I-MethodName
models	O
perform	O
worse	O
than	O
their	O
base	O
variants	O
.	O

This	O
may	O
imply	O
that	O
more	O
training	O
data	O
is	O
needed	O
to	O
train	O
the	O
large	O
variants	O
.	O

For	O
subtask	O
-	O
I	O
,	O
in	O
table	O
2	O
,	O
we	O
also	O
demonstrate	O
the	O
results	O
of	O
BERT	B-MethodName
Ensemble	I-MethodName
,	O
in	O
which	O
we	O
ensemble	O
(	O
i.e.	O
,	O
averaging	O
over	O
the	O
predictions	O
)	O
two	O
checkpoints	O
saved	O
during	O
the	O
training	O
process	O
.	O

When	O
it	O
comes	O
to	O
the	O
Difference	B-MethodName
Method	I-MethodName
using	I-MethodName
Linguistic	I-MethodName
Features	I-MethodName
for	O
imperceptibility	B-TaskName
,	O
we	O
observe	O
an	O
improvement	O
on	O
the	O
dev	O
set	O
,	O
but	O
a	O
slight	O
fall	O
is	O
observed	O
while	O
evaluating	O
it	O
on	O
the	O
test	O
set	O
.	O

This	O
might	O
be	O
solved	O
by	O
careful	O
tuning	O
of	O
the	O
threshold	O
.	O

The	O
polls	O
are	O
already	O
years	O
overdue	O
and	O
were	O
scheduled	O
for	O
Sunday	O
.	O

They	O
were	O
postponed	O
because	O
of	O
an	O
ongoing	O
stalemate	O
between	O
the	O
government	O
and	O
a	O
group	O
of	O
opposition	O
senators	O
over	O
an	O
electoral	O
law	O
.	O

Haiti	O
is	O
the	O
poorest	O
country	O
in	O
the	O
region	O
and	O
is	O
still	O
struggling	O
to	O
recover	O
from	O
a	O
2010	O
earthquake	O
.	O

Protesters	O
lit	O
piles	O
of	O
wood	O
in	O
the	O
central	O
neighbourhood	O
of	O
Bel	O
Aire	O
before	O
marching	O
to	O
a	O
wealthy	O
hillside	O
neighbourhood	O
,	O
where	O
riot	O
police	O
guarded	O
hotels	O
,	O
shops	O
and	O
Haiti	O
'	O
s	O
elections	O
office	O
.	O

Some	O
demanded	O
President	O
Michel	O
Martelly	O
'	O
s	O
resignation	O
for	O
his	O
"	O
inability	O
to	O
organise	O
elections	O
in	O
the	O
country	O
"	O
.	O

Two	O
opposition	O
activists	O
who	O
had	O
organised	O
the	O
protest	O
were	O
arrested	O
by	O
police	O
for	O
"	O
public	O
unrest	O
and	O
inciting	O
violence	O
"	O
.	O

Mid	O
-term	O
senate	O
elections	O
in	O
Haiti	O
had	O
been	O
due	O
in	O
May	O
2012	O
,	O
while	O
the	O
municipal	O
poll	O
is	O
three	O
years	O
behind	O
schedule	O
as	O
Haiti	O
slowly	O
emerges	O
from	O
the	O
earthquake	O
which	O
left	O
much	O
of	O
the	O
country	O
devastated	O
in	O
2010	O
.	O

In	O
June	O
,	O
President	O
Michel	O
Martelly	O
decreed	O
that	O
the	O
elections	O
be	O
held	O
on	O
26	O
October	O
.	O

The	O
date	O
was	O
set	O
after	O
lengthy	O
talks	O
mediated	O
by	O
the	O
president	O
of	O
Haiti	O
'	O
s	O
Bishops	O
'	O
Conference	O
,	O
Cardinal	O
Chibly	O
Langlois	O
,	O
intended	O
to	O
overcome	O
the	O
political	O
deadlock	O
between	O
the	O
opposition	O
and	O
the	O
government	O
.	O

But	O
after	O
the	O
National	O
Assembly	O
failed	O
to	O
pass	O
an	O
electoral	O
law	O
in	O
time	O
,	O
the	O
office	O
of	O
Mr	O
Martelly	O
announced	O
another	O
postponement	O
on	O
Sunday	O
.	O

No	O
new	O
date	O
has	O
been	O
set	O
,	O
but	O
the	O
statement	O
said	O
that	O
"	O
President	O
Michel	O
Martelly	O
,	O
in	O
his	O
constant	O
concern	O
to	O
guarantee	O
political	O
stability	O
,	O
promises	O
to	O
pursue	O
consultations	O
with	O
the	O
different	O
sectors	O
of	O
national	O
life	O
in	O
order	O
to	O
hold	O
the	O
elections	O
as	O
soon	O
as	O
possible	O
"	O
.	O

Opposition	O
politicians	O
accuse	O
President	O
Martelly	O
of	O
wanting	O
to	O
rule	O
by	O
decree	O
-a	O
likely	O
scenario	O
if	O
no	O
elections	O
are	O
held	O
before	O
the	O
lower	O
chamber	O
'	O
s	O
term	O
runs	O
out	O
in	O
January	O
.	O

The	O
government	O
argues	O
that	O
opposition	O
politicians	O
are	O
also	O
dragging	O
their	O
feet	O
in	O
the	O
hope	O
of	O
extending	O
their	O
time	O
in	O
office	O
without	O
elections	O
.	O

Thousands	O
of	O
Haitians	O
marched	O
in	O
the	O
capital	O
Port	O
-au	O
-Prince	O
on	O
Sunday	O
in	O
protest	O
at	O
a	O
delay	O
in	O
the	O
country	O
'	O
s	O
[	O
MASK	O
]	O
and	O
municipal	O
elections	O
.Options	O
:	O
Local	O
,	O
Annual	O
,	O
Legislative	O
,	O
Municipal	O
,	O
Devastating	O
In	O
the	O
future	O
,	O
we	O
aspire	O
to	O
learn	O
embeddings	O
using	O
these	O
Linguistic	O
Features	O
as	O
input	O
to	O
common	O
models	O
such	O
as	O
Word2Vec	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

2013).For	O
non	O
-	O
specificity	O
,	O
with	O
the	O
hypernym	B-MethodName
augmentation	I-MethodName
method	I-MethodName
,	O
BERT	B-MethodName
FitB	I-MethodName
achieves	O
lower	O
accuracy	O
.	O

A	O
possible	O
reason	O
for	O
this	O
could	O
be	O
that	O
replacing	O
the	O
nouns	O
with	O
their	O
hypernyms	O
in	O
some	O
contexts	O
changes	O
the	O
meaning	O
of	O
the	O
sentence	O
(	O
even	O
though	O
we	O
use	B-MethodName
Lesk	I-MethodName
Algorithm	I-MethodName
for	O
WSD	O
,	O
not	O
all	O
hypernyms	O
make	O
sense	O
)	O
.	O

For	O
example	O
,	O
the	O
word	O
"	O
drink	O
"	O
is	O
replaced	O
with	O
"	O
food	O
"	O
.	O

For	O
the	O
hyponyms	B-MethodName
method	I-MethodName
,	O
we	O
can	O
improve	O
our	O
results	O
by	O
recursively	O
generating	O
hyponyms	O
for	O
a	O
particular	O
option	O
,	O
instead	O
of	O
taking	O
the	O
immediate	O
hyponyms	O
.	O

Again	O
,	O
threshold	O
tuning	O
may	O
help	O
.	O

In	O
Table	O
3	O
,	O
a	O
positive	O
sign	O
for	O
the	O
Difference	B-MethodName
Method	I-MethodName
or	O
the	O
Threshold	B-MethodName
Method	I-MethodName
is	O
the	O
improvement	O
in	O
the	O
results	O
of	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching	I-MethodName
)	I-MethodName
when	O
we	O
consider	O
the	O
hyponyms	O
.	O

The	O
accuracy	B-MetricName
jumps	O
from	O
72.86	B-MetricValue
%	I-MetricValue
to	I-MetricValue
75.79	I-MetricValue
%	I-MetricValue
on	O
the	O
dev	O
set	O
and	O
from	O
77.83	B-MetricValue
%	I-MetricValue
to	I-MetricValue
78.98	I-MetricValue
%	I-MetricValue
on	O
the	O
test	O
set	O
.	O

This	O
reinforces	O
our	O
claim	O
that	O
with	O
more	O
careful	O
tuning	O
of	O
the	O
threshold	O
,	O
we	O
might	O
get	O
improvements	O
on	O
the	O
test	O
set	O
in	O
other	O
methods	O
.	O

BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
performs	O
better	O
than	O
vanilla	B-MethodName
BERT	I-MethodName
FitB	I-MethodName
on	O
both	O
subtasks	O
.	O

This	O
is	O
intuitive	O
since	O
in	O
the	O
latter	O
,	O
we	O
truncate	O
the	O
article	O
to	O
512	B-HyperparameterValue
tokens	I-HyperparameterValue
without	O
any	O
consideration	O
of	O
how	O
much	O
context	O
is	O
lost	O
.	O

Voting	O
,	O
on	O
the	O
other	O
hand	O
,	O
considers	O
all	O
contexts	O
and	O
hence	O
,	O
gives	O
a	O
superior	O
performance	O
.	O

For	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
when	O
compared	O
with	O
the	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
baseline	O
,	O
the	B-MetricName
accuracy	I-MetricName
improves	O
from	O
21	B-MetricValue
%	I-MetricValue
to	O
39	B-MetricValue
%	I-MetricValue
on	O
subtask	O
-	O
I	O
dev	O
set	O
.	O

Due	O
to	O
computational	O
restrictions	O
,	O
we	O
could	O
n't	O
pretrain	O
GA	O
-	O
BERT	O
,	O
and	O
only	O
fine	O
-	O
tuned	O
it	O
for	O
subtask	O
-	O
I	O
to	O
get	O
an	O
idea	O
about	O
its	O
performance	O
,	O
which	O
was	O
sub	O
-	O
optimal	O
(	B-MetricValue
19	I-MetricValue
%	I-MetricValue
)	O
.	O

The	O
Answer	B-MethodName
-	I-MethodName
Attention	I-MethodName
system	I-MethodName
gave	O
us	O
a	O
dev	O
score	O
of	O
≈61	B-MetricName
%	I-MetricName
on	O
subtask	O
-	O
I	O
,	O
which	O
is	O
much	O
higher	O
than	O
the	O
baseline	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
achieves	O
≈31	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
on	O
subtask	O
-	O
I	O
dev	O
set	O
.	O

The	O
reasons	O
for	O
this	O
could	O
be	O
lack	O
of	O
pretraining	O
,	O
unlike	O
the	O
original	O
paper	O
,	O
or	O
different	O
way	O
to	O
getting	O
the	O
output	O
logits	O
.	O

We	O
see	O
improvement	O
as	O
we	O
reduced	O
number	O
of	O
layers	O
to	O
1(≈38	B-MetricValue
%	I-MetricValue
)	O
and	O
to	O
0(≈73	B-MetricValue
%	I-MetricValue
)	O
.	O

Hence	O
,	O
we	O
discarded	O
this	O
approach	O
.	O

The	O
Royal	O
College	O
of	O
Physicians	O
of	O
Edinburgh	O
warned	O
that	O
being	O
overweight	O
may	O
now	O
be	O
considered	O
"	O
the	O
norm	O
"	O
.	O

It	O
claimed	O
a	O
tax	O
would	O
help	O
fund	O
the	O
"	O
spiralling	O
"	O
healthcare	O
costs	O
associated	O
with	O
the	O
problem	O
.	O

The	O
British	O
Soft	O
Drinks	O
Association	O
(	O
BSDA	O
)	O
insisted	O
that	O
the	O
case	O
is	O
"	O
not	O
compelling	O
"	O
.	O

It	O
cited	O
research	O
which	O
suggested	O
a	O
20	O
%	O
tax	O
would	O
save	O
just	O
four	O
calories	O
per	O
day	O
.	O

Liverpool	O
University	O
chair	O
of	O
clinical	O
epidemiology	O
,	O
Simon	O
Capewell	O
,	O
is	O
due	O
to	O
speak	O
at	O
a	O
conference	O
on	O
the	O
issue	O
in	O
Edinburgh	O
later	O
,	O
entitled	O
:	O
"	O
Obesity	O
:	O
A	O
21st	O
Century	O
Epidemic	O
"	O
.	O

Professor	O
Capewell	O
will	O
cite	O
Mexico	O
as	O
one	O
example	O
where	O
a	O
10	O
%	O
sugary	O
drinks	O
tax	O
is	O
believed	O
to	O
have	O
contributed	O
to	O
a	O
10	O
%	O
reduction	O
in	O
the	O
consumption	O
of	O
such	O
beverages	O
while	O
Finland	O
,	O
France	O
,	O
Hungary	O
,	O
Latvia	O
and	O
the	O
USA	O
have	O
also	O
introduced	O
sugar	O
taxes	O
.	O

He	O
said	O
:	O
"	O
The	O
revenues	O
raised	O
can	O
then	O
be	O
invested	O
back	O
into	O
initiatives	O
to	O
increase	O
children	O
'	O
s	O
health	O
in	O
these	O
countries	O
,	O
as	O
is	O
happening	O
in	O
Mexico	O
.	O
"	O

Scotland	O
has	O
an	O
excellent	O
track	O
record	O
in	O
addressing	O
public	O
health	O
issues	O
.	O

Notable	O
achievements	O
include	O
smoke	O
-free	O
public	O
places	O
and	O
proposals	O
for	O
minimum	O
unit	O
pricing	O
for	O
alcohol	O
.	O

We	O
need	O
to	O
explore	O
how	O
these	O
developments	O
could	O
be	O
repeated	O
with	O
sugary	O
drinks	O
.	O
"	O

Gavin	O
Partington	O
,	O
BSDA	O
director	O
general	O
,	O
said	O
:	O
"	O
The	O
efforts	O
by	O
soft	O
drinks	O
companies	O
including	O
product	O
reformulation	O
,	O
smaller	O
pack	O
sizes	O
and	O
increased	O
promotion	O
of	O
low	O
and	O
no	O
-calorie	O
drinks	O
have	O
led	O
to	O
a	O
7	O
%	O
reduction	O
in	O
calories	O
from	O
soft	O
drinks	O
in	O
the	O
last	O
three	O
years	O
.	O
"	O

It	O
'	O
s	O
also	O
worth	O
noting	O
that	O
politicians	O
in	O
Belgium	O
and	O
Denmark	O
rejected	O
the	O
notion	O
of	O
a	O
tax	O
in	O
2013	O
and	O
the	O
experience	O
in	O
France	O
shows	O
that	O
while	O
sales	O
of	O
soft	O
drinks	O
initially	O
fell	O
after	O
a	O
tax	O
was	O
introduced	O
in	O
2012	O
,	O
they	O
have	O
increased	O
since	O
.	O
"	O

Doctors	O
have	O
called	O
for	O
the	O
introduction	O
of	O
a	O
tax	O
on	O
sugary	O
[	O
MASK	O
]	O
and	O
drinks	O
to	O
tackle	O
what	O
they	O
describe	O
as	O
an	O
"	O
obesity	O
epidemic	O
"	O
.Options	O
:	O
Food	O
,	O
Terms	O
,	O
Head	O
,	O
Unit	O
,	O
Snacks	O
We	O
use	O
the	O
method	O
of	O
Integrated	O
Gradients	O
(	O
Sundararajan	O
et	O
al	O
.	O
,	O

2017	O
)	O
.	O

We	O
follow	O
Ramnath	O
et	O
al	O
.	O
(	O

2020	O
)	O
to	O
compute	O
the	O
word	O
-	O
wise	O
attribution	O
scores	O
for	O
BERT	B-MethodName
FitB	I-MethodName
for	O
both	O
subtasks	O
.	O

We	O
compute	O
the	O
Integrated	B-MethodName
Gradients	I-MethodName
of	O
the	O
target	O
with	O
respect	O
to	O
the	O
embedding	O
outputs	O
.	O

The	O
Riemann	B-MethodName
Right	I-MethodName
Approximation	I-MethodName
Method	I-MethodName
with	O
n	B-HyperparameterName
steps	I-HyperparameterName
=	O
25	B-HyperparameterValue
is	O
used	O
.	O

After	O
obtaining	O
the	O
token	O
-	O
wise	O
attribution	O
scores	O
,	O
we	O
obtain	O
the	O
word	O
-	O
wise	O
attribution	O
scores	O
by	O
using	O
token	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
word	I-MethodName
offset	I-MethodName
mapping	I-MethodName
.	O

We	O
pick	O
the	O
top-10	O
word	O
-	O
wise	O
attribution	O
scores	O
and	O
normalise	O
them	O
.	O

To	O
implement	O
IG	O
,	O
we	O
use	O
the	O
Captum	O
(	O
Kokhlikyan	O
et	O
al	O
.	O
,	O

2020	O
)	O
library	O
.	O

In	O
favour	O
of	O
brevity	O
,	O
we	O
present	O
one	O
example	O
for	O
each	O
subtask	O
.	O

In	O
Fig	O
.	O

3	O
,	O
the	O
correct	O
answer	O
is	O
"	O
legislative	O
"	O
.	O

The	O
attribution	O
scores	O
of	O
words	O
like	O
senate	O
,	O
senators	O
,	O
municipal	O
and	O
President	O
are	O
high	O
,	O
as	O
is	O
demonstrated	O
by	O
the	O
intensity	O
of	O
the	O
colour	O
.	O

The	O
word	O
"	O
legislative	O
"	O
is	O
,	O
in	O
a	O
sense	O
,	O
more	O
imperceptible	O
than	O
any	O
of	O
the	O
words	O
mentioned	O
above	O
.	O

The	O
senate	O
is	O
the	O
legislative	O
branch	O
of	O
the	O
government	O
,	O
and	O
senators	O
are	O
its	O
members	O
;	O
municipal	O
refers	O
to	O
municipal	O
corporations	O
which	O
are	O
the	O
grassroots	O
governing	O
bodies	O
,	O
etc	O
.	O

Moreover	O
,	O
other	O
words	O
such	O
as	O
elections	O
,	O
political	O
,	O
country	O
also	O
have	O
high	O
attribution	O
scores	O
.	O

These	O
words	O
are	O
related	O
to	O
"	O
legislative	O
"	O
which	O
exhibits	O
the	O
fact	O
that	O
BERT	B-MethodName
FitB	I-MethodName
is	O
not	O
only	O
able	O
to	O
learn	O
the	O
concept	O
of	O
imperceptibility	B-TaskName
,	O
but	O
is	O
also	O
able	O
to	O
predict	O
a	O
suitable	O
word	O
.	O

Similarly	O
,	O
in	O
Fig	O
.	O

4	O
,	O
the	O
correct	O
answer	O
is	O
"	O
food	O
"	O
.	O

Note	O
that	O
"	O
snacks	O
"	O
is	O
also	O
an	O
option	O
;	O
however	O
,	O
food	O
is	O
more	O
non	B-TaskName
-	I-TaskName
specific	I-TaskName
than	O
"	O
snacks	O
"	O
and	O
hence	O
,	O
food	O
is	O
the	O
correct	O
option	O
.	O

Another	O
interesting	O
thing	O
to	O
note	O
is	O
the	O
high	O
attribution	O
scores	O
for	O
words	O
/	O
phrases	O
like	O
calories	O
,	O
beverages	O
,	O
sugar	O
and	O
sugary	O
drinks	O
.	O

This	O
backs	O
the	O
fact	O
that	O
the	O
model	O
is	O
able	O
to	O
learn	O
the	O
concept	O
of	O
non	O
-	O
specificity	O
,	O
i.e.	O
,	O
the	O
above	O
mentioned	O
words	O
are	O
essentially	O
hyponyms	O
of	O
"	O
food	O
"	O
.	O

We	O
tried	O
out	O
myriad	O
approaches	O
,	O
taking	O
care	O
to	O
not	O
only	O
focus	O
on	O
the	O
architecture	O
aspect	O
,	O
but	O
also	O
how	O
we	O
can	O
quantify	O
imperceptibility	O
and	O
nonspecificity	O
.	O

Although	O
we	O
did	O
not	O
achieve	O
favourable	O
improvements	O
in	O
all	O
approaches	O
,	O
we	O
did	O
observe	O
gains	O
in	O
accuracy	O
on	O
the	O
dev	O
set	O
.	O

We	O
reckon	O
that	O
with	O
more	O
careful	O
tuning	O
of	O
parameters	O
such	O
as	O
the	O
threshold	O
in	O
the	O
Difference	B-MethodName
Method	I-MethodName
,	O
we	O
will	O
be	O
able	O
to	O
achieve	O
these	O
gains	O
on	O
the	O
test	O
set	O
.	O

We	O
further	O
interpreted	O
the	O
outputs	O
of	O
transformers	O
-	O
based	O
models	O
using	O
Integrated	B-MethodName
Gradients	I-MethodName
,	O
and	O
demonstrated	O
that	O
transformer	O
models	O
are	O
able	O
to	O
learn	O
the	O
concepts	O
of	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

In	O
the	O
future	O
,	O
we	O
intend	O
to	O
solidify	O
our	O
proposed	O
approaches	O
and	O
carry	O
out	O
further	O
research	O
in	O
this	O
interesting	O
field	O
.	O

We	O
thank	O
Rajaswa	O
Patil	O
1	O
and	O
Somesh	O
Singh	O
2	O
for	O
their	O
support	O
.	O

We	O
would	O
also	O
like	O
to	O
express	O
our	O
gratitude	O
to	O
our	O
colleagues	O
at	O
the	O
Language	O
Research	O
Group	O
(	O
LRG	O
)	O
3	O
,	O
who	O
have	O
been	O
with	O
us	O
at	O
every	O
stepping	O
stone	O
.	O

When	O
evaluating	O
an	O
article	O
and	O
the	O
claims	O
it	O
makes	O
,	O
a	O
critical	O
reader	O
must	O
be	O
able	O
to	O
assess	O
where	O
the	O
information	O
presented	O
comes	O
from	O
,	O
and	O
whether	O
the	O
various	O
claims	O
are	O
mutually	O
consistent	O
and	O
support	O
the	O
conclusion	O
.	O

This	O
motivates	O
the	O
study	O
of	O
claim	B-TaskName
provenance	I-TaskName
,	O
which	O
seeks	O
to	O
trace	O
and	O
explain	O
the	O
origins	O
of	O
claims	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
new	O
techniques	O
to	O
model	O
and	O
reason	O
about	O
the	O
provenance	O
of	O
multiple	O
interacting	O
claims	O
,	O
including	O
how	O
to	O
capture	O
fine	O
-	O
grained	O
information	O
about	O
the	O
context	O
.	O

Our	O
solution	O
hinges	O
on	O
first	O
identifying	O
the	O
sentences	O
that	O
potentially	O
contain	O
important	O
external	O
information	O
.	O

We	O
then	O
develop	O
a	O
query	O
generator	O
with	O
our	O
novel	O
rank	O
-	O
aware	O
cross	O
attention	O
mechanism	O
,	O
which	O
aims	O
at	O
generating	O
metadata	O
for	O
the	O
source	O
article	O
,	O
based	O
on	O
the	O
context	O
and	O
signals	O
collected	O
from	O
a	O
search	O
engine	O
.	O

This	O
establishes	O
relevant	O
search	O
queries	O
,	O
and	O
it	O
allows	O
us	O
to	O
obtain	O
source	O
article	O
candidates	O
for	O
each	O
identified	O
sentence	O
and	O
propose	O
an	O
ILP	B-MethodName
based	I-MethodName
algorithm	I-MethodName
to	O
infer	O
the	O
best	O
sources	O
.	O

We	O
experiment	O
with	O
a	O
newly	O
created	O
evaluation	O
dataset	O
1	O
,	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
,	O
based	O
on	O
fact	O
-	O
checking	O
articles	O
from	O
www.politifa	O
ct.com	O
;	O
our	O
experimental	O
results	O
show	O
that	O
our	O
solution	O
leads	O
to	O
a	O
significant	O
improvement	O
over	O
baselines	O
.	O

Misinformation	O
is	O
on	O
the	O
rise	O
,	O
and	O
people	O
are	O
fighting	O
it	O
with	O
fact	O
checking	O
.	O

However	O
,	O
most	O
of	O
the	O
work	O
in	O
the	O
current	O
literature	O
(	O
Thorne	O
et	O
al	O
.	O
,	O

2018;Zhang	O
et	O
al	O
.	O
,	O

2019;Barrón	O
-	O
Cedeno	O
et	O
al	O
.	O
,	O

2020;Hidey	O
et	O
al	O
.	O
,	O

2020	O
)	O
focuses	O
on	O
automating	O
factchecking	O
for	O
a	O
single	O
claim	O
.	O

In	O
reality	O
,	O
a	O
claim	O
can	O
be	O
complex	O
,	O
and	O
proposed	O
as	O
a	O
conclusion	O
of	O
an	O
article	O
.	O

Therefore	O
,	O
understanding	O
what	O
information	O
supports	O
the	O
article	O
,	O
especially	O
information	O
that	O
was	O
not	O
originated	O
within	O
the	O
same	O
article	O
,	O
and	O
where	O
it	O
originates	O
from	O
,	O
are	O
very	O
important	O
for	O
readers	O
who	O
want	O
to	O
determine	O
whether	O
they	O
can	O
believe	O
the	O
claim	O
.	O

Figure	O
1	O
shows	O
an	O
example	O
of	O
such	O
a	O
claim	O
,	O
"	O
Marco	O
Rubio	O
says	O
Anthony	O
Fauci	O
lies	O
about	O
masks	O
.	O

Fauci	O
did	O
n't	O
.	O
"	O

2	O
with	O
its	O
article	O
from	O
politifact.com	O
.	O

A	O
critical	O
reader	O
of	O
the	O
content	O
will	O
find	O
that	O
several	O
major	O
sources	O
support	O
the	O
author	O
's	O
claim	O
:	O
Source	O
article	O
1	O
in	O
the	O
figure	O
is	O
CBS	O
News,"60	O
Minutes	O
"	O
interview	O
with	O
Anthony	O
Fauci	O
,	O
on	O
March	O
8	O
,	O
2020	O
,	O
which	O
reveals	O
that	O
Dr.	O
Fauci	O
's	O
main	O
point	O
was	O
to	O
preserve	O
masks	O
for	O
those	O
who	O
were	O
already	O
ill	O
and	O
people	O
providing	O
care	O
.	O

If	O
readers	O
can	O
validate	O
all	O
sources	O
used	O
in	O
the	O
article	O
,	O
they	O
will	O
be	O
able	O
to	O
determine	O
whether	O
the	O
article	O
is	O
trustworthy	O
.	O

In	O
this	O
paper	O
,	O
our	O
goal	O
is	O
to	O
automatically	O
find	O
these	O
sources	O
for	O
a	O
given	O
article	O
.	O

This	O
is	O
a	O
different	O
problem	O
from	O
fact	O
-	O
checking	O
:	O
Factchecking	O
seeks	O
evidence	O
for	O
a	O
claim	O
,	O
while	O
here	O
we	O
only	O
care	O
about	O
the	O
information	O
sources	O
the	O
authors	O
used	O
when	O
they	O
were	O
writing	O
.	O

Furthermore	O
,	O
the	O
problem	O
we	O
address	O
is	O
critical	O
also	O
to	O
authors	O
who	O
want	O
to	O
give	O
credit	O
to	O
those	O
who	O
have	O
contributed	O
to	O
their	O
article	O
,	O
and	O
it	O
enables	O
a	O
recursive	O
analysis	O
that	O
can	O
trace	O
back	O
to	O
the	O
starting	O
points	O
of	O
an	O
article	O
.	O

This	O
motivates	O
the	O
study	O
of	O
provenance	B-TaskName
for	I-TaskName
natural	I-TaskName
language	I-TaskName
claims	I-TaskName
,	O
which	O
describes	O
where	O
a	O
specific	O
claim	O
may	O
have	O
come	O
from	O
and	O
how	O
it	O
has	O
spread	O
.	O

Early	O
work	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
proposed	O
a	O
formulation	O
to	O
model	O
,	O
and	O
a	O
solution	O
to	O
infer	O
,	O
the	O
provenance	O
graph	O
for	O
the	O
given	O
claim	O
.	O

However	O
,	O
that	O
model	O
is	O
insufficient	O
to	O
capture	O
the	O
provenance	O
of	O
an	O
article	O
,	O
because	O
(	O
1	O
)	O
an	O
article	O
consists	O
of	O
multiple	O
claims	O
,	O
and	O
it	O
leverages	O
information	O
from	O
other	O
sources	O
,	O
therefore	O
the	O
provenance	O
of	O
all	O
claims	O
should	O
be	O
included	O
in	O
the	O
article	O
's	O
provenance	O
;	O
(	O
2	O
)	O
the	O
inference	O
solution	O
they	O
proposed	O
can	O
only	O
extract	O
domain	O
-	O
level	O
provenance	O
information	O
,	O
e.g.	O
,	O
cbsnews.com	O
,	O
while	O
it	O
can	O
not	O
directly	O
link	O
the	O
claim	O
to	O
its	O
source	O
article	O
,	O
e.g.	O
,	O
https://www.cbsnews.com/news/preventingcoronavirus-facemask-60-minutes-2020-03-08/.	O
Such	O
fine	O
-	O
grained	O
provenance	O
information	O
is	O
important	O
because	O
it	O
can	O
help	O
people	O
understand	O
the	O
original	O
context	O
that	O
influenced	O
the	O
information	O
they	O
read	O
.	O

Therefore	O
,	O
in	O
this	O
work	O
,	O
we	O
argue	O
that	O
the	O
notion	O
of	O
a	O
provenance	O
graph	O
should	O
be	O
extended	O
to	O
incorporate	O
provenance	O
for	O
articles	O
,	O
and	O
that	O
we	O
need	O
a	O
more	O
comprehensive	O
solution	O
that	O
can	O
identify	O
important	O
external	O
information	O
used	O
in	O
the	O
article	O
and	O
infer	O
its	O
corresponding	O
source	O
article	O
:	O
namely	O
,	O
its	O
fine	O
-	O
grained	O
provenance	O
information	O
.	O

Technically	O
,	O
capturing	O
fine	O
-	O
grained	O
provenance	O
for	O
an	O
article	O
is	O
challenging	O
because	O
(	O
1	O
)	O
there	O
may	O
be	O
large	O
numbers	O
of	O
sentences	O
in	O
an	O
article	O
,	O
and	O
not	O
all	O
are	O
from	O
external	O
sources	O
nor	O
important	O
(	O
thus	O
,	O
their	O
provenance	O
may	O
not	O
be	O
worth	O
considering);(2	O
)	O
a	O
sentence	O
in	O
an	O
article	O
is	O
usually	O
just	O
a	O
textual	O
fragment	O
of	O
its	O
source	O
article	O
,	O
and	O
simply	O
looking	O
for	O
other	O
articles	O
with	O
related	O
content	O
may	O
result	O
in	O
low	O
precision	B-MetricName
with	O
regards	O
to	O
finding	O
the	O
correct	O
original	O
article	O
.	O

In	O
our	O
running	O
example	O
,	O
sentence2	O
in	O
Figure	O
1	O
is	O
"	O
On	O
March	O
29	O
,	O
President	O
Donald	O
Trump	O
and	O
the	O
coronavirus	O
task	O
force	O
briefed	O
the	O
press	O
on	O
steps	O
underway	O
to	O
increase	O
...	O
"	O
,	O
whose	O
source	O
is	O
White	O
House	O
's	O
coronavirus	O
task	O
force	O
press	O
briefing	O
on	O
March	O
29	O
,	O
2020	O
.	O

If	O
we	O
directly	O
search	O
for	O
the	O
sentence	O
on	O
the	O
web	O
,	O
it	O
is	O
hard	O
to	O
find	O
this	O
among	O
popular	O
articles	O
from	O
the	O
news	O
.	O

Instead	O
,	O
we	O
need	O
a	O
model	O
that	O
can	O
generate	O
better	O
keywords	O
for	O
a	O
more	O
focused	O
search	O
.	O

The	O
key	O
contributions	O
of	O
this	O
paper	O
are	O
(	O
1	O
)	O
we	O
introduce	O
and	O
formalize	O
the	O
problem	O
of	O
inferring	B-TaskName
finegrained	I-TaskName
provenance	I-TaskName
for	O
an	O
article	O
;	O
(	O
2	O
)	O
we	O
propose	O
a	O
general	O
framework	O
to	O
infer	O
the	O
source	O
articles	O
that	O
have	O
provided	O
important	O
information	O
for	O
the	O
given	O
article	O
,	O
including	O
(	O
a	O
)	O
a	O
ranking	O
module	O
that	O
can	O
identify	O
sentences	O
that	O
contain	O
important	O
external	O
information	O
based	O
on	O
the	O
main	O
topic	O
and	O
the	O
main	O
entities	O
in	O
the	O
article	O
;	O
(	O
b	O
)	O
a	O
query	O
generator	O
that	O
can	O
generate	O
possible	O
metadata	O
for	O
the	O
source	O
article	O
,	O
e.g.	O
,	O
the	O
title	O
,	O
the	O
published	O
date	O
,	O
the	O
source	O
website	O
,	O
based	O
on	O
the	O
context	O
of	O
the	O
selected	O
sentences	O
;	O
(	O
c	O
)	O
an	O
integer	O
linear	O
program	O
(	O
ILP	O
)	O
based	O
algorithm	O
to	O
jointly	O
identify	O
the	O
source	O
articles	O
from	O
all	O
of	O
the	O
candidates	O
.	O
(	O

3	O
)	O
to	O
evaluate	O
our	O
solutions	O
,	O
we	O
collect	O
a	O
new	O
dataset	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
from	O
politifact.com	O
,	O
and	O
our	O
experimental	O
results	O
show	O
that	O
the	O
solution	O
we	O
proposed	O
can	O
lead	O
to	O
a	O
significant	O
improvement	O
compared	O
with	O
baselines	O
.	O

Given	O
an	O
article	O
d	O
,	O
we	O
are	O
to	O
capture	O
its	O
finegrained	B-TaskName
provenance	I-TaskName
,	O
by	O
inferring	O
k	O
source	O
articles	O
SA	O
k	O
(	O
d	O
)	O
that	O
provide	O
the	O
most	O
important	O
information	O
for	O
d.	O
We	O
adopt	O
the	O
notion	O
of	O
provenance	O
from	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
while	O
in	O
this	O
paper	O
,	O
we	O
focus	O
on	O
inferring	O
provenance	O
for	O
a	O
claim	O
based	O
on	O
the	O
information	O
from	O
the	O
given	O
article	O
.	O

To	O
find	O
SA	O
k	O
(	O
d	O
)	O
,	O
there	O
are	O
three	O
subproblems	O
we	O
need	O
to	O
solve	O
.	O

First	O
,	O
we	O
need	O
to	O
locate	O
the	O
important	O
external	O
information	O
in	O
d	O
,	O
which	O
means	O
we	O
need	O
a	O
sentence	O
ranking	O
module	O
that	O
can	O
estimate	O
a	O
score	O
σ	O
i	O
for	O
each	O
sentence	O
in	O
d	O
=	O
{	O
s	O
i	O
}	O
n	O
i=1	O
,	O
based	O
on	O
how	O
likely	O
s	O
i	O
contains	O
external	O
information	O
.	O

Then	O
we	O
will	O
choose	O
top	O
-	O
k	O
sentences	O
based	O
on	O
their	O
score	O
,	O
and	O
try	O
to	O
find	O
source	O
articles	O
for	O
those	O
sentences	O
.	O

Second	O
,	O
for	O
each	O
selected	O
sentence	O
,	O
we	O
need	O
to	O
generate	O
a	O
list	O
of	O
candidate	O
links	O
,	O
which	O
can	O
be	O
its	O
source	O
articles	O
.	O

To	O
achieve	O
this	O
goal	O
,	O
we	O
take	O
advantage	O
of	O
a	O
search	O
engine	O
,	O
based	O
on	O
which	O
we	O
can	O
access	O
all	O
of	O
the	O
articles	O
on	O
the	O
web	O
.	O

As	O
we	O
have	O
discussed	O
in	O
Section	O
1	O
,	O
directly	O
searching	O
the	O
identified	O
sentence	O
on	O
a	O
search	O
engine	O
may	O
result	O
in	O
a	O
low	O
precision	B-MetricName
of	O
finding	O
the	O
correct	O
source	O
article	O
.	O

Therefore	O
,	O
we	O
propose	O
to	O
develop	O
a	O
query	O
generator	O
to	O
generate	O
the	O
possible	O
metadata	O
of	O
the	O
target	O
source	O
article	O
as	O
new	O
search	O
keywords	O
,	O
so	O
that	O
the	O
search	O
engine	O
is	O
more	O
likely	O
to	O
recall	O
source	O
articles	O
.	O

We	O
then	O
collect	O
all	O
of	O
the	O
search	O
results	O
as	O
the	O
candidates	O
for	O
a	O
selected	O
sentence	O
.	O

Finally	O
,	O
we	O
need	O
to	O
infer	O
the	O
correct	O
source	O
article	O
from	O
the	O
candidates	O
,	O
for	O
each	O
identified	O
sentence	O
.	O

Figure	O
2	O
depicts	O
the	O
three	O
steps	O
we	O
need	O
to	O
conduct	O
to	O
infer	O
the	O
fine	B-TaskName
-	I-TaskName
grained	I-TaskName
provenance	I-TaskName
,	O
which	O
correspond	O
to	O
the	O
three	O
subproblems	O
listed	O
above	O
.	O

We	O
will	O
elaborate	O
the	O
details	O
of	O
each	O
step	O
in	O
Section	O
4	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
is	O
no	O
existing	O
dataset	O
that	O
can	O
support	O
inferring	O
fine	O
-	O
grained	O
provenance	O
for	O
an	O
article	O
,	O
therefore	O
we	O
create	O
a	O
new	O
dataset	O
based	O
on	O
the	O
fact	O
-	O
checks	O
from	O
politifact	O
.com	O
to	O
support	O
the	O
training	O
and	O
the	O
evaluation	O
of	O
this	O
problem	O
.	O

Specifically	O
,	O
we	O
crawled	O
all	O
of	O
the	O
fact	O
-	O
check	O
questions	O
from	O
politifact.com	O
on	O
4	O
different	O
issues	O
:	O
Coronavirus	O
,	O
Health	O
Care	O
,	O
Immigration	O
,	O
Taxes	O
in	O
September	O
,	O
2020	O
.	O

For	O
each	O
question	O
,	O
we	O
further	O
crawled	O
its	O
webpage	O
to	O
obtain	O
(	O
1	O
)	O
the	O
title	O
,	O
which	O
is	O
actually	O
the	O
fact	O
-	O
check	O
question	O
itself	O
,	O
(	O
2	O
)	O
the	O
sections	O
of	O
the	O
main	O
text	O
and	O
(	O
3	O
)	O
the	O
"	O
Our	O
Sources	O
"	O
section	O
listing	O
all	O
of	O
the	O
articles	O
(	O
including	O
urls	O
)	O
that	O
provide	O
important	O
information	O
mentioned	O
in	O
the	O
fact	O
-	O
check	O
article	O
.	O

Figure	O
3	O
shows	O
an	O
example	O
of	O
such	O
a	O
section	O
.	O

Furthermore	O
,	O
we	O
extract	O
all	O
of	O
the	O
hyperlinks	O
in	O
the	O
webpage	O
,	O
which	O
can	O
tell	O
us	O
where	O
the	O
source	O
articles	O
are	O
mentioned	O
in	O
the	O
main	O
text	O
.	O

To	O
sum	O
up	O
,	O
we	O
use	O
the	O
main	O
text	O
of	O
each	O
webpage	O
as	O
the	O
given	O
article	O
,	O
and	O
the	O
source	O
articles	O
listed	O
in	O
the	O
section	O
of	O
"	O
Our	O
Sources	O
"	O
as	O
the	O
ground	O
truth	O
our	O
system	O
wants	O
to	O
return	O
.	O

We	O
want	O
to	O
note	O
it	O
is	O
possible	O
that	O
there	O
may	O
be	O
some	O
sources	O
missing	O
in	O
the	O
ground	O
truth	O
we	O
can	O
obtain	O
,	O
therefore	O
,	O
we	O
focus	O
more	O
on	O
the	O
recall	O
in	O
the	O
evaluation	O
.	O

Overall	O
,	O
we	O
collected	O
data	O
from	O
1765	O
articles	O
,	O
where	O
we	O
use	O
883	O
of	O
them	O
for	O
training	O
,	O
and	O
441	O
and	O
441	O
for	O
validation	O
and	O
testing	O
respectively	O
.	O

On	O
average	O
,	O
each	O
article	O
has	O
9.8	O
source	O
articles	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
elaborate	O
how	O
we	O
solve	O
the	O
problems	O
proposed	O
in	O
Section	O
2	O
.	O

Given	O
an	O
article	O
,	O
the	O
first	O
step	O
is	O
to	O
identify	O
the	O
sentences	O
that	O
are	O
most	O
likely	O
to	O
contain	O
important	O
external	O
information	O
.	O

To	O
develop	O
a	O
general	O
data	O
-	O
driven	O
solution	O
,	O
rather	O
than	O
design	O
a	O
ranking	O
function	O
by	O
domain	O
-	O
specific	O
feature	O
engineering	O
,	O
we	O
take	O
advantage	O
of	O
the	O
hyperlinks	O
inserted	O
in	O
the	O
article	O
,	O
so	O
that	O
we	O
can	O
find	O
where	O
the	O
source	O
articles	O
are	O
mentioned	O
.	O

The	O
hyperlink	O
is	O
helpful	O
here	O
because	O
it	O
is	O
standard	O
for	O
the	O
author	O
to	O
provide	O
external	O
information	O
on	O
related	O
topics	O
to	O
the	O
reader	O
.	O

If	O
the	O
hyperlink	O
refers	O
one	O
at	O
the	O
listed	O
source	O
articles	O
,	O
it	O
means	O
the	O
sentence	O
is	O
the	O
one	O
that	O
we	O
are	O
looking	O
for	O
.	O

Then	O
our	O
problem	O
is	O
to	O
learn	O
a	O
model	O
that	O
can	O
distinguish	O
those	O
sentences	O
from	O
the	O
regular	O
ones	O
in	O
the	O
article	O
.	O

Specifically	O
,	O
we	O
first	O
extract	O
all	O
of	O
the	O
hyperlinks	O
with	O
their	O
corresponding	O
sentences	O
in	O
the	O
given	O
article	O
d	O
,	O
and	O
denote	O
the	O
output	O
as	O
Hp(d	O
)	O
=	O
{	O
(	O
l	O
,	O
s)|s	O
∈	O
d	O
}	O
,	O
where	O
l	O
represents	O
the	O
link	O
of	O
the	O
article	O
and	O
s	O
represents	O
the	O
sentence	O
.	O

Then	O
,	O
we	O
create	O
a	O
list	O
of	O
positive	O
sentences	O
for	O
d	O
denoted	O
as	O
P	O
(	O
d	O
)	O
by	O
finding	O
the	O
intersection	O
between	O
the	O
articles	O
in	O
Hp(d	O
)	O
and	O
those	O
inSA	O
k	O
(	O
d	O
)	O
,	O
i.e.	O
,	O
P	O
(	O
d	O
)	O
=	O
{	O
s|s	O
∈	O
d	O
,	O
∃(l	O
,	O
s	O
)	O
∈	O
Hp(d	O
)	O
,	O
s.t	O
.	O
,	O

l	O
∈	O
SA	O
k	O
(	O
d)}.Meanwhile	O
,	O
we	O
create	O
a	O
list	O
of	O
negative	O
sentences	O
for	O
d	O
by	O
randomly	O
sampling	O
from	O
the	O
rest	O
of	O
its	O
sentences	O
,	O
denoted	O
as	O
N	O
(	O
d	O
)	O
.	O

When	O
a	O
new	O
article	O
is	O
given	O
,	O
the	O
job	O
of	O
the	O
model	O
turns	O
out	O
to	O
estimate	O
a	O
score	O
σ	O
i	O
of	O
how	O
likely	O
each	O
sentence	O
s	O
i	O
in	O
d	O
refers	O
to	O
important	O
external	O
information	O
.	O

Since	O
the	O
sentences	O
referring	O
to	O
important	O
external	O
information	O
are	O
always	O
either	O
directly	O
related	O
to	O
the	O
main	O
topic	O
or	O
about	O
the	O
main	O
entities	O
mentioned	O
in	O
the	O
article	O
,	O
we	O
will	O
leverage	O
them	O
to	O
build	O
our	O
model	O
.	O

Denote	O
the	O
title	O
of	O
d	O
as	O
t	O
d	O
,	O
and	O
the	O
most	O
important	O
entities	O
mentioned	O
in	O
the	O
article	O
as	O
E	O
d	O
.	O

Here	O
,	O
we	O
simply	O
use	O
tf	O
-	O
idf	O
to	O
determine	O
the	O
importance	O
of	O
an	O
entity	O
to	O
an	O
article	O
.	O

We	O
build	O
our	O
model	O
by	O
leveraging	O
Roberta	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Using	O
the	O
same	O
notation	O
in	O
the	O
paper	O
,	O
we	O
concatenate	O
t	O
d	O
and	O
each	O
e	O
∈	O
E	O
d	O
,	O
feeding	O
it	O
to	O
the	O
model	O
as	O
sentence	O
A	O
,	O
and	O
s	O
∈	O
P	O
(	O
d	O
)	O
or	O
N	O
(	O
d	O
)	O
as	O
sentence	O
B	O
,	O
as	O
the	O
input	O
of	O
Roberta	B-MethodName
.	O

We	O
then	O
use	O
Roberta	B-MethodName
as	O
a	O
binary	O
classification	O
model	O
,	O
that	O
is	O
,	O
we	O
use	O
its	O
[	O
CLS	O
]	O
vector	O
as	O
input	O
to	O
a	O
two	B-HyperparameterValue
layer	B-HyperparameterName
neural	O
network	O
to	O
obtain	O
the	O
probability	O
of	O
s	O
referring	O
to	O
important	O
external	O
information	O
.	O

Instead	O
of	O
learning	O
the	O
features	O
independently	O
for	O
each	O
example	O
,	O
we	O
want	O
to	O
help	O
the	O
model	O
better	O
capture	O
the	O
discriminative	O
feature	O
between	O
the	O
positive	O
and	O
negative	O
examples	O
.	O

Therefore	O
,	O
we	O
add	O
a	O
margin	O
ranking	O
loss	O
to	O
the	O
learning	O
objective	O
,	O
so	O
that	O
it	O
can	O
enforce	O
the	O
model	O
to	O
distinguish	O
the	O
representations	O
between	O
positive	O
and	O
negative	O
examples	O
.	O

We	O
start	O
training	O
from	O
a	O
pre	O
-	O
trained	O
Roberta	B-MethodName
model	O
and	O
fine	O
-	O
tune	O
it	O
to	O
our	O
ranking	O
task	O
using	O
the	O
following	O
loss	O
,	O
given	O
s	O
i	O
∈	O
P	O
(	O
d	O
)	O
and	O
s	O
j	O
∈	O
N	O
(	O
d):Li	O
,	O
j	O
=	O
−	O
log	O
σi	O
−	O
log	O
(	O
1	O
−	O
σj	O
)	O
+	O
max	O
0	O
,	O
τ	O
(	O
sj	O
)	O
−	O
τ	O
(	O
si	O
)	O
+	O
(	O
1)where	O
τ	O
(	O
s	O
i	O
)	O
and	O
τ	O
(	O
s	O
j	O
)	O
are	O
the	O
representations	O
,	O
obtained	O
by	O
the	O
output	O
of	O
a	O
single	B-HyperparameterValue
layer	B-HyperparameterName
neural	O
network	O
τ	O
on	O
top	O
of	O
the	O
[	O
CLS	O
]	O
vector	O
of	O
Roberta	B-MethodName
.	O

Identifying	O
the	O
sentences	O
that	O
are	O
describing	O
external	O
information	O
provides	O
us	O
with	O
a	O
clue	O
to	O
finding	O
the	O
source	O
articles	O
.	O

The	O
next	O
step	O
is	O
to	O
find	O
candidate	O
articles	O
that	O
can	O
be	O
the	O
source	O
articles	O
based	O
on	O
the	O
identified	O
sentences	O
.	O

However	O
,	O
as	O
we	O
have	O
described	O
in	O
Section	O
1	O
,	O
it	O
is	O
hard	O
to	O
find	O
the	O
source	O
article	O
by	O
directly	O
searching	O
the	O
sentence	O
on	O
the	O
web	O
,	O
since	O
so	O
many	O
articles	O
may	O
be	O
talking	O
about	O
the	O
related	O
information	O
.	O

Therefore	O
,	O
we	O
argue	O
that	O
besides	O
using	O
the	O
sentence	O
as	O
the	O
query	O
,	O
we	O
need	O
a	O
query	O
generator	O
that	O
can	O
generate	O
a	O
better	O
query	O
for	O
searching	O
,	O
so	O
that	O
it	O
can	O
increase	O
the	O
possibility	O
that	O
we	O
can	O
recall	O
the	O
correct	O
source	O
article	O
.	O

To	O
generate	O
a	O
query	O
that	O
can	O
improve	O
the	O
recall	B-MetricName
,	O
the	O
question	O
here	O
is	O
what	O
search	O
keywords	O
are	O
good	O
for	O
finding	O
the	O
source	O
articles	O
besides	O
the	O
identified	O
sentences	O
themselves	O
?	O
In	O
this	O
work	O
,	O
we	O
argue	O
that	O
the	O
metadata	O
of	O
the	O
target	O
article	O
,	O
including	O
its	O
source	O
domain	O
,	O
title	O
and	O
published	O
date	O
is	O
a	O
good	O
choice	O
.	O

Since	O
most	O
of	O
those	O
information	O
may	O
be	O
revealed	O
in	O
the	O
sentence	O
or	O
its	O
context	O
,	O
it	O
is	O
possible	O
that	O
we	O
train	O
a	O
model	O
where	O
we	O
can	O
feed	O
the	O
context	O
of	O
the	O
sentence	O
,	O
and	O
generate	O
a	O
combination	O
of	O
the	O
possible	O
source	O
domain	O
,	O
title	O
and	O
published	O
date	O
of	O
the	O
article	O
it	O
refers	O
to	O
.	O

In	O
our	O
running	O
example	O
in	O
Figure	O
1	O
,	O
the	O
sentence	O
identified	O
(	O
sentence	O
2	O
in	O
the	O
figure	O
)	O
is	O
"	O
...	O
On	O
March	O
29	O
,	O
President	O
...	O
"	O
.	O

The	O
source	O
domain	O
of	O
the	O
article	O
it	O
refers	O
to	O
(	O
source	O
article	O
2	O
in	O
the	O
figure	O
)	O
is	O
white	O
house	O
,	O
the	O
title	O
of	O
the	O
article	O
is	O
coronavirus	O
task	O
force	O
press	O
briefing	O
,	O
and	O
the	O
published	O
date	O
is	O
March	O
29	O
,	O
2020	O
.	O

It	O
is	O
obvious	O
that	O
most	O
of	O
those	O
information	O
has	O
been	O
somehow	O
mentioned	O
in	O
the	O
context	O
or	O
at	O
least	O
can	O
be	O
very	O
easily	O
associated	O
with	O
.	O

Therefore	O
,	O
we	O
treat	O
this	O
problem	O
as	O
a	O
text	O
generation	O
problem	O
,	O
where	O
we	O
feed	O
the	O
identified	O
sentence	O
with	O
its	O
context	O
,	O
and	O
try	O
to	O
generate	O
its	O
metadata	O
.	O

As	O
a	O
baseline	O
,	O
we	O
train	O
this	O
model	O
via	O
fine	O
-	O
tuning	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
a	O
pretrained	O
text	O
generation	O
model	O
.	O

Besides	O
the	O
metadata	O
to	O
generate	O
,	O
the	O
content	O
of	O
the	O
identified	O
sentence	O
itself	O
should	O
be	O
useful	O
for	O
searching	O
,	O
when	O
there	O
is	O
an	O
overlap	O
between	O
the	O
sentence	O
and	O
the	O
content	O
of	O
the	O
target	O
article	O
.	O

In	O
this	O
case	O
,	O
if	O
we	O
search	O
for	O
the	O
identified	O
sentence	O
on	O
a	O
search	O
engine	O
,	O
the	O
results	O
returned	O
can	O
be	O
related	O
articles	O
,	O
and	O
their	O
metadata	O
may	O
provide	O
additional	O
useful	O
information	O
that	O
can	O
tell	O
the	O
model	O
what	O
should	O
be	O
included	O
in	O
the	O
target	O
output	O
.	O

In	O
our	O
running	O
example	O
mentioned	O
in	O
the	O
last	O
section	O
,	O
if	O
we	O
search	O
that	O
sentence	O
on	O
Google	O
,	O
one	O
result	O
it	O
returned	O
is	O
cspan	O
's	O
article	O
"	O
President	O
Trump	O
with	O
Coronavirus	O
Task	O
Press	O
Briefing	O
"	O
,	O
which	O
has	O
been	O
very	O
close	O
to	O
the	O
title	O
of	O
the	O
target	O
article	O
.	O

Therefore	O
,	O
our	O
generation	O
model	O
should	O
leverage	O
those	O
signals	O
,	O
which	O
consist	O
of	O
metadata	O
of	O
related	O
articles	O
to	O
the	O
target	O
article	O
.	O

To	O
incorporate	O
the	O
signals	O
,	O
we	O
first	O
issue	O
the	O
identified	O
sentence	O
as	O
a	O
query	O
to	O
the	O
search	O
engine	O
and	O
collect	O
its	O
top-5	O
returned	O
urls	O
.	O

Then	O
,	O
as	O
what	O
we	O
do	O
to	O
the	O
identified	O
sentence	O
,	O
we	O
crawl	O
its	O
metadata	O
,	O
i.e.	O
,	O
the	O
source	O
domain	O
,	O
title	O
,	O
and	O
published	O
date	O
,	O
and	O
put	O
them	O
together	O
as	O
one	O
document	O
.	O

Then	O
,	O
our	O
problem	O
becomes	O
to	O
generating	O
the	O
metadata	O
of	O
the	O
source	O
article	O
,	O
when	O
we	O
are	O
given	O
the	O
identified	O
sentence	O
,	O
its	O
context	O
,	O
and	O
a	O
concatenation	O
of	O
possible	O
metadata	O
outputs	O
.	O

In	O
this	O
case	O
,	O
we	O
actually	O
have	O
two	O
types	O
of	O
inputs	O
for	O
the	O
model	O
.	O

One	O
is	O
the	O
identified	O
sentence	O
with	O
its	O
context	O
,	O
where	O
we	O
are	O
to	O
infer	O
the	O
metadata	O
from	O
,	O
and	O
the	O
other	O
one	O
is	O
the	O
concatenation	O
of	O
possible	O
outputs	O
,	O
where	O
we	O
want	O
to	O
extract	O
the	O
correct	O
metadata	O
components	O
directly	O
from	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
extend	O
the	O
BART	B-MethodName
baseline	O
to	O
incorporate	O
two	O
sources	O
of	O
inputs	O
,	O
by	O
first	O
feeding	O
the	O
text	O
inputs	O
independently	O
to	O
the	O
BART	B-MethodName
's	O
encoders	O
,	O
then	O
concatenating	O
the	O
outputs	O
of	O
the	O
encoders	O
together	O
,	O
and	O
finally	O
feeding	O
the	O
unified	O
representations	O
to	O
the	O
BART	B-MethodName
's	O
decoder	O
.	O

We	O
collect	O
multiple	O
possible	O
metadata	O
for	O
each	O
source	O
article	O
,	O
so	O
that	O
the	O
integration	O
can	O
help	O
us	O
generate	O
better	O
keywords	O
for	O
the	O
search	O
.	O

However	O
,	O
treating	O
the	O
multiple	O
possible	O
metadata	O
as	O
a	O
single	O
document	O
neglects	O
the	O
rank	O
of	O
the	O
urls	O
returned	O
,	O
which	O
reflects	O
the	O
different	O
possibility	O
for	O
each	O
candidate	O
to	O
be	O
the	O
right	O
metadata	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
rank	O
-	O
aware	O
multi	O
-	O
head	O
cross	O
-	O
attention	O
to	O
relieve	O
this	O
problem	O
.	O

The	O
basic	O
idea	O
is	O
when	O
BART	B-MethodName
's	O
decoders	O
are	O
performing	O
cross	O
-	O
attention	O
over	O
the	O
text	O
input	O
of	O
the	O
sentences	O
and	O
the	O
possible	O
metadata	O
,	O
we	O
require	O
that	O
each	O
set	O
of	O
attention	O
heads	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
derives	O
different	O
attention	O
scores	O
based	O
on	O
different	O
metadata	O
.	O

Concretely	O
,	O
each	O
set	O
of	O
attention	O
heads	O
will	O
explicitly	O
pay	O
attention	O
to	O
different	O
parts	O
of	O
the	O
input	O
corresponding	O
to	O
different	O
pieces	O
of	O
metadata	O
,	O
and	O
neglect	O
the	O
others	O
.	O

Therefore	O
,	O
after	O
training	O
,	O
each	O
set	O
of	O
attention	O
heads	O
can	O
be	O
used	O
to	O
project	O
the	O
input	O
embeddings	O
into	O
different	O
representation	O
subspaces	O
but	O
focusing	O
on	O
a	O
specific	O
set	O
of	O
candidate	O
metadata	O
.	O

For	O
example	O
,	O
we	O
will	O
have	O
a	O
set	O
of	O
attention	O
heads	O
do	O
cross	O
-	O
attention	O
only	O
over	O
the	O
positions	O
of	O
the	O
sentences	O
and	O
the	O
meta	O
-	O
data	O
from	O
the	O
first	O
url	O
,	O
another	O
set	O
do	O
it	O
only	O
over	O
the	O
positions	O
of	O
the	O
sentences	O
and	O
the	O
meta	O
-	O
data	O
from	O
the	O
first	O
and	O
the	O
second	O
urls	O
,	O
and	O
so	O
on	O
.	O

Note	O
that	O
the	O
candidate	O
metadata	O
from	O
the	O
urls	O
ranked	O
higher	O
will	O
always	O
receive	O
more	O
attention	O
than	O
the	O
others	O
in	O
this	O
case	O
.	O

Figure	O
4	O
summarizes	O
our	O
final	O
design	O
of	O
the	O
generation	O
model	O
.	O

Given	O
the	O
identified	O
sentence	O
and	O
the	O
query	O
keywords	O
generated	O
,	O
we	O
can	O
search	O
for	O
them	O
on	O
a	O
search	O
engine	O
and	O
collect	O
a	O
set	O
of	O
links	O
that	O
are	O
the	O
candidates	O
of	O
the	O
source	O
articles	O
.	O

The	O
next	O
problem	O
is	O
to	O
infer	O
the	O
correct	O
ones	O
from	O
them	O
.	O

Figure	O
4	O
:	O
The	O
architecture	O
of	O
the	O
query	O
generator	O
.	O

The	O
model	O
extends	O
(	O
1	O
)	O
BART	B-MethodName
's	O
encoders	O
to	O
incorporate	O
two	O
types	O
of	O
input	O
,	O
one	O
is	O
the	O
context	O
of	O
the	O
selected	O
sentence	O
,	O
and	O
the	O
other	O
one	O
is	O
possible	O
metadata	O
collected	O
from	O
a	O
search	O
engine	O
,	O
(	O
2	O
)	O
BART	B-MethodName
's	O
decoders	O
with	O
a	O
rankaware	B-HyperparameterValue
multi	B-HyperparameterName
-	I-HyperparameterName
head	I-HyperparameterName
cross	I-HyperparameterName
attention	I-HyperparameterName
to	O
generate	O
the	O
gold	O
metadata	O
.	O

Based	O
on	O
our	O
observations	O
,	O
the	O
author	O
is	O
very	O
likely	O
to	O
leverage	O
the	O
external	O
information	O
coming	O
from	O
the	O
same	O
source	O
websites	O
.	O

In	O
our	O
running	O
example	O
introduced	O
in	O
Section	O
1	O
,	O
the	O
author	O
cited	O
8	O
articles	O
in	O
total	O
,	O
and	O
among	O
those	O
articles	O
,	O
two	O
of	O
them	O
come	O
from	O
whitehouse.gov	O
and	O
another	O
two	O
come	O
from	O
politicfact.com	O
,	O
which	O
are	O
actually	O
two	O
claims	O
they	O
have	O
done	O
fact	O
-	O
check	O
before	O
.	O

Besides	O
the	O
sources	O
,	O
the	O
titles	O
of	O
the	O
articles	O
are	O
also	O
very	O
likely	O
to	O
be	O
related	O
.	O

In	O
the	O
same	O
example	O
,	O
some	O
of	O
them	O
are	O
all	O
talking	O
about	O
the	O
interviews	O
done	O
by	O
Anthony	O
Fauci	O
at	O
different	O
time	O
,	O
and	O
some	O
of	O
them	O
are	O
talking	O
about	O
the	O
white	O
house	O
's	O
Coronavirus	O
Task	O
Force	O
in	O
Press	O
Briefing	O
.	O

Therefore	O
,	O
we	O
propose	O
an	O
algorithmic	O
inference	O
framework	O
that	O
can	O
take	O
advantage	O
of	O
those	O
relations	O
between	O
the	O
source	O
articles	O
to	O
determine	O
the	O
correct	O
source	O
articles	O
of	O
identified	O
sentences	O
jointly	O
.	O

We	O
formulate	O
the	O
inference	O
as	O
an	O
Integer	O
Linear	O
Program	O
(	O
ILP	O
)	O
(	O
Roth	O
and	O
tau	O
Yih	O
,	O
2004;Cheng	O
and	O
Roth	O
,	O
2013	O
)	O
,	O
that	O
allows	O
us	O
to	O
jointly	O
determine	O
the	O
best	O
candidate	O
for	O
each	O
identified	O
sentence	O
.	O

Formally	O
,	O
we	O
introduce	O
two	O
types	O
of	O
Boolean	O
variables	O
:	O
x	O
k	O
i	O
,	O
which	O
represents	O
if	O
the	O
k	O
th	O
candidate	O
is	O
the	O
source	O
article	O
of	O
the	O
i	O
th	O
sentence	O
,	O
and	O
z	O
kl	O
ij	O
,	O
which	O
represents	O
if	O
the	O
source	O
article	O
of	O
the	O
i	O
th	O
sentence	O
and	O
the	O
source	O
article	O
of	O
the	O
j	O
th	O
sentence	O
are	O
related	O
,	O
which	O
means	O
either	O
they	O
come	O
from	O
related	O
source	O
websites	O
or	O
provide	O
related	O
content	O
.	O

To	O
infer	O
the	O
value	O
of	O
the	O
Boolean	O
variables	O
,	O
our	O
objective	O
is	O
to	O
assign	O
the	O
best	O
candidate	O
to	O
each	O
identified	O
sentence	O
that	O
can	O
(	O
1	O
)	O
maximize	O
the	O
overall	O
relatedness	O
of	O
the	O
source	O
articles	O
to	O
the	O
query	O
document	O
,	O
and	O
(	O
2	O
)	O
maximize	O
the	O
relatedness	O
between	O
the	O
source	O
articles	O
.	O

To	O
compute	O
the	O
relatedness	O
,	O
we	O
introduce	O
w	O
k	O
i	O
,	O
which	O
represents	O
the	O
relatedness	O
score	O
of	O
the	O
candidate	O
article	O
to	O
the	O
identified	O
sentence	O
,	O
γ	O
kl	O
ij	O
,	O
which	O
represents	O
the	O
similarity	O
score	O
between	O
the	O
representations	O
of	O
the	O
source	O
domain	O
of	O
the	O
i	O
th	O
article	O
's	O
k	O
th	O
candidate	O
and	O
the	O
source	O
domain	O
of	O
the	O
j	O
th	O
article	O
's	O
l	O
th	O
candidate	O
,	O
and	O
τ	O
kl	O
ij	O
,	O
which	O
represents	O
the	O
similarity	O
score	O
between	O
the	O
representations	O
of	O
the	O
title	O
of	O
the	O
i	O
th	O
article	O
's	O
k	O
th	O
candidate	O
and	O
the	O
source	O
domain	O
of	O
the	O
j	O
th	O
article	O
's	O
l	O
th	O
candidate	O
.	O

Then	O
,	O
the	O
optimization	O
goal	O
to	O
find	O
the	O
best	O
assignments	O
Γ	O
d	O
of	O
candidates	O
for	O
the	O
identified	O
sentences	O
is	O
as	O
follows	O
:	O
Γ	O
d	O
=	O
argmax	O
Γ	O
i	O
k	O
ω	O
k	O
i	O
x	O
k	O
i	O
+	O
i	O
,	O
j	O
k	O
,	O
l	O
τ	O
kl	O
ij	O
+	O
γ	O
kl	O
ij	O
z	O
kl	O
ij	O
(	O
2	O
)	O
s.t	O
.	O

x	O
k	O
i	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
z	O
kl	O
ij	O
∈	O
{	O
0	O
,	O
1	O
}	O
∀i	O
,	O
k	O
x	O
k	O
i	O
=	O
1	O
2z	O
kl	O
ij	O
≤	O
x	O
k	O
i	O
+	O
x	O
l	O
j	O
(	O
3)Here	O
,	O
k	O
x	O
k	O
i	O
=	O
1	O
means	O
only	O
one	O
candidate	O
will	O
finally	O
be	O
chosen	O
as	O
the	O
source	O
article	O
of	O
the	O
i	O
th	O
sentence	O
,	O
and	O
2z	O
kl	O
ij	O
≤	O
x	O
k	O
i	O
+	O
x	O
l	O
j	O
means	O
only	O
if	O
the	O
k	O
th	O
candidate	O
of	O
the	O
i	O
th	O
sentence	O
and	O
the	O
l	O
th	O
candidate	O
of	O
the	O
j	O
th	O
sentence	O
have	O
been	O
chosen	O
,	O
we	O
need	O
to	O
consider	O
the	O
relations	O
between	O
them	O
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
the	O
last	O
hidden	O
layer	O
of	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
as	O
the	O
representation	O
for	O
titles	O
and	O
source	O
domains	O
,	O
and	O
use	O
cosine	O
similarity	O
to	O
compute	O
the	O
similarity	O
score	O
.	O

The	O
ILP	O
problem	O
is	O
solved	O
using	O
an	O
off	O
-	O
the	O
-	O
shelf	O
high	O
-	O
performance	O
package	O
3	O
.	O

In	O
this	O
section	O
we	O
aim	O
to	O
answer	O
the	O
following	O
research	O
questions	O
:	O
RQ1	O
Can	O
we	O
correctly	O
identify	O
the	O
sentences	O
that	O
refer	O
to	O
important	O
external	O
information	O
in	O
the	O
given	O
article	O
?	O
RQ2	O
Given	O
the	O
identified	O
sentences	O
,	O
can	O
we	O
generate	O
the	O
metadata	O
of	O
the	O
target	O
articles	O
from	O
the	O
context	O
?	O
RQ3	O
Given	O
a	O
list	O
of	O
candidates	O
for	O
each	O
identified	O
sentence	O
in	O
the	O
article	O
,	O
can	O
we	O
assign	O
the	O
correct	O
candidate	O
to	O
each	O
identified	O
sentence?3	O
https://www.python-mip.com/	O
RQ4	O
Given	O
the	O
identified	O
sentences	O
,	O
can	O
we	O
use	O
the	O
query	O
we	O
generated	O
to	O
find	O
candidates	O
,	O
and	O
successfully	O
use	O
them	O
to	O
improve	O
the	O
inference	O
of	O
source	O
articles	O
?	O
Among	O
those	O
questions	O
,	O
RQ1	O
-	O
RQ3	O
are	O
to	O
evaluate	O
a	O
specific	O
component	O
of	O
our	O
solution	O
,	O
and	O
RQ4	O
is	O
to	O
evaluate	O
the	O
joint	O
performance	O
of	O
candidate	O
generation	O
and	O
source	O
article	O
inference	O
.	O

In	O
the	O
following	O
part	O
,	O
we	O
will	O
elaborate	O
the	O
answers	O
to	O
those	O
questions	O
,	O
and	O
for	O
each	O
question	O
,	O
we	O
will	O
start	O
with	O
describing	O
its	O
experimental	O
setting	O
,	O
baselines	O
and	O
the	O
metrics	O
.	O

Setup	O
We	O
use	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
dataset	O
introduced	O
in	O
Section	O
3	O
.	O

Concretely	O
,	O
we	O
train	O
and	O
validate	O
our	O
models	O
on	O
the	O
articles	O
in	O
the	O
training	O
and	O
validation	O
set	O
,	O
and	O
try	O
to	O
predict	O
the	O
score	O
of	O
a	O
sentence	O
referring	O
to	O
a	O
source	O
article	O
from	O
the	O
article	O
belonging	O
to	O
the	O
test	O
set	O
.	O

To	O
compare	O
the	O
performance	O
,	O
we	O
implement	O
our	O
solution	O
(	O
SR	B-MethodName
-	I-MethodName
TE	I-MethodName
)	O
as	O
described	O
in	O
Section	O
4.1	O
,	O
and	O
compare	O
it	O
with	O
(	O
1	O
)	O
a	O
retrieval	O
baseline	O
that	O
simply	O
computes	O
the	O
cosine	O
similarity	O
between	O
the	O
embedding	O
vectors	O
(	O
using	O
Roberta	B-MethodName
)	O
of	O
the	O
title	O
and	O
the	O
sentence	O
in	O
the	O
article	O
(	O
SR	B-MethodName
)	O
.	O

This	O
retrieval	O
baseline	O
only	O
captures	O
the	O
relatedness	O
between	O
the	O
sentence	O
and	O
the	O
main	O
topic	O
of	O
the	O
article;(2	O
)	O
a	O
retrieval	O
baseline	O
similar	O
to	O
SR	B-MethodName
,	O
but	O
computing	O
the	O
cosine	O
similarity	O
between	O
the	O
embedding	O
vectors	O
of	O
the	O
concatenation	O
of	O
the	O
title	O
and	O
the	O
most	O
important	O
entities	O
(	O
top-50	O
)	O
and	O
the	O
sentence	O
in	O
the	O
article	O
(	O
SR	B-MethodName
-	I-MethodName
E	I-MethodName
)	O
,	O
where	O
we	O
want	O
to	O
show	O
the	O
effect	O
of	O
considering	O
important	O
entities	O
;	O
(	O
3	O
)	O
our	O
learning	O
solution	O
without	O
considering	O
entities	O
(	O
SR	B-MethodName
-	I-MethodName
T	I-MethodName
)	O
.	O

We	O
report	O
the	O
mean	B-MetricName
precision	I-MetricName
and	O
recall	B-MetricName
of	I-MetricName
the	I-MetricName
top	I-MetricName
-	I-MetricName
k	I-MetricName
results	I-MetricName
respectively	O
.	O

The	O
results	O
are	O
reported	O
in	O
Figure	O
5	O
.	O

The	O
gaps	O
between	O
SR	B-MethodName
,	O
SR	B-MethodName
-	I-MethodName
E	I-MethodName
,	O
and	O
SR	B-MethodName
-	I-MethodName
T	I-MethodName
,	O
SR	B-MethodName
-	I-MethodName
TE	I-MethodName
show	O
that	O
considering	O
important	O
entities	O
always	O
results	O
in	O
an	O
improvement	O
on	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
which	O
reveals	O
that	O
the	O
sentences	O
can	O
not	O
be	O
identified	O
based	O
on	O
their	O
relatedness	O
to	O
the	O
title	O
(	O
the	O
main	O
topic	O
)	O
only	O
,	O
but	O
also	O
requires	O
other	O
important	O
information	O
in	O
the	O
article	O
.	O

Furthermore	O
,	O
the	O
figure	O
also	O
shows	O
that	O
the	O
learning	O
method	O
is	O
significantly	O
better	O
than	O
the	O
retrieval	O
baseline	O
without	O
a	O
learning	O
objective	O
.	O

Setup	O
We	O
collect	O
all	O
of	O
the	O
sentences	O
that	O
correspond	O
to	O
the	O
source	O
articles	O
in	O
training	O
,	O
validation	O
and	O
test	O
set	O
of	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
serving	O
as	O
training	O
,	O
validation	O
and	O
testing	O
respectively	O
.	O

Overall	O
,	O
there	O
are	O
5279	O
cases	O
for	O
training	O
,	O
1847	O
for	O
validation	O
,	O
and	O
1538	O
for	O
testing	O
.	O

For	O
each	O
case	O
,	O
the	O
source	O
input	O
is	O
the	O
identified	O
sentence	O
with	O
its	O
context	O
(	O
two	O
sentences	O
which	O
are	O
before	O
and	O
after	O
the	O
sentence	O
respectively	O
)	O
,	O
and	O
the	O
target	O
output	O
to	O
generate	O
is	O
the	O
metadata	O
of	O
the	O
corresponding	O
source	O
article	O
in	O
a	O
form	O
of	O
a	O
concatenation	O
of	O
its	O
source	O
domain	O
,	O
title	O
and	O
published	O
date	O
.	O

To	O
evaluate	O
the	O
performance	O
,	O
we	O
report	O
Rouge	B-MetricName
1	I-MetricName
,	O
Rouge	B-MetricName
2	I-MetricName
and	O
Rouge	B-MetricName
L	I-MetricName
score	O
of	O
the	O
text	O
generated	O
,	O
and	O
compare	O
with	O
the	O
performance	O
produced	O
by	O
(	O
1	O
)	O
the	O
original	O
BART	B-MethodName
,	O
(	O
2	O
)	O
our	O
solution	O
integrating	O
signals	O
from	O
Google	O
(	O
BART	B-MethodName
-	I-MethodName
S	I-MethodName
)	O
,	O
and	O
(	O
3	O
)	O
our	O
solution	O
integrating	O
signals	O
from	O
Google	O
with	O
our	O
rank	O
-	O
aware	O
multi	O
-	O
head	O
cross	O
attention	O
(	O
BART	B-MethodName
-	I-MethodName
SR	I-MethodName
)	O
.	O

We	O
report	O
the	O
results	O
in	O
Table	O
1	O
.	O

As	O
shown	O
in	O
the	O
table	O
,	O
we	O
can	O
observe	O
that	O
integrating	O
the	O
signals	O
from	O
a	O
search	O
engine	O
can	O
significantly	O
improve	O
the	O
performance	O
of	O
generating	O
the	O
metadata	O
,	O
and	O
considering	O
the	O
ranking	O
of	O
the	O
search	O
results	O
can	O
further	O
lead	O
to	O
an	O
improvement	O
.	O

Setup	O
To	O
conduct	O
an	O
isolated	O
evaluation	O
of	O
the	O
ILP	O
based	O
inference	O
,	O
in	O
this	O
experiment	O
,	O
we	O
generate	O
the	O
candidates	O
for	O
each	O
identified	O
sentence	O
based	O
on	O
its	O
metadata	O
from	O
the	O
ground	O
truth	O
.	O

Concretely	O
,	O
we	O
assume	O
there	O
is	O
an	O
oracle	O
that	O
can	O
generate	O
the	O
metadata	O
based	O
on	O
the	O
context	O
for	O
each	O
identified	O
sentence	O
,	O
and	O
we	O
directly	O
search	O
the	O
metadata	O
on	O
Google	O
,	O
and	O
fetch	O
its	O
top-5	O
results	O
returned	O
as	O
candidates	O
for	O
each	O
identified	O
sentence	O
.	O

Then	O
,	O
our	O
inference	O
algorithm	O
is	O
to	O
find	O
the	O
correct	O
source	O
article	O
for	O
each	O
sentence	O
from	O
those	O
candidates	O
.	O

To	O
evaluate	O
the	O
performance	O
,	O
we	O
report	O
the	O
mean	B-MetricName
recall	I-MetricName
of	O
source	O
articles	O
for	O
each	O
article	O
,	O
and	O
compare	O
it	O
with	O
results	O
provided	O
by	O
the	O
baselines	O
,	O
including	O
(	O
1	O
)	O
simply	O
choosing	O
the	O
top-1	O
article	O
from	O
the	O
results	O
returned	O
by	O
directly	O
searching	O
the	O
identified	O
sentence	O
on	O
Google	O
(	O
SS1	B-MethodName
)	O
,	O
(	O
2	O
)	O
choosing	O
the	O
top-1	O
article	O
from	O
the	O
results	O
returned	O
by	O
searching	O
the	O
metadata	O
on	O
Google	O
(	O
MS1	B-MethodName
)	O
,	O
(	O
3	O
)	O
our	O
proposed	O
solution	O
,	O
which	O
conducts	O
ILP	O
inference	O
to	O
find	O
the	O
source	O
article	O
from	O
the	O
search	O
results	O
returned	O
by	O
searching	O
the	O
metadata	O
on	O
Google	O
(	O
MS	O
-	O
ILP	O
)	O
.	O

To	O
have	O
a	O
better	O
understanding	O
of	O
the	O
performance	O
,	O
we	O
also	O
report	O
two	O
upper	O
bounds	O
.	O

The	O
first	O
one	O
is	O
the	O
upper	O
bound	O
of	O
the	O
mean	B-HyperparameterName
recall	I-HyperparameterName
of	O
the	O
results	O
by	O
directly	O
searching	O
the	O
identified	O
sentence	O
on	O
Google	O
(	O
SS	B-MethodName
-	I-MethodName
UB	I-MethodName
)	O
,	O
and	O
the	O
second	O
one	O
is	O
the	O
upper	O
bound	O
of	O
the	O
mean	O
recall	O
of	O
the	O
results	O
by	O
directly	O
searching	O
the	O
meta	O
-	O
data	O
on	O
Google	O
(	O
MS	B-MethodName
-	I-MethodName
UB	I-MethodName
)	O
.	O

To	O
compute	O
the	O
upper	O
bounds	O
,	O
if	O
one	O
of	O
the	O
articles	O
returned	O
by	O
Google	O
is	O
correct	O
,	O
then	O
we	O
consider	O
the	O
sentence	O
is	O
correctly	O
assigned	O
.	O

Actually	O
,	O
they	O
are	O
equivalent	O
to	O
the	O
mean	O
recall	O
of	O
the	O
top-5	O
results	O
,	O
since	O
we	O
only	O
request	O
Google	O
for	O
its	O
top-5	O
search	O
results	O
.	O

We	O
report	O
the	O
performance	O
in	O
Figure	O
6	O
.	O

In	O
the	O
figure	O
,	O
we	O
can	O
observe	O
that	O
the	O
mean	B-MetricName
recall	I-MetricName
of	O
SS1	O
is	O
only	O
0.067	B-MetricValue
,	O
and	O
even	O
its	O
upper	O
bound	O
SS	B-MethodName
-	I-MethodName
UB	I-MethodName
can	O
only	O
achieve	O
0.15	B-MetricValue
,	O
which	O
reveals	O
that	O
directly	O
searching	O
the	O
identified	O
sentence	O
on	O
a	O
search	O
engine	O
to	O
find	O
the	O
source	O
article	O
is	O
not	O
feasible	O
.	O

Using	O
the	O
metadata	O
of	O
the	O
source	O
article	O
to	O
search	O
can	O
improve	O
the	O
mean	B-MetricName
recall	I-MetricName
to	O
around	O
0.3	B-MetricValue
,	O
and	O
considering	O
the	O
relatedness	O
between	O
the	O
source	O
articles	O
by	O
ILP	O
can	O
further	O
improve	O
it	O
to	O
around	O
0.37	B-MetricValue
.	O

It	O
demonstrates	O
that	O
the	O
ILP	O
inference	O
is	O
useful	O
for	O
capturing	O
the	O
relatedness	O
between	O
the	O
source	O
articles	O
,	O
and	O
the	O
result	O
has	O
been	O
very	O
close	O
to	O
the	O
mean	B-MetricName
recall	I-MetricName
of	O
its	O
top-5	O
results	O
(	O
MS	B-MethodName
-	I-MethodName
UB	I-MethodName
)	O
,	O
which	O
is	O
the	O
upper	O
bound	O
of	O
the	O
performance	O
that	O
the	O
inference	O
can	O
achieve	O
with	O
searching	O
by	O
metadata	O
.	O

Setup	O
In	O
this	O
experiment	O
,	O
we	O
issue	O
the	O
queries	O
generated	O
by	O
the	O
query	O
generation	O
module	O
to	O
Google	O
,	O
and	O
fetched	O
the	O
top-5	O
results	O
returned	O
.	O

We	O
combine	O
these	O
results	O
with	O
the	O
top-5	O
links	O
returned	O
by	O
searching	O
the	O
identified	O
sentence	O
directly	O
,	O
as	O
the	O
candidate	O
pool	O
for	O
each	O
identified	O
sentence	O
.	O

Then	O
,	O
we	O
conduct	O
ILP	O
inference	O
to	O
assign	O
the	O
candidate	O
to	O
each	O
sentence	O
.	O

We	O
report	O
the	O
mean	B-MetricName
recall	I-MetricName
of	O
Figure	O
6	O
:	O
The	O
performance	O
of	O
inferring	O
source	O
articles	O
for	O
each	O
article	O
,	O
MS	O
-	O
ILP	O
is	O
our	O
ILP	O
based	O
solution	O
,	O
and	O
MS	O
-	O
UB	O
is	O
the	O
best	O
possible	O
performance	O
that	O
can	O
be	O
achieved	O
when	O
the	O
candidates	O
are	O
the	O
top-5	O
results	O
returned	O
by	O
searching	O
for	O
metadata	O
on	O
Google	O
.	O

the	O
source	O
articles	O
,	O
varying	O
k	O
,	O
which	O
represents	O
the	O
number	O
of	O
the	O
links	O
we	O
returned	O
for	O
each	O
identified	O
sentence	O
.	O

Note	O
that	O
finding	O
the	O
top	O
-	O
k	O
assignments	O
in	O
ILP	O
is	O
actually	O
relaxing	O
the	O
unique	O
solution	O
constraint	O
in	O
Eq	O
3	O
to	O
be	O
∀i	O
,	O
j	O
x	O
j	O
i	O
=	O
k	O
,	O
which	O
makes	O
the	O
problem	O
require	O
an	O
additional	O
significant	O
amount	O
of	O
time	O
to	O
solve	O
.	O

Therefore	O
,	O
here	O
we	O
greedily	O
select	O
the	O
best	O
assignment	O
for	O
each	O
variable	O
as	O
an	O
approximate	O
top	O
-	O
k	O
solution	O
.	O

Results	O
As	O
shown	O
in	O
Figure	O
7	O
,	O
we	O
can	O
observe	O
when	O
k	O
=	O
3	O
,	O
it	O
has	O
already	O
beaten	O
the	O
performance	O
of	O
SS	B-MethodName
-	I-MethodName
UB	I-MethodName
reported	O
in	O
Figure	O
6	O
,	O
which	O
reveals	O
that	O
the	O
candidates	O
found	O
by	O
the	O
queries	O
generated	O
by	O
our	O
query	O
generator	O
are	O
helpful	O
.	O

When	O
k	O
=	O
5	O
,	O
the	O
mean	B-MetricName
recall	I-MetricName
can	O
achieve	O
around	O
0.21	B-MetricValue
,	O
which	O
is	O
much	O
better	O
than	O
0.15	B-MetricValue
,	O
the	O
best	O
performance	O
achieved	O
by	O
searching	O
the	O
identified	O
sentence	O
directly	O
.	O

However	O
,	O
as	O
what	O
we	O
can	O
observe	O
in	O
the	O
figure	O
,	O
there	O
is	O
still	O
a	O
gap	O
to	O
the	O
performance	O
of	O
MS	B-MethodName
-	I-MethodName
UB	I-MethodName
in	O
Figure	O
6	O
.	O

This	O
may	O
result	O
from	O
the	O
insufficiency	O
of	O
the	O
query	O
generation	O
,	O
which	O
implies	O
that	O
a	O
better	O
text	O
generation	O
model	O
may	O
be	O
necessary	O
to	O
further	O
improve	O
the	O
performance	O
,	O
which	O
we	O
think	O
is	O
an	O
interesting	O
topic	O
for	O
future	O
work	O
.	O

Our	O
work	O
builds	O
on	O
earlier	O
work	O
on	O
Claim	B-TaskName
Provenance	I-TaskName
(	O
see	O
Section	O
2	O
for	O
a	O
discussion	O
)	O
.	O

Beyond	O
that	O
,	O
we	O
discuss	O
below	O
additional	O
related	O
work	O
.	O

Fact	O
-	O
checking	O
Fact	O
-	O
checking	O
is	O
related	O
to	O
our	O
problem	O
,	O
since	O
there	O
is	O
usually	O
a	O
document	O
retrieval	O
step	O
to	O
find	O
articles	O
that	O
may	O
provide	O
evidence	O
in	O
most	O
of	O
the	O
solutions	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2018;Thorne	O
et	O
al	O
.	O
,	O

2018;Nadeem	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Typically	O
,	O
the	O
input	O
of	O
fact	O
-	O
checking	O
is	O
a	O
single	O
claim	O
instead	O
of	O
an	O
article	O
,	O
therefore	O
it	O
is	O
hard	O
to	O
directly	O
extend	O
their	O
solutions	O
to	O
our	O
problem	O
.	O

Even	O
though	O
fact	O
-	O
checking	O
may	O
find	O
various	O
evidentiary	O
articles	O
for	O
the	O
claim	O
,	O
the	O
source	O
articles	O
we	O
are	O
looking	O
for	O
are	O
those	O
that	O
have	O
been	O
used	O
by	O
the	O
author	O
,	O
which	O
is	O
actually	O
a	O
specific	O
subset	O
of	O
the	O
articles	O
that	O
fact	O
-	O
checking	O
targets	O
to	O
,	O
and	O
the	O
size	O
is	O
also	O
much	O
smaller	O
.	O

Furthermore	O
,	O
we	O
try	O
to	O
extract	O
the	O
metadata	O
of	O
the	O
source	O
articles	O
from	O
the	O
text	O
to	O
support	O
a	O
better	O
search	O
,	O
which	O
is	O
not	O
considered	O
in	O
the	O
document	O
retrieval	O
step	O
of	O
fact	O
-	O
checking	O
.	O

Recommending	O
Citations	O
Recommending	O
citations	O
for	O
scholarly	O
articles	O
has	O
similarities	O
to	O
our	O
work	O
.	O

The	O
source	O
articles	O
we	O
are	O
looking	O
for	O
can	O
be	O
considered	O
as	O
the	O
citations	O
of	O
the	O
given	O
news	O
article	O
that	O
should	O
be	O
recommended	O
.	O

However	O
,	O
the	O
meaning	O
of	O
the	O
"	O
reference	O
"	O
is	O
different	O
in	O
these	O
two	O
problems	O
.	O

When	O
recommending	O
citations	O
for	O
a	O
paper	O
,	O
the	O
system	O
is	O
to	O
look	O
for	O
previous	O
works	O
that	O
are	O
related	O
to	O
the	O
arguments	O
in	O
the	O
given	O
paper	O
.	O

The	O
argument	O
was	O
created	O
by	O
the	O
author	O
,	O
and	O
the	O
criteria	O
of	O
the	O
recommendation	O
is	O
the	O
relatedness	O
.	O

While	O
inferring	O
provenance	O
is	O
to	O
do	O
reverse	O
engineering	O
to	O
the	O
given	O
article	O
,	O
so	O
that	O
we	O
can	O
find	O
the	O
articles	O
whose	O
information	O
or	O
claims	O
were	O
actually	O
used	O
when	O
the	O
author	O
was	O
writing	O
.	O

Technically	O
,	O
there	O
are	O
two	O
types	O
of	O
citation	O
recommendation	O
systems	O
(	O
Bhagavatula	O
