Lifelong	B-TaskName
learning	I-TaskName
(	I-TaskName
LL	I-TaskName
)	I-TaskName
aims	O
to	O
train	O
a	O
neural	O
network	O
on	O
a	O
stream	O
of	O
tasks	O
while	O
retaining	O
knowledge	O
from	O
previous	O
tasks	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
LL	B-TaskName
framework	O
for	O
language	O
models	O
.	O

In	O
order	O
to	O
alleviate	O
catastrophic	O
forgetting	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
enhances	O
LAMOL	B-MethodName
,	O
a	O
recent	O
LL	B-TaskName
model	O
,	O
by	O
applying	O
critical	O
freezing	O
guided	O
by	O
human	O
rationales	O
.	O

In	O
the	O
experiment	O
,	O
we	O
tested	O
Rational	B-MethodName
LAMOL	I-MethodName
on	O
permutations	O
of	O
three	O
datasets	O
from	O
the	O
ERASER	O
benchmark	O
.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
framework	O
outperformed	O
vanilla	O
LAMOL	B-MethodName
on	O
most	O
permutations	O
.	O

Furthermore	O
,	O
unsupervised	O
rationale	O
generation	O
was	O
able	O
to	O
consistently	O
improve	O
the	O
overall	O
LL	B-TaskName
performance	O
from	O
the	O
baseline	O
without	O
relying	O
on	O
human	O
-	O
annotated	O
rationales	O
.	O

The	O
grounds	O
of	O
lifelong	B-TaskName
learning	I-TaskName
(	O
LL	B-TaskName
)	O
stem	O
from	O
the	O
ability	O
of	O
humans	O
to	O
continually	O
acquire	O
,	O
consolidate	O
,	O
and	O
transfer	O
knowledge	O
and	O
skills	O
throughout	O
their	O
lifespan	O
.	O

We	O
focus	O
on	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
(	O
LLL	B-TaskName
)	O
,	O
which	O
is	O
lifelong	B-TaskName
learning	I-TaskName
on	O
a	O
stream	O
of	O
NLP	O
tasks	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
grounds	O
of	O
LLL	B-TaskName
are	O
left	O
largely	O
underexplored	O
.	O

LAMOL	B-MethodName
is	O
an	O
LLL	B-TaskName
general	O
framework	O
that	O
has	O
garnered	O
recent	O
interest	O
due	O
to	O
its	O
simplicity	O
(	O
Sun	O
et	O
al	O
.	O
,	O

In	O
particular	O
,	O
LAMOL	B-MethodName
transforms	O
all	O
NLP	O
tasks	O
into	O
the	O
question	O
answering	O
(	O
QA	O
)	O
format	O
according	O
to	O
McCann	O
et	O
al	O
.	O
(	O

However	O
,	O
there	O
is	O
still	O
a	O
gap	O
between	O
the	O
performance	O
of	O
LAMOL	B-MethodName
and	O
the	O
result	O
of	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
which	O
is	O
generally	O
considered	O
as	O
the	O
upper	O
bound	O
of	O
LLL	B-TaskName
performance	O
.	O

In	O
this	O
paper	O
,	O
we	O
improve	O
existing	O
LLL	B-TaskName
strategies	O
by	O
proposing	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
rationale	O
-	O
based	O
lifelong	O
learning	O
framework	O
which	O
equips	O
the	O
original	O
LAMOL	B-MethodName
with	O
critical	O
freezing	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O

Particularly	O
,	O
we	O
devise	O
an	O
algorithm	O
to	O
identify	O
critical	O
components	O
in	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
language	I-MethodName
models	I-MethodName
using	O
rationales	O
,	O
and	O
the	O
selected	O
compo	O
-	O
nents	O
will	O
be	O
frozen	O
to	O
maintain	O
learned	O
knowledge	O
while	O
being	O
trained	O
on	O
a	O
new	O
task	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
listed	O
below:•	O
We	O
demonstrate	O
the	O
importance	O
of	O
freezing	O
plastic	O
components	O
(	O
i.e.	O
,	O
components	O
that	O
are	O
most	O
susceptible	O
to	O
change	O
)	O
in	O
transformerbased	B-MethodName
models	I-MethodName
to	O
strengthen	O
memories	O
of	O
the	O
previously	O
learned	O
tasks	O
in	O
the	O
LLL	B-TaskName
setting.•	O
We	O
propose	O
critical	O
component	O
identification	O
algorithm	O
which	O
analyzes	O
the	O
transformerbased	B-MethodName
LLL	I-MethodName
model	I-MethodName
with	O
rationales	O
so	O
as	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
freeze	O
.	O

2020	O
)	O
but	O
we	O
adapted	O
it	O
to	O
NLP.•	O
We	O
propose	O
that	O
unsupervised	O
generated	O
rationales	O
by	O
InvRat	B-MethodName
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
can	O
be	O
effectively	O
used	O
as	O
substitutions	O
of	O
human	O
rationales	O
,	O
allowing	O
our	O
framework	O
to	O
be	O
applied	O
to	O
generic	O
NLP	O
datasets	O
.	O

We	O
evaluated	O
Rational	B-MethodName
LAMOL	I-MethodName
on	O
six	O
task	O
order	O
permutations	O
of	O
three	O
datasets	O
from	O
the	O
ERASER	O
benchmark	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
framework	O
outperformed	O
the	O
original	O
LAMOL	B-MethodName
on	O
five	O
out	O
of	O
the	O
six	O
permutations	O
,	O
achieving	O
average	O
improvements	O
of	O
1.83	B-MetricValue
%	I-MetricValue
with	O
a	O
lower	O
standard	O
deviation	O
of	O
4.57	B-MetricValue
%	I-MetricValue
.	O

Moreover	O
,	O
using	O
unsupervised	O
rationale	O
generation	O
instead	O
of	O
human	O
rationales	O
also	O
yielded	O
competitive	O
performance	O
,	O
achieving	O
average	O
improvements	O
of	O
2.67	B-MetricValue
%	I-MetricValue
from	O
original	O
LAMOL	B-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
introduce	O
the	O
concept	O
of	O
lifelong	B-TaskName
learning	I-TaskName
,	O
catastrophic	O
forgetting	O
,	O
and	O
component	O
freezing	O
which	O
are	O
relevant	O
to	O
the	O
core	O
idea	O
of	O
Rational	B-MethodName
LAMOL	I-MethodName
.	O

We	O
also	O
briefly	O
summarize	O
prominent	O
researches	O
related	O
to	O
rationales	O
.	O

While	O
people	O
fine	O
tune	O
a	O
pre	O
-	O
trained	O
model	O
to	O
perform	O
a	O
single	O
task	O
,	O
lifelong	B-TaskName
learning	I-TaskName
(	O
LL	B-TaskName
)	O
is	O
a	O
setting	O
in	O
which	O
a	O
learner	O
performs	O
sequential	O
learning	O
of	O
infinitely	O
incoming	O
tasks	O
τ	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
...	O
,	O
τ	O
i	O
,	O
...	O
,	O
}	O
,	O
where	O
τ	O
i	O
is	O
the	O
i	O
-	O
th	O
task	O
to	O
learn	O
at	O
a	O
particular	O
point	O
in	O
time	O
.	O

The	O
objective	O
of	O
the	O
LL	B-TaskName
learner	O
is	O
to	O
ideally	O
both	O
optimize	O
the	O
performance	O
on	O
the	O
new	O
task	O
and	O
maintain	O
optimal	O
performance	O
on	O
previous	O
tasks	O
τ	O
t	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
...	O
,	O
i.	O
Moreover	O
,	O
the	O
ability	O
to	O
transfer	O
knowledge	O
across	O
different	O
tasks	O
is	O
also	O
desired	O
.	O

There	O
are	O
multiple	O
existing	O
works	O
that	O
aim	O
to	O
mitigate	O
catastrophic	O
forgetting	O
in	O
LL	B-TaskName
.	O

2016;.Lifelong	B-TaskName
Language	I-TaskName
Learning	I-TaskName
or	O
LLL	B-TaskName
is	O
a	O
scenario	O
where	O
a	O
model	O
sequentially	O
learns	O
from	O
a	O
stream	O
of	O
NLP	O
tasks	O
in	O
an	O
LL	B-TaskName
manner	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
LLL	B-TaskName
has	O
rarely	O
been	O
studied	O
and	O
previous	O
works	O
usually	O
target	O
a	O
single	O
type	O
of	O
NLP	O
tasks	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
proposed	O
LAMOL	B-MethodName
,	O
a	O
learning	O
framework	O
that	O
utilizes	O
a	O
language	O
model	O
to	O
simultaneously	O
predict	O
outputs	O
and	O
learn	O
to	O
generate	O
pseudo	O
-	O
training	O
examples	O
,	O
which	O
are	O
exploited	O
to	O
alleviate	O
catastrophic	O
forgetting	O
.	O

Hence	O
,	O
LAMOL	B-MethodName
,	O
as	O
well	O
as	O
our	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
naturally	O
falls	O
into	O
the	O
data	O
-	O
based	O
LL	B-TaskName
approach	O
since	O
data	O
from	O
previous	O
tasks	O
,	O
albeit	O
generated	O
,	O
is	O
utilized	O
to	O
constrain	O
a	O
model	O
.	O

Component	O
Freezing	O
While	O
component	O
freezing	O
is	O
also	O
a	O
common	O
practice	O
in	O
the	O
fine	O
-	O
tuning	O
process	O
,	O
it	O
is	O
done	O
to	O
prevent	O
loss	O
in	O
general	O
knowledge	O
in	O
lower	O
layers	O
of	O
the	O
model	O
(	O
Raganato	O
and	O
Tiedemann	O
,	O
2018).By	O
contrast	O
,	O
many	O
architecture	O
-	O
based	O
LL	B-TaskName
methods	O
,	O
for	O
example	O
Rusu	O
et	O
al	O
.	O
(	O

Our	O
Rational	B-MethodName
LAMOL	I-MethodName
also	O
uses	O
component	O
freezing	O
,	O
but	O
unlike	O
architecturebased	O
methods	O
,	O
only	O
a	O
small	O
part	O
of	O
the	O
model	O
is	O
frozen	O
and	O
its	O
size	O
is	O
constant	O
throughout	O
the	O
learning	O
process	O
.	O

Rationales	O
could	O
be	O
either	O
annotated	O
by	O
humans	O
or	O
generated	O
by	O
machine	O
learning	O
models	O
.	O

In	O
the	O
experiment	O
,	O
we	O
used	O
human	O
rationales	O
from	O
ERASER	O
in	O
the	O
critical	O
component	O
identification	O
step	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
be	O
frozen	O
.	O

In	O
order	O
to	O
allow	O
Rational	B-MethodName
LAMOL	I-MethodName
to	O
be	O
applied	O
to	O
any	O
NLP	O
dataset	O
,	O
we	O
choose	O
to	O
leverage	O
InvRat	O
to	O
automatically	O
produce	O
rationales	O
due	O
to	O
its	O
superior	O
performance	O
and	O
straightforward	O
application	O
,	O
removing	O
the	O
need	O
for	O
human	O
rationales	O
.	O

We	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section	O
.	O

As	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
based	O
from	O
LAMOL	B-MethodName
(	O
Sun	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
briefly	O
explain	O
LAMOL	B-MethodName
in	O
Section	O
3.1	O
.	O

Then	O
we	O
introduce	O
the	O
core	O
lifelong	B-TaskName
learning	I-TaskName
framework	O
of	O
Rational	B-MethodName
LAMOL	I-MethodName
in	O
Section	O
3.2	O
.	O

Language	B-MethodName
Modeling	I-MethodName
for	I-MethodName
Lifelong	I-MethodName
Language	I-MethodName
Learning	I-MethodName
(	O
LAMOL	B-MethodName
)	O
(	O
Sun	O
et	O
al	O
.	O
,	O

In	O
addition	O
,	O
LAMOL	B-MethodName
trains	O
the	O
LM	O
as	O
a	O
generative	O
model	O
upon	O
receiving	O
a	O
special	O
generation	O
token	O
.	O

Using	O
a	O
single	O
model	O
for	O
both	O
providing	O
answers	O
and	O
generating	O
pseudo	O
-	O
samples	O
,	O
LAMOL	B-MethodName
truly	O
exhibits	O
a	O
model	O
of	O
LM	O
and	O
QA	O
duality	O
.	O

The	O
benefit	O
that	O
comes	O
with	O
the	O
generative	O
part	O
of	O
the	O
model	O
tackles	O
the	O
long	O
-	O
standing	O
issue	O
of	O
LLcatastrophic	B-TaskName
forgetting	O
.	O

2017;Kemker	O
and	O
Kanan	O
,	O
2017	O
)	O
,	O
LAMOL	B-MethodName
transfers	O
all	O
the	O
responsibilities	O
into	O
a	O
single	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
exploiting	O
rationales	O
with	O
LAMOL	B-MethodName
to	O
further	O
improve	O
the	O
LLL	B-TaskName
performance	O
,	O
discussed	O
next	O
.	O

Rational	B-MethodName
LAMOL	I-MethodName
,	O
illustrated	O
in	O
Figure	O
1	O
(	O
right	O
)	O
,	O
is	O
a	O
learning	O
framework	O
revolving	O
around	O
the	O
original	O
methodologies	O
of	O
LAMOL	B-MethodName
.	O

We	O
consider	O
an	O
LL	B-TaskName
setting	O
where	O
τ	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
...	O
,	O
τ	O
i	O
,	O
...	O
}	O
is	O
a	O
stream	O
of	O
learning	O
tasks	O
and	O
τ	O
i	O
is	O
the	O
i	O
-	O
th	O
task	O
to	O
train	O
at	O
a	O
particular	O
point	O
in	O
time	O
.	O

Using	O
these	O
notations	O
and	O
starting	O
from	O
M	O
0	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
works	O
iteratively	O
in	O
four	O
steps	O
as	O
follows	O
.	O

First	O
,	O
given	O
a	O
model	O
M	O
i	O
,	O
it	O
trains	O
M	O
i	O
with	O
the	O
task	O
τ	O
i+1	O
using	O
LAMOL	B-MethodName
's	O
training	O
procedure	O
to	O
obtainM	O
i+1	O
.	O

Note	O
that	O
despite	O
the	O
unique	O
nature	O
of	O
LAMOL	B-MethodName
,	O
our	O
Rational	B-MethodName
LAMOL	I-MethodName
does	O
not	O
limit	O
its	O
usage	O
to	O
a	O
single	O
model	O
architecture	O
.	O

We	O
propose	O
the	O
Critical	O
Component	O
Identification	O
(	O
CCI	O
)	O
algorithm	O
,	O
pointing	O
out	O
the	O
most	O
plastic	O
block	O
of	O
our	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
LL	I-MethodName
model	I-MethodName
before	O
moving	O
on	O
to	O
a	O
new	O
task	O
completely	O
.	O
(	O

The	O
chosen	O
block	O
is	O
the	O
one	O
that	O
forgets	O
what	O
it	O
has	O
learned	O
from	O
the	O
recent	O
task	O
the	O
most	O
when	O
being	O
introduced	O
a	O
new	O
task	O
,	O
so	O
we	O
will	O
freeze	O
the	O
block	O
to	O
prevent	O
catastrophic	O
forgetting	O
in	O
Rational	B-MethodName
LAMOL.As	I-MethodName
shown	O
in	O
Algorithm	O
1	O
,	O
for	O
each	O
validation	O
sample	O
x	O
∈	O
X	O
of	O
task	O
i	O
,	O
the	O
CCI	O
compares	O
the	O
attention	O
maps	O
AT	O
produced	O
by	O
the	O
model	O
M	O
i	O
(	O
i.e.	O
,	O
the	O
old	O
model	O
M	O
O	O
in	O
Algorithm	O
1	O
)	O
andM	O
i+1	O
(	O
i.e.	O
,	O
the	O
new	O
model	O
M	O
N	O
in	O
Algorithm	O
1	O
)	O
to	O
find	O
the	O
most	O
plastic	O
block	O
b	O
with	O
respect	O
to	O
this	O
sample	O
.	O

Then	O
it	O
returns	O
the	O
block	O
F	O
which	O
is	O
the	O
mode	O
of	O
all	O
b	O
,	O
voted	O
by	O
most	O
of	O
the	O
samples	O
in	O
X.	O
Note	O
that	O
most	O
of	O
the	O
variable	O
names	O
are	O
preserved	O
similar	O
to	O
Nguyen	O
et	O
al	O
.	O
(	O

In	O
particular	O
,	O
to	O
find	O
b	O
for	O
the	O
sample	O
x	O
,	O
we	O
iterate	O
over	O
all	O
blocks	O
j	O
=	O
1	O
,	O
...	O
,	O
K	O
and	O
perform	O
two	O
steps	O
.	O

2020	O
)	O
,	O
elementary	O
visualization	O
of	O
attentions	O
are	O
possible	O
in	O
Transformers	B-MethodName
(	O
Vig	O
,	O
2019;Hoover	O
et	O
al	O
.	O
,	O

We	O
hypothesize	O
that	O
the	O
semantic	O
nature	O
of	O
the	O
self	O
-	O
attention	O
mechanisms	O
would	O
opt	O
for	O
tokens	O
most	O
relating	O
to	O
positive	O
evidence	O
vital	O
for	O
predictions	O
,	O
being	O
analogous	O
to	O
rationales	O
-	O
snippets	O
thatAlgorithm	O
1	O
Critical	O
Component	O
Identification	O
Input	O
:	O
Validation	O
set	O
X	O
,	O
ground	O
truth	O
rationale	O
GT	O
,	O
old	O
model	O
M	O
O	O
,	O
new	O
model	O
M	O
N	O
,	O
number	O
of	O
blocks	O
K	O
Output	O
:	O
Critical	O
block	O
F	O
Ł←	O
∅	O
for	O
all	O
validation	O
sample	O
x	O
∈	O
X	O
do	O
:	O
IoUs	O
←	O
∅	O
AT	O
O	O
,	O
AT	O
N	O
←	O
[	O
M	O
O	O
(	O
x	O
)	O
,	O
M	O
N	O
(	O
x	O
)	O
]	O
for	O
j	O
=	O
1	O
,	O
K	O
do	O
:	O
RM	O
M	O
O	O
,	O
GT	O
←	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
with	O
highest	O
IoU	O
M	O
O	O
,	O
GT	O
RM	O
M	O
N	O
,	O
M	O
O	O
←	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
with	O
highest	O
IoU	O
M	O
N	O
,	O
M	O
O	O
APPEND(IoUs	O
,	O
max(IoU	O
M	O
N	O
,	O
M	O
O	O
)	O
)	O
end	O
for	O
b	O
←	O
arg	O
min	O
j	O
IoUs	O
APPEND(Ł	O
,	O
b	O
)	O
end	O
for	O
F	O
=	O
MODE(Ł	O
)	O
return	O
Fsupport	O
outputs	O
.	O

Hence	O
,	O
we	O
propose	O
another	O
algorithm	O
,	O
applying	O
to	O
heads	O
.	O

To	O
overcome	O
the	O
limitation	O
,	O
we	O
leverage	O
a	O
recent	O
unsupervised	O
rationale	O
generation	O
framework	O
,	O
In	B-MethodName
-	I-MethodName
vRat	I-MethodName
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
to	O
generate	O
rationales	O
as	O
substitutions	O
.	O

Originally	O
,	O
InvRat	B-MethodName
was	O
designed	O
for	O
single	O
-	O
input	O
tasks	O
such	O
as	O
sentiment	O
analysis	O
.	O

Table	O
1	O
contains	O
a	O
summary	O
of	O
the	O
datasets	O
,	O
dataset	O
sizes	O
,	O
and	O
metrics.•	O
BoolQ	B-DatasetName
(	O
Clark	O
et	O
al	O
.	O
,	O

2019	O
):	O
a	O
dataset	O
comprises	O
selected	O
passages	O
from	O
Wikipedia	O
and	O
naturally	O
occurring	O
yes	O
/	O
no	O
questions	O
to	O
be	O
answered	O
by	O
the	O
model.•	O
Movie	B-DatasetName
Reviews	I-DatasetName
(	O
Zaidan	O
and	O
Eisner	O
,	O
2008	O
):	O
a	O
dataset	O
composed	O
of	O
movie	O
reviews	O
.	O

It	O
contains	O
positive	O
and	O
negative	O
sentiment	O
labels	O
to	O
be	O
predicted	O
by	O
the	O
model.•	O
SciFact	B-DatasetName
(	O
Wadden	O
et	O
al	O
.	O
,	O

We	O
followed	O
the	O
best	O
LAMOL	B-MethodName
configuration	O
from	O
Sun	O
et	O
al	O
.	O
(	O

All	O
parameters	O
were	O
kept	O
at	O
the	O
default	O
values	O
.	O

2019	O
)	O
as	O
the	O
language	O
model	O
.	O

Each	O
task	O
was	O
trained	O
for	O
five	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

We	O
used	O
β	B-HyperparameterName
=	O
80	B-HyperparameterValue
,	O
i.e.	O
,	O
selecting	O
the	O
top	O
20	O
percentile	O
of	O
attention	O
scores	O
to	O
compare	O
with	O
ground	O
truth	O
rationales	O
.	O

As	O
the	O
ERASER	O
benchmark	O
has	O
an	O
average	O
ratio	O
of	O
rationale	O
tokens	O
to	O
document	O
tokens	O
of	O
around	O
9.4	O
%	O
,	O
we	O
allowed	O
rationale	B-HyperparameterName
selection	I-HyperparameterName
to	O
be	O
two	O
times	O
the	O
average	O
ratio	O
(	O
i.e.	O
,	O
20%).For	B-HyperparameterValue
InvRat	B-MethodName
,	O
we	O
opted	O
for	O
300	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
GloVe	B-HyperparameterName
embeddings	I-HyperparameterName
(	O
Pennington	O
et	O
al	O
.	O
,	O

The	O
generator	O
and	O
the	O
predictor	O
modules	O
of	O
InvRat	B-MethodName
were	O
based	O
on	O
1	B-HyperparameterValue
-	I-HyperparameterValue
layer	I-HyperparameterValue
bidirectional	B-HyperparameterName
gated	I-HyperparameterName
recurrent	I-HyperparameterName
units	I-HyperparameterName
(	O
Chung	O
et	O
al	O
.	O
,	O

2014	O
)	O
with	O
256	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
as	O
in	O
Chang	O
et	O
al	O
.	O
(	O

Maximum	B-HyperparameterName
model	I-HyperparameterName
input	I-HyperparameterName
was	O
set	O
to	O
1,024	B-HyperparameterValue
tokens	I-HyperparameterValue
.	O

This	O
section	O
reports	O
the	O
performance	O
of	O
Rationale	B-MethodName
LAMOL	I-MethodName
and	O
compares	O
it	O
with	O
LAMOL	B-MethodName
as	O
the	O
baseline	O
as	O
well	O
as	O
multitask	B-TaskName
learning	I-TaskName
,	O
which	O
is	O
considered	O
as	O
the	O
upper	O
bound	O
of	O
LL	B-TaskName
.	O

In	O
order	O
to	O
validate	O
if	O
component	O
freezing	O
truly	O
helps	O
reduce	O
catastrophic	O
forgetting	O
,	O
we	O
performed	O
partial	O
brute	O
force	O
block	O
-	O
level	O
freezing	O
on	O
each	O
task	O
permutation	O
to	O
approximately	O
determine	O
the	O
upper	O
bound	O
of	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
block	O
.	O

Brute	O
force	O
was	O
able	O
to	O
outperform	O
vanilla	O
LAMOL	B-MethodName
by	O
a	O
substantial	O
margin	O
of	O
3.68	B-MetricValue
%	I-MetricValue
,	O
only	O
1.36	B-MetricValue
%	I-MetricValue
from	O
the	O
multitask	O
upper	O
bound	O
.	O

This	O
suggests	O
that	O
component	O
freezing	O
is	O
able	O
to	O
further	O
nullify	O
the	O
effect	O
of	O
catastrophic	O
forgetting	O
from	O
LAMOL	B-MethodName
.	O

It	O
also	O
achieved	O
a	O
standard	O
deviation	O
of	O
only	O
2.3	B-MetricValue
%	I-MetricValue
compared	O
with	O
LAMOL	B-MethodName
's	O
5.28	B-MetricValue
%	I-MetricValue
.	O

A	O
sample	O
of	O
accuracy	B-MetricName
graphs	O
(	O
as	O
the	O
learning	O
progressed	O
)	O
of	O
the	O
compared	O
methods	O
,	O
with	O
the	O
BoolQ	B-DatasetName
→	O
SciFact	B-DatasetName
→	O
Movies	B-DatasetName
(	O
BSM	O
)	O
task	O
order	O
is	O
shown	O
in	O
Figure	O
4	O
from	O
top	O
to	O
bottom	O
,	O
respectively	O
.	O

As	O
the	O
first	O
task	O
,	O
BoolQ	B-DatasetName
was	O
not	O
really	O
affected	O
by	O
SciFact	B-DatasetName
,	O
but	O
encountered	O
a	O
heavy	O
drop	O
during	O
the	O
third	O
task	O
of	O
Movies	B-DatasetName
.	O

In	O
the	O
baseline	O
,	O
BoolQ	B-DatasetName
dropped	O
from	O
61	B-MetricValue
%	I-MetricValue
to	O
a	O
mere	O
6	B-MetricValue
%	I-MetricValue
,	O
while	O
only	O
rebounding	O
up	O
to	O
26	B-MetricValue
%	I-MetricValue
at	O
the	O
end	O
.	O

However	O
,	O
after	O
freezing	O
the	O
most	O
plastic	O
block	O
identified	O
by	O
partial	O
brute	O
forcing	O
,	O
BoolQ	B-DatasetName
dropped	O
from	O
62	B-MetricValue
%	I-MetricValue
to	O
15	B-MetricValue
%	I-MetricValue
,	O
and	O
rebounding	O
up	O
to	O
47	B-MetricValue
%	I-MetricValue
.	O

Comparatively	O
,	O
in	O
the	O
second	O
task	O
,	O
SciFact	B-DatasetName
encountered	O
a	O
smaller	O
drop	O
during	O
the	O
third	O
task	O
from	O
63	B-MetricValue
%	I-MetricValue
to	O
55	B-MetricValue
%	I-MetricValue
,	O
and	O
then	O
Green	O
background	O
refers	O
to	O
the	O
epochs	O
on	O
which	O
the	O
model	O
is	O
first	O
introduced	O
with	O
a	O
particular	O
task	O
.	O

In	O
this	O
figure	O
,	O
for	O
example	O
,	O
the	O
model	O
is	O
trained	O
on	O
Bool	B-DatasetName
-	I-DatasetName
Q	I-DatasetName
and	O
evaluated	O
on	O
all	O
the	O
three	O
tasks	O
during	O
epoch	O
1	O
-	O
5	O
.	O

rebounded	O
back	O
to	O
65	B-MetricValue
%	I-MetricValue
.	O

As	O
the	O
last	O
task	O
,	O
movies	B-DatasetName
was	O
not	O
affected	O
by	O
catastrophic	O
forgetting	O
.	O

Accuracy	B-MetricName
graphs	O
for	O
all	O
permutation	O
of	O
tasks	O
is	O
available	O
in	O
Appendix	O
6	O
from	O
which	O
we	O
make	O
several	O
observations	O
concerning	O
the	O
effect	O
of	O
task	O
orders	O
on	O
the	O
overall	O
performance:•	O
There	O
is	O
evidence	O
that	O
Movies	B-DatasetName
accelerate	O
the	O
forgetting	O
process	O
of	O
first	O
task	O
due	O
to	O
the	O
abrupt	O
change	O
in	O
data	O
distribution.•	O
However	O
,	O
the	O
performance	O
on	O
the	O
task	O
Movies	B-DatasetName
itself	O
is	O
barely	O
affected	O
by	O
the	O
task	O
order	O
.	O

We	O
attribute	O
it	O
to	O
the	O
low	O
difficulty	O
of	O
the	O
task.•	O
There	O
is	O
usually	O
no	O
interference	O
between	O
the	O
tasks	O
Bool	B-DatasetName
-	I-DatasetName
Q	I-DatasetName
and	O
SciFact	B-DatasetName
when	O
these	O
tasks	O
are	O
trained	O
in	O
adjacency	O
since	O
they	O
are	O
similar	O
.	O

Combined	O
with	O
time	O
required	O
for	O
CCI	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
required	O
approximately	O
2.4	O
times	O
more	O
time	O
than	O
vanilla	O
LAMOL	B-MethodName
to	O
completely	O
train	O
a	O
model	O
as	O
shown	O
in	O
Figure	O
3	O
.	O

From	O
Table	O
2	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
block	O
outperformed	O
LAMOL	B-MethodName
by	O
1.83	B-MetricValue
%	I-MetricValue
average	O
accuracy	B-MetricName
(	O
0.97	B-MetricValue
%	I-MetricValue
average	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
over	O
all	O
permutations	O
while	O
having	O
smaller	O
standard	O
deviation	O
,	O
indicating	O
that	O
it	O
is	O
also	O
more	O
robust	O
to	O
task	O
orders	O
.	O

Rational	B-MethodName
LAMOL	I-MethodName
head	O
was	O
able	O
to	O
match	O
or	O
outperform	O
LAMOL	B-MethodName
in	O
five	O
out	O
of	O
six	O
task	O
orders	O
,	O
but	O
the	O
significant	O
decrease	O
in	O
the	O
SBM	O
order	O
lowered	O
the	O
average	O
to	O
a	O
0.43	B-MetricValue
%	I-MetricValue
gain	O
(	O
and	O
a	O
slight	O
decrease	O
in	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
from	O
the	O
baseline	O
.	O

Upon	O
further	O
inspection	O
,	O
we	O
found	O
that	O
the	O
pseudo	O
-	O
samples	O
of	O
SciFact	B-DatasetName
contained	O
high	O
variance	O
in	O
quality	O
during	O
pseudodata	O
replay	O
.	O

In	O
addition	O
to	O
generation	O
token	O
mismatch	O
,	O
i.e.	O
,	O
a	O
situation	O
where	O
a	O
pseudo	O
-	O
sample	O
has	O
an	O
answer	O
token	O
from	O
a	O
wrong	O
task	O
,	O
the	O
low	O
volume	O
of	O
SciFact	B-DatasetName
training	O
data	O
affected	O
the	O
quality	O
of	O
the	O
pseudo	O
-	O
samples	O
generated	O
.	O

Without	O
the	O
SBM	O
drop	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
head	O
performed	O
comparatively	O
well	O
or	O
slightly	O
higher	O
with	O
the	O
block	O
-	O
level	O
.	O

Performing	O
a	O
one	O
-	O
tailed	O
paired	O
ttest	O
on	O
all	O
data	O
points	O
of	O
the	O
total	O
3	O
random	O
seeds	O
,	O
we	O
observed	O
that	O
block	O
-	O
level	O
freezing	O
is	O
able	O
to	O
win	O
against	O
the	O
original	O
LAMOL	B-MethodName
with	O
statistical	O
significance	O
(	O
p	O
-	O
value	O
of	O
0.023	O
and	O
0.042	O
for	O
block	O
-	O
level	O
and	O
generated	O
block	O
-	O
level	O
respectively	O
)	O
.	O

With	O
the	O
SBM	O
result	O
neglected	O
as	O
an	O
outlier	O
,	O
both	O
block	O
-	O
level	O
and	O
head	O
-	O
level	O
significantly	O
improved	O
the	O
results	O
compared	O
with	O
the	O
original	O
LAMOL	B-MethodName
(	O
p	O
-	O
value	O
of	O
0.015	O
,	O
0.014	O
,	O
0.010	O
,	O
0.049	O
for	O
block	O
-	O
level	O
,	O
generated	O
block	O
-	O
level	O
,	O
head	O
-	O
level	O
,	O
and	O
generated	O
headlevel	O
respectively	O
)	O
.	O

Even	O
though	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
outperformed	O
the	O
baseline	O
,	O
there	O
was	O
still	O
a	O
gap	O
from	O
the	O
brute	O
force	O
upper	O
bound	O
.	O

Due	O
to	O
the	O
difference	O
in	O
focus	O
between	O
human	O
and	O
machines	O
,	O
it	O
is	O
conceivable	O
that	O
the	O
rationales	O
generated	O
by	O
InvRat	B-MethodName
would	O
be	O
mostly	O
misaligned	O
with	O
human	O
rationales	O
.	O

This	O
is	O
shown	O
in	O
Table	O
3	O
,	O
where	O
the	O
F1	B-MetricName
scores	O
of	O
InvRat	B-MethodName
are	O
quite	O
low	O
when	O
compared	O
with	O
human	O
rationales	O
.	O

Figure	O
5	O
shows	O
an	O
example	O
of	O
generated	O
rationales	O
output	O
by	B-MethodName
InvRat	I-MethodName
compared	O
with	O
human	O
rationales	O
.	O

Despite	O
that	O
,	O
Generated	O
Rational	B-MethodName
LAMOL	I-MethodName
block	O
outperformed	O
both	O
Rational	B-MethodName
LAMOL	I-MethodName
and	O
LAMOL	B-MethodName
baseline	O
by	O
0.84	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
(	O
0.31	B-MetricValue
%	I-MetricValue
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
and	O
2.67	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
(	O
1.27	B-MetricValue
%	I-MetricValue
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
respectively	O
,	O
further	O
reducing	O
the	O
gap	O
to	O
Brute	O
Force	O
,	O
the	O
approximate	O
upper	O
bound	O
of	O
the	O
proposed	O
CCI	O
.	O

This	O
suggests	O
that	O
rationales	O
chosen	O
by	O
InvRat	B-MethodName
,	O
regardless	O
of	O
how	O
nonsensical	O
they	O
appear	O
,	O
still	O
carry	O
information	O
that	O
eliminates	O
the	O
need	O
for	O
human	O
rationales	O
.	O

The	O
results	O
are	O
consistent	O
with	O
Bao	O
et	O
al	O
.	O
(	O

Last	O
but	O
not	O
least	O
,	O
Figure	O
3	O
shows	O
that	O
the	O
process	O
of	O
generating	O
rationales	O
using	O
InvRat	B-MethodName
,	O
including	O
training	O
and	O
inference	O
,	O
contributed	O
only	O
marginally	O
,	O
about	O
15	O
minutes	O
,	O
to	O
the	O
total	O
time	O
used	O
in	O
the	O
training	O
process	O
.	O

To	O
effectively	O
retain	O
learned	O
knowledge	O
in	O
LL	B-TaskName
for	O
NLP	O
tasks	O
,	O
we	O
proposed	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
learning	O
framework	O
that	O
uses	O
rationales	O
to	O
identify	O
and	O
freeze	O
the	O
most	O
critical	O
components	O
of	O
the	O
model	O
while	O
being	O
trained	O
on	O
a	O
new	O
task	O
.	O

We	O
showed	O
that	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
able	O
to	O
outperform	O
LAMOL	B-MethodName
by	O
a	O
significant	O
margin	O
.	O

Overall	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
bridges	O
the	O
gap	O
between	O
LL	B-TaskName
in	O
NLP	O
with	O
model	O
understanding	O
through	O
rationales	O
,	O
exhibiting	O
potential	O
for	O
a	O
true	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
as	O
well	O
as	O
limiting	O
catastrophic	O
forgetting	O
.	O

However	O
,	O
we	O
uncover	O
a	O
striking	O
contrast	O
to	O
this	O
promise	O
:	O
across	O
5	O
models	O
and	O
4	O
datasets	O
on	O
the	O
task	O
of	O
visual	B-TaskName
question	I-TaskName
answering	I-TaskName
,	O
a	O
wide	O
variety	O
of	O
active	O
learning	O
approaches	O
fail	O
to	O
outperform	O
random	O
selection	O
.	O

Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
VQA	B-TaskName
)	O
,	O
the	O
task	O
of	O
answering	O
questions	O
about	O
Figure	O
1	O
:	O
We	O
systematically	O
evaluate	O
active	O
learning	O
on	O
VQA	B-TaskName
datasets	O
and	O
isolate	O
their	O
inability	O
to	O
perform	O
better	O
than	O
random	O
sampling	O
due	O
to	O
the	O
presence	O
of	O
collective	O
outliers	O
.	O

Unfortunately	O
,	O
today	O
's	O
VQA	B-TaskName
models	O
are	O
data	O
hungry	O
:	O
Their	O
performance	O
scales	O
monotonically	O
with	O
more	O
train	O
-	O
ing	O
data	O
(	O
Lu	O
et	O
al	O
.	O
,	O

2019	O
)	O
show	O
little	O
to	O
no	O
improvement	O
in	O
sample	O
efficiency	O
across	O
5	O
models	O
on	O
4	O
VQA	B-TaskName
datasets	O
-indeed	O
,	O
in	O
some	O
cases	O
performing	O
worse	O
than	O
randomly	O
selecting	O
data	O
to	O
label	O
.	O

We	O
mitigate	O
the	O
cold	O
start	O
challenge	O
of	O
needing	O
a	O
representative	O
initial	O
dataset	O
by	O
varying	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
seed	I-HyperparameterName
set	I-HyperparameterName
in	O
our	O
experiments	O
.	O

Finally	O
,	O
we	O
use	O
deep	B-MethodName
Bayesian	I-MethodName
active	I-MethodName
learning	I-MethodName
to	O
calibrate	O
model	O
uncertainty	O
to	O
high	O
-	O
dimensional	O
data	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011;Gal	O
and	O
Ghahramani	O
,	O
2016;.After	O
concluding	O
that	O
negative	O
results	O
are	O
consistent	O
across	O
all	O
experimental	O
conditions	O
,	O
we	O
investigate	O
active	O
learning	O
's	O
ineffectiveness	O
on	O
VQA	B-TaskName
as	O
a	O
data	O
problem	O
and	O
identify	O
the	O
existence	O
of	O
collective	O
outliers	O
(	O
Han	O
and	O
Kamber	O
,	O
2000	O
)	O
as	O
the	O
source	O
of	O
the	O
problem	O
.	O

While	O
global	O
outliers	O
deviate	O
from	O
the	O
rest	O
of	O
the	O
data	O
and	O
are	O
often	O
a	O
consequence	O
of	O
labeling	O
error	O
,	O
collective	O
outliers	O
cluster	O
together	O
;	O
they	O
may	O
not	O
individually	O
be	O
identifiable	O
as	O
outliers	O
but	O
collectively	O
deviate	O
from	O
other	O
examples	O
in	O
the	O
dataset	O
.	O

For	O
instance	O
,	O
VQA-2	B-DatasetName
(	O
Goyal	O
et	O
al	O
.	O
,	O

Similarly	O
,	O
GQA	B-DatasetName
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
asks	O
underspecified	O
questions	O
(	O
e.g.	O
,	O
"	O
what	O
is	O
the	O
person	O
wearing	O
?	O
"	O
which	O
can	O
have	O
multiple	O
correct	O
answers	O
)	O
.	O

Collective	O
outliers	O
are	O
not	O
specific	O
to	O
VQA	B-TaskName
,	O
but	O
can	O
similarly	O
be	O
found	O
in	O
many	O
open	O
-	O
ended	O
tasks	O
,	O
including	O
visual	O
navigation	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

This	O
allows	O
us	O
to	O
conclude	O
that	O
collective	O
outliers	O
are	O
,	O
indeed	O
,	O
responsible	O
for	O
the	O
ineffectiveness	O
of	O
active	O
learning	O
for	O
VQA	B-TaskName
.	O

Our	O
work	O
tests	O
the	O
utility	O
of	O
multiple	O
recent	O
active	O
learning	O
methods	O
on	O
the	O
open	O
-	O
ended	O
understanding	O
task	O
of	O
VQA	B-TaskName
.	O

We	O
draw	O
on	O
the	O
dataset	O
analysis	O
literature	O
to	O
identify	O
collective	O
outliers	O
as	O
the	O
bottleneck	O
hindering	O
active	O
learning	O
methods	O
in	O
this	O
setting	O
.	O

2003;Culotta	O
and	O
McCallum	O
,	O
2005	O
)	O
,	O
named	O
entity	O
recognition	O
(	O
Hachey	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
semantic	O
parsing	O
(	O
Dong	O
et	O
al	O
.	O
,	O

However	O
,	O
these	O
same	O
methods	O
struggle	O
to	O
outperform	O
a	O
random	O
baseline	O
when	O
applied	O
to	O
the	O
task	O
of	O
VQA	B-TaskName
(	O
Lin	O
and	O
Parikh	O
,	O
2017;Jedoui	O
et	O
al	O
.	O
,	O

To	O
study	O
this	O
discrepancy	O
,	O
we	O
systematically	O
apply	O
8	O
diverse	O
active	O
learning	O
methods	O
to	O
VQA	B-TaskName
,	O
including	O
methods	O
that	O
use	O
model	B-MethodName
uncertainty	I-MethodName
(	O
Abramson	O
and	O
Freund	O
,	O
2004;Collins	O
et	O
al	O
.	O
,	O

2009	O
)	O
,	O
Bayesian	B-MethodName
uncertainty	I-MethodName
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016;Kendall	O
and	O
Gal	O
,	O
2017	O
)	O
,	O
disagreement	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011	O
;	O
,	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018).Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
.	O

Progress	O
on	O
VQA	B-TaskName
has	O
been	O
heralded	O
as	O
a	O
marker	O
for	O
progress	O
on	O
general	O
open	O
-	O
ended	O
understanding	O
tasks	O
,	O
resulting	O
in	O
several	O
benchmarks	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2015;Ren	O
et	O
al	O
.	O
,	O

2019	O
)	O
versus	O
those	O
that	O
are	O
easy	O
to	O
learn	O
(	O
Bras	O
et	O
al	O
.	O
,	O

Unlike	O
prior	O
datasets	O
analyzed	O
by	O
Dataset	O
Maps	O
that	O
have	O
a	O
small	O
number	O
of	O
global	O
outliers	O
as	O
hard	O
examples	O
,	O
we	O
discover	O
that	O
VQA	B-TaskName
datasets	O
contain	O
copious	O
amounts	O
of	O
collective	O
outliers	O
,	O
which	O
are	O
difficult	O
or	O
even	O
impossible	O
for	O
models	O
to	O
learn	O
.	O

We	O
adopt	O
the	O
standard	O
pool	O
-	O
based	O
active	O
learning	O
setup	O
from	O
prior	O
work	O
(	O
Lewis	O
and	O
Gale	O
,	O
1994;Settles	O
,	O
2009;Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
consisting	O
of	O
a	O
model	O
M	O
,	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
of	O
labeled	O
examples	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
∈	O
D	O
seed	O
used	O
to	O
initialize	O
M	O
,	O
an	O
unlabeled	O
pool	O
of	O
data	O
D	O
pool	O
,	O
and	O
an	O
acquisition	O
function	O
A(x	O
,	O
M	O
)	O
.	O

We	O
follow	O
prior	O
work	O
to	O
simulate	O
an	O
oracle	O
using	O
existing	O
datasets	O
,	O
forming	O
D	O
seed	O
from	O
a	O
fixed	O
percentage	O
of	O
the	O
full	O
dataset	O
,	O
and	O
using	O
the	O
remainder	O
as	O
D	O
pool	O
Lin	O
and	O
Parikh	O
,	O
2017;Siddhant	O
and	O
Lipton	O
,	O
2018	O
)	O
.	O

We	O
re	O
-	O
train	O
M	O
after	O
each	O
acquisition	O
iteration	O
.	O

Prior	O
work	O
has	O
noted	O
the	O
impact	O
of	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
on	O
active	O
learning	O
performance	O
(	O
Lin	O
and	O
Parikh	O
,	O
2017;Misra	O
et	O
al	O
.	O
,	O

We	O
run	O
multiple	O
active	O
learning	O
evaluations	O
with	O
varying	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
(	O
ranging	O
from	O
5	B-HyperparameterValue
%	I-HyperparameterValue
to	I-HyperparameterValue
50	I-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
size	I-HyperparameterValue
)	O
.	O

We	O
keep	O
the	O
size	O
of	O
each	O
acquisition	O
batch	B-HyperparameterName
B	I-HyperparameterName
to	O
a	O
constant	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
overall	I-HyperparameterValue
pool	I-HyperparameterValue
size	I-HyperparameterValue
.	O

Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
VQA	B-TaskName
)	O
requires	O
reasoning	O
over	O
two	O
modalities	O
:	O
images	O
and	O
text	O
.	O

We	O
evaluate	O
with	O
a	O
representative	O
sample	O
of	O
existing	O
VQA	B-TaskName
models	O
,	O
including	O
the	O
following	O
:	O
2LogReg	B-MethodName
is	O
a	O
logistic	O
regression	O
model	O
that	O
uses	O
either	O
ResNet-101	O
or	O
Faster	O
R	O
-	O
CNN	O
image	O
features	O
with	O
mean	O
-	O
pooled	O
GloVe	O
question	O
embeddings	O
(	O
Pennington	O
et	O
al	O
.	O
,	O

Although	O
these	O
models	O
are	O
not	O
as	O
performant	O
as	O
the	O
subsequent	O
models	O
,	O
logistic	O
regression	O
has	O
been	O
effective	O
on	O
VQA	B-TaskName
(	O
Suhr	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
and	O
is	O
pervasive	O
in	O
the	O
active	O
learning	O
literature	O
(	O
Schein	O
and	O
Ungar	O
,	O
2007;Yang	O
and	O
Loog	O
,	O
2018;Mussmann	O
and	O
Liang	O
,	O
2018).LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
is	O
a	O
standard	O
model	O
introduced	O
with	O
VQA-1	B-TaskName
(	O
Agrawal	O
et	O
al	O
.	O
,	O

We	O
use	O
more	O
performant	O
ResNet-101	O
features	O
instead	O
of	O
the	O
original	O
VGGNet	O
features	O
as	O
our	O
visual	O
backbone	O
.	O

BUTD	B-MethodName
(	B-MethodName
Bottom	I-MethodName
-	I-MethodName
Up	I-MethodName
Top	I-MethodName
-	I-MethodName
Down	I-MethodName
Attention	I-MethodName
)	O
uses	O
object	O
-	O
based	O
features	O
in	O
tandem	O
with	O
attention	O
over	O
objects	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

BUTD	B-MethodName
won	O
the	O
2017	O
VQA	B-TaskName
Challenge	O
(	O
Teney	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
and	O
has	O
been	O
a	O
consistent	O
baseline	O
for	O
recent	O
work	O
in	O
VQA.LXMERT	B-TaskName
is	O
a	O
large	O
multi	O
-	O
modal	O
transformer	O
model	O
that	O
uses	O
BUTD	B-MethodName
's	O
object	O
features	O
and	O
contextualized	O
BERT	O
language	O
features	O
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
.	O

LXMERT	B-MethodName
is	O
pretrained	O
on	O
a	O
corpus	O
of	O
aligned	O
image	O
-	O
and	O
-	O
textual	O
data	O
spanning	O
MS	O
COCO	O
,	O
Visual	O
Genome	O
,	O
VQA-2	B-DatasetName
,	O
NLVR-2	O
,	O
and	O
GQA	B-DatasetName
(	O
Lin	O
et	O
al	O
.	O
,	O

3	O
Several	O
active	B-MethodName
learning	I-MethodName
methods	O
have	O
been	O
developed	O
to	O
account	O
for	O
different	O
aspects	O
of	O
the	O
machine	O
learning	O
training	O
pipeline	O
:	O
while	O
some	O
acquire	O
examples	O
with	O
high	O
aleotoric	O
uncertainty	O
(	O
Settles	O
,	O
2009	O
)	O
(	O
having	O
to	O
do	O
with	O
the	O
natural	O
uncertainty	O
in	O
the	O
data	O
)	O
or	O
epistemic	O
uncertainty	O
(	O
having	O
to	O
do	O
with	O
the	O
uncertainty	O
in	O
the	O
modeling	O
/	O
learning	O
process	O
)	O
,	O
others	O
attempt	O
to	O
acquire	O
examples	O
that	O
reflect	O
the	O
distribution	O
of	O
data	O
in	O
the	O
pool	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018	O
)	O
.	O

We	O
sample	O
a	O
diverse	O
set	O
of	O
these	O
methods	O
:	O
Random	B-MethodName
Sampling	I-MethodName
serves	O
as	O
our	O
baseline	O
passive	O
approach	O
for	O
acquiring	O
examples	O
.	O

Least	B-MethodName
Confidence	I-MethodName
acquires	O
examples	O
with	O
lowest	O
model	O
prediction	O
probability	O
(	O
Settles	O
,	O
2009).Entropy	B-MethodName
acquires	O
examples	O
with	O
the	O
highest	O
entropy	O
in	O
the	O
model	O
's	O
output	O
(	O
Settles	O
,	O
2009	O
)	O
.	O

MC	B-MethodName
-	I-MethodName
Dropout	I-MethodName
Entropy	I-MethodName
(	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
with	I-MethodName
Entropy	I-MethodName
acquisition	I-MethodName
)	O
acquires	O
examples	O
with	O
high	O
entropy	O
in	O
the	O
model	O
's	O
output	O
averaged	O
over	O
multiple	O
passes	O
through	O
a	O
neural	O
network	O
with	O
different	O
dropout	O
masks	O
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016	O
)	O
.	O

BALD	B-MethodName
(	O
Bayesian	B-MethodName
Active	I-MethodName
Learning	I-MethodName
by	I-MethodName
Disagreement	I-MethodName
)	O
builds	O
upon	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
by	O
proposing	O
a	O
decision	O
theoretic	O
objective	O
;	O
it	O
acquires	O
examples	O
that	O
maximise	O
the	O
decrease	O
in	O
expected	O
posterior	O
entropy	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

Since	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
operates	O
over	O
a	O
representation	O
space	O
(	O
and	O
not	O
an	O
output	O
distribution	O
,	O
like	O
prior	O
strategies	O
)	O
and	O
VQA	B-TaskName
models	O
operate	O
over	O
two	O
modalities	O
,	O
we	O
employ	O
three	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
variants	O
:	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Language	O
)	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Vision	O
)	O
operate	O
over	O
their	O
respective	O
representation	O
spaces	O
while	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Fused	O
)	O
operates	O
over	O
the	O
"	O
fused	O
"	O
vision	O
and	O
language	O
representation	O
space	O
.	O

Due	O
to	O
space	O
constraints	O
,	O
we	O
only	O
visualize	O
4	O
active	O
learning	O
strategies	O
-Least	B-MethodName
-	I-MethodName
Confidence	I-MethodName
,	O
BALD	B-MethodName
,	O
CoreSet	B-MethodName
-	O
Fused	O
,	O
and	O
the	O
Random	B-MethodName
Baseline	I-MethodName
-using	O
3	O
models	O
(	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
BUTD	B-MethodName
,	O
LXMERT	B-MethodName
)	O
.	O

4	O
Results	O
and	O
trends	O
are	O
consistent	O
across	O
the	O
different	O
acquisition	O
functions	O
,	O
models	O
and	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
(	O
see	O
the	O
appendix	O
for	O
results	O
with	O
other	O
models	O
,	O
acquisition	O
functions	O
,	O
and	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
)	O
.	O

Strategies	O
perform	O
on	O
par	O
with	O
or	O
worse	O
than	O
the	O
random	O
baseline	O
,	O
when	O
using	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
dataset	I-HyperparameterValue
as	O
the	O
seed	B-HyperparameterName
set.4	I-HyperparameterName
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-VQA-2	B-DatasetName
Random	O
Baseline	O
Least	O
-	O
Confidence	O
BALD	O
Core	O
-	O
Set	O
(	O
Fused	O
)	O
4	O
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
BUTD	B-MethodName
-VQA-2	B-DatasetName
4	O
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
LXMERT	B-MethodName
-VQA-2Figure	B-DatasetName
3	O
:	O
Results	O
for	O
the	O
full	O
VQA-2	B-DatasetName
dataset	O
,	O
also	O
using	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
dataset	I-HyperparameterValue
as	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
.	O

One	O
complexity	O
of	O
VQA	B-TaskName
is	O
the	O
size	O
of	O
the	O
output	O
space	O
and	O
the	O
number	O
of	O
examples	O
present	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2017	O
)	O
;	O
VQA-2	B-DatasetName
has	O
400k	O
training	O
examples	O
,	O
and	O
in	O
excess	O
of	O
3k	O
possible	O
answers	O
(	O
see	O
Table	O
1	O
)	O
.	O

To	O
ensure	O
our	O
results	O
and	O
conclusions	O
are	O
not	O
due	O
to	O
the	O
size	O
of	O
the	O
output	O
space	O
,	O
we	O
build	O
two	O
meaningful	O
,	O
but	O
narrow	O
-	O
domain	O
VQA	B-TaskName
datasets	O
from	O
subsets	O
of	O
VQA-2	B-DatasetName
.	O

VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
.	O

We	O
generate	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
by	O
compiling	O
a	O
list	O
of	O
20	O
popular	O
sports	O
(	O
e.g.	O
,	O
soccer	O
,	O
football	O
,	O
tennis	O
,	O
etc	O
.	O
)	O

in	O
VQA-2	B-DatasetName
,	O
and	O
restricting	O
the	O
set	O
of	O
questions	O
to	O
those	O
with	O
answers	O
in	O
this	O
list	O
.	O

We	O
picked	O
the	O
sports	O
categories	O
by	O
ranking	O
the	O
GloVe	O
vector	O
similarity	O
between	O
the	O
word	O
"	O
sports	O
"	O
to	O
answers	O
in	O
VQA-2	B-DatasetName
,	O
and	O
selected	O
the	O
20	O
most	O
commonly	O
occurring	O
answers	O
.	O

VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

We	O
generate	O
the	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
dataset	O
similarly	O
,	O
compiling	O
a	O
list	O
of	O
the	O
20	O
commonly	O
occurring	O
food	O
categories	O
by	O
GloVe	O
vector	O
similarity	O
to	O
the	O
word	O
"	O
food	O
.	O
"	O

Results	O
.	O

Figure	O
2	O
presents	O
results	O
for	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
with	O
an	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
restricted	O
to	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
500	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

The	O
appendix	O
reports	O
similar	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

For	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
Least	O
-	O
Confidence	O
appears	O
to	O
be	O
slightly	O
more	O
sample	O
efficient	O
,	O
while	O
all	O
other	O
strategies	O
perform	O
on	O
par	O
with	O
or	O
worse	O
than	O
random	O
.	O

For	O
BUTD	B-MethodName
,	O
all	O
methods	O
are	O
on	O
par	O
with	O
random	O
;	O
for	O
LXMERT	B-MethodName
,	O
they	O
perform	O
worse	O
than	O
random	O
.	O

Generally	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
active	O
learning	O
performance	O
varies	O
,	O
but	O
fails	O
to	O
outperform	O
random	O
acquisition	O
.	O

VQA-2	B-DatasetName
is	O
the	O
canonical	O
dataset	O
for	O
evaluating	O
VQA	O
models	O
(	O
Goyal	O
et	O
al	O
.	O
,	O

Unlike	O
traditional	O
VQA-2	B-DatasetName
evaluation	O
,	O
which	O
treats	O
the	O
task	O
as	O
a	O
multi	O
-	O
label	O
binary	O
classification	O
problem	O
,	O
we	O
follow	O
prior	O
active	O
learning	O
work	O
on	O
VQA	O
(	O
Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
which	O
formulates	O
it	O
as	O
a	O
multi	O
-	O
class	O
classification	O
problem	O
,	O
enabling	O
the	O
use	O
of	O
acquisition	O
functions	O
such	O
as	O
uncertainty	O
sampling	O
and	O
BALD	O
.	O

We	O
use	O
the	O
standard	O
GQA	B-DatasetName
training	O
set	O
of	O
943k	O
questions	O
,	O
900k	O
of	O
which	O
we	O
use	O
for	O
the	O
active	O
learning	O
pool	O
.	O

Figure	O
5	O
shows	O
results	O
on	O
GQA	B-DatasetName
using	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
of	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
90k	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

Despite	O
its	O
notable	O
differences	O
in	O
question	O
structure	O
to	O
VQA-2	B-DatasetName
,	O
active	O
learning	O
still	O
performs	O
on	O
par	O
with	O
or	O
slightly	O
worse	O
than	O
random	O
.	O

A	O
simple	O
question	O
remains	O
-why	O
?	O
One	O
hypothesis	O
is	O
that	O
sample	O
inefficiency	O
stems	O
from	O
the	O
data	O
itself	O
:	O
there	O
is	O
only	O
a	O
2	B-MetricValue
%	I-MetricValue
gain	O
in	O
validation	O
accuracy	B-MetricName
when	O
training	O
on	O
half	O
versus	O
the	O
whole	O
dataset	O
.	O

For	O
instance	O
(	O
Figure	O
7	O
)	O
,	O
in	O
VQA-2	B-DatasetName
,	O
we	O
identify	O
clusters	O
of	O
hard	O
-	O
to	O
-	O
learn	O
examples	O
that	O
require	O
optical	O
character	O
recognition	O
(	O
OCR	O
)	O
for	O
reasoning	O
about	O
text	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
first	O
word	O
on	O
the	O
black	O
car	O
?	O
"	O
)	O
;	O
another	O
cluster	O
requires	O
external	O
knowledge	O
to	O
answer	O
(	O
"	O
What	O
is	O
the	O
symbol	O
on	O
the	O
hood	O
often	O
associated	O
with	O
?	O
"	O
)	O
.	O

In	O
GQA	B-DatasetName
,	O
we	O
identify	O
different	O
clusters	O
of	O
collective	O
outliers	O
;	O
one	O
cluster	O
stems	O
from	O
innate	O
underspecification	O
(	O
e.g.	O
,	O
"	O
what	O
is	O
on	O
the	O
shelf	O
?	O
"	O
with	O
multiple	O
objects	O
present	O
on	O
the	O
shelf	O
)	O
;	O
another	O
cluster	O
requires	O
multiple	O
reasoning	O
hops	O
difficult	O
for	O
current	O
models	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
vehicle	O
that	O
is	O
driving	O
down	O
the	O
road	O
the	O
box	O
is	O
on	O
the	O
side	O
of	O
?	O
"	O
)	O
.	O

We	O
sample	O
100	O
random	O
"	O
hard	O
-	O
to	O
-	O
learn	O
"	O
examples	O
from	O
both	O
VQA-2	B-DatasetName
and	O
GQA	B-DatasetName
and	O
find	O
that	O
100	O
%	O
Ablating	O
Outliers	O
.	O

To	O
verify	O
that	O
collective	O
outliers	O
are	O
responsible	O
for	O
the	O
degradation	O
of	O
active	O
learning	O
performance	O
,	O
we	O
re	O
-	O
run	O
our	O
experiments	O
using	O
active	O
learning	O
pools	O
with	O
varying	O
numbers	B-HyperparameterName
of	I-HyperparameterName
outliers	I-HyperparameterName
removed	I-HyperparameterName
.	O

We	O
systematically	O
remove	O
examples	O
with	O
a	O
low	O
product	O
value	O
and	O
observe	O
how	O
active	O
learning	O
performance	O
changes	O
(	O
see	O
Figure	O
8).We	O
observe	O
a	O
2	O
-	O
3x	O
improvement	O
in	O
sample	O
efficiency	O
when	O
removing	O
50	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
entire	I-HyperparameterValue
data	I-HyperparameterValue
pool	I-HyperparameterValue
,	O
consisting	O
mainly	O
of	O
collective	O
outliers	O
(	O
Figure	O
8c	O
)	O
.	O

This	O
improvement	O
decreases	O
if	O
we	O
only	O
remove	O
25	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
Figure	O
8b	O
)	O
,	O
and	O
further	O
degrades	O
if	O
we	O
remove	O
only	O
10	B-HyperparameterValue
%	I-HyperparameterValue
(	O
Figure	O
8a	O
)	O
.	O

This	O
paper	O
asks	O
a	O
simple	O
question	O
-why	O
does	O
the	O
modern	O
neural	O
active	O
learning	O
toolkit	O
fail	O
when	O
applied	O
to	O
complex	O
,	O
open	O
ended	O
tasks	O
?	O
While	O
we	O
focus	O
on	O
VQA	B-TaskName
,	O
collective	O
outliers	O
are	O
abundant	O
in	O
tasks	O
such	O
as	O
natural	O
language	O
inference	O
(	O
Bowman	O
et	O
al	O
.	O
,	O

Other	O
work	O
learns	O
to	O
identify	O
novel	O
utterances	O
by	O
learning	O
to	O
intelligently	O
set	O
thresholds	O
in	O
representation	O
space	O
(	O
Karamcheti	O
et	O
al	O
.	O
,	O

Overall	O
,	O
we	O
hope	O
that	O
our	O
experiments	O
serve	O
as	O
a	O
catalyst	O
for	O
future	O
work	O
on	O
evaluating	O
active	O
learning	O
methods	O
with	O
inputs	O
drawn	O
from	O
open	O
-	O
world	O
datasets	O
.	O

Generally	O
,	O
any	O
combination	O
of	O
{	O
active	O
learning	O
strategy	O
×	O
model	O
×	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
×	O
analysis	O
/	O
acquisition	O
plot	O
}	O
is	O
present	O
in	O
this	O
paper	O
,	O
and	O
is	O
available	O
in	O
the	O
public	O
code	O
repository	O
.	O

For	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
,	O
we	O
base	O
our	O
implementation	O
off	O
of	O
this	O
repository	O
:	O
https://github.com/	O
Shivanshu	O
-	O
Gupta	O
/	O
Visual	B-TaskName
-	I-TaskName
Question	I-TaskName
-	I-TaskName
Answering	I-TaskName
,	O
while	O
for	O
the	O
Bottom	O
-	O
Up	O
Top	O
-	O
Down	O
Attention	O
Model	O
,	O
we	O
use	O
this	O
repository	O
:	O
https://github.com/	O
hengyuan	O
-	O
hu	O
/	O
bottom	O
-	O
up	O
-	O
attention	O
-	O
vqa	O
,	O
keeping	O
default	O
hyperparameters	O
the	O
same	O
.	O

Logistic	O
Regression	O
.	O

LXMERT	B-MethodName
.	O

As	O
mentioned	O
in	O
Section	O
3	O
,	O
the	O
default	O
LXMERT	B-MethodName
checkpoint	O
and	O
fine	O
-	O
tuning	O
code	O
made	O
publicly	O
available	O
in	O
Tan	O
and	O
Bansal	O
(	O
2019	O
)	O
(	O
associated	O
code	O
repository	O
:	O
https://github.com/	O
airsplay	O
/	O
lxmert	O
)	O
is	O
pretrained	O
on	O
data	O
from	O
VQA-2	B-DatasetName
and	O
GQA	B-DatasetName
,	O
leaking	O
information	O
that	O
could	O
substantially	O
affect	O
our	O
active	O
learning	O
results	O
.	O

To	O
mitigate	O
this	O
,	O
we	O
contacted	O
the	O
authors	O
,	O
who	O
kindly	O
provided	O
us	O
with	O
a	O
checkpoint	O
of	O
the	O
model	O
without	O
VQA	B-TaskName
pretraining	O
.	O

We	O
perform	O
a	O
coarse	O
grid	O
search	O
over	O
hyperparameters	O
,	O
using	O
the	O
LXMERT	B-MethodName
implementation	O
provided	O
by	O
HuggingFace	O
Transformers	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

For	O
our	O
implementations	O
of	O
the	O
deep	O
Bayesian	O
active	O
learning	O
methods	O
(	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
w/	I-MethodName
Entropy	I-MethodName
,	O
BALD	B-MethodName
)	O
,	O
we	O
follow	O
Gal	O
and	O
Ghahramani	O
(	O
2016	O
)	O
and	O
estimate	O
a	O
Dropout	O
distribution	O
via	O
test	O
-	O
time	O
dropout	O
,	O
running	O
multiple	O
forward	O
passes	O
through	O
our	O
neural	O
networks	O
,	O
with	O
different	O
,	O
randomly	O
sampled	O
Dropout	O
masks	O
.	O

In	O
the	O
original	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
active	O
learning	O
work	O
introduced	O
by	O
Sener	O
and	O
Savarese	O
(	O
2018	O
)	O
,	O
it	O
is	O
shown	O
that	O
Core	O
-	O
Set	O
selection	O
for	O
active	O
learning	O
can	O
be	O
reduced	O
to	O
a	O
version	O
of	O
the	O
k	O
-	O
centers	O
problem	O
,	O
which	O
can	O
be	O
solved	O
approximately	O
(	O
2	O
-	O
OPT	O
)	O
with	O
a	O
greedy	O
algorithm	O
.	O

While	O
we	O
can	O
run	O
this	O
out	O
completely	O
for	O
smaller	O
datasets	O
(	O
and	O
indeed	O
,	O
this	O
is	O
what	O
we	O
do	O
for	O
our	O
small	O
datasets	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
and	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
)	O
,	O
a	O
single	O
acquisition	O
iteration	O
for	O
a	O
large	O
dataset	O
for	O
the	O
full	O
VQA-2	B-DatasetName
dataset	O
takes	O
approximately	O
20	O
GPU	O
-	O
hours	O
on	O
the	O
resources	O
we	O
have	O
available	O
,	O
or	O
up	O
to	O
9	O
days	O
for	O
a	O
single	O
Core	O
-	O
Set	O
selection	O
run	O
.	O

For	O
GQA	B-DatasetName
,	O
performing	O
exact	O
Core	O
-	O
Set	O
selection	O
takes	O
at	O
least	O
twice	O
as	O
long	O
.	O

Then	O
,	O
rather	O
than	O
updating	O
distances	O
from	O
examples	O
in	O
our	O
acquired	O
set	O
to	O
points	O
in	O
our	O
pool	O
after	O
each	O
acquisitionx	O
,	O
we	O
delay	O
updates	O
,	O
instead	O
only	O
refreshing	O
the	O
distance	O
computation	O
every	O
2000	O
acquisitions	O
(	O
roughly	O
5	O
%	O
of	O
an	O
acquisition	O
batch	O
for	O
VQA-2	B-DatasetName
)	O
.	O

This	O
allows	O
us	O
to	O
report	O
results	O
for	O
Core	O
-	O
Set	O
selection	O
with	O
the	O
three	O
different	O
proposed	O
representations	O
(	O
Fused	O
,	O
Language	O
-	O
Only	O
,	O
Vision	O
-	O
Only	O
)	O
for	O
VQA-2	B-DatasetName
;	O
unfortunately	O
,	O
for	O
GQA	B-DatasetName
and	O
LXMERT	B-MethodName
(	O
due	O
to	O
the	O
high	O
cost	O
of	O
training	O
)	O
,	O
even	O
running	O
this	O
amortized	O
version	O
of	O
Core	O
-	O
Set	O
selection	O
is	O
prohibitive	O
,	O
so	O
we	O
report	O
a	O
subset	O
of	O
results	O
,	O
and	O
omit	O
the	O
rest	O
.	O

We	O
include	O
further	O
results	O
from	O
our	O
study	O
of	O
active	O
learning	O
applied	O
to	O
VQA	B-TaskName
,	O
including	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
(	O
not	O
included	O
in	O
the	O
main	O
body	O
)	O
,	O
active	O
learning	O
results	O
for	O
the	O
two	O
logistic	O
regression	O
models	O
-Log	O
-	O
Reg	O
(	O
ResNet-101	O
)	O
and	O
Log	O
-	O
Reg	O
(	O
Faster	O
R	O
-	O
CNN	O
)	O
,	O
as	O
well	O
as	O
with	O
the	O
4	O
acquisition	O
strategies	O
not	O
included	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
-Entropy	O
,	O
Monte	O
-	O
Carlo	O
Dropout	O
w/	O
Entropy	O
,	O
Core	O
-	O
Set	O
(	O
Language	O
)	O
,	O
and	O
Core	O
-	O
Set	O
(	O
Vision	O
)	O
.	O

Figure	O
9	O
shows	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
with	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
BUTD	B-MethodName
,	O
and	O
LXMERT	B-MethodName
models	O
,	O
with	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
comprised	O
of	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
.	O

The	O
results	O
are	O
mostly	O
similar	O
to	O
those	O
reported	O
in	O
the	O
paper	O
;	O
strategies	O
track	O
or	O
underperform	O
random	O
sampling	O
,	O
with	O
the	O
exception	O
of	O
Least	O
-	O
Confidence	O
for	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
-however	O
,	O
this	O
is	O
the	O
sole	O
exception	O
,	O
and	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
has	O
the	O
highest	O
training	O
variance	O
of	O
all	O
the	O
models	O
we	O
try	O
.	O

Figure	O
10	O
shows	O
active	O
learning	O
results	O
for	O
the	O
Lo	O
-	O
gReg	O
(	O
ResNet-101	O
)	O
model	O
on	O
VQA	O
-	O
Sports	O
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
,	O
and	O
VQA-2	B-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
50	B-HyperparameterValue
%	I-HyperparameterValue
)	O
.	O

Figure	O
11	O
presents	O
the	O
same	O
set	O
of	O
experiments	O
as	O
the	O
prior	O
section	O
,	O
except	O
with	O
the	O
LogReg	B-MethodName
(	O
Faster	O
R	O
-	O
CNN	O
)	O
model	O
.	O

Figure	O
12	O
presents	O
results	O
for	O
the	O
four	O
other	O
active	O
learning	O
strategies	O
we	O
implement	O
-Entropy	O
,	O
Monte	O
Carlo	O
Dropout	O
w/	O
Entropy	O
,	O
Core	O
-	O
Set	O
(	O
Language	O
)	O
,	O
and	O
Core	O
-	O
Set	O
(	O
Vision	O
)	O
-for	O
the	O
BUTD	B-MethodName
model	O
.	O

Results	O
are	O
across	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
,	O
and	O
VQA-2	B-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
50	B-HyperparameterValue
%	I-HyperparameterValue
)	O
-despite	O
the	O
unique	O
features	O
of	O
each	O
strategy	O
,	O
the	O
trends	O
remain	O
consistent	O
with	O
those	O
in	O
the	O
paper	O
.	O

Figure	O
12	O
:	O
Results	O
with	O
the	O
BUTD	B-MethodName
on	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
VQA-2	B-DatasetName
and	O
GQA	O
using	O
the	O
alternative	O
4	O
acquisition	O
strategies	O
not	O
included	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
.	O

Unsurprisingly	O
,	O
results	O
are	O
consistent	O
with	O
those	O
reported	O
in	O
the	O
paper	O
.	O

Given	O
that	O
the	O
map	O
for	O
GQA	B-DatasetName
is	O
similar	O
to	O
the	O
map	O
for	O
VQA-2	B-DatasetName
,	O
it	O
is	O
not	O
surprising	O
that	O
the	O
active	O
learning	O
acquisitions	O
follow	O
a	O
similar	O
trend	O
,	O
preferring	O
to	O
select	O
"	O
hard	O
-	O
to	O
-	O
learn	O
"	O
examples	O
.	O

We	O
are	O
also	O
grateful	O
to	O
Hao	O
Tan	O
for	O
providing	O
us	O
with	O
the	O
LXMERT	B-MethodName
checkpoint	O
trained	O
without	O
access	O
to	O
VQA	B-TaskName
datasets	O
,	O
as	O
well	O
as	O
for	O
general	O
LXMERT	B-MethodName
fine	O
-	O
tuning	O
pointers	O
.	O

In	O
this	O
work	O
,	O
we	O
explore	O
the	O
method	O
of	O
employing	O
contrastive	O
learning	O
to	O
improve	O
the	O
text	O
representation	O
from	O
the	O
BERT	B-MethodName
model	O
for	O
relation	B-TaskName
extraction	I-TaskName
.	O

The	O
key	O
knob	O
of	O
our	O
framework	O
is	O
a	O
unique	O
contrastive	O
pre	O
-	O
training	O
step	O
tailored	O
for	O
the	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
by	O
seamlessly	O
integrating	O
linguistic	O
knowledge	O
into	O
the	O
data	O
augmentation	O
.	O

Furthermore	O
,	O
we	O
investigate	O
how	O
large	O
-	O
scale	O
data	O
constructed	O
from	O
the	O
external	O
knowledge	O
bases	O
can	O
enhance	O
the	O
generality	O
of	O
contrastive	O
pre	O
-	O
training	O
of	O
BERT	B-MethodName
.	O

The	O
experimental	O
results	O
on	O
three	O
relation	O
extraction	O
benchmark	O
datasets	O
demonstrate	O
that	O
our	O
method	O
can	O
improve	O
the	O
BERT	B-MethodName
model	O
representation	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
addition	O
,	O
we	O
explore	O
the	O
interpretability	O
of	O
models	O
by	O
showing	O
that	O
BERT	B-MethodName
with	O
contrastive	O
pre	O
-	O
training	O
relies	O
more	O
on	O
rationales	O
for	O
prediction	O
.	O

Contrastive	O
learning	O
can	O
encode	O
general	O
properties	O
(	O
e.g.	O
invariance	O
)	O
in	O
the	O
learned	O
representation	O
while	O
it	O
is	O
relatively	O
hard	O
for	O
other	O
representation	O
learning	O
methods	O
to	O
achieve	O
(	O
Bengio	O
et	O
al	O
.	O
,	O

2020).Despite	O
its	O
advancement	O
,	O
contrastive	O
learning	O
has	O
not	O
been	O
well	O
studied	O
in	O
biomedical	O
natural	O
language	O
processing	O
(	O
BioNLP	O
)	O
,	O
especially	O
for	O
relation	B-TaskName
extraction	I-TaskName
(	O
RE	B-TaskName
)	O
tasks	O
.	O

Compared	O
to	O
computer	O
vision	O
,	O
it	O
is	O
more	O
challenging	O
to	O
design	O
a	O
general	O
and	O
efficient	O
data	O
augmentation	O
method	O
to	O
construct	O
positive	O
pairs	O
.	O

2019	O
)	O
.	O

Therefore	O
,	O
leveraging	O
contrastive	O
learning	O
in	O
the	O
large	O
pre	O
-	O
trained	O
language	O
models	O
to	O
learn	O
more	O
general	O
representation	O
for	O
RE	B-TaskName
tasks	O
remains	O
unexplored	O
.	O

To	O
bridge	O
this	O
gap	O
,	O
this	O
paper	O
presents	O
an	O
innovative	O
method	O
of	O
contrastive	O
pre	O
-	O
training	O
to	O
improve	O
the	O
language	O
model	O
representation	O
for	O
biomedical	B-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

As	O
the	O
main	O
difference	O
from	O
the	O
existing	O
contrastive	O
learning	O
framework	O
,	O
we	O
augment	O
the	O
datasets	O
for	O
RE	B-TaskName
tasks	O
by	O
randomly	O
changing	O
the	O
words	O
that	O
do	O
not	O
affect	O
the	O
relation	O
expression	O
.	O

We	O
hence	O
keep	O
words	O
on	O
SDP	O
fixed	O
during	O
the	O
data	O
augmentation	O
.	O

2016).To	O
verify	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
,	O
we	O
use	O
the	O
transformer	O
-	O
based	O
BERT	B-MethodName
model	O
as	O
a	O
backbone	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
evaluate	O
our	O
method	O
on	O
three	O
widely	O
studied	O
RE	B-TaskName
tasks	O
in	O
the	O
biomedical	O
domain	O
:	O
the	O
chemical	B-TaskName
-	I-TaskName
protein	I-TaskName
interactions	I-TaskName
(	O
ChemProt	B-TaskName
)	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
the	O
drug	B-TaskName
-	I-TaskName
drug	I-TaskName
interactions	I-TaskName
(	O
DDI	B-TaskName
)	O
(	O
Herrero	O
-	O
Zazo	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
and	O
the	O
protein	B-TaskName
-	I-TaskName
protein	I-TaskName
interactions	I-TaskName
(	O
PPI	B-TaskName
)	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2008	O
)	O
.	O

The	O
experimental	O
results	O
show	O
that	O
our	O
method	O
boosts	O
the	O
BERT	B-MethodName
model	O
performance	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
three	O
tasks	O
.	O

Interest	O
has	O
also	O
grown	O
in	O
designing	O
interpretable	O
BioNLP	O
models	O
that	O
are	O
both	O
plausible	O
(	O
accurate	O
)	O
and	O
rely	O
on	O
a	O
specific	O
part	O
of	O
the	O
input	O
(	O
faithful	O
rationales	O
)	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

1	O
)	O
We	O
propose	O
a	O
new	O
method	O
that	O
utilizes	O
contrastive	O
learning	O
to	O
improve	O
the	O
BERT	B-MethodName
model	O
on	O
biomedical	B-TaskName
relation	I-TaskName
extraction	I-TaskName
tasks	O
.	O
(	O

com	O
/	O
udel	O
-	O
biotm	O
-	O
lab	O
/	O
BERT	O
-	O
CLRE	O
.	O

The	O
contrastive	O
representation	O
has	O
all	O
the	O
properties	O
that	O
a	O
good	O
representation	O
should	O
have	O
:	O
1	O
)	O
Distributed	O
property	O
;	O
2	O
)	O
Abstraction	O
and	O
invariant	O
property	O
;	O
3	O
)	O
Disentangled	O
representation	O
(	O
Bengio	O
et	O
al	O
.	O
,	O

2020	O
)	O
propose	O
a	O
pre	O
-	O
trained	O
language	O
representation	O
model	O
(	O
CERT	B-MethodName
)	O
using	O
contrastive	O
learning	O
at	O
the	O
sentence	O
level	O
to	O
benefit	O
the	O
language	O
understanding	O
tasks	O
.	O

2020	O
)	O
propose	O
a	O
self	O
-	O
supervised	O
pretraining	O
framework	O
for	O
relation	O
extraction	O
to	O
explore	O
the	O
encoded	O
information	O
for	O
the	O
textual	O
context	O
and	O
entity	O
type	O
.	O

Relation	O
extraction	O
is	O
usually	O
seen	O
as	O
a	O
classification	O
problem	O
when	O
the	O
entity	O
mentions	O
are	O
given	O
in	O
the	O
text	O
.	O

Many	O
different	O
methods	O
have	O
been	O
proposed	O
to	O
solve	O
the	O
relation	B-TaskName
extraction	I-TaskName
problem	O
(	O
Culotta	O
and	O
Sorensen	O
,	O
2004;Sierra	O
et	O
al	O
.	O
,	O

2019;Radford	O
et	O
al	O
.	O
,	O

Among	O
all	O
the	O
language	O
models	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Several	O
BERT	B-MethodName
models	O
have	O
been	O
adapted	O
for	O
biomedical	O
domain	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
SciBERT	B-MethodName
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
Blue	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Peng	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

BioBERT	B-MethodName
,	O
SciBERT	B-MethodName
and	O
BlueBERT	B-MethodName
are	O
pre	O
-	O
trained	O
based	O
on	O
the	O
general	O
-	O
domain	O
BERT	B-MethodName
using	O
different	O
pre	O
-	O
training	O
data	O
.	O

In	O
contrast	O
,	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

employed	O
easy	O
data	O
augmentation	O
techniques	O
to	O
improve	O
model	O
performance	O
on	O
text	O
classification	O
tasks	O
.	O

As	O
the	O
preliminary	O
study	O
,	O
we	O
experiment	O
with	O
three	O
techniques	O
to	O
randomly	O
replace	O
the	O
tokens	O
to	O
generate	O
the	O
augmented	O
data	O
and	O
choose	O
the	O
best	O
one	O
for	O
our	O
contrastive	O
learning	O
method	O
:	O
1	O
)	O
Synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
,	O
2	O
)	O
Random	B-MethodName
swap	I-MethodName
(	O
RS	B-MethodName
)	O
,	O
and	O
3	O
)	O
Random	B-MethodName
deletion	I-MethodName
(	O
RD).Table	B-MethodName
1	O
gives	O
some	O
samples	O
after	O
applying	O
the	O
three	O
operations	O
on	O
a	O
sentence	O
from	O
the	O
PPI	B-TaskName
task	O
.	O

For	O
the	O
synonym	B-MethodName
replacement	I-MethodName
,	O
we	O
randomly	O
replace	O
n	O
words	O
with	O
their	O
synonyms	O
.	O

For	O
the	O
random	O
deletion	O
,	O
we	O
delete	O
some	O
words	O
with	O
the	O
probability	B-HyperparameterName
p.	I-HyperparameterName
The	O
probability	B-HyperparameterName
p	I-HyperparameterName
is	O
set	O
to	O
0.1	B-HyperparameterValue
in	O
our	O
experiments	O
and	O
the	O
parameter	O
n	O
for	O
SR	B-MethodName
and	O
RS	B-MethodName
is	O
calculated	O
by	O
p	O
×	O
l	O
,	O
where	O
l	O
is	O
the	O
length	O
of	O
the	O
sentence	O
.	O

To	O
examine	O
which	O
operation	O
performs	O
better	O
for	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
,	O
we	O
train	O
three	O
BERT	B-MethodName
models	O
using	O
the	O
three	O
types	O
of	O
augmented	O
data	O
(	O
combined	O
with	O
the	O
original	O
training	O
data	O
)	O
.	O

Table	O
4	O
shows	O
that	O
the	O
synonym	B-TaskName
replacement	I-TaskName
(	O
SR	B-TaskName
)	O
operation	O
achieves	O
the	O
best	O
performance	O
on	O
all	O
three	O
tasks	O
and	O
we	O
will	O
employ	O
this	O
operation	O
in	O
our	O
data	O
augmentation	O
module	O
in	O
our	O
contrastive	O
learning	O
experiments	O
(	O
We	O
will	O
further	O
discuss	O
it	O
in	O
Section	O
5.2	O
)	O
.	O

In	O
this	O
work	O
,	O
we	O
employ	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

As	O
demonstrated	O
in	O
(	O
Chen	O
et	O
al	O
.	O
,	O

We	O
follow	O
the	O
work	O
of	O
(	O
Chen	O
et	O
al	O
.	O
,	O

Therefore	O
,	O
we	O
have	O
2N	O
views	O
from	O
the	O
batch	O
.	O

Please	O
see	O
Algorithm	O
1	O
for	O
calculating	O
the	O
contrastive	O
loss	O
in	O
one	O
batch	O
.	O

Then	O
we	O
can	O
update	O
the	O
parameters	O
of	O
the	O
BERT	B-MethodName
model	O
and	O
projection	O
head	O
g	O
to	O
minimize	O
the	O
loss	O
L.	O
Input	O
:	O
encoder	O
f	O
(	O
BERT	B-MethodName
)	O
,	O
project	O
head	O
g	O
,	O
data	O
augmentation	O
module	O
,	O
data	O
batch	O
{	O
s	O
k	O
}	O
N	O
k=1	O
;	O
for	O
k=1	O
,	O
...	O
,	O
N	O
do	O
v	O
,	O
v	O
=	O
data_augment(s	O
k	O
)	O
;	O
z	O
2k−1	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
z	O
2k	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
end	O
L	O
=	O
1	O
2N	O
N	O
k=1	O
[	O
l(z	O
2k−1	O
,	O
z	O
2k	O
)	O
+	O
l(z	O
2k	O
,	O
z	O
2k−1	O
)	O
]	O
Figure	O
2	O
shows	O
the	O
training	O
procedure	O
of	O
our	O
framework	O
.	O

It	O
consists	O
of	O
three	O
stages	O
.	O

First	O
,	O
we	O
pretrain	O
the	O
BERT	B-MethodName
model	O
on	O
a	O
large	O
amount	O
of	O
unlabeled	O
data	O
from	O
a	O
specific	O
domain(e.g	O
.	O
,	O

Second	O
,	O
we	O
conduct	O
contrastive	O
pretraining	O
on	O
task	O
-	O
specific	O
data	O
as	O
a	O
continual	O
pretraining	O
step	O
after	O
the	O
domain	O
pre	O
-	O
training	O
of	O
BERT	B-MethodName
model	O
.	O

In	O
this	O
way	O
,	O
we	O
retain	O
the	O
learned	O
knowledge	O
from	O
general	O
pre	O
-	O
training	O
,	O
and	O
add	O
the	O
new	O
features	O
from	O
contrastive	O
learning	O
.	O

Finally	O
,	O
we	O
finetune	O
the	O
model	O
on	O
the	O
RE	B-TaskName
tasks	O
to	O
further	O
gain	O
taskspecific	O
knowledge	O
through	O
supervised	O
training	O
on	O
the	O
labeled	O
datasets	O
.	O

The	O
domain	O
pre	O
-	O
training	O
stage	O
follows	O
that	O
of	O
the	O
BERT	B-MethodName
using	O
the	O
masked	O
language	O
model	O
and	O
next	O
sentence	O
prediction	O
technique	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019).In	O
our	O
experiments	O
,	O
we	O
use	O
two	O
pre	O
-	O
trained	O
versions	O
for	O
the	O
biomedical	O
domain	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

Formally	O
,	O
assuming	O
a	O
curated	O
database	O
for	O
relation	O
r	O
contains	O
all	O
the	O
relevant	O
entities	O
and	O
text	O
,	O
we	O
consider	O
every	O
combination	O
of	O
the	O
entity	O
pairs	O
in	O
one	O
sentence	O
and	O
use	O
them	O
as	O
examples	O
for	O
this	O
relation	O
.	O

2006	O
)	O
are	O
utilized	O
for	O
DDI	B-TaskName
and	O
ChemProt	B-TaskName
,	O
respectively	O
.	O

As	O
discussed	O
before	O
,	O
we	O
will	O
utilize	O
the	O
BERT	B-MethodName
model	O
as	O
the	O
encoder	O
for	O
the	O
inputs	O
.	O

In	O
particular	O
,	O
we	O
will	O
employ	O
two	O
BERT	B-MethodName
models	O
pre	O
-	O
trained	O
for	O
the	O
biomedical	O
domain	O
in	O
our	O
experiments	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

The	O
statistics	O
of	O
these	O
datasets	O
is	O
shown	O
in	O
Table	O
2	O
.	O

For	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
tasks	O
,	O
we	O
employ	O
the	O
corpora	O
in	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2013	O
)	O
PubMedBERT	B-MethodName
model	O
(	O
Gu	O
et	O
al	O
.	O
,	O

We	O
utilize	O
the	O
AIMed	B-DatasetName
corpus	O
for	O
the	O
PPI	B-TaskName
task	O
,	O
and	O
we	O
will	O
employ	O
10	B-HyperparameterValue
-	O
fold	O
cross	O
-	O
validation	O
on	O
it	O
since	O
there	O
is	O
no	O
standard	O
split	O
of	O
training	O
and	O
test	O
.	O

PPI	B-TaskName
is	O
a	O
binary	O
classification	O
problem	O
,	O
and	O
we	O
will	O
use	O
the	O
standard	O
precision	B-MetricName
(	O
P	B-MetricName
)	O
,	O
recall	B-MetricName
(	O
R	B-MetricName
)	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
F	B-MetricName
)	O
to	O
measure	O
the	O
model	O
performance	O
.	O

However	O
,	O
the	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
tasks	O
are	O
multiclass	O
classification	O
problems	O
.	O

The	O
ChemProt	B-DatasetName
corpus	O
is	O
labeled	O
with	O
five	O
positive	O
classes	O
and	O
the	O
negative	O
class	O
:	O
CPR:3	O
,	O
CPR:4	O
,	O
CPR:5	O
,	O
CPR:6	O
,	O
CPR:9	O
and	O
negative	O
.	O

Similar	O
to	O
the	O
DDI	B-DatasetName
corpus	O
,	O
there	O
are	O
four	O
positive	O
labels	O
and	O
one	O
negative	O
label	O
:	O
AD	O
-	O
VICE	O
,	O
EFFECT	O
,	O
INT	O
,	O
MECHANISM	O
and	O
negative	O
.	O

The	O
models	O
for	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
will	O
be	O
evaluated	O
utilizing	O
micro	B-MetricName
precision	I-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
non	O
-	O
negative	O
classes	O
.	O

One	O
instance	O
of	O
relation	B-TaskName
extraction	I-TaskName
task	O
contains	O
two	O
parts	O
:	O
the	O
text	O
and	O
the	O
entity	O
mentions	O
.	O

In	O
order	O
to	O
make	O
the	O
BERT	B-MethodName
model	O
identify	O
the	O
positions	O
of	O
the	O
entities	O
,	O
we	O
replace	O
the	O
relevant	O
entity	O
names	O
with	O
predefined	O
tags	O
by	O
following	O
the	O
standard	O
pre	O
-	O
processing	O
step	O
for	O
relation	O
extraction	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

For	O
the	O
fine	O
-	O
tuning	O
of	O
the	O
BioBERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	B-HyperparameterValue
,	O
training	B-HyperparameterName
epoch	I-HyperparameterName
of	O
10	B-HyperparameterValue
,	O
and	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
128.During	B-HyperparameterValue
the	O
fine	O
-	O
tuning	O
of	O
PubMedBERT	B-MethodName
models	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	B-HyperparameterValue
,	O
training	B-HyperparameterName
epoch	I-HyperparameterName
of	O
10	B-HyperparameterValue
and	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
256	B-HyperparameterValue
are	O
utilized	O
.	O

In	O
the	O
contrastive	O
pre	O
-	O
training	O
step	O
of	O
the	O
BERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
with	O
the	O
fine	O
-	O
tuning	O
,	O
and	O
the	O
training	B-HyperparameterName
epoch	I-HyperparameterName
is	O
selected	O
from	O
[	O
2,4,6,8,10	B-HyperparameterValue
]	O
based	O
on	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O

If	O
there	O
is	O
no	O
development	O
set	O
(	O
e.g.	O
,	O
PPI	B-TaskName
task	O
)	O
,	O
we	O
will	O
use	O
6	B-HyperparameterValue
as	O
the	O
default	O
training	O
epoch	B-HyperparameterName
.	O

2020	O
)	O
,	O
we	O
utilize	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
and	O
128	B-HyperparameterValue
for	O
BioBERT	B-MethodName
and	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
respectively	O
.	O

In	O
addition	O
,	O
the	O
temperature	B-HyperparameterName
parameter	I-HyperparameterName
τ	I-HyperparameterName
is	O
set	O
to	O
0.1	B-HyperparameterValue
during	O
the	O
training	O
.	O

5.1	O
BERT	B-MethodName
model	O
performance	O
with	O
contrastive	O
pre	O
-	O
training	O
ever	O
,	O
contrastive	O
pre	O
-	O
training	O
on	O
human	O
-	O
labeled	O
dataset	O
only	O
improves	O
the	O
model	O
with	O
a	O
small	O
margin	O
.	O

Compared	O
with	O
the	O
BERT	B-MethodName
models	O
without	O
contrastive	O
pre	O
-	O
training	O
,	O
we	O
observe	O
an	O
averaged	O
F1	B-MetricName
score	I-MetricName
improvement	O
(	O
on	O
the	O
two	O
BERT	B-MethodName
models	O
)	O
of	O
1.2	B-MetricValue
%	I-MetricValue
,	O
1.2	B-MetricValue
%	I-MetricValue
,	O
and	O
0.85	B-MetricValue
%	I-MetricValue
on	O
ChemProt	B-DatasetName
,	O
DDI	B-DatasetName
,	O
and	O
PPI	B-DatasetName
datasets	O
,	O
respectively	O
.	O

Since	O
PubMedBERT	B-MethodName
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
model	O
on	O
these	O
three	O
tasks	O
,	O
we	O
further	O
improve	O
its	O
performance	O
by	O
adding	O
contrastive	O
pretraining	O
.	O

Table	O
4	O
shows	O
the	O
BERT	B-MethodName
model	O
performance	O
after	O
including	O
three	O
types	O
of	O
augmented	O
data	O
.	O

We	O
can	O
see	O
that	O
the	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
operation	O
yields	O
the	O
best	O
results	O
on	O
all	O
three	O
tasks	O
.	O

We	O
also	O
notice	O
that	O
the	O
augmented	O
data	O
from	O
the	O
random	B-MethodName
swap	I-MethodName
(	O
RS	B-MethodName
)	O
operation	O
hurt	O
the	O
model	O
performance	O
on	O
the	O
DDI	B-TaskName
and	O
PPI	B-TaskName
tasks	O
,	O
which	O
indicates	O
that	O
this	O
operation	O
might	O
change	O
the	O
relation	O
expression	O
in	O
the	O
sentence	O
.	O

Therefore	O
,	O
the	O
model	O
should	O
make	O
its	O
predictions	O
based	O
on	O
them	O
.	O

In	O
this	O
work	O
,	O
we	O
define	O
a	O
new	O
metric	O
to	O
measure	O
the	O
faithfulness	O
of	O
the	O
rationales	O
:	O
"	O
prediction	B-MetricName
shift	I-MetricName
"	O
.	O

If	O
the	O
model	O
predicts	O
one	O
test	O
example	O
(	O
nonnegative	O
)	O
with	O
label	O
L	O
t	O
,	O
but	O
changes	O
its	O
prediction	O
on	O
its	O
neighbor	O
(	O
the	O
augmented	O
data	O
point	O
)	O
with	O
another	O
label	O
L	O
t	O
,	O
we	O
will	O
say	O
a	O
"	O
prediction	O
shift	O
"	O
happens	O
(	O
In	O
Table	O
5	O
,	O
we	O
give	O
two	O
examples	O
of	O
pre	O
-	O
diction	O
shift	O
on	O
PubMedBERT	B-MethodName
model	O
)	O
.	O

To	O
generate	O
a	O
similar	O
set	O
(	O
with	O
test	O
set	O
)	O
for	O
the	O
measurement	O
of	O
"	O
prediction	O
shift	O
"	O
,	O
we	O
apply	O
the	O
same	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
technique	O
on	O
the	O
original	O
test	O
data	O
.	O

We	O
compare	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
on	O
two	O
types	O
of	O
BERT	B-MethodName
model	O
:	O
the	O
original	O
BERT	B-MethodName
and	O
the	O
BERT	B-MethodName
model	O
with	O
contrastive	O
pre	O
-	O
training	O
.	O

Table	O
6	O
illustrates	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
dramatically	O
reduce	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
.	O

Those	O
results	O
indicate	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
rely	O
more	O
on	O
the	O
information	O
of	O
shortest	O
dependency	O
path	O
for	O
prediction	O
,	O
a.k.a	O
.	O
,	O

From	O
another	O
perspective	O
,	O
the	O
results	O
in	O
Table	O
6	O
also	O
demonstrate	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
are	O
resilient	O
to	O
small	O
changes	O
of	O
the	O
inputs	O
,	O
which	O
means	O
the	O
models	O
are	O
more	O
robust	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
contrastive	O
pre	O
-	O
training	O
method	O
to	O
improve	O
the	O
text	O
representation	O
of	O
the	O
BERT	B-MethodName
model	O
.	O

The	O
experimental	O
results	O
demonstrate	O
that	O
our	O
method	O
outperforms	O
the	O
original	O
BERT	B-MethodName
model	O
on	O
three	O
relation	O
extraction	O
benchmarks	O
.	O

Additionally	O
,	O
our	O
method	O
shows	O
robustness	O
to	O
slightly	O
changed	O
inputs	O
over	O
the	O
BERT	B-MethodName
models	O
.	O

To	O
study	B-DatasetName
customer	I-DatasetName
service	I-DatasetName
dialogue	I-DatasetName
systems	I-DatasetName
in	I-DatasetName
more	I-DatasetName
realistic	I-DatasetName
settings	I-DatasetName
,	O
we	O
introduce	O
the	B-DatasetName
Action	I-DatasetName
-	I-DatasetName
Based	I-DatasetName
Conversations	I-DatasetName
Dataset	O
(	O
ABCD	B-DatasetName
)	O
,	O
a	O
fully	O
-	O
labeled	O
dataset	O
with	O
over	O
10	O
K	O
human	O
-	O
to	O
-	O
human	O
dialogues	O
containing	O
55	O
distinct	O
user	O
intents	O
requiring	O
unique	O
sequences	O
of	O
actions	O
constrained	O
by	O
policies	O
to	O
achieve	O
task	O
success	O
.	O

We	O
propose	O
two	O
additional	O
dialog	O
tasks	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
and	O
establish	O
a	O
series	O
of	O
baselines	O
involving	O
large	O
-	O
scale	O
,	O
pre	O
-	O
trained	O
language	O
models	O
on	O
this	O
dataset	O
.	O

Empirical	O
results	O
demonstrate	O
that	O
while	O
more	O
sophisticated	O
networks	O
outperform	O
simpler	O
models	O
,	O
a	O
considerable	O
gap	O
(	O
50.8	B-MetricValue
%	I-MetricValue
absolute	B-MetricName
accuracy	I-MetricName
)	O
still	O
exists	O
to	O
reach	O
human	O
-	O
level	O
performance	O
on	O
ABCD	O
.	O

2019;Rastogi	O
et	O
al	O
.	O
,	O

Figure	O
1	O
:	O
An	O
interaction	O
from	O
ABCD	B-DatasetName
(	O
left	O
)	O
starts	O
with	O
the	O
customer	O
receiving	O
a	O
prompt	O
(	O
top	O
right	O
)	O
to	O
ground	O
the	O
dialogue	O
.	O

See	O
Figure	O
1)To	O
more	O
closely	O
model	O
real	O
customer	O
service	O
agents	O
,	O
we	O
present	O
the	O
Action	B-DatasetName
-	I-DatasetName
Based	I-DatasetName
Conversations	I-DatasetName
Dataset	O
(	O
ABCD	B-DatasetName
)	O
consisting	O
of	O
10,042	O
conversations	O
containing	O
numerous	O
actions	O
with	O
precise	O
procedural	O
requirements	O
.	O

Thus	O
,	O
the	O
major	O
difference	O
between	O
ABCD	O
and	O
other	O
dialogue	O
datasets	O
,	O
such	O
as	O
Mul	B-DatasetName
-	I-DatasetName
tiWOZ	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

While	O
the	O
prevalent	O
data	O
collection	O
paradigm	O
involves	O
Wizard	O
-	O
of	O
-	O
Oz	O
techniques	O
,	O
our	O
situation	O
containing	O
asymmetric	O
speakers	O
compelled	O
the	O
design	B-TaskName
of	I-TaskName
a	I-TaskName
novel	I-TaskName
Expert	I-TaskName
Live	I-TaskName
Chat	I-TaskName
system	I-TaskName
.	O

Based	O
on	O
the	O
unique	O
aspects	O
of	O
ABCD	B-DatasetName
,	O
we	O
propose	O
two	O
new	O
tasks	O
.	O

To	O
start	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
(	O
AST	B-MetricName
)	O
closely	O
mirrors	O
the	O
format	O
of	O
Dialogue	O
State	O
Tracking	O
where	O
the	O
user	O
intent	O
is	O
inferred	O
from	O
the	O
dialogue	O
history	O
.	O

AST	B-MetricName
then	O
differs	O
since	O
the	O
correct	O
state	O
must	O
also	O
be	O
reconciled	O
with	O
the	O
requirements	O
outlined	O
in	O
the	O
Agent	O
Guidelines	O
.	O

As	O
a	O
second	O
task	O
,	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
(	O
CDS	B-MetricName
)	O
extends	O
this	O
notion	O
across	O
the	O
entire	O
conversation	O
.	O

Experiments	O
show	O
that	O
in	O
addition	O
to	O
conversation	O
history	O
,	O
conditioning	O
on	O
the	O
Agent	O
Guidelines	O
further	O
boosts	O
performance	O
,	O
with	O
top	O
models	O
relying	O
on	O
both	O
aspects	O
to	O
reach	O
31.9	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
.	O

Lastly	O
,	O
human	O
evaluation	O
reaches	O
82.7	B-MetricValue
%	I-MetricValue
,	O
demonstrating	O
ample	O
room	O
for	O
future	O
improvement	O
.	O

2	O
)	O
We	O
establish	O
a	O
new	O
technique	O
called	O
Expert	B-MethodName
Live	I-MethodName
Chat	I-MethodName
for	O
capturing	O
natural	O
dialogue	O
between	O
two	O
unequal	O
interlocutors	O
.	O
(	O

3	O
)	O
We	O
propose	O
two	O
metrics	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
for	O
measuring	O
dialogue	O
comprehension	O
with	O
policy	O
constraints	O
.	O

Unlike	O
opendomain	O
chatbots	O
often	O
built	O
for	O
entertainment	O
,	O
task	O
-	O
oriented	O
dialogue	O
systems	O
trained	O
on	O
such	O
datasets	O
are	O
intended	O
for	O
solving	O
user	O
issues	O
.	O

Rather	O
than	O
expanding	O
wider	O
,	O
ABCD	B-DatasetName
instead	O
focuses	O
deeper	O
by	O
increasing	O
the	O
count	O
and	O
diversity	O
of	O
actions	O
within	O
a	O
single	O
domain	O
.	O

All	O
actions	O
are	O
also	O
shown	O
.	O

The	O
closest	O
prior	O
work	O
to	O
ABCD	B-DatasetName
is	O
the	O
Schema	B-DatasetName
Guided	I-DatasetName
Dialogue	I-DatasetName
(	O
SGD	B-DatasetName
)	O
dataset	O
,	O
which	O
contains	O
dozens	O
of	O
API	O
calls	O
that	O
can	O
be	O
interpreted	O
as	O
individual	O
actions	O
sending	O
commands	O
to	O
a	O
SQL	O
engine	O
(	O
Rastogi	O
et	O
al	O
.	O
,	O

The	O
action	O
restrictions	O
within	O
ABCD	B-DatasetName
are	O
made	O
explicit	O
by	O
the	O
Agent	O
Guidelines	O
manual	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
task	O
setting	O
of	O
ABCD	B-DatasetName
by	O
following	O
along	O
with	O
the	O
example	O
dialog	O
shown	O
in	O
Figure	O
1	O
.	O

Accordingly	O
,	O
customers	O
within	O
ABCD	B-DatasetName
remain	O
oblivious	O
towards	O
what	O
values	O
apply	O
to	O
which	O
actions	O
,	O
nor	O
are	O
they	O
aware	O
that	O
actions	O
exist	O
in	O
first	O
place	O
.	O

ABCD	B-DatasetName
then	O
diverges	O
as	O
the	O
next	O
step	O
involves	O
interpreting	O
the	O
Agent	O
Guidelines	O
,	O
a	O
document	O
representing	O
the	O
internal	O
policies	O
of	O
a	O
company	O
in	O
the	O
online	O
retail	O
domain	O
(	O
See	O
Table	O
1	O
)	O
.	O

While	O
identifying	O
a	O
subflow	O
may	O
seem	O
straightforward	O
,	O
information	O
asymmetry	O
prevents	O
the	O
customers	O
from	O
directly	O
revealing	O
the	O
name	O
of	O
their	O
intent	O
.	O

In	O
our	O
case	O
,	O
the	O
agent	O
eventually	O
figures	O
out	O
the	O
correct	O
subflow	O
and	O
begins	O
to	O
execute	O
actions	O
,	O
which	O
consists	O
of	O
recording	O
values	O
given	O
by	O
the	O
customer	O
,	O
namely	O
the	O
customer	O
's	O
full	O
name	O
or	O
account	O
ID	O
in	O
order	O
to	O
[	O
Pull	O
up	O
Account	O
]	O
.	O

Dialogue	O
success	O
demands	O
that	O
agents	O
execute	O
a	O
chain	O
of	O
such	O
actions	O
in	O
the	O
right	O
order	O
with	O
the	O
right	O
values	O
,	O
while	O
simultaneously	O
engaging	O
the	O
customer	O
in	O
natural	O
language	O
conversation	O
.	O

To	O
start	O
,	O
the	O
permitted	O
actions	O
in	O
a	O
given	O
state	O
are	O
determined	O
not	O
only	O
by	O
Agent	O
Guidelines	O
,	O
but	O
also	O
by	O
the	O
user	O
's	O
desire	O
,	O
which	O
may	O
be	O
in	O
conflict	O
.	O

Keeping	O
the	O
bar	O
high	O
,	O
we	O
set	O
a	O
minimum	O
threshold	O
of	O
80	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
of	O
the	O
quiz	O
which	O
resulted	O
in	O
a	O
low	O
20	O
%	O
pass	O
rate	O
.	O

After	O
passing	O
the	O
exam	O
,	O
we	O
offered	O
the	O
answer	O
key	O
to	O
agents	O
which	O
further	O
improved	O
understanding	O
.	O

Rather	O
than	O
utilizing	O
Wizard	O
-	O
of	O
-	O
Oz	O
techniques	O
(	O
such	O
as	O
in	O
MultiWOZ	B-DatasetName
)	O
,	O
we	O
developed	O
Expert	O
Live	O
Chat	O
which	O
contains	O
three	O
unique	O
aspects:(1	O
)	O
Conversations	O
are	O
conducted	O
continuously	O
in	O
real	O
-	O
time	O
.	O
(	O

2	O
)	O
Users	O
involved	O
are	O
not	O
interchangeable	O
.	O
(	O

2018	O
)	O
to	O
produce	O
conversations	O
.	O

Despite	O
the	O
time	O
-	O
consuming	O
nature	O
,	O
some	O
datasets	O
have	O
produced	O
synchronous	O
dialogues	O
between	O
two	O
humans	O
(	O
Lewis	O
et	O
al	O
.	O
,	O

However	O
,	O
the	O
skill	O
sets	O
of	O
ABCD	B-DatasetName
workers	O
are	O
notably	O
unequal	O
,	O
exacerbating	O
the	O
matching	O
problem	O
.	O

With	O
these	O
changes	O
,	O
we	O
successfully	O
increased	O
the	O
pairing	O
rate	O
from	O
18	O
out	O
of	O
80	O
active	O
users	O
up	O
to	O
72	O
out	O
of	O
83	O
,	O
an	O
increase	O
of	O
nearly	O
400	O
%	O
,	O
while	O
maintaining	O
wait	B-HyperparameterName
times	I-HyperparameterName
under	O
10	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

In	O
particular	O
,	O
we	O
observed	O
the	O
greatest	O
gains	O
by	O
grounding	O
the	O
conversation	O
to	O
the	O
relatable	O
scenario	O
of	O
online	O
shopping	O
,	O
which	O
provided	O
immediate	O
context	O
to	O
participants	O
without	O
requiring	O
any	O
extra	O
training	O
.	O

We	O
validate	O
all	O
dialogues	O
to	O
pass	O
quality	O
thresholds	O
such	O
as	O
including	O
a	O
minimum	O
number	O
of	O
actions	O
and	O
avoiding	O
copy	O
/	O
paste	O
behavior	O
.	O

Unsurprisingly	O
,	O
ABCD	B-DatasetName
includes	O
more	O
actions	O
per	O
dialogue	O
than	O
other	O
datasets	O
,	O
by	O
at	O
least	O
a	O
factor	O
of	O
two	O
.	O

ABCD	B-DatasetName
also	O
contains	O
a	O
lower	O
absolute	O
number	O
of	O
tokens	O
,	O
but	O
also	O
has	O
the	O
highest	O
variance	O
in	O
the	O
number	O
of	O
tokens	O
per	O
turn	O
.	O
(	O

See	O
Table	O
2.)Since	O
each	O
subflow	O
represents	O
a	O
unique	O
customer	O
intent	O
,	O
ABCD	B-DatasetName
contains	O
55	O
user	O
intents	O
evenly	O
distributed	O
through	O
the	O
dataset	O
.	O

By	O
interpreting	O
buttons	O
as	O
domains	O
,	O
the	O
dataset	O
contains	O
30	O
domains	O
and	O
231	O
associated	O
slots	O
,	O
compared	O
to	O
7	O
domains	O
and	O
24	O
slots	O
within	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

Furthermore	O
,	O
the	O
unconstrained	O
nature	O
of	O
Expert	O
Live	O
Chat	O
allows	O
users	O
to	O
chat	O
with	O
each	O
other	O
in	O
a	O
free	O
-	O
form	O
style	O
.	O

Dialogues	O
exhibited	O
normal	O
texting	O
behavior	O
such	O
as	O
users	O
speaking	O
for	O
many	O
turns	O
in	O
a	O
row	O
or	O
fixing	O
typos	O
with	O
a	O
star	O
in	O
the	O
subsequent	O
line	O
.	O

Other	O
examples	O
of	O
linguistic	O
phenomenon	O
can	O
be	O
observed	O
in	O
Table	O
5	O
.	O

The	O
novel	O
features	O
in	O
ABCD	B-DatasetName
brings	O
two	O
new	O
dialog	O
tasks	O
,	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
and	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

We	O
also	O
build	O
baseline	O
systems	O
that	O
are	O
variants	O
of	O
standard	O
dialogue	O
models	O
and	O
report	O
their	O
results	O
on	O
ABCD	B-DatasetName
.	O

Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
(	O
AST	B-TaskName
)	O
aims	O
at	O
detecting	O
the	O
pertinent	O
intent	O
by	O
interpreting	O
customer	O
utterances	O
while	O
taking	O
into	O
account	O
constraints	O
from	O
the	O
Agent	O
Guidelines	O
,	O
an	O
aspect	O
not	O
considered	O
in	O
traditional	O
dialog	B-TaskName
state	I-TaskName
tracking	I-TaskName
(	O
DST	B-TaskName
)	O
.	O

In	O
contrast	O
,	O
the	O
appropriate	O
next	O
step	O
within	O
AST	B-TaskName
is	O
governed	O
by	O
the	O
Agent	O
Guidelines	O
,	O
which	O
might	O
require	O
[	O
Verify	O
Identity	O
]	O
of	O
the	O
customer	O
first	O
,	O
or	O
any	O
number	O
of	O
other	O
actions	O
,	O
before	O
executing	O
the	O
password	O
reset	O
.	O

Despite	O
the	O
similar	O
structure	O
,	O
AST	B-TaskName
deviates	O
from	O
DST	B-TaskName
since	O
predicting	O
the	O
right	O
action	O
requires	O
not	O
only	O
parsing	O
the	O
customer	O
utterance	O
,	O
but	O
also	O
adhering	O
to	O
Agent	O
Guidelines	O
.	O

To	O
measure	O
a	O
model	O
's	O
ability	O
to	O
comprehend	O
such	O
nuanced	O
situations	O
,	O
we	O
adopt	O
overall	O
accuracy	O
as	O
the	O
evaluation	O
metric	O
for	O
AST	B-TaskName
.	O

Since	O
the	O
appropriate	O
action	O
often	O
depends	O
on	O
the	O
situation	O
,	O
we	O
propose	O
the	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
(	O
CDS	B-TaskName
)	O
task	O
to	O
measure	O
a	O
model	O
's	O
ability	O
to	O
understand	O
actions	O
in	O
context	O
.	O

Whereas	O
AST	O
assumes	O
an	O
action	O
occurs	O
in	O
the	O
current	O
turn	O
,	O
CDS	B-TaskName
gives	O
an	O
agent	O
the	O
additional	O
options	O
of	O
responding	O
with	O
an	O
utterance	O
or	O
ending	O
the	O
conversation	O
.	O

Formally	O
,	O
given	O
C	O
t	O
=	O
[	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O

Therefore	O
,	O
CDS	B-TaskName
is	O
scored	O
using	O
a	O
variation	O
on	O
Cascading	O
Evaluation	O
(	O
Suhr	O
et	O
al	O
.	O
,	O

We	O
also	O
run	O
several	O
baselines	O
on	O
these	O
new	O
tasks	O
.	O

We	O
break	O
down	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
(	O
AST	B-TaskName
)	O
into	O
two	O
sub	O
-	O
problems	O
,	O
button	O
-	O
slot	O
prediction	O
and	O
value	O
-	O
filling	O
.	O

The	O
gate	O
is	O
conditioned	O
on	O
the	O
hidden	O
state	O
h	O
enc	O
as	O
well	O
as	O
a	O
learned	O
context	O
vector	O
c	O
i	O
.	O

When	O
the	O
next	O
step	O
is	O
to	O
take	O
action	O
,	O
the	O
AST	B-TaskName
model	O
is	O
reused	O
to	O
determine	O
the	O
button	O
-	O
slot	O
and	O
value	O
.	O

We	O
performed	O
experiments	O
on	O
the	O
two	O
newly	O
proposed	O
tasks	O
,	O
AST	B-TaskName
and	O
CDS	B-TaskName
.	O

AST	B-TaskName
consists	O
of	O
two	O
subtasks	O
,	O
button	O
-	O
slot	O
prediction	O
and	O
value	O
-	O
filling	O
,	O
while	O
CDS	O
builds	O
on	O
this	O
with	O
three	O
additional	O
subtasks	O
of	O
next	O
step	O
selection	O
,	O
utterance	O
ranking	O
,	O
and	O
intent	O
classification	O
.	O

For	O
both	O
tasks	O
,	O
we	O
experimented	O
with	O
two	O
types	O
of	O
frameworks	O
,	O
a	O
pipeline	O
version	O
and	O
an	O
end	O
-	O
to	O
-	O
end	O
version	O
.	O

2020).The	O
pipeline	O
model	O
uses	O
a	O
BERT	B-MethodName
model	O
trained	O
with	O
the	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
.	O

To	O
test	O
the	O
performance	O
of	O
different	O
pretrained	O
models	O
under	O
the	O
end	O
-	O
to	O
-	O
end	O
framework	O
,	O
we	O
experiment	O
with	O
three	O
additional	O
encoders	O
,	O
Al	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
RoBERTa	B-MethodName
-	I-MethodName
Large	I-MethodName
.	O

AlBERT	B-MethodName
model	O
has	O
an	O
inter	O
-	O
sentence	O
coherence	O
task	O
and	O
a	O
lighter	O
memory	O
footprint	O
compared	O
to	O
BERT	B-MethodName
,	O
while	O
RoBERTa	B-MethodName
model	O
has	O
substantially	O
more	O
data	O
and	O
hyper	O
-	O
parameter	O
tuning	O
in	O
pretraining	O
than	O
BERT.In	B-MethodName
the	O
future	O
,	O
we	O
also	O
plan	O
to	O
include	O
GPT	O
-	O
based	O
models	O
,	O
such	O
as	O
DialoGPT	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O

As	O
hinted	O
at	O
in	O
prior	O
works	O
(	O
Liang	O
et	O
al	O
.	O
,	O

In	O
the	O
AST	B-TaskName
task	O
,	O
we	O
found	O
steady	O
improvements	O
as	O
we	O
move	O
from	O
the	O
older	O
to	O
the	O
newer	O
models	O
with	O
vanilla	O
BERT	B-MethodName
at	O
59.5	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
and	O
RoBERTa	B-MethodName
doing	O
the	O
best	O
at	O
65.8	B-MetricValue
%	I-MetricValue
.	O

For	O
the	O
CDS	B-TaskName
task	O
,	O
we	O
found	O
a	O
similar	O
trend	O
where	O
RoBERTa	B-MethodName
-	I-MethodName
Large	I-MethodName
outperforms	O
BERT	B-MethodName
,	O
but	O
only	O
by	O
a	O
mere	O
0.6	B-MetricValue
%	I-MetricValue
.	O

Separately	O
,	O
we	O
evaluate	O
CDS	B-TaskName
subtask	O
difficulty	O
by	O
asking	O
human	O
volunteers	O
to	O
select	O
the	O
correct	O
label	O
from	O
a	O
list	O
of	O
possible	O
options	O
.	O

As	O
an	O
example	O
,	O
workers	O
would	O
be	O
presented	O
with	O
55	O
different	O
classes	O
for	O
Intent	B-TaskName
Classification	I-TaskName
and	O
asked	O
to	O
choose	O
the	O
right	O
one	O
.	O

On	O
the	O
other	O
hand	O
,	O
human	O
evaluation	O
for	O
the	O
overall	O
CDS	B-TaskName
task	O
was	O
judged	O
by	O
measuring	O
the	O
success	O
rate	O
in	O
a	O
standard	O
conversational	O
scenarios	O
where	O
behavioral	O
instincts	O
are	O
activated	O
,	O
so	O
humans	O
were	O
able	O
to	O
excel	O
on	O
this	O
environment	O
.	O

We	O
perform	O
an	O
ablation	O
study	O
to	O
test	O
the	O
significance	O
of	O
the	O
key	O
features	O
in	O
ABCD	B-DatasetName
.	O

See	O
Appendix	O
E	O
for	O
details.)We	O
observe	O
that	O
supplying	O
the	O
intent	O
information	O
to	O
the	O
BERT	B-MethodName
model	O
causes	O
a	O
noticeable	O
boost	O
in	O
dialog	O
success	O
,	O
bringing	O
the	O
score	O
to	O
32.3	B-MetricValue
%	I-MetricValue
.	O

However	O
,	O
augmenting	O
the	O
model	O
with	O
knowledge	O
of	O
the	O
guidelines	O
unexpectedly	O
dropped	O
performance	O
down	O
to	O
30.6	B-MetricValue
%	I-MetricValue
.	O

This	O
model	O
reached	O
the	O
peak	O
observed	O
performance	O
of	O
32.7	B-TaskName
%	I-TaskName
,	O
highlighting	O
the	O
importance	O
of	O
both	O
components	O
.	O

In	O
conclusion	O
,	O
we	O
have	O
presented	O
ABCD	B-DatasetName
which	O
includes	O
over	O
10	O
K	O
dialogues	O
that	O
incorporate	O
procedural	O
,	O
dual	O
-	O
constrained	O
actions	O
.	O

We	O
found	O
that	O
pre	O
-	O
trained	O
models	O
perform	O
decent	O
on	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
but	O
there	O
is	O
a	O
large	O
gap	O
between	O
humans	O
agents	O
and	O
the	O
top	O
systems	O
for	B-TaskName
Cascading	I-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

By	O
grounding	O
dialogues	O
to	O
in	O
-	O
depth	O
scenarios	O
with	O
explicit	O
policies	O
,	O
we	O
hope	O
to	O
have	O
pushed	O
towards	O
a	O
better	O
understanding	O
of	O
dialogue	O
success	O
.	O

Optimizing	O
agents	O
performance	O
can	O
be	O
split	O
into	O
preparation	O
before	O
the	O
HIT	B-TaskName
(	O
Human	B-TaskName
Intelligence	I-TaskName
Task	I-TaskName
)	O
,	O
improving	O
HIT	B-TaskName
itself	O
,	O
and	O
ongoing	O
training	O
afterwards	O
.	O

Starting	O
with	O
the	O
pre	O
-	O
HIT	B-TaskName
phase	O
,	O
the	O
major	O
steps	O
largely	O
center	O
around	O
multiple	O
rounds	O
of	O
qualifications	O
to	O
filter	O
for	O
the	O
highest	O
quality	O
workers	O
available	O
.	O

During	O
the	O
post	O
-	O
HIT	B-TaskName
phase	O
,	O
effort	O
shifts	O
to	O
ensuring	O
that	O
each	O
worker	O
becomes	O
increasingly	O
comfortable	O
with	O
the	O
task	O
.	O

During	O
-	O
HIT	B-TaskName
Phase	O
The	O
HIT	B-TaskName
itself	O
was	O
priced	O
at	O
$	O
1.50	O
for	O
completing	O
the	O
conversation	O
with	O
an	O
extra	O
$	O
1.00	O
bonus	O
for	O
identifying	O
the	O
correct	O
customer	O
intent	O
at	O
the	O
end	O
-	O
of	O
-	O
chat	O
survey	O
.	O

However	O
,	O
by	O
encouraging	O
agents	O
to	O
focus	O
on	O
the	O
customer	O
intent	O
,	O
they	O
were	O
forced	O
to	O
peruse	O
the	O
Agent	O
Guidelines	O
for	O
the	O
associated	O
subflow	O
.	O

Post	O
-	O
HIT	B-TaskName
Phase	O
For	O
ongoing	O
training	O
,	O
we	O
began	O
producing	O
small	O
lists	O
of	O
bulletpoints	O
to	O
the	O
agents	O
on	O
areas	O
they	O
could	O
improve	O
on	O
.	O

Let	O
us	O
consider	O
the	O
number	O
of	O
agents	O
available	O
as	O
A	O
and	O
the	O
number	O
of	O
customers	O
available	O
as	O
C.	O
Given	O
budget	O
constraints	O
,	O
we	O
can	O
only	O
pay	O
some	O
maximum	O
number	O
of	O
workers	O
M	O
.	O

Although	O
the	O
customer	O
side	O
of	O
ABCD	B-DatasetName
is	O
a	O
simpler	O
task	O
,	O
there	O
is	O
still	O
a	O
minimum	O
bar	O
to	O
be	O
met	O
to	O
prevent	O
(	O
a	O
)	O
customers	O
who	O
spam	O
with	O
random	O
text	O
(	O
b	O
)	O
customers	O
who	O
fake	O
scenarios	O
or	O
(	O
c	O
)	O
customers	O
who	O
hoard	O
HITs	O
and	O
never	O
show	O
up	O
to	O
the	O
chat	O
.	O

In	O
a	O
typical	O
scenario	O
,	O
a	O
customer	O
might	O
leave	O
the	O
tab	O
open	O
to	O
work	O
on	O
other	O
tasks	O
,	O
but	O
when	O
they	O
are	O
eventually	O
paired	O
,	O
the	O
customer	O
is	O
often	O
busy	O
doing	O
something	O
else	O
,	O
leaving	O
the	O
chat	O
to	O
flounder	O
.	O

To	O
resolve	O
this	O
situation	O
,	O
we	O
begin	O
with	O
the	O
maximum	O
number	O
of	O
workers	O
M	O
as	O
the	O
starting	O
constraint	O
given	O
a	O
fixed	O
budget	O
.	O

If	O
we	O
qualify	O
too	O
many	O
workers	O
,	O
then	O
we	O
will	O
not	O
have	O
enough	O
budget	O
left	O
for	O
the	O
actual	O
conversations	O
,	O
so	O
instead	O
we	O
qualify	O
workers	O
in	O
mini	O
-	O
batches	O
.	O

We	O
also	O
establish	O
an	O
exam	O
that	O
is	O
purposely	O
very	O
easy	O
(	O
to	O
minimize	O
costs	O
)	O
,	O
but	O
just	O
hard	O
enough	O
to	O
deter	O
bots	O
and	O
spammers	O
.	O

To	O
raise	O
the	O
likelihood	O
that	O
the	O
customer	O
will	O
show	O
up	O
,	O
we	O
include	O
a	O
question	O
in	O
the	O
quiz	O
which	O
simply	O
asks	O
when	O
the	O
customer	O
is	O
available	O
to	O
perform	O
the	O
HIT	O
.	O

We	O
believe	O
our	O
modifications	O
have	O
only	O
scratched	O
the	O
surface	O
and	O
that	O
improving	O
the	O
user	O
experience	O
for	O
data	O
collection	O
offers	O
an	O
interesting	O
line	O
of	O
HCI	B-TaskName
research	O
to	O
explore	O
.	O

To	O
motivate	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
(	O
CDS	B-TaskName
)	O
over	O
typical	O
other	O
accuracy	O
metrics	O
,	O
consider	O
the	O
scenario	O
where	O
a	O
model	O
gets	O
80	B-MetricValue
%	I-MetricValue
of	O
turns	O
correct	O
,	O
while	O
still	O
achieving	O
0	O
%	O
accuracy	O
on	O
the	O
conversation	O
level	O
because	O
it	O
always	O
messes	O
up	O
somewhere	O
right	O
at	O
the	O
end	O
of	O
the	O
dialogue	O
.	O

Instead	O
,	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
creates	O
an	O
evaluation	O
example	O
for	O
the	O
remainder	O
of	O
each	O
conversation	O
starting	O
from	O
each	O
turn	O
.	O

Now	O
imagine	O
the	O
model	O
consistently	O
predicted	O
turn	O
C	O
incorrectly	O
,	O
and	O
everything	O
else	O
correct	O
.	O

Averaging	O
across	O
all	O
turns	O
would	O
yield	O
a	O
final	O
cascading	B-MetricName
success	I-MetricName
rate	I-MetricName
of	O
45.8	B-MetricValue
%	I-MetricValue
.	O

A	O
turn	O
-	O
based	O
metric	O
would	O
yield	O
75	B-MetricValue
%	I-MetricValue
while	O
a	O
conversation	O
-	O
based	O
metric	O
would	O
yield	O
0	B-MetricValue
%	I-MetricValue
.	O

Thus	O
,	O
CDS	B-TaskName
allows	O
a	O
model	O
to	O
earn	O
partial	O
credit	O
on	O
what	O
it	O
has	O
learned	O
without	O
severe	O
penalties	O
in	O
either	O
direction	O
.	O

When	O
training	O
the	O
best	O
model	O
for	B-TaskName
Action	I-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
we	O
ended	O
up	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-5	B-HyperparameterValue
,	B-HyperparameterName
hidden	I-HyperparameterName
dimension	I-HyperparameterName
of	O
1024	B-HyperparameterValue
,	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.05	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
10	B-HyperparameterValue
examples	O
.	O

Training	O
lasted	O
for	O
14	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
where	O
we	O
early	O
stopped	O
if	O
overall	O
accuracy	B-MetricName
failed	O
to	O
improve	O
for	O
three	O
epochs	O
in	O
a	O
row	O
.	O

The	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
had	O
a	O
linear	O
warm	O
-	O
up	O
for	O
three	O
epochs	O
,	O
with	O
hyperparameters	O
kept	O
at	O
their	O
defaults	O
of	O
0.9	B-HyperparameterValue
and	O
0.999	B-HyperparameterValue
.	O

For	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
,	O
our	O
best	O
model	O
had	O
a	O
1e-5	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
1024	B-HyperparameterValue
hidden	B-HyperparameterName
dimension	I-HyperparameterName
and	O
no	O
weight	O
decay	O
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
shrunk	O
to	O
3	B-HyperparameterValue
examples	O
,	O
but	O
this	O
was	O
due	O
purely	O
to	O
memory	O
rather	O
than	O
performance	O
reasons	O
.	O

Second	O
,	O
the	O
intent	O
classifier	O
is	O
directly	O
fed	O
the	O
solution	O
,	O
which	O
is	O
what	O
allows	O
it	O
to	O
trivially	O
reach	O
perfect	O
accuracy	B-MetricName
.	O

Since	O
ABCD	B-DatasetName
was	O
collected	O
using	O
Expert	O
Live	O
Chat	O
rather	O
than	O
templates	O
,	O
we	O
observe	O
various	O
linguistic	O
diversity	O
in	O
the	O
chats	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Tao	O
Lei	O
,	O
Felix	O
Wu	O
and	O
Anmol	O
Kabra	O
for	O
their	O
feedback	O
and	O
support	O
.	O

This	O
paper	O
presents	O
a	O
new	O
dataset	O
which	O
was	O
collected	O
through	O
the	O
use	O
of	O
crowdworkers	O
.	O

Nowadays	O
,	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
which	O
aims	O
to	O
verify	O
whether	O
a	O
news	O
document	O
is	O
trusted	O
or	O
fake	O
,	O
has	O
become	O
urgent	O
and	O
important	O
.	O

Most	O
existing	O
methods	O
rely	O
heavily	O
on	O
linguistic	O
and	O
semantic	O
features	O
from	O
the	O
news	O
content	O
,	O
and	O
fail	O
to	O
effectively	O
exploit	O
external	O
knowledge	O
which	O
could	O
help	O
determine	O
whether	O
the	O
news	O
document	O
is	O
trusted	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
called	O
CompareNet	B-MethodName
,	O
which	O
compares	O
the	O
news	O
to	O
the	O
knowledge	O
base	O
(	O
KB	O
)	O
through	O
entities	O
for	B-TaskName
fake	I-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Considering	O
that	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
is	O
correlated	O
with	O
topics	O
,	O
we	O
also	O
incorporate	O
topics	O
to	O
enrich	O
the	O
news	O
representation	O
.	O

Specifically	O
,	O
we	O
first	O
construct	O
a	O
directed	B-MethodName
heterogeneous	I-MethodName
document	I-MethodName
graph	I-MethodName
for	O
each	O
news	O
incorporating	O
topics	O
and	O
entities	O
.	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
for	O
learning	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
content	O
.	O

Experimental	O
results	O
on	O
two	O
benchmark	O
datasets	O
demonstrate	O
that	O
CompareNet	B-MethodName
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

2017;Khurana	O
and	O
Intelligentie	O
,	O
2017;Shu	O
et	O
al	O
.	O
,	O

To	O
avoid	O
feature	O
engineering	O
,	O
deep	O
neural	O
models	O
such	O
as	O
Bi	B-MethodName
-	I-MethodName
LSTM	I-MethodName
and	I-MethodName
convolutional	I-MethodName
neural	I-MethodName
networks	I-MethodName
(	O
CNN	B-MethodName
)	O
have	O
been	O
employed	O
(	O
Oshikawa	O
et	O
al	O
.	O
,	O

They	O
modeled	O
a	O
news	O
document	O
as	O
a	O
fully	O
connected	O
sentence	O
graph	O
and	O
proposed	O
a	O
graph	B-MethodName
attention	I-MethodName
model	I-MethodName
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

External	O
KB	O
such	O
as	O
Wikipedia	B-DatasetName
contains	O
a	O
large	O
amount	O
of	O
high	O
-	O
quality	O
structured	O
subjectpredicate	O
-	O
object	O
triplets	O
and	O
unstructured	O
entity	O
descriptions	O
,	O
which	O
could	O
serve	O
as	O
evidence	O
for	O
detecting	O
fake	O
news	O
.	O

proposed	O
to	O
construct	O
knowledge	O
graphs	O
from	O
positive	O
and	O
negative	O
news	O
,	O
and	O
apply	O
TransE	B-MethodName
to	O
learn	O
triplet	O
scores	O
for	O
fake	O
news	O
detection	O
(	O
Pan	O
et	O
al	O
.	O
,	O

In	O
this	O
paper	O
,	O
to	O
take	O
full	O
advantage	O
of	O
the	O
external	O
knowledge	O
,	O
we	O
propose	O
a	O
novel	O
endto	B-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
CompareNet	I-MethodName
which	O
directly	O
compares	O
the	O
news	O
to	O
the	O
KB	O
through	O
entities	O
for	O
fake	O
news	O
detection	O
.	O

In	O
CompareNet	B-MethodName
,	O
we	O
also	O
consider	O
using	O
topics	O
to	O
enrich	O
the	O
news	O
document	O
representation	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
since	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
and	O
topics	O
are	O
highly	O
correlated	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Based	O
on	O
the	O
directed	O
heterogeneous	O
document	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
topic	O
-	O
enriched	O
news	O
representations	O
and	O
contextual	O
entity	O
representations	O
.	O

To	O
facilitate	O
related	O
researches	O
,	O
we	O
release	O
both	O
our	O
code	O
and	O
dataset	O
to	O
the	O
public	O
2	O
.1	O
https://en.wikipedia.org/wiki/Mammography	O
2	O
https://github.com/ytc272098215/FakeNewsDetection	O
In	O
summary	O
,	O
our	O
main	O
contributions	O
include	O
:	O
1	O
)	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
CompareNet	I-MethodName
which	O
compares	O
the	O
news	O
to	O
the	O
external	O
knowledge	O
through	O
entities	O
for	O
fake	O
news	O
detection.2	O
)	O
In	O
CompareNet	B-MethodName
,	O
we	O
also	O
consider	O
the	O
useful	O
topic	O
information	O
.	O

Generally	O
,	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
usually	O
focuses	O
on	O
news	O
events	O
while	O
fact	O
-	O
checking	O
is	O
broader	O
(	O
Oshikawa	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
approaches	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
can	O
be	O
divided	O
into	O
two	O
categories	O
:	O
social	O
-	O
based	O
and	O
content	O
-	O
based	O
.	O

Social	B-MethodName
based	I-MethodName
models	O
basically	O
include	O
stance	B-MethodName
-	I-MethodName
based	I-MethodName
and	O
propagation	B-MethodName
-	I-MethodName
based	I-MethodName
.	O

Stance	B-MethodName
-	I-MethodName
based	I-MethodName
models	O
utilize	O
users	O
'	O
opinions	O
to	O
infer	O
news	O
veracity	O
(	O
Jin	O
et	O
al	O
.	O
,	O

Propagation	B-MethodName
-	I-MethodName
based	I-MethodName
approaches	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
are	O
based	O
on	O
the	O
basic	O
assumption	O
that	O
the	O
credibility	O
of	O
a	O
news	O
event	O
is	O
highly	O
related	O
to	O
the	O
credibilities	O
of	O
relevant	O
social	O
media	O
posts	O
.	O

Both	O
homogeneous	B-MethodName
(	O
Jin	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
heterogeneous	B-MethodName
credibility	I-MethodName
networks	I-MethodName
(	O
Gupta	O
et	O
al	O
.	O
,	O

2020	O
)	O
constructed	O
a	O
heterogeneous	O
network	O
of	O
news	O
articles	O
,	O
creators	O
and	O
news	O
subjects	O
,	O
and	O
proposed	O
a	O
deep	B-MethodName
diffusive	I-MethodName
network	I-MethodName
model	O
for	O
incorporating	O
the	O
network	O
structure	O
information	O
to	O
simultaneously	O
detect	B-TaskName
fake	I-TaskName
news	I-TaskName
articles	I-TaskName
,	O
creators	O
and	O
subjects	O
.	O

2015;Rubin	O
et	O
al	O
.	O
,	O

proposed	O
to	O
construct	O
knowledge	O
graphs	O
from	O
positive	O
and	O
negative	O
news	O
,	O
and	O
apply	O
TransE	B-MethodName
to	O
learn	O
triplet	O
scores	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
(	O
Pan	O
et	O
al	O
.	O
,	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
graph	O
neural	O
model	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
which	O
directly	O
compares	O
the	O
news	O
to	O
external	O
knowledge	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Considering	O
that	O
the	O
detection	B-TaskName
of	I-TaskName
fake	I-TaskName
news	I-TaskName
is	O
correlated	O
with	O
topics	O
,	O
we	O
also	O
use	O
topics	O
to	O
enrich	O
the	O
news	O
representation	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

2019	O
;)	O
also	O
consider	O
incorporating	O
multi	O
-	O
modal	O
features	O
such	O
as	O
images	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
detail	O
our	O
proposed	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
model	O
CompareNet	B-MethodName
,	O
which	O
directly	O
compares	O
the	O
news	O
to	O
external	O
knowledge	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

As	O
shown	O
in	O
Figure	O
2	O
,	O
we	O
also	O
consider	O
topics	O
for	O
enriching	O
news	O
representation	O
since	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
is	O
highly	O
correlated	O
with	O
topics	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
document	O
.	O

Finally	O
,	O
the	O
obtained	O
entity	O
comparison	O
features	O
are	O
combined	O
with	O
the	O
topic	O
-	O
enriched	O
news	O
document	O
representation	O
for	O
fake	O
news	O
detection	O
.	O

There	O
are	O
three	O
kinds	O
of	O
nodes	O
in	O
the	O
graph	O
:	O
sentences	O
S	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
•	O
•	O
•	O
,	O
s	O
m	O
}	O
,	O
topics	O
T	O
=	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
•	O
•	O
•	O
,	O
t	O
K	O
}	O
and	O
entities	O
E	O
=	O
{	O
e	O
1	O
,	O
e	O
2	O
,	O
•	O
•	O
•	O
,	O
e	O
n	O
}	O
,	O
i.e.	O
,	O
V	O
=	O
S	O
∪	O
T	O
∪	O
E.The	O
set	O
of	O
edges	O
E	O
represent	O
the	O
relations	O
among	O
sentences	O
,	O
topics	O
and	O
entities	O
.	O

Since	O
topic	O
information	O
is	O
important	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
apply	O
the	O
unsupervised	B-MethodName
LDA	I-MethodName
(	O
Blei	O
et	O
al	O
.	O
,	O

2003	O
)	O
(	O
the	O
total	B-HyperparameterName
topic	I-HyperparameterName
number	I-HyperparameterName
K	B-HyperparameterName
is	O
set	O
as	O
100	B-HyperparameterValue
)	O
to	O
mine	O
the	O
latent	O
topics	O
T	O
from	O
all	O
the	O
sentences	O
of	O
all	O
the	O
documents	O
in	O
our	O
dataset	O
.	O

Specifically	O
,	O
each	O
sentence	O
is	O
taken	O
as	O
a	O
pseudo	O
-	O
document	O
and	O
is	O
assigned	O
to	O
the	O
top	B-HyperparameterName
P	I-HyperparameterName
relevant	I-HyperparameterName
topics	I-HyperparameterName
with	O
the	O
largest	O
probabilities	O
.	O

Thus	O
,	O
each	O
sentence	O
is	O
also	O
connected	O
with	O
its	O
top	B-HyperparameterName
P	I-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
in	O
bi	O
-	O
direction	O
,	O
allowing	O
the	O
useful	O
topic	O
information	O
to	O
propagate	O
among	O
the	O
sentences	O
.	O

Note	O
that	O
we	O
can	O
also	O
deal	O
with	O
new	O
coming	O
news	O
documents	O
by	O
inferring	O
the	O
topics	O
with	O
trained	O
LDA	B-MethodName
.	O

We	O
identify	O
the	O
entities	O
E	O
in	O
the	O
document	O
d	O
and	O
map	O
them	O
to	O
Wikipedia	B-DatasetName
using	O
the	O
entity	O
linking	O
tool	O
TAGME	O
3	O
.	O

Based	O
on	O
the	O
above	O
directed	O
heterogeneous	O
document	O
graph	O
G	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
for	O
learning	O
the	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
.	O

The	O
heterogeneous	O
convolution	O
layer	O
updates	O
the	O
(	O
l	O
+	O
1)-th	O
layer	O
representation	O
of	O
the	O
nodes	O
H	O
(	O
l+1	O
)	O
by	O
aggregating	O
the	O
features	O
of	O
their	O
neighboring	O
nodes	O
H	O
(	O
l	O
)	O
τ	O
with	O
different	O
types	O
τ	O
.	O
(	O

We	O
believe	O
entity	O
comparison	O
features	O
could	O
improve	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
based	O
on	O
the	O
assumption	O
that	O
e	O
c	O
learned	O
from	O
trusted	O
news	O
document	O
can	O
be	O
better	O
aligned	O
with	O
the	O
corresponding	O
e	O
KB	O
;	O
while	O
inverse	O
for	O
fake	O
news	O
.	O

Due	O
to	O
the	O
simplicity	O
of	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
we	O
adopted	O
TransE	B-MethodName
to	O
learn	O
entity	O
representations	O
e	O
s	O
∈	O
R	O
M	O
from	O
the	O
triplets	O
.	O

Formally	O
,	O
given	O
a	O
triplet	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
TransE	B-MethodName
regards	O
a	O
relationship	O
r	O
as	O
a	O
translation	O
vector	O
r	O
from	O
the	O
head	O
entity	O
h	O
to	O
the	O
tail	O
entity	O
t	O
,	O
namely	O
h	O
+	O
r	O
=	O
t.	O
Textual	O
Embedding	O
.	O

For	O
each	O
entity	O
,	O
we	O
take	O
the	O
first	O
paragraph	O
of	O
the	O
corresponding	O
Wikipedia	O
page	O
as	O
its	O
text	O
description	O
.	O

2019	O
)	O
,	O
we	O
use	O
SLN	B-DatasetName
:	O
Satirical	B-DatasetName
and	I-DatasetName
Legitimate	I-DatasetName
News	I-DatasetName
Database	I-DatasetName
(	O
Rubin	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
LUN	B-DatasetName
:	O
Labeled	B-DatasetName
Unreliable	I-DatasetName
News	I-DatasetName
Dataset	O
(	O
Rashkin	O
et	O
al	O
.	O
,	O

Our	O
baseline	O
models	O
include	O
deep	O
neural	O
models	O
:	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
CNN	B-MethodName
(	O
Kim	O
,	O
2014	O
)	O
,	O
BERT+LSTM	B-MethodName
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
(	O
BERT	B-MethodName
for	O
sentence	O
encoder	O
and	O
then	O
LSTM	B-MethodName
for	O
document	O
encoder	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

We	O
also	O
compare	O
our	O
model	O
with	O
graph	O
neural	O
models	O
:	O
GCN	B-MethodName
and	O
GAT	B-MethodName
based	O
on	O
an	O
undirected	O
fully	O
-	O
connected	O
sentence	O
graph	O
,	O
which	O
use	O
attention	O
pooling	O
or	O
max	O
pooling	O
for	O
learning	O
news	O
document	O
representation	O
.	O

2019	O
)	O
,	O
we	O
use	O
LSTM	B-MethodName
to	O
encode	O
sentences	O
with	O
randomly	O
initialized	O
word	O
embeddings	O
,	O
which	O
is	O
the	O
same	O
as	O
all	O
the	O
graph	O
neural	O
baselines	O
.	O

We	O
run	O
our	O
model	O
5	O
times	O
and	O
report	O
the	O
micro	O
-	O
averaged	O
(	O
Precision	O
=	O
Recall	O
=	O
F1	O
)	O
and	O
macro	O
-	O
averaged	O
scores	O
(	O
Precision	O
,	O
Recall	O
,	O
F1	O
)	O
in	O
all	O
the	O
settings	O
including	O
2	O
-	O
way	O
and	O
4	O
-	O
way	O
classification.2	O
-	O
way	O
classification	O
:	O
We	O
use	O
the	O
satirical	O
and	O
trusted	O
news	O
articles	O
from	O
LUN	B-DatasetName
-	I-DatasetName
train	I-DatasetName
for	O
training	O
,	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
for	O
validation	O
and	O
evaluate	O
our	O
model	O
on	O
the	O
entire	O
SLN	B-DatasetName
dataset	O
.	O

This	O
is	O
done	O
to	O
emulate	O
a	O
real	O
-	O
world	O
scenario	O
where	O
we	O
want	O
to	O
see	O
the	O
performance	O
of	O
our	O
model	O
on	O
an	O
out	O
-	O
of	O
-	O
domain	O
dataset.4	O
-	O
way	O
classification	O
:	O
We	O
split	O
the	O
LUN	B-DatasetName
-	I-DatasetName
train	I-DatasetName
into	O
a	O
80:20	O
split	O
to	O
create	O
our	O
training	O
and	O
validation	O
set	O
.	O

We	O
use	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
as	O
our	O
in	O
-	O
domain	O
test	O
set	O
.	O

In	O
our	O
experiments	O
,	O
we	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
topics	I-HyperparameterName
K	I-HyperparameterName
=	O
100	B-HyperparameterValue
in	O
LDA	B-MethodName
.	O

Each	O
sentence	O
is	O
assigned	O
to	O
top	B-HyperparameterName
P	I-HyperparameterName
=	O
2	B-HyperparameterValue
topics	O
with	O
the	O
largest	O
probabilities	O
.	O

The	O
layer	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
our	I-HyperparameterName
heterogeneous	I-HyperparameterName
graph	I-HyperparameterName
convolution	I-HyperparameterName
is	O
set	O
as	B-HyperparameterName
L	I-HyperparameterName
=	O
1	B-HyperparameterValue
.	O

Specifically	O
,	O
all	O
the	O
hidden	B-HyperparameterName
dimensions	I-HyperparameterName
used	O
in	O
our	O
model	O
are	O
set	O
as	O
M	B-HyperparameterName
=	O
100	B-HyperparameterValue
.	O

The	O
node	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
N	B-HyperparameterName
=	O
32	B-HyperparameterValue
.	O

For	O
GCN	B-HyperparameterName
,	O
GAT	B-HyperparameterName
and	O
CompareNet	B-HyperparameterName
,	O
we	O
set	O
the	O
activation	B-HyperparameterValue
function	I-HyperparameterValue
as	O
LeakyRelU	B-HyperparameterValue
with	O
slope	B-HyperparameterName
0.2	B-HyperparameterValue
.	O

For	O
model	O
training	O
,	O
we	O
train	O
the	O
models	O
for	O
a	O
maximum	O
of	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	B-HyperparameterValue
.	O

We	O
set	O
L2	B-HyperparameterName
normalization	I-HyperparameterName
factor	I-HyperparameterName
η	B-HyperparameterName
as	O
1e-6	B-HyperparameterValue
.	O

We	O
report	O
only	O
micro	B-MetricName
F1	I-MetricName
since	O
micro	B-MetricName
Precision	I-MetricName
=	O
Recall	B-MetricName
=	O
F1	B-MetricName
.	O

As	O
we	O
can	O
see	O
,	O
our	O
proposed	O
model	O
CompareNet	B-HyperparameterName
significantly	O
outperforms	O
all	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
in	O
terms	O
of	O
all	O
the	O
metrics	O
.	O

Compared	O
to	O
the	O
best	O
baseline	O
model	O
,	O
CompareNet	B-HyperparameterName
improves	O
both	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
by	O
nearly	O
3	B-MetricValue
%	I-MetricValue
.	O

We	O
can	O
also	O
find	O
that	O
the	O
graph	O
neural	O
network	O
based	O
models	O
GCN	B-MethodName
and	O
GAT	B-MethodName
all	O
perform	O
better	O
than	O
the	O
deep	O
neural	O
models	O
including	O
CNN	B-MethodName
,	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
.	O

Our	O
model	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
further	O
improves	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
by	O
effectively	O
exploiting	O
the	O
topics	O
as	O
well	O
as	O
the	O
external	O
KB	O
.	O

The	O
topics	O
enrich	O
the	O
news	O
representation	O
,	O
and	O
the	O
external	O
KB	O
offers	O
evidences	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Our	O
model	O
CompareNet	B-MethodName
achieves	O
the	O
best	O
performance	O
in	O
terms	O
of	O
all	O
metrics	O
.	O

We	O
believe	O
that	O
our	O
model	O
CompareNet	B-MethodName
benefits	O
from	O
the	O
topics	O
and	O
external	O
knowledge	O
.	O

In	O
this	O
subsection	O
,	O
we	O
conduct	O
experiments	O
to	O
study	O
the	O
effectiveness	O
of	O
each	O
module	O
in	O
CompareNet	B-MethodName
and	O
the	O
way	O
we	O
incorporate	O
external	O
knowledge	O
.	O

We	O
study	O
the	O
average	O
performance	O
of	O
5	O
runs	O
on	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
set	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
we	O
test	O
the	O
performance	O
of	O
CompareNet	B-MethodName
removing	O
structured	O
triplets	O
,	O
removing	O
the	O
entire	O
external	O
knowledge	O
,	O
removing	O
topics	O
,	O
and	O
removing	O
both	O
topics	O
and	O
external	O
knowledge	O
.	O

Removing	O
both	O
topics	O
and	O
external	O
knowledge	O
(	O
i.e.	O
,	O
w/o	O
Both	O
)	O
will	O
lead	O
to	O
substantial	O
performance	O
drop	O
(	O
4.0	B-MetricValue
-	I-MetricValue
5.0	I-MetricValue
%	I-MetricValue
)	O
.	O

The	O
variant	O
model	O
CompareNet	B-MethodName
(	I-MethodName
undirected	I-MethodName
)	I-MethodName
although	O
incorporating	O
both	O
topics	O
and	O
external	O
knowledge	O
achieves	O
lower	O
performance	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
and	O
CompareNet	B-MethodName
w/o	I-MethodName
Topics	I-MethodName
.	O

The	O
reason	O
could	O
be	O
that	O
CompareNet	B-MethodName
(	I-MethodName
undirected	I-MethodName
)	I-MethodName
directly	O
aggregates	O
the	O
true	O
entity	O
knowledge	O
into	O
the	O
news	O
representation	O
in	O
graph	O
convolution	O
without	O
considering	O
the	O
directed	O
edges	O
,	O
which	O
misleads	O
the	O
classifier	O
for	O
differentiating	O
fake	O
news	O
.	O

The	O
last	O
variant	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
(	I-MethodName
concatenation	I-MethodName
)	I-MethodName
also	O
performs	O
lower	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
,	O
further	O
indicating	O
that	O
directly	O
concatenating	O
true	O
entity	O
knowledge	O
is	O
not	O
a	O
good	O
way	O
for	O
incorporating	O
entity	O
knowledge	O
.	O

Its	O
performance	O
drops	O
by	O
around	O
2.0	B-MetricValue
%	I-MetricValue
compared	O
to	O
CompareNet	B-MethodName
.	O

These	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
carefully	O
designed	O
entity	O
comparison	O
network	O
in	O
CompareNet	B-MethodName
.	O

Figure	O
3	O
shows	O
the	O
performance	O
(	O
micro	B-MetricName
and	I-MetricName
macro	I-MetricName
F1	I-MetricName
)	O
of	O
our	O
model	O
CompareNet	B-MethodName
on	O
LUN	B-DatasetName
validation	I-DatasetName
set	O
with	O
different	O
number	O
of	O
top	B-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
P	I-HyperparameterName
to	O
each	O
sentence	O
.	O

As	O
we	O
can	O
see	O
clearly	O
,	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
first	O
consistently	O
rises	O
with	O
the	O
increase	O
of	O
P	B-HyperparameterName
and	O
then	O
drops	O
when	O
P	B-HyperparameterName
is	O
larger	O
than	O
2	O
.	O

Thus	O
,	O
in	O
our	O
experiments	O
,	O
we	O
set	O
P	B-HyperparameterName
=	O
2	B-HyperparameterValue
.	O

To	O
further	O
illustrate	O
why	O
our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baseline	O
GAT+Attn	B-MethodName
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

We	O
believe	O
that	O
our	O
model	O
CompareNet	B-MethodName
benefits	O
from	O
the	O
comparison	O
to	O
Wikipedia	B-DatasetName
knowledge	I-DatasetName
by	O
the	O
entity	O
comparison	O
network	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
graph	O
neural	O
model	O
CompareNet	B-MethodName
which	O
compares	O
the	O
news	O
to	O
the	O
external	O
knowledge	O
for	O
fake	O
news	O
detection	O
.	O

This	O
paper	O
describes	O
three	O
open	O
access	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
corpora	O
and	O
presents	O
the	O
results	O
and	O
implications	O
of	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
for	I-TaskName
endangered	I-TaskName
language	I-TaskName
documentation	I-TaskName
.	O

Two	O
issues	O
are	O
addressed	O
.	O

First	O
,	O
the	O
advantage	O
for	O
ASR	B-TaskName
accuracy	O
of	O
targeting	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
in	O
addition	O
to	O
,	O
or	O
in	O
substitution	O
of	O
,	O
linguistic	O
units	O
(	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
morae	B-MethodName
)	O
and	O
then	O
using	O
ROVER	B-MethodName
for	O
system	O
combination	O
.	O

BPE	B-MethodName
units	O
consistently	O
outperform	O
linguistic	O
units	O
although	O
the	O
best	O
results	O
are	O
obtained	O
by	O
system	O
combination	O
of	O
different	O
BPE	B-MethodName
targets	O
.	O

Second	O
,	O
a	O
case	O
is	O
made	O
that	O
for	O
endangered	O
language	O
documentation	O
,	O
ASR	B-TaskName
contributions	O
should	O
be	O
evaluated	O
according	O
to	O
extrinsic	O
criteria	O
(	O
e.g.	O
,	O
positive	B-MetricName
impact	I-MetricName
on	I-MetricName
downstream	I-MetricName
tasks	I-MetricName
)	O
and	O
not	O
simply	O
intrinsic	O
metrics	O
(	O
e.g.	O
,	O
CER	B-MetricName
and	O
WER	B-MetricName
)	O
.	O

2006	O
)	O
,	O
and	O
financial	O
support	O
for	O
endangered	O
language	O
documentation	O
(	O
the	O
Volkswagen	O
Foundation	O
,	O
the	O
NSF	O
Documenting	O
Endangered	O
Language	O
Program	O
,	O
and	O
the	O
SOAS	O
Endangered	O
Language	O
Documentation	O
Programme	O
)	O
.	O

1992	O
)	O
and	O
Himmelmann	O
(	O
1998	O
)	O
have	O
been	O
published	O
by	O
Seifart	O
et	O
al	O
.	O
(	O

Within	O
the	O
last	O
decade	O
,	O
the	O
National	O
Science	O
Foundation	O
supported	O
a	O
series	O
of	O
three	O
workshops	O
,	O
under	O
the	O
acronym	O
AARDVARC	O
(	O
Automatically	O
Annotated	O
Repository	O
of	O
Digital	O
Audio	O
and	O
Video	O
Resources	O
Community	O
)	O
to	O
bring	O
together	O
field	O
linguists	O
working	O
on	O
endangered	O
languages	O
and	O
computational	O
linguists	O
working	O
on	O
automatic	O
annotation	O
-	O
particularly	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR)-to	B-TaskName
address	O
the	O
impact	O
of	O
what	O
has	O
been	O
called	O
the	O
"	O
transcription	O
bottleneck	O
"	O
(	O
Whalen	O
and	O
Damir	O
,	O
2012	O
)	O
.	O

Finally	O
,	O
articles	O
directly	O
referencing	O
ASR	B-TaskName
of	O
endangered	O
languages	O
have	O
become	O
increasingly	O
common	O
over	O
the	O
last	O
five	O
years	O
(	O
Adams	O
et	O
al	O
.	O
,	O
,	O

2021).This	O
article	O
continues	O
work	O
on	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
ASR	B-TaskName
(	O
Mitra	O
et	O
al	O
.	O
,	O

The	O
most	O
recent	O
efforts	O
(	O
2020	O
and	O
2021	O
)	O
have	O
adopted	O
the	O
ESPNet	O
toolkit	O
for	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
E2E	B-TaskName
ASR	I-TaskName
)	O
.	O

This	O
approach	O
has	O
proven	O
to	O
be	O
very	O
efficient	O
in	O
terms	O
of	O
time	O
needed	O
to	O
develop	O
the	O
ASR	B-TaskName
recipe	O
(	O
Shi	O
et	O
al	O
.	O
,	O

2021	O
)	O
and	O
in	O
yielding	O
ASR	B-TaskName
hypotheses	O
of	O
an	O
accuracy	O
capable	O
of	O
significantly	O
reducing	O
the	O
extent	O
of	O
human	O
effort	O
needed	O
to	O
finalize	O
accurate	O
transcribed	O
audio	O
for	O
permanent	O
archiving	O
as	O
here	O
demonstrated	O
.	O

Section	O
2	O
discusses	O
the	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
corpora	O
,	O
and	O
Section	O
3	O
explores	O
the	O
general	O
goals	O
of	O
EL	O
documentation	O
.	O

Section	O
4	O
reviews	O
the	O
E2E	B-TaskName
ASR	I-TaskName
and	O
corresponding	O
results	O
using	O
ESPNet	O
.	O

The	O
conclusion	O
is	O
offered	O
in	O
Section	O
5.2	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
:	O
Corpus	O
characteristics	O
and	O
development	O
Much	O
work	O
on	O
computer	O
-	O
assisted	O
EL	O
documentation	O
is	O
closely	O
related	O
to	O
work	O
on	O
low	O
-	O
resource	O
languages	O
,	O
for	O
the	O
obvious	O
reason	O
that	O
most	O
ELs	O
have	O
limited	O
resources	O
,	O
be	O
they	O
time	O
-	O
coded	O
transcriptions	O
,	O
interlinearized	O
texts	O
,	O
or	O
corpora	O
in	O
parallel	O
translation	O
.	O

The	O
resources	O
for	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
,	O
the	O
language	O
targeted	O
in	O
this	O
present	O
study	O
,	O
are	O
,	O
however	O
,	O
relatively	O
abundant	O
by	O
EL	O
standards	O
(	O
119.32	O
hours	O
over	O
three	O
corpora	O
)	O
,	O
the	O
result	O
of	O
over	O
a	O
decade	O
of	O
linguistic	O
and	O
anthropological	O
research	O
by	O
Amith	O
and	O
Castillo	O
García	O
(	O
2020	O
)	O
.	O

Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
(	O
henceforth	O
YM	B-DatasetName
)	O
,	O
an	O
endangered	O
Mixtecan	O
language	O
spoken	O
in	O
the	O
municipality	O
of	O
San	O
Luis	O
Acatlán	O
,	O
Guerrero	O
,	O
Mexico	O
,	O
is	O
one	O
of	O
some	O
50	O
languages	O
in	O
the	O
Mixtec	O
language	O
family	O
,	O
which	O
is	O
within	O
a	O
larger	O
unit	O
,	O
Otomanguean	O
,	O
that	O
Suárez	O
(	O
1983	O
)	O
considers	O
a	O
hyper	O
-	O
family	O
or	O
stock	O
.	O

YM	B-DatasetName
is	O
spoken	O
in	O
four	O
communities	O
:	O
Yoloxóchitl	O
,	O
Cuanacaxtitlan	O
,	O
Arroyo	O
Cumiapa	O
,	O
and	O
Buena	O
Vista	O
.	O

Mutual	O
intelligibility	O
among	O
the	O
four	O
communities	O
is	O
high	O
despite	O
differences	O
in	O
phonology	O
,	O
morphology	O
,	O
and	O
syntax	O
.	O

YMC	B-DatasetName
(	O
referring	O
only	O
to	O
the	O
Mixtec	O
of	O
the	O
community	O
of	O
Yoloxóchitl	O
[	O
16.81602	O
,	O
-98.68597	O
]	O
)	O
manifests	O
28	O
distinct	O
tonal	O
patterns	O
on	O
1,451	O
to	O
-	O
date	O
identified	O
bimoraic	O
lexical	O
stems	O
.	O

For	O
example	O
,	O
24	O
distinct	O
tonal	O
patterns	O
on	O
the	O
bimoraic	O
segmental	O
sequence	O
[	O
nama	O
]	O
yield	O
30	O
words	O
(	O
including	O
five	O
homophones	O
)	O
.	O

In	O
a	O
not	O
-	O
insignificant	O
number	O
of	O
cases	O
,	O
suppletive	O
stems	O
exist	O
,	O
generally	O
manifesting	O
variation	O
in	O
a	O
stem	O
-	O
initial	O
consonant	O
and	O
often	O
the	O
stem	O
-	O
initial	O
vowel	O
.	O

The	O
ample	O
tonal	O
inventory	O
of	O
YMC	B-DatasetName
presents	O
obstacles	O
to	O
native	O
speaker	O
literacy	O
and	O
an	O
ASR	B-TaskName
system	O
learning	O
to	O
convert	O
an	O
acoustic	O
signal	O
to	O
text	O
.	O

It	O
also	O
complicates	O
the	O
construction	O
of	O
a	O
language	O
lexicon	O
for	O
HMM	B-MethodName
-	I-MethodName
based	I-MethodName
systems	I-MethodName
,	O
a	O
lexicon	O
that	O
is	O
not	O
required	O
in	O
E2E	B-TaskName
ASR	I-TaskName
.	O

The	O
phonological	O
and	O
morphological	O
differences	O
between	O
YMC	B-DatasetName
and	O
the	O
Mixtec	O
of	O
the	O
three	O
other	O
YM	B-DatasetName
communities	O
create	O
challenges	O
for	O
transcription	O
and	O
,	O
by	O
extension	O
,	O
for	O
applying	O
YMC	B-DatasetName
ASR	B-TaskName
to	O
speech	O
recordings	O
from	O
these	O
other	O
villages	O
.	O

This	O
ample	O
size	O
has	O
yielded	O
lower	O
character	B-MetricName
(	O
CER	B-MetricName
)	O
and	O
word	B-MetricName
(	O
WER	B-MetricName
)	O
error	O
rates	O
than	O
would	O
usually	O
occur	O
with	O
truly	O
low	O
-	O
resource	O
EL	O
documentation	O
projects	O
.	O

A	O
second	O
YMC	B-DatasetName
corpus	O
(	B-DatasetName
YMC	I-DatasetName
-	I-DatasetName
FB	I-DatasetName
;	O
for	O
'	O
field	O
botany	O
'	O
)	O
was	O
developed	O
during	O
ethno	O
-	O
botanical	O
fieldwork	O
.	O

Kenia	O
Velasco	O
Gutiérrez	O
(	O
a	O
Spanish	O
-	O
speaking	O
botanist	O
)	O
and	O
Esteban	O
Guadalupe	O
Sierra	O
(	O
a	O
native	O
speaker	O
from	O
Yoloxóchitl	O
)	O
led	O
105	O
days	O
of	O
fieldwork	O
that	O
yielded	O
888	O
distinct	O
plant	O
collections	O
.	O

A	O
total	O
of	O
584	O
recordings	O
were	O
made	O
in	O
all	O
four	O
YM	O
communities	O
;	O
only	O
452	O
were	O
in	O
Yoloxóchitl	O
,	O
and	O
of	O
these	O
,	O
435	O
,	O
totaling	O
15.17	O
hours	O
with	O
only	O
three	O
speakers	O
,	O
were	O
used	O
as	O
a	O
second	O
test	O
case	O
for	O
E2E	B-TaskName
ASR	I-TaskName
.	O

This	O
Spanish	O
section	O
has	O
not	O
been	O
factored	O
into	O
the	O
duration	O
of	O
the	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
,	O
nor	O
has	O
it	O
been	O
evaluated	O
for	O
character	O
and	O
word	O
error	O
rates	O
at	O
this	O
time	O
(	O
pending	O
future	O
implementation	O
of	O
a	O
multilingual	O
model	O
)	O
.	O

2021).•	O
178	O
recordings	O
(	O
6.81	O
hours	O
)	O
were	O
processed	O
by	O
E2E	B-TaskName
ASR	I-TaskName
,	O
then	O
corrected	O
by	O
Castillo	O
.	O

This	O
set	O
was	O
not	O
used	O
to	O
teach	O
or	O
evaluate	O
novice	O
trainee	O
transcription	O
skills	O
but	O
only	O
to	O
determine	O
CER	B-MetricName
and	O
WER	B-MetricName
for	O
E2E	B-TaskName
ASR	I-TaskName
with	O
the	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
.	O

No	O
training	O
or	O
validation	O
sets	O
were	O
created	O
from	O
this	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
,	O
which	O
for	O
this	O
present	O
paper	O
was	O
used	O
solely	O
to	O
test	O
E2E	B-TaskName
ASR	I-TaskName
efficiency	O
using	O
the	O
recipe	O
developed	O
from	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
scores	O
for	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
were	O
only	O
produced	O
after	O
Castillo	O
used	O
the	O
ELAN	O
interface	O
to	O
correct	O
the	O
ASR	B-TaskName
hypotheses	O
for	O
this	O
corpus	O
(	O
see	O
Appendix	O
A	O
for	O
an	O
example	O
ASR	B-TaskName
output	O
)	O
.	O

The	O
recordings	O
involved	O
some	O
speakers	O
not	O
represented	O
in	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
.	O

This	O
environment	O
may	O
have	O
introduced	O
reverb	O
or	O
other	O
effects	O
that	O
might	O
have	O
negatively	O
affected	O
ASR	B-TaskName
CER	B-MetricName
and	O
WER.Accessibility	B-MetricName
:	O
All	O
three	O
corpora	O
(	O
119.32	O
hours	O
)	O
are	O
available	O
at	O
the	O
OpenSLR	O
data	O
portal	O
(	O
Amith	O
and	O
Castillo	O
García	O
,	O
2020	O
)	O
3	O
Goals	O
and	O
challenges	O
of	O
corpora	O
-	O
based	O
endangered	O
language	O
documentation	O
The	O
oft	O
-	O
cited	O
Boasian	O
trilogy	O
of	O
grammar	O
,	O
dictionaries	O
,	O
and	O
texts	O
is	O
a	O
common	O
foundation	O
for	O
EL	O
documentation	O
.	O

And	O
a	O
grammar	O
itself	O
would	O
benefit	O
greatly	O
from	O
a	O
large	O
set	O
of	O
annotated	O
natural	O
speech	O
recordings	O
not	O
simply	O
to	O
provide	O
examples	O
of	O
particular	O
structures	O
but	O
to	O
facilitate	O
a	O
statistical	O
analysis	O
of	O
speech	O
patterns	O
(	O
e.g.	O
,	O
for	O
YMC	B-DatasetName
,	O
the	O
relative	O
frequency	O
of	O
completive	O
verbs	O
marked	O
solely	O
by	O
tone	O
vs.	O
those	O
marked	O
by	O
the	O
prefix	O
ni	O
1	O
-	O
)	O
.	O

End	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
is	O
used	O
to	O
rapidly	O
increase	O
corpus	O
size	O
while	O
offering	O
the	O
opportunity	O
to	O
target	O
certain	O
genres	O
(	O
such	O
as	O
expert	O
conversations	O
on	O
the	O
nomenclature	O
,	O
classification	O
,	O
and	O
use	O
of	O
local	O
flora	O
and	O
fauna	O
;	O
ritual	O
discourse	O
;	O
material	O
cultural	O
production	O
;	O
techniques	O
for	O
fishing	O
and	O
hunting	O
)	O
that	O
are	O
of	O
ethnographic	O
interest	O
but	O
are	O
often	O
insufficiently	O
covered	O
in	O
EL	O
documentation	O
projects	O
that	O
struggle	O
to	O
produce	O
large	O
and	O
varied	O
corpora	O
.	O

With	O
the	O
human	O
effortreducing	O
advances	O
in	O
ASR	B-TaskName
for	O
YMC	B-DatasetName
presented	O
in	O
this	O
paper	O
,	O
such	O
extensive	O
targeted	O
recording	O
of	O
endangered	O
cultural	O
knowledge	O
can	O
now	O
easily	O
be	O
included	O
in	O
the	O
documentation	O
effort	O
.	O

The	O
evaluation	O
metric	O
,	O
therefore	O
,	O
is	O
not	O
intrinsic	O
(	O
e.g.	O
,	O
reduced	B-MetricName
CER	I-MetricName
and	O
WER	B-MetricName
)	O
but	O
rather	O
extrinsic	O
:	O
the	O
impact	B-MetricName
of	I-MetricName
ASR	I-MetricName
on	I-MetricName
the	I-MetricName
downstream	I-MetricName
task	I-MetricName
of	I-MetricName
creating	I-MetricName
a	I-MetricName
large	I-MetricName
and	I-MetricName
varied	I-MetricName
corpus	I-MetricName
of	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
.	O

ASR	B-TaskName
for	I-TaskName
endangered	I-TaskName
languages	I-TaskName
is	O
made	O
difficult	O
not	O
simply	O
because	O
of	O
limited	O
resources	O
for	O
training	O
a	O
robust	O
system	O
but	O
by	O
a	O
series	O
of	O
factors	O
briefly	O
discussed	O
in	O
this	O
section	O
.	O

Recording	O
conditions	O
:	O
Noisy	O
environments	O
,	O
including	O
overlapping	O
speech	O
,	O
reverberation	O
in	O
indoor	O
recordings	O
,	O
natural	O
sounds	O
in	O
outdoor	O
recordings	O
,	O
less	O
than	O
optimal	O
microphone	O
placement	O
(	O
e.g.	O
,	O
a	O
boom	O
mic	O
in	O
video	O
recordings	O
)	O
,	O
and	O
failure	O
to	O
separately	O
mike	O
speakers	O
for	O
multichannel	O
recordings	O
all	O
negatively	O
impact	O
the	O
accuracy	B-MetricName
of	O
ASR	B-TaskName
output	O
.	O

Also	O
to	O
the	O
point	O
,	O
field	O
recordings	O
are	O
seldom	O
made	O
with	O
an	O
eye	O
to	O
seeding	O
a	O
corpus	O
in	O
ways	O
that	O
would	O
specifically	O
benefit	O
ASR	B-TaskName
results	O
(	O
e.g.	O
,	O
recording	O
a	O
large	O
number	O
of	O
speakers	O
for	O
shorter	O
durations	O
,	O
rather	O
than	O
fewer	O
speakers	O
for	O
longer	O
times	O
)	O
.	O

To	O
date	O
,	O
then	O
,	O
processing	O
a	O
corpus	O
through	O
ASR	B-TaskName
techniques	O
of	O
any	O
nature	O
(	O
HMM	O
,	O
end	O
-	O
to	O
-	O
end	O
)	O
has	O
been	O
more	O
of	O
an	O
afterthought	O
than	O
planned	O
at	O
project	O
beginning	O
.	O

Development	O
of	O
a	O
corpus	O
from	O
the	O
beginning	O
with	O
an	O
eye	O
to	O
subsequent	O
ASR	B-TaskName
potential	O
would	O
be	O
immensely	O
helpful	O
to	O
these	O
computational	O
efforts	O
.	O

It	O
could	O
,	O
perhaps	O
should	O
,	O
be	O
increasingly	O
considered	O
in	O
the	O
initial	O
project	O
design	O
.	O

Indeed	O
,	O
just	O
as	O
funding	O
agencies	O
such	O
as	O
NSF	O
require	O
that	O
projects	O
address	O
data	O
management	O
issues	O
,	O
it	O
might	O
be	O
worth	O
considering	O
the	O
suggested	O
inclusion	O
of	O
how	O
to	O
make	O
documentation	O
materials	O
more	O
amenable	O
to	O
ASR	B-TaskName
and	O
NLP	O
processing	O
as	O
machine	O
learning	O
technologies	O
are	O
getting	O
more	O
robust	O
.	O

Preliminary	O
,	O
though	O
not	O
quantified	O
,	O
CER	O
analysis	O
for	O
YMC	B-DatasetName
ASR	B-TaskName
suggests	O
that	O
"	O
Spanish	O
-	O
origin	O
"	O
words	O
provoke	O
a	O
significantly	O
higher	O
error	O
rate	O
than	O
the	O
YMC	B-DatasetName
lexicon	O
uninfluenced	O
by	O
Spanish	O
.	O

Tone	O
-	O
based	O
inflectional	O
morphology	O
is	O
not	O
separated	O
in	O
any	O
YMC	B-DatasetName
transcriptions	O
.	O

2	O
The	O
transcription	O
strategy	O
for	O
YMC	B-DatasetName
was	O
unusual	O
in	O
that	O
the	O
practical	O
orthography	O
was	O
a	O
deep	O
,	O
underlying	O
system	O
that	O
represented	O
segmental	O
morpheme	O
boundaries	O
and	O
showed	O
elided	O
tones	O
in	O
parentheses	O
.	O

2021	O
,	O
§	O
2.3).Only	O
after	O
documentation	O
(	O
recording	O
and	O
timecoded	O
transcriptions	O
)	O
was	O
well	O
advanced	O
did	O
work	O
begin	O
on	O
a	O
finite	O
state	O
transducer	O
for	O
the	O
YMC	B-DatasetName
corpus	O
.	O

Note	O
,	O
then	O
,	O
that	O
the	O
surface	O
forms	O
in	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
are	O
based	O
on	O
FST	O
generation	O
from	O
an	O
underlying	O
transcription	O
as	O
input	O
and	O
not	O
from	O
the	O
direct	O
transcription	O
of	O
the	O
acoustic	O
signal	O
.	O

This	O
could	O
increase	O
the	O
CER	B-MetricName
and	O
WER	B-MetricName
for	O
ASR	B-TaskName
of	O
surface	O
forms	O
,	O
given	O
that	O
the	O
reference	O
for	O
evaluation	O
is	O
not	O
directly	O
derived	O
from	O
the	O
acoustic	O
signal	O
while	O
the	O
ASR	B-TaskName
hypothesis	O
is	O
so	O
derived	O
.	O

In	O
an	O
evaluation	O
across	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
development	O
and	O
test	O
sets	O
(	O
total	O
6.53	O
hours	O
)	O
of	O
the	O
relative	O
accuracy	O
of	O
ASR	B-TaskName
when	O
using	O
underlying	O
versus	O
surface	O
orthography	O
,	O
it	O
was	O
found	O
that	O
training	O
on	O
underlying	O
orthography	O
produced	O
slightly	O
greater	O
accuracy	O
than	O
training	O
on	O
surface	O
forms	O
:	O
Underlying	O
=	O
7.7/16.0	B-MetricValue
[	O
CER	B-MetricName
/	O
WER	B-MetricName
]	O
compared	O
to	O
Surface	O
=	O
7.8/16.5	B-MetricValue
[	O
CER	B-MetricName
/	O
WER	B-MetricName
]	O
(	O
Shi	O
et	O
al	O
.	O
,	O

The	O
decision	O
to	O
use	O
underlying	O
representations	O
in	O
ASR	B-TaskName
training	O
has	O
,	O
however	O
,	O
several	O
more	O
important	O
advantages	O
.	O

As	O
the	O
example	O
in	O
Appendix	O
A	O
demonstrates	O
,	O
ASR	O
output	O
includes	O
basic	O
segmentation	O
at	O
the	O
morphological	O
level	O
.	O

3.3	O
Intrinsic	O
metrics	O
:	O
CER	B-MetricName
,	O
WER	B-MetricName
,	O
and	O
consistency	B-MetricName
in	I-MetricName
transcriptions	I-MetricName
used	I-MetricName
as	I-MetricName
reference	I-MetricName
:	O
Although	O
both	O
CER	B-MetricName
and	O
WER	B-MetricName
reference	O
"	O
error	O
rate	O
"	O
in	O
regards	O
to	O
character	O
and	O
word	O
,	O
respectively	O
,	O
the	O
question	O
of	O
the	O
accuracy	O
of	O
the	O
reference	O
itself	O
is	O
rarely	O
explored	O
(	O
but	O
cf	O
.	O

For	O
YMC	B-DatasetName
,	O
only	O
one	O
speaker	O
,	O
Castillo	O
García	O
,	O
is	O
capable	O
of	O
accurate	O
transcription	O
,	O
which	O
in	O
YMC	B-DatasetName
is	O
the	O
sole	O
gold	O
standard	O
for	O
ASR	B-TaskName
training	O
,	O
validation	O
,	O
and	O
testing	O
.	O

Three	O
native	O
speaker	O
experts	O
have	O
worked	O
with	O
Amith	O
on	O
transcription	O
for	O
over	O
six	O
years	O
,	O
but	O
the	O
reference	O
for	O
ASR	B-TaskName
development	O
are	O
native	O
-	O
speaker	O
transcriptions	O
carefully	O
proofed	O
by	O
Amith	O
,	O
a	O
process	O
that	O
both	O
corrected	O
simple	O
errors	O
and	O
applied	O
a	O
single	O
standard	O
implemented	O
by	O
one	O
researcher	O
.	O

When	O
all	O
three	O
native	O
speaker	O
experts	O
were	O
asked	O
to	O
transcribe	O
the	O
same	O
90	O
minutes	O
or	O
recordings	O
,	O
and	O
the	O
results	O
were	O
compared	O
,	O
there	O
was	O
not	O
an	O
insignificant	O
level	O
of	O
variation	O
(	O
9%).The	O
aforementioned	O
scenario	O
suggests	O
the	O
impact	O
on	O
ASR	O
intrinsic	O
metrics	O
of	O
variation	O
in	O
transcriptions	O
across	O
multiple	O
annotators	O
,	O
or	O
even	O
inconsistencies	O
of	O
one	O
skilled	O
annotator	O
in	O
the	O
context	O
of	O
incipient	O
writing	O
systems	O
.	O

This	O
affects	O
not	O
only	O
ASR	B-TaskName
output	O
but	O
also	O
the	O
evaluation	O
of	O
ASR	B-TaskName
accuracy	O
via	O
character	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rates	I-MetricName
.	O

It	O
may	O
be	O
that	O
rather	O
than	O
character	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
,	O
it	O
would	O
be	O
advisable	O
to	O
consider	O
the	O
character	O
and	O
word	O
discrepancy	O
rate	O
a	O
change	O
in	O
terminology	O
that	O
perhaps	O
better	O
communicates	O
the	O
idea	O
that	O
the	O
differences	O
between	O
REF	O
and	O
HYP	O
are	O
often	O
as	O
much	O
a	O
matter	O
of	O
opinion	O
as	O
fact	O
.	O

The	O
nature	O
and	O
value	O
of	O
utilizing	O
intrinsic	O
metrics	O
(	O
e.g.	O
,	O
CER	B-MetricName
and	O
WER	B-MetricName
)	O
for	O
evaluating	O
ASR	B-TaskName
effectiveness	I-TaskName
for	I-TaskName
endangered	I-TaskName
language	I-TaskName
documentation	I-TaskName
merits	O
rethinking	O
.	O

An	O
additional	O
factor	O
that	O
has	O
emerged	O
in	O
the	O
YMC	B-DatasetName
corpora	I-DatasetName
,	O
which	O
contains	O
very	O
rapid	O
speech	O
,	O
is	O
what	O
may	O
be	O
called	O
"	O
hypercorrection	O
"	O
.	O

In	O
both	O
cases	O
,	O
ASR	B-TaskName
"	O
errors	O
"	O
might	O
represent	O
a	O
more	O
accurate	O
representation	O
of	O
the	O
acoustic	O
signal	O
than	O
the	O
transcription	O
of	O
even	O
the	O
most	O
highly	O
capable	O
native	O
speakers	O
.	O

Parity	O
could	O
perhaps	O
best	O
be	O
considered	O
as	O
not	O
based	O
on	O
CER	B-MetricName
and	O
WER	B-MetricName
alone	O
but	O
on	O
whether	O
ASR	B-TaskName
output	O
achieves	O
a	O
lower	O
error	O
rate	O
in	O
these	O
two	O
measurements	O
as	O
compared	O
to	O
what	O
another	O
skilled	O
human	O
transcriber	O
might	O
achieve	O
.	O

Given	O
the	O
nature	O
of	O
EL	O
documentation	O
,	O
which	O
requires	O
high	O
levels	O
of	O
accuracy	O
if	O
the	O
corpus	O
is	O
to	O
be	O
easily	O
used	O
for	O
future	O
linguistic	O
research	O
,	O
it	O
is	O
essential	O
that	O
ASR	B-TaskName
-	O
generated	O
hypotheses	O
be	O
reviewed	O
by	O
an	O
expert	O
human	O
annotator	O
before	O
permanent	O
archiving	O
.	O

Certainly	O
,	O
audio	O
can	O
be	O
archived	O
with	O
metadata	O
alone	O
or	O
with	O
unchecked	O
ASR	B-TaskName
transcriptions	O
(	O
see	O
Michaud	O
et	O
al	O
.	O
,	O

2018	O
,	O
§	O
4.3	O
and	O
4.4	O
)	O
,	O
but	O
the	O
workflow	O
envisioned	O
for	O
YMC	B-DatasetName
is	O
to	O
use	O
ASR	O
to	O
reduce	O
human	O
effort	O
while	O
the	O
archived	O
corpus	O
of	O
audio	O
and	O
text	O
maintains	O
results	O
equivalent	O
to	O
those	O
that	O
would	O
be	O
obtained	O
by	O
careful	O
,	O
and	O
laborintensive	O
,	O
expert	O
transcription	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
were	O
measured	O
for	O
YMC	B-DatasetName
corpora	O
with	O
training	O
sets	O
of	O
10	O
,	O
20	O
,	O
50	O
,	O
and	O
92	O
hours	O
.	O

and	O
7.7/16.1	O
(	O
92	O
hrs	O
.	O
)	O
;	O

Measurement	O
of	O
human	O
effort	O
reduction	O
suggests	O
that	O
with	O
a	O
corpus	O
of	O
30	O
-	O
50	O
hours	O
,	O
even	O
for	O
a	O
relatively	O
challenging	O
language	O
such	O
as	O
YMC	B-DatasetName
,	O
E2E	B-TaskName
ASR	I-TaskName
can	O
achieve	O
the	O
level	O
of	O
accuracy	O
that	O
allows	O
a	O
reduction	O
of	O
human	O
effort	O
by	O
>	O
75	O
percent	O
(	O
e.g.	O
,	O
from	O
40	O
to	O
10	O
hours	O
,	O
approximately	O
)	O
.	O

These	O
totals	O
are	O
derived	O
from	O
measurements	O
with	O
the	O
FB	O
and	O
VN	O
corpora	O
,	O
the	O
two	O
corpora	O
for	O
which	O
ASR	B-TaskName
provided	O
the	O
initial	O
transcription	O
,	O
and	O
Castillo	O
subsequently	O
corrected	O
the	O
output	O
,	O
keeping	O
track	O
of	O
the	O
time	O
he	O
spent	O
.	O

For	O
the	O
first	O
corpus	O
,	O
Castillo	O
required	O
58.20	O
hours	O
to	O
correct	O
6.65	O
hours	O
of	O
audio	O
(	O
from	O
173	O
of	O
the	O
178	O
files	O
that	O
had	O
not	O
been	O
first	O
transcribed	O
by	O
a	O
speaker	O
trainee	O
)	O
.	O

Over	O
the	O
entire	O
set	O
of	O
197	O
files	O
(	O
11.81	O
hours	O
)	O
,	O
human	O
effort	O
was	O
111.27	O
hours	O
,	O
or	O
9.42	O
hours	O
to	O
correct	O
1	O
hour	O
of	O
audio	O
.	O

Given	O
that	O
the	O
ASR	B-TaskName
system	I-TaskName
was	O
trained	O
on	O
an	O
underlying	O
orthography	O
,	O
the	O
final	O
result	O
of	O
<	O
10	O
hours	O
of	O
human	O
effort	O
per	O
hour	O
of	O
audio	O
is	O
a	O
transcribed	O
and	O
partially	O
parsed	O
corpus	O
.	O

Table	O
3	O
presents	O
an	O
analysis	O
of	O
two	O
lines	O
of	O
a	O
recording	O
that	O
was	O
first	O
processed	O
by	O
E2E	B-TaskName
ASR	I-TaskName
and	O
corrected	O
by	O
Castillo	O
García	O
.	O

This	O
focus	O
on	O
extrinsic	O
metrics	O
reflects	O
the	O
realization	O
that	O
the	O
ultimate	O
goal	O
of	O
computational	O
systems	O
is	O
not	O
to	O
achieve	O
the	O
lowest	O
CER	B-MetricName
and	O
WER	B-MetricName
but	O
to	O
help	O
documentation	O
initiatives	O
more	O
efficiently	O
produce	O
results	O
that	O
will	O
benefit	O
future	O
stakeholders	O
.	O

Recently	O
,	O
E2E	B-TaskName
ASR	I-TaskName
has	O
reached	O
comparable	O
or	O
better	O
performances	O
than	O
conventional	O
Hidden	B-MethodName
-	I-MethodName
Markov	I-MethodName
-	I-MethodName
Model	I-MethodName
-	O
based	O
ASR	B-TaskName
(	O
Graves	O
and	O
Jaitly	O
,	O
2014;Chiu	O
et	O
al	O
.	O
,	O

In	O
practice	O
,	O
E2E	B-TaskName
ASR	I-TaskName
systems	I-TaskName
are	O
less	O
affected	O
by	O
linguistic	O
constraints	O
and	O
are	O
generally	O
easier	O
to	O
train	O
.	O

The	O
benefits	O
of	O
such	O
systems	O
are	O
reflected	O
in	O
the	O
recent	O
trends	O
of	O
using	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
for	O
EL	O
documentation	O
(	O
Adams	O
et	O
al	O
.	O
,	O

2020;Matsuura	O
et	O
al	O
.	O
,	O

2021).In	O
developing	O
E2E	B-TaskName
ASR	I-TaskName
recipes	O
for	O
YMC	B-DatasetName
,	O
we	O
have	O
adopted	O
transformer	B-MethodName
and	I-MethodName
conformerbased	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
networks	I-MethodName
with	I-MethodName
hybrid	I-MethodName
CTC	I-MethodName
/	I-MethodName
attention	I-MethodName
training	O
(	O
Karita	O
et	O
al	O
.	O
,	O

We	O
used	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
(	O
trainsplit	O
)	O
for	O
training	O
and	O
other	O
YMC	B-DatasetName
corpora	O
for	O
evaluation	O
.	O

Four	O
systems	O
employ	O
the	O
byte	B-MethodName
-	I-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
method	O
trained	O
from	O
unigram	O
language	O
models	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
,	O
with	O
transcription	B-HyperparameterName
alphabets	I-HyperparameterName
limited	O
to	O
the	O
150	B-HyperparameterName
,	O
500	B-HyperparameterName
,	O
1000	B-HyperparameterName
,	O
and	O
1500	B-HyperparameterName
most	O
frequent	O
byte	O
-	O
pairs	O
in	O
the	O
training	O
set	O
.	O

The	O
other	O
three	O
ASR	O
systems	O
adopt	O
linguistic	O
units	O
,	O
including	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
mora	B-MethodName
.	O

The	O
YM	B-DatasetName
word	O
is	O
defined	O
as	O
a	O
stem	O
with	O
all	O
prefixes	O
(	O
such	O
as	O
completetive	O
ni	O
1	O
-	O
,	O
causative	O
sa	O
4	O
-	O
,	O
and	O
iterative	O
nda	O
3	O
-	O
)	O
separated	O
from	O
the	O
stem	O
by	O
a	O
hyphen	O
;	O
and	O
all	O
enclitics	O
(	O
particularly	O
person	O
markers	O
for	O
subjects	O
,	O
objects	O
,	O
and	O
possessors	O
,	O
such	O
as	O
=	O
yu	O
3	O
,	O
1sg	O
;	O
=	O
un	O
4	O
,	O
2sg	O
;	O
=	O
an	O
4	O
,	O
3sgFem	O
;	O
=	O
o	O
4	O
,	O
1plIncl	O
;	O
as	O
well	O
as	O
=	O
lu	O
3	O
,	O
augmentive	O
)	O
.	O

ASR	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
coco	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
Exp	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
ko	O
4	O
ko	O
13	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
Note	O
ASR	B-TaskName
suggested	O
Spanish	O
'	O
coco	O
'	O
coconut	O
for	O
Mixtec	O
ko	O
4	O
ko	O
13	O
(	O
'	O
to	O
be	O
abundant[plants	O
]	O
'	O
)	O
itics	O
have	O
alternative	O
tones	O
,	O
depending	O
on	O
stemfinal	O
vowel	O
and	O
tone	O
,	O
respectively	O
.	O

Three	O
combinations	O
have	O
been	O
evaluated	O
:	O
(	O
1	O
)	O
ROVER	B-MethodName
among	O
only	O
linguistic	O
units	O
(	O
i.e.	O
,	O
morae	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
word	B-MethodName
)	O
,	O
(	O
2	O
)	O
ROVER	B-MethodName
among	O
only	O
sub	O
-	O
word	O
units	O
(	O
in	O
this	O
case	O
BPE	B-MethodName
)	O
;	O
and	O
(	O
3	O
)	O
ROVER	B-MethodName
combination	O
utilizing	O
all	O
seven	O
systems	O
.	O

The	O
first	O
addresses	O
the	O
performance	O
of	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
across	O
three	O
corpora	O
,	O
each	O
with	O
slightly	O
different	O
recording	O
systems	O
and	O
content	O
.	O

As	O
clear	O
from	O
the	O
preceding	O
discussion	O
and	O
illustrated	O
in	O
Table	O
2	O
,	O
in	O
addition	O
to	O
training	O
on	O
the	O
word	O
unit	O
,	O
the	O
YMC	O
E2E	B-TaskName
ASR	I-TaskName
system	I-TaskName
was	O
trained	O
on	O
six	O
additional	O
linguistic	O
and	O
informational	O
sub	O
-	O
word	O
units	O
.	O

ROVER	B-MethodName
was	O
then	O
used	O
to	O
produce	O
composite	O
systems	O
in	O
which	O
the	O
outputs	O
of	O
all	O
seven	O
systems	O
were	O
combined	O
in	O
three	O
distinct	O
manners	O
.	O

In	O
all	O
cases	O
,	O
ROVER	B-MethodName
combinations	O
improved	O
the	O
result	O
of	O
any	O
individual	O
system	O
,	O
including	O
the	O
averages	O
for	O
either	O
of	O
the	O
two	O
types	O
of	O
units	O
:	O
linguistic	O
and	O
informational	O
.	O

The	O
average	O
CER	B-MetricName
/	O
WER	B-MetricName
for	O
linguistic	O
units	O
(	O
rows	O
A	O
-	O
C	O
)	O
was	O
10.4/	B-MetricValue
19.5	B-MetricValue
(	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
,	O
13.6/23.3	B-MetricValue
(	O
FB	B-DatasetName
)	O
,	O
and	O
10.7/21.7	B-MetricValue
(	O
VN	B-DatasetName
)	O
.	O

The	O
corresponding	O
figures	O
for	O
the	O
BPE	O
units	O
(	O
rows	O
D	O
-	O
G	O
)	O
were	O
7.7/16.0	B-MetricValue
(	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
,	O
9.7/19.5	B-MetricValue
(	O
FB	B-DatasetName
)	O
,	O
and	O
6.8/16.8	B-MetricValue
(	O
VN	B-DatasetName
)	O
.	O

In	O
terms	O
of	O
percentage	O
differences	O
between	O
the	O
two	O
types	O
of	O
units	O
,	O
the	O
numbers	O
are	O
not	O
insignificant	O
.	O

In	O
regards	O
to	O
CER	B-MetricName
,	O
performance	O
improved	O
from	O
linguistic	O
to	O
informational	O
units	O
by	O
26.0	B-MetricValue
,	O
28.7	B-MetricValue
,	O
and	O
36.4	B-MetricValue
percent	O
across	O
the	O
Exp(Test	B-DatasetName
)	I-DatasetName
,	O
FB	B-DatasetName
,	O
and	O
VN	B-DatasetName
corpora	O
.	O

In	O
regards	O
to	O
WER	B-MetricName
,	O
performance	O
improved	O
by	O
17.9	B-MetricValue
,	O
16.3	B-MetricValue
,	O
and	O
22.6	B-MetricValue
percent	O
across	O
the	O
same	O
three	O
corpora	O
.	O

The	O
experiments	O
also	O
addressed	O
two	O
remaining	O
questions	O
:	O
(	O
1	O
)	O
does	O
unweighted	O
ROVER	B-MethodName
combination	O
improve	O
the	O
accuracy	O
of	O
ASR	B-TaskName
results	O
;	O
(	O
2	O
)	O
does	O
adding	O
linguistic	O
unit	O
performance	O
units	O
to	O
the	O
ROVER	B-MethodName
"	O
voting	O
pool	O
"	O
improve	O
results	O
over	O
a	O
combination	O
of	O
only	O
BPE	B-MethodName
units	I-MethodName
.	O

In	O
regards	O
to	O
the	O
first	O
question	O
:	O
ROVER	B-MethodName
always	O
improves	O
results	O
over	O
any	O
individual	O
system	O
(	O
compare	O
row	O
H	O
to	O
rows	O
A	O
,	O
B	O
,	O
and	O
C	O
,	O
and	O
row	O
I	O
to	O
rows	O
D	O
,	O
E	O
,	O
F	O
,	O
and	O
G	O
)	O
.	O

The	O
second	O
question	O
is	O
addressed	O
by	O
comparing	O
rows	O
I	O
(	O
ROVER	B-MethodName
applied	O
only	O
to	O
the	O
four	O
BPE	B-MethodName
results	O
)	O
to	O
J	O
(	O
adding	O
the	O
ASR	O
results	O
for	O
the	O
three	O
linguistic	O
units	O
into	O
the	O
combination	O
)	O
.	O

In	O
only	O
one	O
of	O
the	O
six	O
cases	O
(	O
CER	B-MetricName
of	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
does	O
including	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
morae	O
lower	O
the	O
error	O
rate	O
from	O
the	O
results	O
of	O
a	O
simple	O
combination	O
of	O
the	O
four	O
BPE	B-MethodName
results	O
(	O
in	O
this	O
case	O
from	O
7.6	B-MetricValue
[	O
row	O
I	O
]	O
to	O
7.4	B-MetricValue
[	O
row	O
J	O
]	O
)	O
.	O

In	O
one	O
case	O
,	O
there	O
is	O
no	O
change	O
(	O
CER	B-MetricName
for	O
the	O
VN	B-DatasetName
corpus	O
)	O
and	O
in	O
four	O
cases	O
,	O
including	O
linguistic	O
units	O
slightly	O
worsens	O
the	O
score	O
from	O
the	O
combination	O
of	O
BPE	B-MethodName
units	I-MethodName
alone	O
(	O
row	O
I	O
with	O
bold	O
numbers	O
)	O
.	O

The	O
implication	O
of	O
the	O
preceding	O
is	O
that	O
ASR	O
using	O
linguistic	O
units	O
yields	O
significantly	O
lower	O
accuracy	O
than	O
ASR	B-TaskName
that	O
uses	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
.	O

Combining	O
the	O
former	O
with	O
the	O
latter	O
in	O
an	O
unweighted	O
ROVER	B-MethodName
system	O
in	O
most	O
cases	O
does	O
not	O
improve	O
results	O
.	O

The	O
project	O
here	O
discussed	O
suggests	O
a	O
path	O
to	O
creating	O
such	O
corpora	O
using	O
end	O
-	O
to	O
-	O
end	O
ASR	O
technology	O
to	O
build	O
up	O
the	O
resources	O
(	O
30	O
-	O
50	O
hours	O
)	O
necessary	O
to	O
train	O
an	O
ASR	B-TaskName
system	O
with	O
perhaps	O
a	O
6	B-MetricValue
-	I-MetricValue
10	I-MetricValue
percent	I-MetricValue
CER	B-MetricName
.	O

Once	O
this	O
threshold	O
is	O
reached	O
,	O
it	O
is	O
unlikely	O
that	O
further	O
improvement	O
will	O
significantly	O
reduce	O
the	O
human	O
effort	O
needed	O
to	O
check	O
the	O
ASR	B-TaskName
output	O
for	O
accuracy	O
.	O

Indeed	O
,	O
even	O
if	O
there	O
are	O
no	O
"	O
errors	O
"	O
in	O
the	O
ASR	B-TaskName
output	O
,	O
confirmation	O
of	O
this	O
through	O
careful	O
revision	O
of	O
the	O
recording	O
of	O
the	O
transcription	O
would	O
probably	O
still	O
take	O
3	O
-	O
4	O
hours	O
.	O

The	O
effort	O
reduction	O
of	O
75	O
percent	O
documented	O
here	O
for	O
YMC	B-DatasetName
is	O
,	O
therefore	O
,	O
approaching	O
what	O
may	O
be	O
considered	O
the	O
minimum	O
amount	O
of	O
time	O
to	O
proofread	O
transcription	O
of	O
natural	O
speech	O
in	O
an	O
endangered	O
language	O
.	O

In	O
a	O
relatively	O
isolating	O
language	O
such	O
as	O
YM	O
,	O
such	O
a	O
system	O
is	O
not	O
difficult	O
for	O
native	O
speakers	O
to	O
write	O
nor	O
for	O
ASR	B-TaskName
systems	O
to	O
learn	O
.	O

It	O
has	O
the	O
advantage	O
of	O
creating	O
a	O
workflow	O
in	O
which	O
parsed	O
text	O
is	O
the	O
direct	O
output	O
of	O
E2E	B-TaskName
ASR	I-TaskName
.	O

The	O
error	O
rate	O
evaluations	O
across	O
the	O
spectrum	O
of	O
corpora	O
and	O
CER	B-MetricName
/	O
WER	B-MetricName
also	O
demonstrate	O
the	O
advantage	O
of	O
using	O
subword	O
units	O
such	O
as	O
BPE	O
and	O
subsequent	O
processing	O
by	O
ROVER	B-MethodName
for	O
system	O
combination	O
(	O
see	O
above	O
and	O
Table	O
2	O
)	O
.	O

A	O
final	O
question	O
concerns	O
additional	O
steps	O
once	O
CER	B-MetricName
is	O
reduced	O
to	O
6	B-MetricValue
-	I-MetricValue
8	I-MetricValue
percent	I-MetricValue
,	O
and	O
additional	O
improvements	O
to	O
ASR	B-TaskName
would	O
not	O
significantly	O
affect	O
the	O
human	O
effort	O
needed	O
to	O
produce	O
a	O
high	O
-	O
quality	O
time	O
-	O
coded	O
transcription	O
and	O
segmentation	O
.	O

Four	O
topics	O
are	O
suggested	O
:	O
(	O
1	O
)	O
address	O
issues	O
of	O
noise	O
,	O
overlapping	O
speech	O
,	O
and	O
other	O
challenging	O
recording	O
situations	O
;	O
(	O
2	O
)	O
focus	O
on	O
transfer	O
learning	O
to	O
related	O
languages	O
;	O
(	O
3	O
)	O
explore	O
the	O
impact	O
of	O
"	O
colonialization	O
"	O
by	O
a	O
dominant	O
language	O
;	O
and	O
(	O
4	O
)	O
focus	O
additional	O
ASR	B-TaskName
-	O
supported	O
corpus	O
development	O
on	O
producing	O
material	O
for	O
documentation	O
of	O
endangered	O
cultural	O
knowledge	O
,	O
a	O
facet	O
of	O
documentation	O
that	O
is	O
often	O
absent	O
from	O
endangered	O
language	O
documentation	O
projects	O
.	O

A	O
Analysis	O
of	O
ASR	B-TaskName
errors	O
in	O
one	O
recording	O
from	O
the	O
FB	O
corpus	O
Unique	O
identifier	O
:	O
2017	O
-	O
12	O
-	O
01	O
-	O
b	O
Speakers	O
:	O
Constantino	O
Teodoro	O
Bautista	O
and	O
Esteban	O
Guadalupe	O
Sierra	O
Spanish	O
:	O
The	O
first	O
13	O
seconds	O
(	O
3	O
segments	O
)	O
of	O
the	O
recording	O
were	O
of	O
a	O
Spanish	O
speaker	O
describing	O
the	O
plant	O
being	O
collected	O
(	O
Passiflora	O
biflora	O
Lam	O
.	O
)	O

Note	O
:	O
A	O
total	O
16	O
out	O
of	O
33	O
segments	O
/	O
utterances	O
are	O
without	O
ASR	B-TaskName
error	O
.	O

Notes	O
:	O
ASR	B-TaskName
does	O
not	O
output	O
caps	O
or	O
punctuation	O
.	O

00:00:17.105	O
-	O
>	O
00:00:19.477	O
ASR	B-TaskName
ya	O
1	O
mi	O
4	O
i	O
4	O
tu	O
1	O
tu	O
'	O
4	O
un	O
4	O
ku	O
3	O
rra	O
42	O
Exp	O
Ya	O
1	O
mi	O
4	O
i	O
4	O
tu	O
1	O
tu	O
'	O
4	O
un	O
4	O
ku	O
3	O
rra	O
42	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.6	O
.	O

00:00:19.477	O
-	O
>	O
00:00:23.688	O
ASR	O
ta	O
1	O
mas	O
4	O
tru	O
2	O
tela	O
ya	O
1	O
i	O
3	O
chi	O
4	O
ya	O
3	O
tin	O
3	O
ye	O
'	O
1	O
4e	O
4	O
ku	O
3	O
rra	O
42	O
ndi	O
4	O
covalentín	O
yo	O
'	O
4	O
o	O
4	O
Exp	O
ta	O
1	O
mas	O
4	O
tru	O
2	O
Tele	O
ya	O
1	O
i	O
3	O
chi	O
4	O
ya	O
3	O
tin	O
3	O
ye	O
'	O
1	O
4e	O
4	O
ku	O
3	O
rra	O
42	O
Nicu	O
Valentín	O
yo	O
'	O
4	O
o	O
4	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
the	O
proper	O
name	O
,	O
Nicu	O
Valentín	O
(	O
short	O
for	O
Nicolás	O
Valentín	O
)	O
but	O
did	O
get	O
the	O
accent	O
on	O
Valentín	O
,	O
while	O
mistaking	O
the	O
first	O
name	O
Nicu	O
for	O
ndi	O
4	O
co[valentín	O
]	O
7	O
*	O
.	O

00:00:23.688	O
-	O
>	O
00:00:31.086	O
ASR	B-TaskName
ya	O
1	O
i	O
3	O
chi	O
4	O
kwa	O
'	O
1	O
an	O
(	O
1	O
)	O
=	O
e	O
4	O
tan	O
3	O
xa	O
1	O
a	O
(	O
1	O
)	O
=	O
e	O
4	O
ku	O
3	O
rra	O
42	O
chi	O
4	O
ñu	O
3	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
ya	O
1	O
kwa	O
'	O
1	O
an	O
1	O
ni	O
1	O
nu	O
3	O
yo	O
'	O
4	O
o	O
4	O
ju	O
13	O
ta	O
'	O
3	O
an	O
2	O
=	O
ndu	O
1	O
ya	O
1	O
ko	O
4	O
ndo	O
3	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
Exp	O
ya	O
1	O
i	O
3	O
chi	O
4	O
kwa	O
'	O
1	O
an	O
(	O
1	O
)	O
=	O
e	O
4	O
tan	O
3	O
xa	O
1	O
a	O
(	O
1	O
)	O
=	O
e	O
4	O
ku	O
3	O
rra	O
42	O
chi	O
4	O
ñu	O
3	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
ya	O
1	O
kwa	O
'	O
1	O
an	O
1	O
ni	O
1	O
nu	O
3	O
yo	O
'	O
4	O
o	O
4	O
ju	O
13	O
ta	O
'	O
3	O
an	O
2	O
=	O
ndu	O
1	O
ya	O
1	O
ko	O
4	O
ndo	O
3	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.8	O
*	O
.	O

00:00:31.086	O
-	O
>	O
00:00:37.318	O
ASR	B-TaskName
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
kwi	O
4	O
i	O
24	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
ya	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
i	O
4	O
yo	O
(	O
2	O
)	O
=	O
a	O
2	O
mi	O
4	O
i	O
4	O
bi	O
1	O
xin	O
3	O
tan	O
3	O
Exp	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
kwi	O
4	O
i	O
24	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
ya	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
i	O
4	O
yo	O
(	O
2	O
)	O
=	O
a	O
2	O
mi	O
4	O
i	O
4	O
bi	O
1	O
xin	O
3	O
tan	O
3	O
Notes	O
:	O
The	O
ASR	B-TaskName
hypothesis	O
missed	O
the	O
inanimate	O
enclitic	O
after	O
the	O
verb	O
su	O
4	O
kun	O
1	O
and	O
as	O
a	O
result	O
failed	O
to	O
mark	O
the	O
elision	O
of	O
the	O
stem	O
-	O
final	O
low	O
tone	O
as	O
would	O
occur	O
before	O
a	O
following	O
low	O
-	O
tone	O
enclitic.9	O
.	O

00:00:37.318	O
-	O
>	O
00:00:42.959	O
ASR	O
yo	O
'	O
3	O
o	O
4	O
xi	O
13	O
i	O
2	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
yo	O
4	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
Exp	O
yo	O
'	O
3	O
o	O
4	O
xi	O
1	O
i	O
32	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
ji	O
'	O
4	O
in	O
(	O
4	O
)	O
=	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
,	O
Notes	O
:	O
ASR	O
missed	O
the	O
word	O
ji	O
'	O
4	O
in	O
4	O
(	O
'	O
with	O
'	O
,	O
comitative	O
)	O
and	O
as	O
a	O
result	O
wrote	O
the	O
1plInclusive	O
as	O
an	O
independent	O
pronoun	O
and	O
not	O
an	O
enclitic.10	O
.	O

00:00:42.959	O
-	O
>	O
00:00:49.142	O
ASR	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
coco	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2Exp	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
ko	O
4	O
ko	O
13	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
,	O
Notes	O
:	O
ASR	B-TaskName
suggested	O
Spanish	O
'	O
coco	O
'	O
coconut	O
for	O
Mixtec	O
ko	O
4	O
ko	O
13	O
(	O
'	O
to	O
be	O
abundant[plants	O
]	O
'	O
)	O
.	O

Note	O
that	O
'	O
coco	O
'	O
was	O
spelled	O
as	O
it	O
is	O
in	O
Spanish	O
and	O
no	O
tones	O
were	O
included	O
in	O
the	O
ASR	B-TaskName
output.11	O
.	O

00:00:49.142	O
-	O
>	O
00:00:53.458	O
ASR	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
43	O
Exp	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
42	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
tone	O
42	O
,	O
writing	O
43	O
instead	O
.	O

00:00:57.279	O
-	O
>	O
00:01:02.728	O
ASR	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
(	O
4	O
)	O
=	O
a	O
2	O
ni	O
1	O
-xa	O
'	O
3	O
nda	O
2	O
=	O
e	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
tun	O
4	O
si	O
13	O
su	O
2	O
kan	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
tan	O
3	O
ndi	O
4	O
Exp	O
Yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
(	O
4	O
)	O
=	O
a	O
2	O
ni	O
1	O
-xa	O
'	O
3	O
nda	O
2	O
=	O
e	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
tun	O
4	O
si	O
13	O
su	O
2	O
kan	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
tan	O
3	O
ndi	O
4	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.14	O
.	O

00:01:02.728	O
-	O
>	O
00:01:06.296	O
ASR	O
su	O
14	O
u	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
1	O
ba	O
42	O
ndi	O
4	O
su	O
14	O
u	O
3	O
ki	O
3	O
ti	O
4	O
ja	O
4	O
xi	O
24	O
=	O
ri	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
mi	O
4	O
i	O
4	O
ba	O
(	O
3	O
)	O
=	O
e	O
3	O
Exp	O
su	O
14	O
u	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
1	O
ba	O
42	O
tan	O
3	O
ni	O
4	O
su	O
14	O
u	O
3	O
ki	O
3	O
ti	O
4	O
ja	O
4	O
xi	O
24	O
=	O
ri	O
4	O
,	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
mi	O
4	O
i	O
4	O
ba	O
(	O
3	O
)	O
=	O
e	O
3	O
,	O
Notes	O
:	O
ASR	B-TaskName
mistakenly	O
proposed	O
ndi	O
4	O
for	O
tan	O
3	O
ni	O
4	O
.15	O
*	O
.	O

00:01:14.768	O
-	O
>	O
00:01:18.281	O
ASR	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
Exp	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
,	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis	O
.	O

The	O
authors	O
gratefully	O
acknowledge	O
the	O
following	O
support	O
for	O
documenting	O
and	O
studying	O
Yoloxóchitl	O
Mixtec	O
:	O
National	O
Science	O
Foundation	O
,	O
Documenting	O
Endangered	O
Languages	O
(	O
DEL	O
):	O
Awards	O
1761421	O
,	O
1500595	O
,	O
0966462	O
(	O
Amith	O
,	O
PI	O
on	O
all	O
three	O
;	O
the	O
second	O
was	O
a	O
collaborative	O
project	O
with	O
SRI	O
International	O
,	O
Award	O
1500738	O
,	O
Andreas	O
Kathol	O
,	O
PI	O
)	O
;	O
Endangered	O
Language	O
Documentation	O
Programme	O
:	O
Awards	O
MDP0201	O
,	O
PPG0048	O
(	O
Amith	O
,	O
PI	O
on	O
both	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
conduct	O
comprehensive	O
evaluation	O
and	O
analysis	O
with	O
respect	O
to	O
the	O
robustness	B-TaskName
of	I-TaskName
natural	I-TaskName
language	I-TaskName
understanding	I-TaskName
models	I-TaskName
,	O
and	O
introduce	O
three	O
important	O
aspects	O
related	O
to	O
language	O
understanding	O
in	O
realworld	O
dialog	O
systems	O
,	O
namely	O
,	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

We	O
propose	O
a	O
model	O
-	O
agnostic	O
toolkit	O
LAUG	B-MethodName
to	O
approximate	O
natural	O
language	O
perturbations	O
for	O
testing	O
the	O
robustness	B-TaskName
issues	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
.	O

Four	O
data	O
augmentation	O
approaches	O
covering	O
the	O
three	O
aspects	O
are	O
assembled	O
in	O
LAUG	B-MethodName
,	O
which	O
reveals	O
critical	O
robustness	O
issues	O
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

The	O
augmented	O
dataset	O
through	O
LAUG	B-MethodName
can	O
be	O
used	O
to	O
facilitate	O
future	O
research	O
on	O
the	O
robustness	B-TaskName
testing	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
.	O

2019;Shah	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
their	O
robustness	O
to	O
changes	O
in	O
the	O
input	O
distribution	O
is	O
still	O
one	O
of	O
the	O
biggest	O
challenges	O
in	O
practical	O
use	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

1	O
,	O
user	O
expressions	O
can	O
be	O
of	O
high	O
lexical	O
and	O
syntactic	O
diversity	O
when	O
a	O
system	O
is	O
deployed	O
to	O
users	O
;	O
typed	O
texts	O
may	O
differ	O
significantly	O
from	O
those	O
recognized	O
from	O
voice	O
speech	O
;	O
interaction	O
environments	O
may	O
be	O
full	O
of	O
chaos	O
and	O
even	O
users	O
themselves	O
may	O
introduce	O
irrelevant	O
noises	O
such	O
that	O
the	O
system	O
can	O
hardly	O
get	O
clean	O
user	O
input	O
.	O

Although	O
many	O
studies	O
have	O
discussed	O
the	O
LU	B-TaskName
robustness	I-TaskName
(	O
Ray	O
et	O
al	O
.	O
,	O

2018;Zhu	O
et	O
al	O
.	O
,	O

In	O
order	O
to	O
study	O
the	O
real	O
-	O
world	O
robustness	O
issues	O
,	O
we	O
define	O
the	O
LU	B-TaskName
robustness	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
.	O

Therefore	O
,	O
we	O
propose	O
an	O
automatic	O
method	O
LAUG	B-MethodName
for	O
Language	B-MethodName
understanding	I-MethodName
AUGmentation	I-MethodName
in	O
this	O
paper	O
to	O
approximate	O
the	O
natural	O
perturbations	O
to	O
existing	O
data	O
.	O

LAUG	B-MethodName
is	O
a	O
black	O
-	O
box	O
testing	O
toolkit	O
on	O
LU	B-TaskName
robustness	I-TaskName
composed	O
of	O
four	O
data	O
augmentation	O
methods	O
,	O
including	O
word	B-TaskName
perturbation	I-TaskName
,	O
text	B-TaskName
paraphrasing	I-TaskName
,	O
speech	B-TaskName
recognition	I-TaskName
,	O
and	O
speech	B-TaskName
disfluency	I-TaskName
.	O

We	O
instantiate	O
LAUG	B-MethodName
on	O
two	O
dialog	O
corporaFrames	B-DatasetName
(	O
El	O
Asri	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MultiWOZ	B-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

Quality	O
evaluation	O
by	O
annotators	O
indicates	O
that	O
the	O
utterances	O
augmented	O
by	O
LAUG	B-MethodName
are	O
reasonable	O
and	O
appropriate	O
with	O
regards	O
to	O
each	O
augmentation	O
approach	O
's	O
target	O
.	O

Real	O
user	O
evaluation	O
further	O
verifies	O
that	O
LAUG	B-MethodName
well	O
reflects	O
real	O
-	O
world	O
robustness	O
issues	O
.	O

Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
classify	O
the	O
LU	B-TaskName
robustness	I-TaskName
systematically	O
into	O
three	O
aspects	O
that	O
occur	O
in	O
real	O
-	O
world	O
dialog	O
,	O
including	O
linguistic	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
;	O
(	O
2	O
)	O
We	O
propose	O
a	O
general	O
and	O
model	O
-	O
agnostic	O
toolkit	O
,	O
LAUG	B-MethodName
,	O
which	O
is	O
an	O
integration	O
of	O
four	O
data	O
augmentation	O
methods	O
on	O
LU	O
that	O
covers	O
the	O
three	O
aspects	O
.	O
(	O

3	O
)	O
We	O
conduct	O
an	O
in	O
-	O
depth	O
analysis	O
of	O
LU	B-TaskName
robustness	I-TaskName
on	O
two	O
dialog	O
corpora	O
with	O
a	O
variety	O
of	O
baselines	O
and	O
standardized	O
evaluation	O
measures	O
.	O
(	O

4	O
)	O
Quality	O
and	O
user	O
evaluation	O
results	O
demonstrate	O
that	O
the	O
augmented	O
data	O
are	O
representative	O
of	O
real	O
-	O
world	O
noisy	O
data	O
,	O
therefore	O
can	O
be	O
used	O
for	O
future	O
research	O
to	O
test	O
the	O
LU	B-TaskName
robustness	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
1	O
.	O

2020;He	O
et	O
al	O
.	O
,	O

2018).Noise	O
Perturbation	O
Most	O
dialog	O
systems	O
are	O
trained	O
only	O
on	O
noise	O
-	O
free	O
interactions	O
.	O

From	O
another	O
perspective	O
,	O
four	O
operations	O
from	O
EDA	O
perform	O
an	O
Invariance	O
test	O
,	O
while	O
slot	O
value	O
replacement	O
conducts	O
a	O
Directional	O
Expectation	O
test	O
according	O
to	O
CheckList	O
(	O
Ribeiro	O
et	O
al	O
.	O
,	O

2020).Text	O
Paraphrasing	O
The	O
target	O
of	O
text	O
paraphrasing	O
is	O
to	O
generate	O
a	O
new	O
utterance	O
x	O
=	O
x	O
while	O
maintaining	O
its	O
dialog	O
act	O
unchanged	O
,	O
i.e.	O
y	O
=	O
y.	O
We	O
applied	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
,	O
a	O
finetuned	O
language	O
model	O
conditioned	O
on	O
the	O
dialog	O
acts	O
,	O
to	O
paraphrase	O
the	O
sentences	O
as	O
data	O
augmentation	O
.	O

Then	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
is	O
finetuned	O
on	O
the	O
processed	O
data	O
so	O
that	O
it	O
can	O
be	O
aware	O
of	O
dialog	O
context	O
when	O
generating	O
paraphrases	O
.	O

As	O
a	O
result	O
,	O
we	O
find	O
that	O
the	O
average	O
token	O
length	O
of	O
generated	O
utterances	O
with	O
/	O
without	O
"	O
*	O
"	O
is	O
15.96/12.67	O
respectively	O
after	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
's	O
finetuning	O
on	O
MultiWOZ.It	B-DatasetName
should	O
be	O
noted	O
that	O
slot	O
values	O
of	O
an	O
utterance	O
can	O
be	O
paraphrased	O
by	O
models	O
,	O
resulting	O
in	O
a	O
different	O
semantic	O
meaning	O
y	O
.	O

To	O
prevent	O
generating	O
irrelevant	O
sentences	O
,	O
we	O
apply	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
in	O
paraphrases	O
with	O
original	O
slot	O
values	O
by	O
fuzzy	O
matching	O
3	O
,	O
and	O
replace	O
the	O
detected	O
values	O
in	O
bad	O
paraphrases	O
with	O
original	O
values	O
.	O

Speech	B-TaskName
Recognition	I-TaskName
We	O
simulate	O
the	O
speech	B-TaskName
recognition	I-TaskName
(	O
SR	B-TaskName
)	O
process	O
with	O
a	O
TTS	B-MethodName
-	I-MethodName
ASR	I-MethodName
pipeline	I-MethodName
(	O
Park	O
et	O
al	O
.	O
,	O

First	O
we	O
transfer	O
textual	O
user	O
utterance	O
x	O
to	O
its	O
audio	O
form	O
a	O
using	O
gTTS	B-MethodName
4	I-MethodName
(	O
Oord	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
a	O
Text	O
-	O
to	O
-	O
Speech	O
system	O
.	O

Then	O
audio	O
data	O
is	O
translated	O
back	O
into	O
text	O
x	O
by	O
DeepSpeech2	B-MethodName
(	O
Amodei	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
an	O
Automatic	B-MethodName
Speech	I-MethodName
Recognition	I-MethodName
(	I-MethodName
ASR	I-MethodName
)	I-MethodName
system	I-MethodName
.	O

We	O
directly	O
use	O
the	O
released	O
models	O
in	O
the	O
DeepSpeech2	B-MethodName
repository	O
5	O
with	O
the	O
original	O
configuration	O
,	O
where	O
the	O
speech	O
model	O
is	O
trained	O
on	O
Baidu	B-DatasetName
Internal	I-DatasetName
English	I-DatasetName
Dataset	O
,	O
and	O
the	O
language	O
model	O
is	O
trained	O
on	O
CommonCrawl	B-DatasetName
Data	I-DatasetName
.	O

ASR	B-TaskName
sometimes	O
wrongly	O
identifies	O
one	O
word	O
as	O
another	O
with	O
similar	O
pronunciation	O
.	O

Most	O
slot	O
values	O
could	O
be	O
relocated	O
by	O
our	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
rules	O
.	O

Repeats	O
I	O
,	O
I	O
want	O
to	O
go	O
to	O
,	O
go	O
to	O
Cambridge	O
.	O

We	O
present	O
some	O
examples	O
of	O
SD	B-TaskName
in	O
Table	O
5	O
.	O

In	O
order	O
to	O
approximate	O
the	O
real	O
distribution	O
of	O
disfluency	O
,	O
the	O
interruption	O
points	O
of	O
filled	O
pauses	O
and	O
repeats	O
are	O
predicted	O
by	O
a	O
Bi	B-MethodName
-	I-MethodName
LSTM+CRF	I-MethodName
model	O
(	O
Zayats	O
et	O
al	O
.	O
,	O

2016	O
)	O
trained	O
on	O
an	O
annotated	O
dataset	O
SwitchBoard	B-DatasetName
(	O
Godfrey	O
et	O
al	O
.	O
,	O

The	O
filler	O
words	O
,	O
restart	O
terms	O
,	O
and	O
edit	O
terms	O
and	O
their	O
occurrence	O
frequency	O
are	O
all	O
sampled	O
from	O
their	O
distribution	O
in	O
SwitchBoard	B-DatasetName
.	O

Therefore	O
,	O
SD	B-TaskName
augmentation	O
do	O
not	O
change	O
the	O
original	O
semantic	O
and	O
labels	O
of	O
the	O
utterance	O
,	O
i.e.	O
y	O
=	O
y.	O
In	O
our	O
experiments	O
we	O
adopt	O
Frames	B-DatasetName
6	O
(	O
El	O
Asri	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MultiWOZ	B-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

In	O
particular	O
,	O
MultiWOZ	O
is	O
one	O
of	O
the	O
most	O
challenging	O
datasets	O
due	O
to	O
its	O
multi	O
-	O
domain	O
setting	O
and	O
complex	O
ontology	O
,	O
and	O
we	O
conduct	O
our	O
experiments	O
on	O
the	O
latest	O
annotation	O
-	O
enhanced	O
version	O
MultiWOZ	B-DatasetName
2.3	I-DatasetName
(	O
Han	O
et	O
al	O
.	O
,	O

Table	O
7	O
shows	O
the	O
change	O
rates	O
in	O
different	O
as-	O
7	O
See	O
appendix	O
for	O
the	O
hyperparameter	O
setting	O
of	O
LAUG.pects	B-MethodName
by	O
comparing	O
our	O
augmented	O
utterances	O
with	O
the	O
original	O
counterparts	O
.	O

For	O
instance	O
,	O
TP	B-TaskName
rewrites	O
the	O
text	O
without	O
changing	O
the	O
original	O
meaning	O
,	O
thus	O
lexical	O
and	O
syntactic	O
representations	O
dramatically	O
change	O
,	O
while	O
most	O
slot	O
values	O
remain	O
unchanged	O
.	O

To	O
ensure	O
the	O
quality	O
of	O
our	O
augmented	O
test	O
set	O
,	O
we	O
conduct	O
human	O
annotation	O
on	O
1,000	O
sampled	O
utterances	O
in	O
each	O
augmented	O
test	O
set	O
of	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
.	O

According	O
to	O
the	O
feature	O
of	O
each	O
augmentation	O
method	O
,	O
different	O
evaluation	O
protocols	O
are	O
used	O
.	O

For	O
TP	B-TaskName
and	O
SD	B-TaskName
,	O
annotators	O
check	O
whether	O
the	O
meaning	O
of	O
utterances	O
and	O
dialog	O
acts	O
are	O
unchanged	O
.	O

For	O
WP	B-TaskName
,	O
changing	O
slot	O
values	O
is	O
allowed	O
due	O
to	O
slot	O
value	O
replacement	O
,	O
but	O
the	O
slot	O
name	O
should	O
be	O
the	O
same	O
.	O

To	O
support	O
a	O
multi	O
-	O
intent	O
setting	O
in	O
classificationbased	O
models	O
,	O
we	O
decouple	O
the	O
LU	B-TaskName
process	O
as	O
follows	O
:	O
first	O
perform	O
domain	O
classification	O
and	O
intent	O
detection	O
,	O
then	O
concatenate	O
two	O
special	O
tokens	O
which	O
indicate	O
the	O
detected	O
domain	O
and	O
intent	O
(	O
e.g.[restaurant][inf	O
orm	O
]	O
)	O
at	O
the	O
beginning	O
of	O
the	O
input	O
sequence	O
,	O
and	O
last	O
encode	O
the	O
new	O
sequence	O
to	O
predict	O
slot	O
tags	O
.	O

We	O
conduct	O
robustness	O
testing	O
on	O
all	O
three	O
capacities	O
for	O
five	O
base	O
models	O
using	O
four	O
augmentation	O
methods	O
in	O
LAUG	B-MethodName
.	O

Overall	O
F1	B-MetricName
-	O
measure	O
performance	O
on	O
Frames	B-DatasetName
and	O
MultiWOZ	B-DatasetName
is	O
shown	O
in	O
Table	O
8	O
.	O

This	O
indicates	O
the	O
effectiveness	O
of	O
LAUG	B-MethodName
in	O
improving	O
the	O
model	O
's	O
robustness	O
.	O

ToD	B-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
the	O
state-	O
of	O
-	O
the	O
-	O
art	O
model	O
which	O
was	O
further	O
pre	O
-	O
trained	O
on	O
task	O
-	O
oriented	O
dialog	O
data	O
,	O
has	O
comparable	O
performance	O
with	O
BERT	B-MethodName
.	O

With	O
most	O
augmentation	O
methods	O
,	O
ToD	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
slightly	O
better	O
robustness	O
than	O
BERT.Since	B-MethodName
the	O
data	O
volume	O
of	O
Frames	B-DatasetName
is	O
far	O
less	O
than	O
that	O
of	O
MultiWOZ	B-DatasetName
,	O
the	O
performance	O
improvement	O
of	O
pre	O
-	O
trained	O
models	O
on	O
Frames	B-DatasetName
is	O
larger	O
than	O
that	O
on	O
MultiWOZ	B-DatasetName
.	O

test	O
set	O
more	O
remarkably	O
in	O
Frames	B-DatasetName
where	O
data	O
is	O
not	O
sufficient	O
.	O

The	O
dramatic	O
performance	O
drop	O
when	O
testing	O
on	O
SR	O
and	O
SD	O
data	O
indicates	O
that	O
robustness	O
for	O
speech	O
characteristics	O
may	O
be	O
the	O
most	O
challenging	O
issue	O
.	O

3	O
shows	O
how	O
the	O
performance	O
of	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
changes	O
on	O
MultiWOZ	B-DatasetName
when	O
the	O
ratio	O
of	O
augmented	O
training	O
data	O
to	O
the	O
original	O
data	O
varies	O
from	O
0.1	O
to	O
4.0	O
.	O

F1	B-MetricName
scores	O
on	O
augmented	O
test	O
sets	O
increase	O
when	O
there	O
are	O
more	O
augmented	O
data	O
for	O
training	O
.	O

The	O
performance	O
of	O
BERT	B-MethodName
on	O
augmented	O
test	O
sets	O
is	O
improved	O
when	O
augmentation	O
ratio	O
is	O
less	O
than	O
0.5	O
but	O
becomes	O
almost	O
unchanged	O
after	O
0.5	O
while	O
GPT-2	B-MethodName
keeps	O
increasing	O
stably	O
.	O

Between	O
augmentation	O
approaches	O
In	O
order	O
to	O
study	O
the	O
influence	O
of	O
each	O
augmentation	O
approach	O
in	O
LAUG	B-MethodName
,	O
we	O
test	O
the	O
performance	O
changes	O
when	O
one	O
augmentation	O
approach	O
is	O
removed	O
from	O
constructing	O
augmented	O
training	O
data	O
.	O

Results	O
on	O
Mul	B-DatasetName
-	I-DatasetName
tiWOZ	I-DatasetName
are	O
shown	O
in	O
Table	O
10	O
.	O

We	O
can	O
also	O
observe	O
an	O
increase	O
in	O
performance	O
when	O
it	O
is	O
removed	O
,	O
especially	O
for	O
MILU	B-MethodName
.	O

This	O
implies	O
a	O
lack	O
of	O
LU	B-TaskName
robustness	I-TaskName
in	O
detecting	O
unseen	O
entities	O
.	O

Table	O
11b	O
shows	O
the	O
results	O
of	O
ablation	O
study	O
on	O
SD	B-TaskName
.	O

We	O
collected	O
240	O
speech	O
utterances	O
from	O
real	O
humans	O
as	O
follows	O
:	O
First	O
,	O
we	O
sampled	O
120	O
combinations	O
of	O
DA	O
from	O
the	O
test	O
set	O
of	O
MultiWOZ	B-DatasetName
.	O

This	O
is	O
because	O
multiple	O
robustness	O
issues	O
may	O
be	O
included	O
in	O
one	O
real	O
case	O
,	O
while	O
each	O
augmentation	O
method	O
in	O
LAUG	B-MethodName
evaluates	O
them	O
separately	O
.	O

Despite	O
the	O
difference	O
,	O
model	O
performance	O
on	O
the	O
real	O
data	O
is	O
remarkably	O
improved	O
after	O
every	O
model	O
is	O
finetuned	O
on	O
the	O
augmented	O
data	O
,	O
verifying	O
that	O
LAUG	B-MethodName
effectively	O
enhances	O
the	O
model	O
's	O
real	O
-	O
world	O
robustness	O
.	O

Table	O
13	O
investigates	O
which	O
error	O
type	O
the	O
model	O
has	O
made	O
on	O
the	O
real	O
test	O
set	O
by	O
manually	O
checking	O
all	O
the	O
error	O
outputs	O
of	O
BERT	B-MethodName
Ori	O
.	O
"	O

It	O
can	O
be	O
observed	O
that	O
the	O
model	O
seriously	O
suffers	O
to	O
LU	B-TaskName
robustness	I-TaskName
(	O
over	O
70	O
%	O
)	O
,	O
and	O
that	O
almost	O
half	O
of	O
the	O
error	O
is	O
due	O
to	O
Language	O
Variety	O
.	O

This	O
shows	O
that	O
BERT	B-MethodName
Aug.	O
can	O
solve	O
these	O
two	O
kinds	O
of	O
problems	O
better	O
.	O

Robustness	B-TaskName
in	I-TaskName
LU	I-TaskName
has	O
always	O
been	O
a	O
challenge	O
in	O
task	O
-	O
oriented	O
dialog	O
.	O

This	O
paper	O
aims	O
to	O
provide	O
an	O
automatic	O
way	O
to	O
test	O
the	O
LU	B-TaskName
robustness	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
.	O

2016;Ebrahimi	O
et	O
al	O
.	O
,	O

While	O
data	O
augmentation	O
can	O
be	O
an	O
efficient	O
method	O
to	O
address	O
data	O
sparsity	O
,	O
it	O
can	O
improve	O
the	O
generalization	O
abilities	O
and	O
measure	O
the	O
model	B-TaskName
robustness	I-TaskName
as	O
well	O
(	O
Eshghi	O
et	O
al	O
.	O
,	O

2019;Iyyer	O
et	O
al	O
.	O
,	O

Word	O
-	O
level	O
operations	O
(	O
Kolomiyets	O
et	O
al	O
.	O
,	O

2019;Xu	O
and	O
Sarikaya	O
,	O
2014	O
)	O
worked	O
on	O
the	O
out	O
-	O
of	O
-	O
vocabulary	O
problem	O
when	O
facing	O
unseen	O
user	O
expression	O
.	O

Simulating	O
ASR	B-TaskName
errors	O
(	O
Schatzmann	O
et	O
al	O
.	O
,	O

As	O
most	O
work	O
tackles	O
LU	O
robustness	O
from	O
only	O
one	O
perspective	O
,	O
we	O
present	O
a	O
comprehensive	O
study	O
to	O
reveal	O
three	O
critical	O
issues	O
in	O
this	O
paper	O
,	O
and	O
shed	O
light	O
on	O
a	O
thorough	O
robustness	O
evaluation	O
of	O
LU	B-TaskName
in	I-TaskName
dialog	I-TaskName
systems	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
systematic	O
robustness	B-TaskName
evaluation	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
(	I-TaskName
LU	I-TaskName
)	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

In	O
-	O
depth	O
experiments	O
and	O
analysis	O
are	O
conducted	O
on	O
MultiWOZ	B-DatasetName
and	O
Frames	B-DatasetName
,	O
with	O
both	O
classification	O
-	O
and	O
generation	O
-	O
based	O
LU	O
models	O
.	O

In	O
addition	O
to	O
the	O
four	O
approaches	O
in	O
LAUG	B-MethodName
,	O
more	O
methods	O
to	O
evaluate	O
LU	O
robustness	O
can	O
be	O
considered	O
in	O
the	O
future	O
.	O

As	O
for	O
hyperparameters	O
in	O
LAUG	B-MethodName
,	O
we	O
set	O
the	O
ratio	B-HyperparameterName
of	I-HyperparameterName
perturbation	I-HyperparameterName
number	I-HyperparameterName
to	I-HyperparameterName
text	I-HyperparameterName
length	I-HyperparameterName
α	B-HyperparameterName
=	O
n	O
/	O
l	O
=	O
0.1	B-HyperparameterValue
in	O
EDA	O
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
used	O
to	O
finetune	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
in	O
TP	O
is	O
1e-4	B-HyperparameterValue
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epoch	I-HyperparameterName
is	O
5	B-HyperparameterValue
,	O
and	O
the	O
beam	B-HyperparameterName
size	I-HyperparameterName
during	O
inference	O
is	O
5	B-HyperparameterValue
.	O

In	O
SR	B-TaskName
,	O
the	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
the	O
language	O
model	O
in	O
DeepSpeech2	O
is	O
set	O
to	O
50	B-HyperparameterValue
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
Bi	B-MethodName
-	I-MethodName
LSTM+CRF	I-MethodName
in	O
SD	B-TaskName
is	O
1e-3	B-HyperparameterValue
.	O

The	O
threshold	B-HyperparameterName
of	I-HyperparameterName
fuzzy	I-HyperparameterName
matching	I-HyperparameterName
in	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
is	O
set	O
to	O
0.9	B-HyperparameterValue
in	O
TP	B-TaskName
and	O
0.7	B-HyperparameterValue
in	O
SR.For	B-TaskName
hyperparameters	O
of	O
base	O
models	O
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
1e-4	B-HyperparameterValue
for	O
BERT	B-MethodName
,	O
1e-5	B-HyperparameterValue
for	O
GPT2	B-MethodName
,	O
and	O
1e-3	B-HyperparameterName
for	O
MILU	B-MethodName
and	O
CopyNet	B-MethodName
.	O

The	O
beam	B-HyperparameterName
-	I-HyperparameterName
size	I-HyperparameterName
of	O
GPT2	B-MethodName
and	O
CopyNet	B-MethodName
is	O
5	B-HyperparameterValue
during	O
the	O
decoding	O
step	O
.	O

We	O
use	O
the	O
same	O
settings	O
of	O
DeepSpeech2	B-MethodName
in	O
SR	O
to	O
recognize	O
the	O
collected	O
audios	O
.	O

After	O
automatic	B-TaskName
span	I-TaskName
detection	I-TaskName
(	O
also	O
the	O
same	O
as	O
SR	O
's	O
)	O
are	O
applied	O
,	O
we	O
conduct	O
human	O
check	O
and	O
annotation	O
to	O
ensure	O
the	O
quality	O
of	O
labels	O
.	O

14	O
:	O
Robustness	O
on	O
different	O
schemes	O
on	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
.	O

In	O
this	O
section	O
,	O
we	O
study	O
the	O
influence	O
of	O
training	O
/	O
prediction	O
schemes	O
on	O
LU	B-TaskName
robustness	I-TaskName
.	O

4.3	O
of	O
the	O
main	O
paper	O
,	O
the	O
process	O
of	O
classification	O
-	O
based	O
LU	O
models	O
is	O
decoupled	O
into	O
two	O
steps	O
to	O
handle	O
multiple	O
labels	O
:	O
one	O
for	O
domain	O
/	O
intent	O
classification	O
and	O
the	O
other	O
for	O
slot	O
tagging	O
.	O

The	O
classificationbased	O
models	O
can	O
predict	O
the	O
dialog	O
acts	O
within	O
a	O
single	O
step	O
in	O
this	O
way	O
.	O

Table	O
14	O
shows	O
that	O
MILU	B-MethodName
and	O
BERT	B-MethodName
gain	O
from	O
the	O
decoupled	O
scheme	O
on	O
the	O
original	O
test	O
set	O
.	O

This	O
indicates	O
that	O
the	O
decoupled	O
scheme	O
decreases	O
the	O
model	O
complexity	O
by	O
decomposing	O
the	O
output	O
space	O
.	O

MILU	B-MethodName
via	O
the	O
coupled	O
scheme	O
behaves	O
more	O
robustly	O
than	O
the	O
decoupled	O
counterpart	O
(	O
-2.61	B-MetricValue
vs.-7.05	B-MetricValue
)	O
,	O
while	O
BERT	B-MethodName
with	O
the	O
decoupled	O
scheme	O
outperforms	O
its	O
coupled	O
version	O
in	O
robustness	O
(	O
-6.45	B-MetricValue
vs.	O
-8.61	B-MetricValue
)	O
.	O

Meanwhile	O
,	O
BERT	B-MethodName
benefits	O
from	O
the	O
decoupled	O
scheme	O
and	O
still	O
achieves	O
86.95	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
but	O
BERT	B-MethodName
training	O
with	O
the	O
coupled	O
scheme	O
seems	O
more	O
susceptible	O
.	O

In	O
addition	O
,	O
both	O
MILU	B-MethodName
and	O
BERT	B-MethodName
recover	O
more	O
performance	O
by	O
the	O
proposed	O
decoupled	O
scheme	O
.	O

All	O
these	O
results	O
demonstrate	O
the	O
superiority	O
of	O
the	O
decoupled	O
scheme	O
in	O
classification	B-TaskName
-	I-TaskName
based	I-TaskName
LU	I-TaskName
models	O
.	O

In	O
Table	O
15	O
,	O
we	O
present	O
some	O
examples	O
of	O
augmented	O
utterances	O
in	O
MultiWOZ	B-DatasetName
.	O

In	O
terms	O
of	O
model	O
performance	O
,	O
MILU	B-MethodName
,	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
perform	O
well	O
on	O
WP	B-TaskName
and	O
TP	B-TaskName
in	O
the	O
example	O
while	O
Copy	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
misses	O
some	O
dialog	O
acts	O
.	O

For	O
the	O
SR	O
utterance	O
,	O
only	O
BERT	B-MethodName
obtains	O
all	O
the	O
correct	O
labels	O
.	O

MILU	B-MethodName
and	O
Copynet	O
both	O
fail	O
to	O
find	O
the	O
changed	O
value	O
spans	O
"	O
lester	O
"	O
and	O
"	O
thirteen	O
forty	O
five	O
"	O
.	O

Copynet	B-MethodName
's	O
copy	O
mechanism	O
is	O
fully	O
confused	O
by	O
recognition	O
error	O
and	O
even	O
predicts	O
discontinuous	O
slot	O
values	O
.	O

GPT-2	B-MethodName
successfully	O
finds	O
the	O
non	O
-	O
numerical	O
time	O
but	O
misses	O
"	O
leseter	O
"	O
.	O

Overall	O
,	O
in	O
this	O
example	O
,	O
BERT	B-MethodName
performs	O
quite	O
well	O
while	O
MILU	B-MethodName
and	O
Copy	B-MethodName
-	I-MethodName
Net	I-MethodName
expose	O
some	O
of	O
their	O
defects	O
in	O
robustness	O
.	O

Case-2	O
could	O
be	O
regarded	O
as	O
a	O
Speech	O
Characteristics	O
or	O
Noise	O
Perturbation	O
case	O
because	O
"	O
please	O
"	O
is	O
wrongly	O
recognized	O
as	O
"	O
police	O
"	O
by	O
ASR	B-TaskName
models	O
.	O

MILU	B-MethodName
and	O
BERT	B-MethodName
failed	O
in	O
most	O
of	O
these	O
cases	O
but	O
fixed	O
some	O
error	O
after	O
augmented	O
training	O
.	O

Extensive	O
experiments	O
show	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
model	O
obtained	O
by	O
our	O
method	O
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
on	O
various	O
downstream	O
tasks	O
.	O

For	O
example	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
achieves	O
78.8	B-MetricValue
on	O
the	O
GLUE	B-MetricName
testing	O
set	O
,	O
1.8	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
.	O

1	O
In	O
recent	O
years	O
,	O
pre	O
-	O
trained	O
language	O
models	O
,	O
such	O
as	O
the	O
representative	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
GPT-3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O

Except	O
BERT	B-MethodName
pre	O
-	O
trained	O
with	O
the	O
Masked	O
Language	O
Modeling	O
objective	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

We	O
first	O
consider	O
the	O
layer	O
types	O
.	O

2018Wu	O
et	O
al	O
.	O
,	O
,	O

It	O
has	O
been	O
shown	O
that	O
the	O
sandwich	O
order	O
can	O
bring	O
improvement	O
on	O
language	O
modeling	O
task	O
,	O
indicating	O
the	O
layer	O
order	O
contributes	O
to	O
model	O
performance	O
.	O

We	O
show	O
the	O
different	O
layer	O
variety	O
designs	O
of	O
existing	O
models	O
in	O
Figure	O
1(b	O
)	O
,	O
including	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019)/ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
Dynamic	B-MethodName
-	I-MethodName
Conv	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
Sandwich	B-MethodName
(	O
Press	O
et	O
al	O
.	O
,	O

It	O
can	O
be	O
seen	O
that	O
layer	O
variety	O
significantly	O
influences	O
model	O
performance	O
.	O

Pre	O
-	O
training	O
a	O
single	O
language	O
model	O
already	O
needs	O
to	O
consume	O
a	O
large	O
amount	O
of	O
computation	O
,	O
e.g.	O
,	O
2400	O
P100	O
GPU	O
days	O
for	O
pre	O
-	O
training	O
BERT	B-MethodName
.	O

After	O
obtaining	O
the	O
pre	O
-	O
trained	O
supernet	O
,	O
we	O
develop	O
an	O
evolutionary	O
algorithm	O
guided	O
by	O
MLM	O
evaluation	O
accuracy	B-MetricName
to	O
search	O
an	O
effective	O
architecture	O
with	O
specific	O
layer	O
variety	O
.	O

We	O
call	O
the	O
resulted	O
model	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
.	O

Extensive	O
experiments	O
show	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

Secondly	O
,	O
our	O
obtained	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
superiority	O
over	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

For	O
example	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	O
small	O
achieves	O
79.8	O
on	O
GLUE	B-DatasetName
testing	O
set	O
,	O
1.8	O
higher	O
than	O
the	O
baseline	O
ELECTRAsmall	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

2017	O
and	O
Peters	O
et	O
al	O
.	O
(	O

2018b	O
)	O
propose	O
CoVe	B-MethodName
and	O
ELMo	B-MethodName
respectively	O
which	O
both	O
utilize	O
LSTM	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
to	O
generate	O
contextualized	O
word	O
representations	O
.	O

2018	O
)	O
introduce	O
GPT	B-MethodName
that	O
changes	O
the	O
backbone	O
to	O
transformers	O
where	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
are	O
arrayed	O
interleavedly	O
.	O

BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Besides	O
designing	O
pre	O
-	O
training	O
objectives	O
,	O
some	O
other	O
works	O
try	O
to	O
extend	O
BERT	B-MethodName
by	O
incorporating	O
knowledge	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
or	O
with	O
multiple	O
languages	O
Conneau	O
and	O
Lample	O
,	O
2019;Chi	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

2019	O
)	O
.	O

To	O
solve	O
this	O
,	O
many	O
neural	O
architecture	O
search	O
algorithms	O
are	O
proposed	O
.	O

leverage	O
differentiable	O
neural	O
architecture	O
to	O
automatically	O
compress	O
BERT	B-MethodName
with	O
task	O
-	O
oriented	O
knowledge	O
distillation	O
for	O
specific	O
tasks	O
.	O

2020	O
)	O
utilize	O
architecture	O
search	O
to	O
improve	O
models	O
based	O
on	O
pre	O
-	O
trained	O
BERT	B-MethodName
for	O
the	O
relation	O
classification	O
task	O
.	O

Besides	O
,	O
Khetan	O
and	O
Karnin	O
(	O
2020	O
)	O
employ	O
pre	O
-	O
training	O
loss	O
to	O
help	O
prune	O
BERT	B-MethodName
,	O
but	O
their	O
method	O
can	O
not	O
find	O
new	O
architectures	O
.	O

The	O
layer	O
type	O
set	O
of	O
current	O
BERTlike	B-MethodName
models	O
consists	O
of	O
self	O
-	O
attention	O
for	O
information	O
communication	O
and	O
feed	O
-	O
forward	O
for	O
nonlinear	O
transformation	O
.	O

2019b	O
;	O
.	O

We	O
notice	O
that	O
convolution	O
(	O
LeCun	O
et	O
al	O
.	O
,	O

For	O
a	O
model	O
with	O
24	B-HyperparameterValue
layers	B-HyperparameterName
,	O
the	O
interleaved	O
order	O
can	O
be	O
expressed	O
by	O
the	O
following	O
list,[L	O
SA	O
1	O
,	O
L	O
FF	O
2	O
,	O
L	O
SA	O
3	O
,	O
L	O
FF	O
4	O
,	O
...	O
,	O
L	O
SA	O
23	O
,	O
L	O
FF	O
24	O
]	O
.	O
(	O

2020	O
)	O
can	O
be	O
expressed	O
as	O
}	O
The	O
accuracy	B-MetricName
is	O
used	O
to	O
guide	O
the	O
evolutionary	O
algorithm	O
for	O
generating	O
new	O
candidate	O
models.~After	O
T	O
iterations	O
,	O
the	O
candidate	O
with	O
best	O
pre	O
-	O
training	O
accuracy	B-MetricName
is	O
output	O
as	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
.	O

LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
can	O
be	O
scaled	O
up	O
to	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
medium	I-MethodName
/	I-MethodName
base	I-MethodName
with	O
larger	O
hidden	O
size	O
.	O

Beyond	O
the	O
above	O
manually	O
designed	O
orders	O
,	O
we	O
take	O
advantage	O
of	O
neural	O
architecture	O
search	O
to	O
identify	O
more	O
effective	O
layer	O
orders	O
for	O
pre	O
-	O
trained	O
models	O
.	O

The	O
order	O
to	O
be	O
discovered	O
can	O
be	O
expressed	O
as[L	O
1	O
,	O
L	O
2	O
,	O
...	O
,	O
L	O
i	O
,	O
...	O
,	O
L	O
N	O
]	O
,	O
(	O
4)where	O
L	B-HyperparameterName
i	O
∈	O
L	O
type	O
and	O
N	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
.	O

Here	O
,	O
N	B-HyperparameterName
is	O
set	O
to	O
24	B-HyperparameterValue
,	O
following	O
common	O
practice	O
.	O

To	O
reduce	O
the	O
search	O
computations	O
,	O
recent	O
NAS	O
works	O
(	O
Pham	O
et	O
al	O
.	O
,	O

Inspired	O
by	O
this	O
strategy	O
,	O
we	O
construct	O
a	O
supernet	O
where	O
each	O
layer	O
contains	O
all	O
types	O
of	O
layers	O
,	O
i.e.	O
,	O
self	O
-	O
attention	O
,	O
feedforward	O
,	O
and	O
dynamic	O
convolution	O
.	O

2019	O
)	O
is	O
utilized	O
as	O
the	O
pre	O
-	O
training	O
objective	O
to	O
pretrain	O
the	O
supernet	O
since	O
MLM	O
accuracy	B-MetricName
can	O
reflect	O
the	O
model	O
performance	O
on	O
downstream	O
tasks	O
(	O
Lan	O
et	O
al	O
.	O
,	O

However	O
,	O
BooksCorpus	B-DatasetName
is	O
no	O
longer	O
publicly	O
available	O
.	O

To	O
ease	O
reproduction	O
,	O
we	O
train	O
models	O
on	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
that	O
is	O
open	O
-	O
sourced	O
and	O
of	O
similar	O
size	O
with	O
the	O
corpus	O
used	O
by	O
BERT	B-DatasetName
.	O

Fine	O
-	O
tuning	O
Datasets	O
To	O
compare	O
our	O
model	O
with	O
other	O
pre	O
-	O
trained	O
models	O
,	O
we	O
fine	O
-	O
tune	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
including	O
various	O
tasks	O
for	O
general	O
language	O
understanding	O
,	O
and	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2018	O
for	O
question	B-TaskName
answering	I-TaskName
.	O

2020	O
)	O
,	O
we	O
define	O
different	O
model	O
sizes	O
,	O
i.e.	O
,	O
"	O
small	O
"	O
,	O
"	O
medium	O
"	O
and	O
"	O
base	O
"	O
,	O
with	O
the	O
same	O
layer	O
number	O
of	O
24	B-HyperparameterValue
but	O
different	O
hidden	B-HyperparameterName
sizes	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
384	B-HyperparameterValue
,	O
and	O
768	B-HyperparameterValue
,	O
respectively	O
.	O

The	O
detailed	O
hyperparameters	O
are	O
shown	O
in	O
Appendix	O
.	O

Since	O
the	O
layer	O
number	O
of	O
models	O
in	O
medium	O
and	O
base	O
sizes	O
are	O
the	O
same	O
as	O
that	O
of	O
the	O
small	O
-	O
sized	O
one	O
,	O
the	O
obtained	O
architecture	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
can	O
be	O
easily	O
scaled	O
up	O
to	O
the	O
ones	O
of	O
medium	O
and	O
base	O
sizes	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
to	O
pre	O
-	O
train	O
the	O
supernet	O
with	O
MLM	O
loss	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-4	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	B-HyperparameterValue
,	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
128	B-HyperparameterValue
and	O
pre	B-HyperparameterName
-	I-HyperparameterName
training	I-HyperparameterName
step	I-HyperparameterName
number	I-HyperparameterName
of	O
2	B-HyperparameterValue
million	I-HyperparameterValue
.	O

Evaluation	O
Setup	O
To	O
compare	O
with	O
other	O
pretrained	O
models	O
,	O
we	O
pre	O
-	O
train	O
the	O
searched	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
architecture	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
from	O
scratch	O
on	O
the	O
Open	B-DatasetName
-	I-DatasetName
WebText	I-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
using	O
Re	O
-	O
placed	O
Token	O
Detection	O
(	O
Clark	O
et	O
al	O
.	O
,	O

We	O
fine	O
-	O
tune	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2018	O
downstream	O
tasks	O
with	O
most	O
hyperparameters	O
the	O
same	O
as	O
those	O
of	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

For	O
GLUE	B-DatasetName
tasks	O
,	O
the	O
evaluation	O
metrics	O
are	O
Matthews	B-MetricName
correlation	I-MetricName
for	O
CoLA	B-DatasetName
,	O
Spearman	B-MetricName
correlation	I-MetricName
for	O
STS	B-DatasetName
,	O
and	O
accuracy	B-MetricName
for	O
other	O
tasks	O
,	O
which	O
are	O
averaged	O
to	O
get	O
GLUE	O
score	O
.	O

We	O
utilize	O
evaluation	O
metrics	O
of	O
Exact	B-MetricName
-	I-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
for	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
.	O

Similar	O
to	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

See	O
Appendix	O
for	O
more	O
evaluation	O
details	O
.	O

Layer	O
Variety	O
Various	O
models	O
are	O
constructed	O
with	O
different	O
layer	O
variety	O
designs	O
,	O
and	O
their	O
results	O
on	O
GLUE	B-DatasetName
development	O
set	O
are	O
shown	O
in	O
Table	O
1	O
.	O

For	O
the	O
layer	O
types	O
,	O
if	O
only	O
two	O
layer	O
types	O
are	O
provided	O
,	O
selecting	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
yields	O
the	O
best	O
result	O
,	O
which	O
can	O
always	O
achieve	O
performance	O
higher	O
than	O
80	B-MetricValue
under	O
different	O
search	O
methods	O
.	O

With	O
only	O
dynamic	O
convolution	O
and	O
feedforward	O
,	O
the	O
performance	O
drops	O
dramatically	O
to	O
around	O
65	B-MetricValue
.	O

Surprisingly	O
,	O
without	O
feed	O
-	O
forward	O
,	O
the	O
layer	O
set	O
of	O
dynamic	O
convolution	O
and	O
self	O
-	O
attention	O
can	O
still	O
achieve	O
relatively	O
good	O
score	O
,	O
near	O
80	B-MetricValue
.	O

When	O
using	O
all	O
the	O
three	O
layer	O
types	O
,	O
we	O
can	O
obtain	O
the	O
best	O
81.8	B-MetricValue
score	O
,	O
1.4	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
(	O
80.4	B-MetricValue
)	O
and	O
0.6	B-MetricValue
higher	O
than	O
the	O
model	O
searched	O
with	O
only	O
self	O
-	O
attention	O
and	O
feedforward	O
(	O
81.2	B-MetricValue
)	O
.	O

For	O
example	O
,	O
with	O
the	O
same	O
layer	O
types	O
of	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
,	O
the	O
EA	O
searched	O
model	O
obtains	O
81.2	B-MetricValue
score	O
,	O
improving	O
BERT	B-MethodName
/	O
ELECTRA	B-MethodName
by	O
6.1/0.8	B-MetricValue
as	O
well	O
as	O
Sandwich	B-MethodName
by	O
2.6	B-MetricValue
.	O

Blue	O
and	O
yellow	O
dots	O
denote	O
the	O
accuracy	B-MetricName
of	O
top	O
10	O
candidates	O
for	O
each	O
method	O
respectively	O
,	O
while	O
the	O
plots	O
mean	O
their	O
averages	O
.	O

Randomly	O
searched	O
"	O
produces	O
candidate	O
models	O
at	O
random	O
for	O
estimation	O
while	O
"	O
EA	O
searched	O
"	O
generates	O
candidate	O
models	O
with	O
evolutionary	O
algorithm	O
guided	O
by	O
the	O
pre	O
-	O
training	O
MLM	O
accuracy	B-MetricName
.	O

With	O
the	O
same	O
layer	O
types	O
,	O
EA	O
searched	O
orders	O
are	O
generally	O
better	O
than	O
randomly	O
searched	O
ones	O
while	O
the	O
randomly	O
searched	O
ones	O
are	O
generally	O
better	O
than	O
random	O
ones	O
.	O

Figure	O
3	O
plots	O
the	O
pre	O
-	O
trianing	O
MLM	O
evaluation	O
accuracy	B-MetricName
over	O
search	O
iterations	O
with	O
both	O
random	O
and	O
evolutionary	O
search	O
methods	O
.	O

It	O
shows	O
that	O
the	O
accuracy	B-MetricName
of	O
evolutionary	O
search	O
is	O
obviously	O
higher	O
than	O
that	O
of	O
random	O
search	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
evolutionary	O
search	O
.	O

According	O
to	O
these	O
observation	O
,	O
for	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
,	O
if	O
we	O
replace	O
the	O
bottom	O
two	O
layers	O
with	O
dynamic	O
convolutions	O
or	O
the	O
top	O
layer	O
with	O
self	O
-	O
attention	O
,	O
the	O
performance	O
can	O
be	O
improved	O
by	O
0.3	B-MetricValue
or	O
0.5	B-MetricValue
respectively	O
on	O
GLUE	B-DatasetName
development	O
set	O
.	O

If	O
we	O
replace	O
the	O
bottom	O
8	B-HyperparameterValue
layers	B-HyperparameterName
with	O
manually	O
designed	O
'	O
ccsfccsf	O
'	O
(	O
'	O
c	O
'	O
,	O
's	O
'	O
and	O
'	O
f	O
'	O
denote	O
dynamic	O
convolution	O
,	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
,	O
respectively	O
)	O
and	O
replace	O
the	O
top	O
8	B-HyperparameterValue
layers	B-HyperparameterName
with	O
manually	O
designed	O
'	O
ssfsssfs	O
'	O
together	O
,	O
we	O
observe	O
0.7	B-MetricValue
performance	O
improvement	O
.	O

For	O
larger	O
model	O
size	O
"	O
medium	O
"	O
and	O
"	O
base	O
"	O
,	O
LV	B-MethodName
-	I-MethodName
BERTs	I-MethodName
still	O
outperform	O
other	O
baseline	O
models	O
,	O
demonstrating	O
the	O
good	O
generalization	O
in	O
terms	O
of	O
model	O
size	O
.	O

We	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pretrained	O
models	O
(	O
Radford	O
et	O
al	O
.	O
,	O

2020	O
;	O
on	O
GLUE	B-DatasetName
testing	O
set	O
and	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
to	O
show	O
its	O
advantages	O
.	O

2020	O
)	O
,	O
due	O
to	O
the	O
computation	O
resource	O
limit	O
,	O
we	O
only	O
pre	O
-	O
train	O
our	O
models	O
in	O
small	O
/	O
medium	O
/	O
base	O
sizes	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
.	O

However	O
,	O
note	O
that	O
these	O
methods	O
rely	O
on	O
a	O
pre	O
-	O
trained	O
large	O
teacher	O
network	O
and	O
thus	O
are	O
orthogonal	O
to	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
methods	O
.	O

Table	O
3	O
presents	O
the	O
performance	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
pre	O
-	O
trained	O
models	O
on	O
GLUE	B-DatasetName
testing	O
set	O
.	O

It	O
shows	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
other	O
pre	O
-	O
trained	O
models	O
with	O
similar	O
model	O
size	O
.	O

Remarkably	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
/	I-MethodName
base	I-MethodName
achieve	O
79.8/85.1	B-MetricValue
,	O
1.8/1.6	B-MetricValue
higher	O
than	O
strong	O
baselines	O
ELECTRAsmall	B-MethodName
/	I-MethodName
base	I-MethodName
.	O

Even	O
compared	O
with	O
knowledge	O
distillation	O
based	O
model	O
MobileBERT	B-MethodName
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
medium	I-MethodName
still	O
outperforms	O
it	O
by	O
0.3	B-MetricValue
.	O

Since	O
there	O
is	O
nearly	O
no	O
single	O
model	O
submission	O
on	O
SQuAD	B-DatasetName
leaderboard	O
2	O
,	O
we	O
only	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
other	O
pre	O
-	O
trained	O
models	O
on	O
the	O
development	O
sets	O
.	O

We	O
find	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
outperforms	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
significantly	O
,	O
like	O
F1	B-MetricName
score	O
73.7	B-MetricName
versus	O
69.4	B-MetricName
on	O
SQuAD	B-DatasetName
2.0	I-DatasetName
.	O

However	O
,	O
when	O
we	O
generalize	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
to	O
base	O
size	O
,	O
the	O
gap	O
between	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
ELECTRA	B-MethodName
with	O
base	O
size	O
is	O
narrower	O
than	O
that	O
with	O
small	O
size	O
.	O

One	O
reason	O
may	O
be	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
is	O
searched	O
by	O
our	O
method	O
while	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
base	I-MethodName
is	O
only	O
generalized	O
from	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
with	O
larger	O
hidden	O
size	O
.	O

Experiment	O
results	O
show	O
our	O
obtained	O
model	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
on	O
various	O
downstream	O
tasks.program	O
and	O
Google	O
Cloud	O
Research	O
Credits	O
Program	O
for	O
the	O
support	O
of	O
computational	O
resources	O
.	O

Notice	O
that	O
h	B-HyperparameterName
×	O
d	B-HyperparameterName
=	O
cwhere	O
h	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
heads	I-HyperparameterName
and	O
d	B-HyperparameterName
is	O
the	O
head	B-HyperparameterName
dimension	I-HyperparameterName
.	O

The	O
above	O
K	O
and	O
Q	O
are	O
used	O
to	O
compute	O
their	O
similarity	O
matrix	O
M	O
which	O
is	O
then	O
used	O
to	O
generate	O
new	O
value	O
V	O
:	O
M	O
=	O
Softmax(KQ	B-HyperparameterValue
/	O
√	O
d	O
)	O
V	O
=	O
Reshape(M	O
V	O
)	O
,	O
(	O
10)where	O
M	O
∈	O
R	O
h×s×s	O
and	O
V	O
∈	O
R	O
s×c	O
.	O

Finally	O
,	O
a	O
linear	O
transformation	O
is	O
used	O
to	O
exchange	O
information	O
between	O
different	O
heads	O
,	O
followed	O
by	O
shortcut	O
connection	O
and	O
layer	O
normalization	O
,	O
O	O
=	O
Norm(V	B-HyperparameterValue
W	O
O	O
+	O
b	O
O	O
+	O
I),(11)whereW	O
O	O
∈	O
R	O
c×c	O
and	O
b	O
O	O
∈	O
R	O
c	O
.Feed	O
-	O
Forward	O
The	O
feed	O
-	O
forward	O
layer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
includes	O
two	O
linear	O
transformations	O
with	O
a	O
non	O
-	O
linear	O
activation	O
,	O
followed	O
by	O
a	O
shortcut	O
connection	O
and	O
layer	O
normalization	O
,	O
N	O
=	O
GELU(IW	B-HyperparameterValue
1	O
+	O
b	O
1	O
)	O
O	O
=	O
Norm(N	O
W	O
2	O
+	O
b	O
2	O
+	O
I),(12)where	O
W	O
1	O
∈	O
R	O
c×rc	O
and	O
W	O
2	O
∈	O
R	O
rc×c	O
with	O
a	O
ratio	O
r.	O
GELU(•	B-HyperparameterValue
)	O
denotes	O
the	O
Gaussian	O
Error	O
Linear	O
Unit	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
.	O

2018	O
)	O
,	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
GLUE	B-DatasetName
)	O
benchmark	O
is	O
a	O
collection	O
of	O
nine	O
tasks	O
for	O
natural	O
language	O
understanding	O
,	O
where	O
testing	O
set	O
labels	O
are	O
hidden	O
and	O
predictions	O
need	O
to	O
be	O
submitted	O
to	O
the	O
evaluation	O
server	O
3	O
.	O

We	O
provide	O
details	O
about	O
the	O
GLUE	B-DatasetName
tasks	O
below	O
.	O

CoLA	B-DatasetName
The	I-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Linguistic	I-DatasetName
Acceptability	I-DatasetName
(	O
Warstadt	O
et	O
al	O
.	O
,	O

2019	O
)	O
is	O
a	O
binary	B-TaskName
single	I-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
dataset	O
for	O
predicting	O
whether	O
an	O
sentence	O
is	O
grammatical	O
or	O
not	O
.	O

MRPC	O
The	O
Microsoft	B-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
is	O
a	O
dataset	O
for	O
the	O
task	O
to	O
predict	O
whether	O
two	O
sentences	O
are	O
semantically	O
equivalent	O
or	O
not	O
.	O

MNLI	B-DatasetName
The	I-DatasetName
Multi	I-DatasetName
-	I-DatasetName
Genre	I-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
Corpus	I-DatasetName
(	O
Williams	O
et	O
al	O
.	O
,	O

SST	B-DatasetName
The	O
Stanford	B-DatasetName
Sentiment	I-DatasetName
Treebank	I-DatasetName
(	O
Socher	O
et	O
al	O
.	O
,	O

2007	O
)	O
,	O
and	O
RTE5	B-DatasetName
(	O
Bentivogli	O
et	O
al	O
.	O
,	O

QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
dataset	O
converted	O
from	O
The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

QQP	B-DatasetName
The	O
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
dataset	O
(	O
Chen	O
et	O
al	O
.	O
,	O

STS	B-DatasetName
The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	O
(	O
Cer	O
et	O
al	O
.	O
,	O

WNLI	B-DatasetName
Winograd	B-DatasetName
NLI	I-DatasetName
(	O
Levesque	O
et	O
al	O
.	O
,	O

The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	O
(	O
SQuAD	O
1.1	O
)	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

Besides	O
this	O
data	O
,	O
the	O
upgraded	O
version	O
SQuAD	B-DatasetName
2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

For	O
supernet	O
,	O
We	O
pre	O
-	O
train	O
it	O
for	O
2	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
hyperparameters	O
listed	O
in	O
Table	O
5	O
,	O
using	O
Masked	O
Language	O
Modeling	O
(	O
MLM	O
)	O
pre	O
-	O
training	O
objective	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

This	O
objective	O
masks	O
15	O
%	O
input	O
tokens	O
that	O
require	O
the	O
model	O
to	O
predict	O
.	O

The	O
reason	O
to	O
use	O
this	O
objective	O
is	O
that	O
the	O
MLM	O
valida-4	O
https://gluebenchmark.com/faq	O
tion	O
accuracy	B-MetricName
can	O
reflect	O
the	O
performance	O
of	O
models	O
on	O
downstream	O
tasks	O
(	O
Lan	O
et	O
al	O
.	O
,	O

2020).For	O
pre	O
-	O
training	O
LV	B-MethodName
-	I-MethodName
BERTs	I-MethodName
and	O
other	O
compared	O
baselines	O
like	O
DynamicConv	B-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
Sandwich	B-MethodName
(	O
Press	O
et	O
al	O
.	O
,	O

We	O
pre	O
-	O
train	O
the	O
models	O
for	O
1	O
M	O
steps	O
,	O
mostly	O
using	O
the	O
same	O
hyperparameters	O
as	O
ELEC	B-MethodName
-	I-MethodName
TRA	I-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

For	O
downstream	O
task	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
that	O
needs	O
longer	O
input	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
,	O
we	O
pre	O
-	O
train	O
more	O
10	B-HyperparameterValue
%	I-HyperparameterValue
steps	B-HyperparameterName
with	O
the	O
sequence	B-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
to	O
learn	O
the	O
position	O
embedding	O
before	O
fine	O
-	O
tuning	O
.	O

For	O
fine	O
-	O
tuning	O
on	O
downstream	O
tasks	O
,	O
most	O
of	O
the	O
hyperparameters	O
are	O
the	O
same	O
as	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

See	O
Table	O
6	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O

This	O
paper	O
presents	O
a	O
technical	O
report	O
of	O
our	O
submission	O
to	O
the	O
4th	O
task	O
of	O
SemEval-2021	O
,	O
titled	O
:	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
.	O

Thus	O
,	O
common	O
contextualized	O
language	O
models	O
like	O
BERT	B-MethodName
miss	O
fine	O
representation	O
and	O
performance	O
due	O
to	O
the	O
limited	O
capacity	O
of	O
the	O
input	O
tokens	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
used	O
the	O
longformer	B-MethodName
model	O
to	O
better	O
process	O
the	O
sequences	O
.	O

Furthermore	O
,	O
we	O
utilized	O
the	O
method	O
proposed	O
in	O
the	O
longformer	B-MethodName
benchmark	O
on	O
wikihop	O
dataset	O
which	O
improved	O
the	O
accuracy	B-MetricName
on	O
our	O
task	O
data	O
from	O
(	O
23.01	B-MetricValue
%	I-MetricValue
and	O
22.95	B-MetricValue
%	I-MetricValue
)	O
achieved	O
by	O
the	O
baselines	O
for	O
subtask	O
1	O
and	O
2	O
,	O
respectively	O
,	O
to	O
(	O
70.30	B-MetricValue
%	I-MetricValue
and	O
64.38	B-MetricValue
%	I-MetricValue
)	O
.	O

Reading	B-TaskName
comprehension	I-TaskName
is	O
the	O
ability	O
to	O
understand	O
a	O
passage	O
either	O
by	O
human	O
or	O
machine	O
.	O

Generally	O
,	O
this	O
problem	O
can	O
contain	O
single	O
or	O
multiple	O
documents	O
as	O
context	O
(	O
containing	O
relevant	O
information	O
needed	O
to	O
understand	O
and	O
answer	O
the	O
question	O
)	O
,	O
a	O
question	O
(	O
a	O
sentence	O
with	O
at	O
least	O
one	O
asking	O
parameter	O
)	O
,	O
and	O
an	O
answer	O
(	O
which	O
is	O
the	O
parameter	O
value	O
of	O
the	O
question).In	O
the	O
Task	O
of	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
(	O
ReCAM	B-TaskName
)	O
,	O
we	O
have	O
one	O
passage	O
as	O
a	O
context	O
,	O
one	O
question	O
and	O
five	O
candidate	O
answers	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

For	O
each	O
instance	O
of	O
the	O
data	O
,	O
there	O
is	O
a	O
passage	O
,	O
a	O
question	O
with	O
a	O
missing	O
word	O
that	O
should	O
be	O
filled	O
based	O
on	O
the	O
passage	O
,	O
and	O
five	O
candidate	O
answers	O
to	O
the	O
question	O
.	O
...	O

In	O
this	O
weekend	O
's	O
upper	O
house	O
elections	O
...	O
Question	O
Abenomics	O
:	O
The	O
@Placeholder	O
and	O
the	O
risks	O
Answer	O
(	O
A	O
)	O
chances	O
(	O
B	O
)	O
prospective	O
(	O
C	O
)	O
security	O
(	O
D	O
)	O
objectives	O
(	O
E	O
)	O
threats	O
The	O
task	O
divides	O
into	O
two	O
subtasks	O
:	O
imperceptibility	O
and	O
non	O
-	O
specificity	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

Since	O
we	O
use	O
the	O
long	B-MethodName
document	I-MethodName
transformer	I-MethodName
model	O
(	O
Longformer	B-MethodName
(	O
Beltagy	O
et	O
al	O
.	O
,	O

We	O
have	O
evaluated	O
this	O
model	O
both	O
on	O
subtask-1	O
and	O
subtask-2	O
which	O
resulted	O
in	O
70	B-MetricValue
%	I-MetricValue
and	O
64	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
respectively	O
.	O

Therefore	O
,	O
we	O
have	O
about	O
40	B-MetricValue
%	I-MetricValue
improvement	O
compared	O
to	O
the	O
baseline	O
,	O
which	O
is	O
a	O
Gated	B-MethodName
Attention	I-MethodName
(	O
GA	B-MethodName
)	O
model	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

2020):•	O
Language	O
representation	O
:	O
this	O
module	O
is	O
responsible	O
to	O
encode	O
the	O
inputs	O
.	O

While	O
great	O
progress	O
has	O
been	O
made	O
in	O
this	O
field	O
by	O
using	O
contextual	O
word	O
representation	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

Deep	O
contextualized	O
language	O
models	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Generally	O
,	O
this	O
can	O
be	O
an	O
arbitrary	O
length	O
based	O
on	O
the	O
dataset	O
.	O

We	O
used	O
the	O
Longformer	B-MethodName
model	O
introduced	O
in	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
as	O
the	O
pre	O
-	O
trained	O
contextual	O
embedding	O
model	O
in	O
our	O
method	O
.	O

The	O
size	O
of	O
this	O
sequence	O
is	O
B	O
so	O
B	O
=	O
L	O
+	O
A.After	O
feeding	O
the	O
input	O
b	O
to	O
the	O
Longformer	O
model	O
,	O
we	O
apply	O
a	O
global	O
attention	O
only	O
on	O
a	O
(	O
concatenated	O
question	O
and	O
answer	O
candidates	O
)	O
,	O
and	O
the	O
rest	O
is	O
the	O
context	O
.	O

As	O
the	O
longformer	O
model	O
utilizes	O
a	O
base	O
model	O
(	O
like	O
RoBERTa	B-MethodName
without	O
the	O
self	O
-	O
attention	O
layer	O
,	O
in	O
our	O
case	O
)	O
,	O
we	O
denote	O
this	O
as	O
basemodel	O
function	O
that	O
outputs	O
the	O
encoded	O
sequence	O
of	O
the	O
input	O
.	O

If	O
GAttn	B-MethodName
denotes	O
the	O
global	O
attention	O
function	O
,	O
we	O
have	O
:	O
d	O
i	O
=	O
basemodel(b	O
)	O
(	O
3	O
)	O
g	O
i	O
=	O
GAttn(d	O
i	O
)	O
.1(i	O
∈	O
A	O
)	O
(	O
4)where	O
d	O
i	O
is	O
the	O
raw	O
output	O
vector	O
for	O
each	O
input	O
token	O
.	O

The	O
concatenated	O
input	O
vector	O
will	O
be	O
encoded	O
using	O
the	O
base	O
model	O
(	O
like	O
RoBERTa	B-MethodName
without	O
the	O
self	O
-	O
attention	O
layer	O
,	O
in	O
our	O
case	O
)	O
.	O

The	O
logit	O
(	O
score	O
)	O
of	O
each	O
ent	O
token	O
will	O
be	O
calculated	O
using	O
a	O
linear	O
transformation	O
function	O
,	O
then	O
the	O
prediction	O
distribution	O
over	O
the	O
answer	O
candidates	O
(	O
ent	O
tokens	O
)	O
will	O
be	O
outputted	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
.	O

And	O
the	O
probability	O
distribution	O
over	O
the	O
candidates	O
will	O
be	O
calculated	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
on	O
the	O
logits	O
.	O

Although	O
we	O
only	O
participated	O
in	O
the	O
second	O
subtask	O
,	O
we	O
will	O
evaluate	O
our	O
model	O
on	O
both	O
subtasks	O
here	O
.	O

We	O
will	O
explain	O
our	O
configurations	O
for	O
utilizing	O
the	O
model	O
on	O
the	O
task	O
as	O
well	O
as	O
other	O
baselines	O
which	O
are	O
the	O
BERT	B-MethodName
-	O
base	O
as	O
an	O
alternative	O
model	O
and	O
the	O
Gate	B-MethodName
-	I-MethodName
Attention	I-MethodName
(	I-MethodName
GA	I-MethodName
)	I-MethodName
as	O
our	O
task	O
baseline	O
.	O

Finally	O
,	O
a	O
brief	O
discussion	O
will	O
be	O
done	O
based	O
on	O
the	O
results	O
.	O

Popular	O
metrics	O
to	O
evaluate	O
these	O
models	O
are	O
F1	B-MetricName
,	O
EM	B-MetricName
(	O
Exact	B-MetricName
Match	I-MetricName
or	O
accuracy	B-MetricName
)	O
,	O
and	O
MRR	B-MetricName
(	O
Mean	B-MetricName
Reciprocal	I-MetricName
Rank	I-MetricName
)	O
.	O

As	O
the	O
precision	B-MetricName
and	O
recall	B-MetricName
in	O
our	O
task	O
are	O
equal	O
,	O
so	O
F1	B-MetricName
=	O
Precision	B-MetricName
=	O
Recall	B-MetricName
.	O

Also	O
,	O
F1	B-MetricName
and	O
EM	B-MetricName
are	O
the	O
same	O
.	O

And	O
,	O
the	O
use	O
of	O
MRR	B-MetricName
is	O
optional	O
,	O
so	O
the	O
metrics	O
used	O
to	O
evaluate	O
the	O
result	O
are	O
the	O
accuracy	O
and	O
the	O
F1	B-MetricName
.	O

The	O
baseline	O
model	O
(	O
GA	B-MethodName
)	O
is	O
trained	O
for	O
30	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
each	O
epoch	O
containing	O
101	B-HyperparameterValue
mini	B-HyperparameterName
-	I-HyperparameterName
batches	I-HyperparameterName
.	O

The	O
train	B-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	B-HyperparameterValue
.	O

Dropout	B-HyperparameterName
with	O
the	O
rate	O
of	O
0.5	B-HyperparameterValue
is	O
also	O
applied	O
to	O
the	O
hidden	O
states	O
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
0.001	B-HyperparameterValue
.	O

The	O
dimensionality	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
GloVe	I-HyperparameterName
embedding	I-HyperparameterName
is	O
300	B-HyperparameterValue
,	O
and	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
128	B-HyperparameterValue
.	O

In	O
fact	O
,	O
we	O
consider	O
the	O
output	O
vector	O
of	O
each	O
chunk	O
as	O
our	O
final	O
vector	O
to	O
be	O
linearly	O
transformed	O
into	O
single	O
logit	O
,	O
followed	O
by	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
using	O
the	O
crossentropy	O
loss	O
.	O

Note	O
that	O
the	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
here	O
is	O
bounded	O
to	O
512	B-HyperparameterValue
tokens	I-HyperparameterValue
,	O
and	O
the	O
model	O
includes	O
the	O
n	O
2	O
attention	O
mechanism	O
.	O

The	O
model	O
was	O
initialized	O
using	O
the	O
Longformer	B-MethodName
-	O
base	O
pre	O
-	O
training	O
weights	O
,	O
then	O
finetuned	O
in	O
each	O
of	O
the	O
subtasks	O
.	O

Due	O
to	O
the	O
performance	O
issues	O
,	O
the	O
model	B-HyperparameterName
max	I-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
is	O
set	O
to	O
4096	B-HyperparameterValue
tokens	I-HyperparameterValue
which	O
are	O
sufficient	O
in	O
our	O
case	O
.	O

We	O
also	O
used	O
the	O
RoBERTa	B-MethodName
-	O
large	O
tokenizer	O
to	O
tokenize	O
the	O
input	O
sequence	O
as	O
the	O
Longformer	B-MethodName
model	O
has	O
been	O
trained	O
on	O
using	O
this	O
configuration	O
.	O

We	O
used	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
and	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-5	B-HyperparameterValue
using	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
beta2=0.98	B-HyperparameterName
.	O

And	O
a	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
has	O
been	O
considered	O
to	O
regularize	O
the	O
model	O
and	O
avoid	O
overfitting	O
.	O

Our	O
proposed	O
model	O
is	O
trained	O
for	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
for	O
each	O
task	O
.	O

We	O
have	O
achieved	O
an	O
accuracy	B-MetricName
of	O
70	B-MetricValue
%	I-MetricValue
on	O
the	O
validation	O
set	O
,	O
which	O
improves	O
our	O
baseline	O
by	O
about	O
40	B-MetricValue
percent	I-MetricValue
.	O

Subtask2	O
measures	O
the	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
level	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
in	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

It	O
in	O
-	O
cludes	O
3318	O
training	O
samples	O
,	O
851	O
validation	O
samples	O
,	O
and	O
2017	O
test	O
samples	O
.	O

The	O
best	O
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
is	O
64	B-MetricValue
%	I-MetricValue
.	O

We	O
used	O
two	O
baselines	O
to	O
find	O
out	O
the	O
effect	O
of	O
using	O
a	O
pre	O
-	O
trained	O
model	O
rather	O
than	O
a	O
simple	O
RNN	B-MethodName
model	O
.	O

As	O
most	O
of	O
the	O
available	O
texts	O
for	O
training	O
consist	O
of	O
concrete	O
words	O
,	O
it	O
is	O
more	O
likely	O
to	O
leverage	O
the	O
language	O
understanding	O
to	O
less	O
abstract	O
words	O
to	O
achieve	O
a	O
better	O
result	O
.	O

Comparing	O
our	O
method	O
which	O
is	O
based	O
on	O
longformer	B-MethodName
model	O
to	O
usual	O
language	O
models	O
like	O
BERT	B-MethodName
indicates	O
a	O
new	O
insight	O
in	O
terms	O
of	O
passage	O
length	O
and	O
the	O
attention	O
mechanism	O
.	O

Popular	O
language	O
models	O
like	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
use	O
a	O
n	O
2	O
attention	O
which	O
requires	O
a	O
large	O
receptive	O
field	O
to	O
represent	O
long	O
passages	O
.	O

In	O
contrast	O
,	O
the	O
longformer	B-MethodName
global	O
attention	O
mechanism	O
relaxes	O
this	O
limitation	O
as	O
we	O
only	O
need	O
to	O
pay	O
attention	O
to	O
a	O
small	O
factor	O
of	O
context	O
and	O
more	O
focus	O
on	O
the	O
local	O
window	O
.	O

We	O
have	O
shown	O
how	O
different	O
approaches	O
can	O
be	O
leveraged	O
to	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
.	O

We	O
reformulated	O
the	O
longformer	B-MethodName
model	O
to	O
learn	O
abstract	B-TaskName
meaning	I-TaskName
as	I-TaskName
a	I-TaskName
new	I-TaskName
level	I-TaskName
of	I-TaskName
semantic	I-TaskName
in	I-TaskName
machine	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
contribution	O
in	O
SemEval-2021	B-DatasetName
Task	O
1	O
:	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
,	O
where	O
we	O
integrate	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
properties	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
as	O
features	O
within	O
a	O
Machine	O
Learning	O
(	O
ML	O
)	O
framework	O
for	O
predicting	O
lexical	O
complexity	O
.	O

In	O
particular	O
,	O
we	O
use	O
BERT	B-MethodName
contextualized	I-MethodName
word	I-MethodName
embeddings	I-MethodName
to	O
represent	O
the	O
semantic	O
meaning	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
.	O

At	O
the	O
beginning	O
,	O
most	O
of	O
these	O
methods	O
assumed	O
that	B-MetricName
lexical	I-MetricName
complexity	I-MetricName
is	O
binary	O
,	O
words	O
are	O
either	O
"	O
difficult	O
"	O
or	O
"	O
not	O
difficult	O
"	O
.	O

Thus	O
,	O
the	O
first	O
Complex	B-TaskName
Word	I-TaskName
Identification	I-TaskName
(	O
CWI	O
)	O
shared	O
task	O
referred	O
to	O
binary	O
identification	O
of	O
complex	O
words	O
(	O
Zampieri	O
et	O
al	O
.	O
,	O

Therefore	O
,	O
three	O
years	O
ago	O
,	O
the	O
CWI	B-TaskName
included	O
an	O
additional	O
probabilistic	O
classification	O
task	O
where	O
the	O
participants	O
were	O
asked	O
to	O
give	O
a	O
probability	O
of	O
the	O
given	O
target	O
word	O
in	O
particular	O
context	O
being	O
complex	O
(	O
Štajner	O
et	O
al	O
.	O
,	O

2018).Recently	O
,	O
CompLex	B-DatasetName
,	O
a	O
new	O
English	O
corpus	O
for	O
lexical	O
complexity	O
prediction	O
was	O
introduced	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

The	O
corpus	O
is	O
annotated	O
using	O
a	O
5	O
-	O
point	O
Likert	B-MetricName
scale	I-MetricName
(	O
1	O
-	O
5	O
)	O
(	O
corresponding	O
to	O
very	O
easy	O
,	O
easy	O
,	O
neutral	O
,	O
difficult	O
,	O
and	O
very	O
difficult	O
)	O
,	O
and	O
covers	O
3	O
genres	O
:	O
Bible	O
translation	O
,	O
European	O
Pariliament	O
proceedings	O
,	O
and	O
biomedical	O
articles	O
.	O

SemEval-2021	B-DatasetName
(	O
Task	O
1	O
)	O
shared	O
task	O
on	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
(	I-TaskName
LCP	I-TaskName
)	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

Linguistics	O
features	O
,	O
such	O
as	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tag	O
,	O
dependency	O
parsing	O
relations	O
,	O
and	O
syllable	O
counts	O
,	O
as	O
well	O
as	O
statistical	O
features	O
,	O
such	O
as	O
word	O
length	O
and	O
word	O
frequency	O
,	O
have	O
been	O
widely	O
used	O
for	O
predicting	O
lexical	B-MetricName
complexity	I-MetricName
(	O
Mukherjee	O
et	O
al	O
.	O
,	O

Some	O
of	O
these	O
works	O
found	O
WordNet	B-MethodName
(	O
Miller	O
,	O
1998	O
)	O
as	O
a	O
valuable	O
source	O
of	O
lexical	O
features	O
.	O

The	O
main	O
extracted	O
feature	O
is	O
the	O
number	O
of	O
synsets	O
,	O
but	O
also	O
information	O
on	O
hypernyms	O
,	O
hyponyms	O
,	O
holonym	O
,	O
and	O
meronym	O
is	O
useful	O
(	O
Gooding	O
and	O
Kochmar	O
,	O
2018;Hartmann	O
and	O
Dos	O
Santos	O
,	O
2018;Wani	O
et	O
al	O
.	O
,	O

These	O
word	O
embeddings	O
were	O
generated	O
using	O
Word2Vec	B-MethodName
context	O
-	O
independent	O
models	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

Word2Vec	B-MethodName
models	O
combine	O
different	O
senses	O
of	O
the	O
word	O
into	O
one	O
single	O
vector	O
.	O

However	O
,	O
recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
contextualized	O
word	O
representations	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

BERT	B-MethodName
model	O
generates	O
context	O
-	O
dependent	O
embeddings	O
that	O
allow	O
a	O
word	O
to	O
have	O
several	O
vector	O
representations	O
depending	O
on	O
the	O
context	O
in	O
which	O
it	O
is	O
used	O
.	O

In	O
contrast	O
to	O
previous	O
works	O
that	O
only	O
use	O
context	O
-	O
independent	O
embeddings	O
,	O
our	O
system	O
uses	O
the	O
BERT	B-MethodName
-	O
based	O
contextdependent	O
embeddings	O
.	O

We	O
adopt	O
a	O
supervised	O
Machine	O
Learning	O
(	O
ML	O
)	O
approach	O
for	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

Our	O
dataset	O
contains	O
three	O
corpora	O
:	O
Bible	B-DatasetName
,	I-DatasetName
Europarl	I-DatasetName
,	I-DatasetName
and	I-DatasetName
Biomedical	I-DatasetName
,	O
to	O
add	O
variation	O
.	O

The	O
POS	O
is	O
extracted	O
by	O
the	O
Spacy	B-MethodName
's	O
statistical	O
POS	O
tagger	O
1	O
.	O

Then	O
,	O
we	O
calculate	O
the	O
number	O
of	O
punctuation	O
marks	O
and	O
stopwords	O
in	O
the	O
sentence	O
(	O
two	O
features).Next	O
,	O
we	O
represent	O
syntactic	O
forms	O
by	O
POS	O
patterns	O
.	O

The	O
POS	O
pattern	O
refers	O
to	O
seven	O
words	O
,	O
the	O
target	O
word	O
and	O
three	O
words	O
before	O
and	O
after	O
it	O
.	O

We	O
also	O
measure	O
the	O
polysemy	O
degree	O
of	O
the	O
target	O
word	O
using	O
the	O
number	O
of	O
senses	O
in	O
WordNet	B-MethodName
.	O

Then	O
,	O
we	O
extract	O
the	O
target	O
word	O
frequency	O
using	O
Google	B-MethodName
N	I-MethodName
-	I-MethodName
gram	I-MethodName
4	I-MethodName
word	I-MethodName
frequencies	I-MethodName
.	O

We	O
use	O
the	O
BERT	B-MethodName
semantic	O
space	O
.	O

BERT	B-MethodName
is	O
a	O
bidirectional	B-MethodName
transformer	I-MethodName
pre	O
-	O
trained	O
on	O
a	O
large	O
corpus	O
containing	O
the	O
Toronto	B-DatasetName
Book	I-DatasetName
Corpus	I-DatasetName
and	O
Wikipedia	B-DatasetName
using	O
a	O
combination	O
of	O
masked	O
language	O
modeling	O
objective	O
and	O
next	O
sentence	O
prediction	O
.	O

BERT	B-MethodName
contextualizing	O
vectors	O
are	O
used	O
to	O
represent	O
the	O
semantic	O
meaning	O
of	O
the	O
sentence	O
by	O
averaging	O
the	O
BERT	B-MethodName
vectors	O
of	O
seven	O
words	O
,	O
the	O
target	O
word	O
and	O
three	O
words	O
before	O
and	O
after	O
it	O
.	O

Thus	O
,	O
our	O
semantic	O
representation	O
add	O
768	O
features	O
(	O
the	O
size	O
of	O
BERT	O
output	O
layer).To	O
extract	O
additional	O
features	O
,	O
we	O
use	O
two	O
machine	O
learning	O
algorithm	O
:	O
K	B-MethodName
-	I-MethodName
Means	I-MethodName
and	I-MethodName
k	I-MethodName
-	I-MethodName
Nearest	I-MethodName
Neighbors	I-MethodName
(	I-MethodName
KNN	I-MethodName
)	I-MethodName
algorithm	I-MethodName
.	O

K	B-MethodName
-	I-MethodName
Means	I-MethodName
is	O
an	O
unsupervised	O
learning	O
algorithm	O
used	O
for	O
clustering	O
.	O

We	O
encode	O
the	O
K	B-MethodName
-	I-MethodName
Mean	I-MethodName
results	O
by	O
four	O
binary	O
features	O
,	O
a	O
feature	O
per	O
cluster	O
(	O
k=4	B-HyperparameterName
)	O
.	O

The	O
results	O
of	O
the	O
KNN	B-MethodName
algorithm	O
are	O
encoded	O
similarly	O
.	O

KNN	B-MethodName
classifies	O
an	O
unseen	O
sentence	O
using	O
it	O
k	B-MethodName
nearest	I-MethodName
neighbors	I-MethodName
voting	O
.	O

We	O
use	O
four	O
complexity	B-HyperparameterName
classes	O
:	O
0	B-HyperparameterValue
-	I-HyperparameterValue
0.25	I-HyperparameterValue
,	I-HyperparameterValue
0.26	I-HyperparameterValue
-	I-HyperparameterValue
0.5	I-HyperparameterValue
,	I-HyperparameterValue
0.51	I-HyperparameterValue
-	I-HyperparameterValue
0.75	I-HyperparameterValue
,	I-HyperparameterValue
0.76	I-HyperparameterValue
-	I-HyperparameterValue
1	I-HyperparameterValue
.	O

First	O
,	O
we	O
discharged	O
features	O
that	O
decrease	O
the	O
system	O
performance	O
on	O
the	O
training	O
set	O
,	O
namely	O
,	O
the	O
POS	B-MethodName
pattern	I-MethodName
features	O
,	O
the	O
WordNet	B-MethodName
features	O
,	O
and	O
the	O
K	B-MethodName
-	I-MethodName
Means	I-MethodName
and	I-MethodName
KNN	I-MethodName
features	O
.	O

These	O
features	O
were	O
selected	O
using	O
the	O
Linear	B-MethodName
Regression	I-MethodName
algorithm	O
,	O
which	O
was	O
also	O
selected	O
as	O
a	O
baseline	O
algorithm	O
by	O
the	O
task	O
organizers	O
.	O

To	O
further	O
improve	O
the	O
performance	O
of	O
our	O
systems	O
,	O
we	O
used	O
additional	O
ML	O
algorithms	O
,	O
such	O
as	O
SVM	B-MethodName
and	O
XGBoost	B-MethodName
(	O
see	O
more	O
details	O
in	O
Section	O
3.3).Next	O
,	O
since	O
correlated	O
features	O
do	O
not	O
carry	O
unique	O
information	O
and	O
may	O
interfere	O
the	O
learning	O
,	O
we	O
tried	O
to	O
discharge	O
highly	O
correlated	O
features	O
.	O

First	O
,	O
we	O
define	O
an	O
initial	O
correlation	O
threshold	O
(	O
0.9	O
)	O
.	O

Next	O
,	O
if	O
we	O
still	O
have	O
more	O
features	O
than	O
desired	O
,	O
we	O
will	O
lower	O
the	O
correlation	B-HyperparameterName
threshold	I-HyperparameterName
(	O
by	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
and	O
repeat	O
the	O
process	O
.	O

This	O
approach	O
improved	O
the	O
performance	O
of	O
the	O
SVM	B-MethodName
and	O
Linear	B-MethodName
Regression	I-MethodName
models	O
(	O
selecting	O
97	O
features	O
)	O
,	O
but	O
did	O
not	O
increase	O
the	O
performance	O
of	O
the	O
XGBOOST	B-MethodName
method	O
.	O

We	O
note	O
that	O
we	O
also	O
tried	O
to	O
filter	O
out	O
feature	O
using	O
the	O
principal	B-MethodName
component	I-MethodName
analysis	I-MethodName
(	I-MethodName
PCA	I-MethodName
)	I-MethodName
feature	O
selection	O
method	O
(	O
Song	O
et	O
al	O
.	O
,	O

2010	O
)	O
.	O

PCA	B-MethodName
aims	O
to	O
pick	O
a	O
subset	O
of	O
features	O
that	O
retains	O
as	O
much	O
information	O
present	O
in	O
the	O
full	O
data	O
as	O
possible	O
.	O

PCA	B-MethodName
was	O
performed	O
both	O
on	O
the	O
full	O
feature	O
list	O
and	O
on	O
specific	O
features	O
,	O
such	O
as	O
BERT	B-MethodName
features	O
,	O
but	O
it	O
was	O
not	O
successful	O
.	O

We	O
resulted	O
with	O
the	O
following	O
list	O
of	O
101	O
features:•	O
Biomedical	O
corpus	O
indicator	O
•	O
94	O
features	O
from	O
BERT	B-MethodName
vector	O
It	O
is	O
interesting	O
to	O
note	O
that	O
even	O
though	O
,	O
there	O
are	O
12	O
POS	O
tags	O
,	O
only	O
2	O
are	O
informative	O
for	O
the	O
complexity	O
prediction	O
task	O
.	O

Out	O
of	O
the	O
BERT	B-MethodName
768	O
features	O
,	O
only	O
94	O
remained	O
(	O
12.2	O
%	O
of	O
the	O
vector).The	O
BERT	B-MethodName
representation	O
of	O
the	O
sentence	O
is	O
generated	O
by	O
pre	O
-	O
trained	O
language	O
representation	O
model	O
.	O

Since	O
one	O
of	O
our	O
corpora	O
is	O
from	O
the	O
Biomedical	O
domain	O
,	O
we	O
examined	O
the	O
system	O
performance	O
using	O
the	O
domain	O
specific	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

Figure	O
1	O
shows	O
a	O
comparison	O
between	O
the	O
error	O
rate	O
of	O
our	O
system	O
using	O
the	O
classic	O
BERT	B-MethodName
and	O
BioBERT	B-MethodName
(	O
BERT	B-MethodName
on	O
the	O
left	O
and	O
BioBERT	B-MethodName
on	O
the	O
right	O
)	O
.	O

Columns	O
from	O
left	O
to	O
right	O
:	B-DatasetName
Bible	I-DatasetName
,	I-DatasetName
Biomedical	I-DatasetName
,	I-DatasetName
and	I-DatasetName
Europarl	I-DatasetName
.	O

Surprisingly	O
,	O
the	O
error	O
rate	O
of	O
the	O
BioBERT	B-MethodName
on	O
the	O
Biomedical	O
domain	O
is	O
higher	O
than	O
that	O
of	O
the	O
classic	O
BERT	B-MethodName
.	O

We	O
combined	O
the	O
features	O
in	O
a	O
supervised	O
classification	O
framework	O
using	O
five	O
ML	O
methods	O
:	O
Linear	B-MethodName
Regression	I-MethodName
,	I-MethodName
Supported	I-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	I-MethodName
SVM	I-MethodName
)	I-MethodName
,	I-MethodName
XGBoost	I-MethodName
(	I-MethodName
XGB	I-MethodName
)	I-MethodName
,	I-MethodName
KNN	I-MethodName
,	I-MethodName
and	I-MethodName
Stacking	I-MethodName
(	I-MethodName
Stack	I-MethodName
)	I-MethodName
.	O

We	O
ran	O
these	O
ML	O
methods	O
by	O
the	O
scikit	B-MethodName
-	I-MethodName
learn	I-MethodName
open	O
-	O
source	O
machine	O
-	O
learning	O
package	O
in	O
python	O
5	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O

The	O
MAE	B-MethodName
is	O
omitted	O
from	O
the	O
table	O
because	O
it	O
is	O
similar	O
for	O
all	O
the	O
ML	O
algorithms	O
(	O
0.01	O
)	O
.	O

In	O
Figure	O
2	O
,	O
we	O
present	O
the	O
classification	O
confusion	O
matrix	O
of	O
the	O
XGBoost	B-MethodName
algorithm	O
.	O

Most	O
of	O
the	O
classification	B-MetricName
errors	I-MetricName
(	O
18.54	B-MetricValue
%	I-MetricValue
)	O
were	O
due	O
to	O
incorrect	O
classification	O
of	O
very	O
easy	O
words	O
as	O
easy	O
.	O

There	O
were	O
also	O
errors	O
in	O
the	O
opposite	O
direction	O
(	O
4.36	B-MetricValue
%	I-MetricValue
)	O
.	O

Most	O
of	O
the	O
rest	O
of	O
the	O
classifications	O
were	O
between	O
neutral	O
and	O
easy	O
in	O
both	O
directions	O
(	O
7.42	B-MetricValue
%	I-MetricValue
+	I-MetricValue
6.43	I-MetricValue
%	I-MetricValue
=	I-MetricValue
13.85	I-MetricValue
%	I-MetricValue
)	O
.	O

We	O
have	O
implemented	O
a	O
system	O
that	O
incorporates	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
features	O
to	O
predict	O
lexical	O
complexity	O
of	O
target	O
word	O
in	O
context	O
.	O

BERT	B-MethodName
semantic	O
space	O
was	O
used	O
to	O
represent	O
the	O
word	O
and	O
its	O
context	O
.	O

Even	O
though	O
our	O
system	O
was	O
not	O
highly	O
ranked	O
,	O
we	O
believe	O
that	O
some	O
of	O
the	O
presented	O
ideas	O
can	O
be	O
useful	O
for	O
future	O
research	O
on	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

In	O
particular	O
,	O
we	O
think	O
that	O
BERT	B-MethodName
is	O
a	O
powerful	O
model	O
that	O
should	O
be	O
explored	O
.	O

Perhaps	O
,	O
fine	O
-	O
tuning	O
BERT	B-MethodName
for	O
the	O
complexity	O
prediction	O
task	O
would	O
increase	O
the	O
system	O
performance	O
.	O

We	O
investigate	O
the	O
less	O
-	O
explored	O
task	O
of	O
generating	B-TaskName
open	I-TaskName
-	I-TaskName
ended	I-TaskName
questions	I-TaskName
that	I-TaskName
are	I-TaskName
typically	I-TaskName
answered	I-TaskName
by	I-TaskName
multiple	I-TaskName
sentences	I-TaskName
.	O

We	O
then	O
propose	O
a	O
novel	O
question	B-TaskName
type	I-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
,	O
augmented	O
by	O
a	O
semantic	B-TaskName
graph	I-TaskName
representation	I-TaskName
,	O
to	O
jointly	O
predict	O
question	O
focuses	O
and	O
produce	O
the	O
question	O
.	O

Human	O
judges	O
also	O
rate	O
our	O
model	O
outputs	O
highly	O
in	O
answerability	B-MetricName
,	O
coverage	B-MetricName
of	I-MetricName
scope	I-MetricName
,	O
and	O
overall	B-MetricName
quality	I-MetricName
.	O

Question	B-TaskName
-	I-TaskName
asking	I-TaskName
has	O
long	O
served	O
as	O
an	O
effective	O
instrument	O
for	O
knowledge	B-TaskName
learning	I-TaskName
(	O
Andre	O
,	O
1979;Tobin	O
,	O
1990	O
)	O
and	O
assessing	O
learning	O
progress	O
(	O
Holme	O
,	O
2003;Downing	O
and	O
Yudkowsky	O
,	O
2009;Livingston	O
,	O
2009	O
)	O
.	O

2019	O
)	O
,	O
this	O
work	O
is	O
interested	O
in	O
generating	B-TaskName
open	I-TaskName
-	I-TaskName
ended	I-TaskName
questions	I-TaskName
that	I-TaskName
require	I-TaskName
deep	I-TaskName
comprehension	I-TaskName
and	I-TaskName
long	I-TaskName
-	I-TaskName
form	I-TaskName
answers	I-TaskName
(	O
Labutov	O
et	O
al	O
.	O
,	O

2015	O
)	O
.	O

It	O
also	O
provides	O
a	O
little	O
bit	O
of	O
respect	O
on	O
the	O
street	O
...	O
Figure	O
1	O
:	O
Open	O
-	O
ended	O
questions	O
generated	O
by	O
different	O
models	O
after	O
reading	O
the	O
same	O
input	O
:	O
(	O
1	O
)	O
BART	B-MethodName
decoded	O
with	O
nucleus	O
sampling	O
,	O
(	O
2	O
)	O
BART	B-MethodName
that	O
considers	O
different	O
question	O
words	O
,	O
and	O
(	O
3	O
)	O
our	O
type	B-MethodName
-	I-MethodName
aware	I-MethodName
generator	I-MethodName
TPLGEN	I-MethodName
,	O
that	O
predicts	O
focuses	O
and	O
operates	O
with	O
generated	O
templates	O
(	O
to	O
the	O
left	O
of	O
the	O
arrows	O
)	O
.	O

Questions	O
generated	O
by	O
our	O
model	O
have	O
diverse	O
TYPEs	O
.	O

2019	O
)	O
and	O
building	O
open	B-MethodName
-	I-MethodName
domain	I-MethodName
dialogue	I-MethodName
systems	I-MethodName
(	O
Shum	O
et	O
al	O
.	O
,	O

We	O
first	O
introduce	O
a	O
new	O
question	O
type	O
ontology	O
,	O
drawn	O
upon	O
researches	O
in	O
cognitive	O
science	O
and	O
psychology	O
(	O
Graesser	O
et	O
al	O
.	O
,	O

1992	O
)	O
,	O
to	O
capture	O
deeper	O
levels	O
of	O
cognition	O
,	O
such	O
as	O
causal	B-TaskName
reasoning	I-TaskName
and	I-TaskName
judgments	I-TaskName
.	O

2018	O
)	O
)	O
,	O
our	O
framework	O
is	O
built	O
on	O
large	O
pre	O
-	O
trained	B-MethodName
BART	I-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

It	O
is	O
further	O
augmented	O
by	O
a	O
semantic	B-MethodName
graph	I-MethodName
that	O
leverages	O
both	O
semantic	O
roles	O
and	O
dependency	O
relations	O
,	O
facilitating	O
long	O
text	O
comprehension	O
to	O
pinpoint	O
salient	O
concepts	O
.	O

Moreover	O
,	O
to	O
achieve	O
the	O
goal	O
of	O
producing	O
various	O
types	O
of	O
questions	O
from	O
the	O
same	O
input	O
,	O
we	O
investigate	O
two	O
model	O
variants	O
that	O
use	O
templates	O
to	O
improve	O
controllability	O
and	O
generation	O
diversity	O
:	O
one	O
using	O
pre	O
-	O
identified	O
exemplars	O
,	O
the	O
other	O
employing	O
generated	O
templates	O
to	O
guide	O
question	B-TaskName
writing	I-TaskName
,	O
with	O
sample	O
outputs	O
displayed	O
in	O
Figure	O
1.For	O
experiments	O
,	O
we	O
collect	O
two	O
new	O
large	O
-	O
scale	O
datasets	O
consisting	O
of	O
open	O
-	O
ended	O
questions	O
with	O
answers	O
from	O
(	O
1	O
)	O
Yahoo	B-DatasetName
Answers	I-DatasetName
2	I-DatasetName
L6	I-DatasetName
dataset	O
and	O
(	O
2	O
)	O
popular	O
question	O
-	O
asking	O
communities	O
on	O
Reddit	B-DatasetName
3	I-DatasetName
,	O
consisting	O
of	O
291	O
K	O
and	O
720	O
K	O
question	O
-	O
answer	O
pairs	O
,	O
respectively	O
.	O

Compared	O
to	O
existing	O
popular	O
QA	O
datasets	O
,	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
.	O
,	O

Automatic	O
metrics	O
show	O
that	O
our	O
type	B-MethodName
-	I-MethodName
aware	I-MethodName
question	I-MethodName
generation	I-MethodName
model	O
outperforms	O
competitive	O
comparisons	O
,	O
highlighting	O
the	O
effectiveness	O
of	O
semantic	B-MethodName
graph	I-MethodName
-	I-MethodName
augmented	I-MethodName
representation	I-MethodName
and	I-MethodName
joint	I-MethodName
modeling	I-MethodName
of	O
focus	O
prediction	O
and	O
question	O
generation	O
.	O

Adding	O
templates	B-MethodName
further	O
promotes	O
question	O
diversity	O
,	O
as	O
evaluated	O
by	O
both	O
automatic	O
evaluation	O
and	O
human	O
assessment	O
.	O

Question	B-TaskName
generation	I-TaskName
has	O
long	O
been	O
studied	O
to	O
reduce	O
human	O
efforts	O
in	O
constructing	O
questions	O
for	O
knowledge	O
learning	O
evaluation	O
(	O
Mitkov	O
and	O
Ha	O
,	O
2003;Brown	O
et	O
al	O
.	O
,	O

Early	O
work	O
relies	O
on	O
syntactic	B-MethodName
transformation	I-MethodName
to	O
convert	O
declarative	O
sentences	O
to	O
questions	O
(	O
Heilman	O
and	O
Smith	O
,	O
2010;Chali	O
and	O
Hasan	O
,	O
2015	O
)	O
.	O

2018;Zhou	O
et	O
al	O
.	O
,	O

Attempts	O
are	O
also	O
made	O
toward	O
creating	O
complex	O
questions	O
that	O
require	O
multi	B-TaskName
-	I-TaskName
hop	I-TaskName
reasoning	I-TaskName
over	O
the	O
given	O
text	O
,	O
and	O
graph	O
-	O
based	O
representations	O
have	O
been	O
an	O
enabling	O
tool	O
to	O
facilitate	O
the	O
access	O
to	O
both	O
entities	O
and	O
relations	O
(	O
Pan	O
et	O
al	O
.	O
,	O

While	O
our	O
model	O
also	O
enhances	O
the	O
input	O
with	O
a	O
semantic	B-MethodName
graph	I-MethodName
,	O
it	O
boasts	O
a	O
richer	O
representation	O
by	O
including	O
both	O
dependency	O
and	O
semantic	O
relations	O
,	O
with	O
predicted	O
question	O
focuses	O
highlighted	O
via	O
extra	B-MethodName
node	I-MethodName
embeddings	I-MethodName
.	O

Given	O
the	O
data	O
-	O
driven	O
nature	O
of	O
question	O
generation	O
and	O
answering	O
tasks	O
,	O
recent	O
studies	O
take	O
advantage	O
of	O
the	O
availability	O
of	O
large	O
-	O
scale	O
QA	O
datasets	O
,	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
.	O
,	O

A	O
dataset	O
closer	O
to	O
ours	O
is	O
ELI5	B-DatasetName
,	O
which	O
also	O
obtains	O
open	O
-	O
ended	O
questionanswer	O
pairs	O
from	O
Reddit	B-DatasetName
,	O
while	O
one	O
of	O
our	O
datasets	O
includes	O
more	O
Reddit	O
communities	O
and	O
thus	O
covers	O
a	O
wider	O
range	O
of	O
topics	O
.	O

Generating	O
diverse	O
questions	O
is	O
much	O
less	O
studied	O
,	O
with	O
existing	O
approaches	O
mainly	O
focusing	O
on	B-MethodName
entity	I-MethodName
replacement	I-MethodName
(	O
Cho	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
sampling	B-MethodName
decoding	I-MethodName
(	O
Sultan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
post	B-MethodName
-	I-MethodName
filtering	I-MethodName
.	O

DISJUNCTIVE	O
the	O
true	O
one	O
given	O
multiple	O
events	O
or	O
concepts	O
,	O
where	O
comparison	O
among	O
options	O
is	O
not	O
needed	O
.	O

EXTENT	O
the	O
extent	O
or	O
quantity	O
of	O
an	O
event	O
or	O
a	O
concept	O
.	O

CONSEQUENCE	O
the	O
consequences	O
or	O
results	O
of	O
an	O
event	O
.	O

A	O
second	O
dataset	O
with	O
question	O
-	O
answer	O
pairs	O
is	O
collected	O
from	O
the	O
Yahoo	B-DatasetName
Answers	I-DatasetName
L6	I-DatasetName
corpus	O
4	O
,	O
which	O
covers	O
a	O
broader	O
range	O
of	O
topics	O
than	O
the	O
Reddit	B-DatasetName
data	O
.	O

To	O
ensure	O
both	O
questions	O
and	O
answers	O
are	O
well	O
-	O
formed	O
,	O
human	O
inspection	O
is	O
conducted	O
in	O
multiple	O
iterations	O
to	O
design	O
rules	O
to	O
filter	O
out	O
improper	O
samples	O
.	O

Each	O
dataset	O
is	O
then	O
divided	O
into	O
train	O
,	O
validation	O
and	O
test	O
sets	O
with	O
a	O
90%/5%/5	B-HyperparameterValue
%	I-HyperparameterValue
split	O
.	O

The	O
annotation	O
guideline	O
and	O
examples	O
for	O
each	O
question	O
type	O
are	O
shown	O
in	O
Table	O
12	O
in	O
Appendix	O
A.Training	O
Question	O
Type	O
Classifiers	O
.	O

Both	O
classifiers	O
are	O
based	O
on	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

γ	O
q	O
achieves	O
a	O
macro	B-MetricName
F1	I-MetricName
score	O
of	O
0.80	B-MetricValue
on	O
a	O
reserved	O
test	O
set	O
,	O
with	O
data	O
splits	O
detailed	O
in	O
Appendix	O
B.	O
To	O
train	O
γ	O
a	O
,	O
in	O
addition	O
to	O
the	O
annotated	O
questions	O
,	O
we	O
run	O
γ	O
q	O
on	O
unlabeled	O
questions	O
in	O
Reddit	O
and	O
Yahoo	O
and	O
include	O
samples	O
whose	O
type	O
prediction	B-MetricName
confidence	I-MetricName
score	I-MetricName
is	O
above	O
0.9	B-MetricValue
.	O

γ	O
a	O
obtains	O
macro	B-MetricName
F1	I-MetricName
scores	O
of	O
0.48	B-MetricValue
and	O
0.46	B-MetricValue
on	O
the	O
same	O
reserved	O
test	O
set	O
over	O
all	O
types	O
after	O
training	O
on	O
Yahoo	O
and	O
Reddit	O
,	O
respectively	O
.	O

Yahoo	B-DatasetName
dataset	O
is	O
more	O
balanced	O
,	O
with	O
PROCEDURAL	O
questions	O
being	O
the	O
most	O
frequent	O
type	O
(	O
19.9	O
%	O
of	O
all	O
samples	O
)	O
.	O

Distri-	O
butions	O
of	O
question	O
types	O
for	O
the	O
two	O
datasets	O
are	O
listed	O
in	O
Table	O
8	O
in	O
Appendix	O
B.4	O
Type	B-TaskName
-	I-TaskName
aware	I-TaskName
Open	I-TaskName
-	I-TaskName
ended	I-TaskName
Question	I-TaskName
GenerationIn	I-TaskName
this	O
section	O
,	O
we	O
present	O
our	O
type	B-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
.	O

Our	O
generator	O
is	O
built	O
on	O
top	O
of	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

Question	O
focuses	O
are	O
first	O
detected	O
based	O
on	O
the	O
semantic	B-MethodName
graph	I-MethodName
,	O
which	O
then	O
guide	O
question	O
generation	O
via	O
cross	O
-	O
attentions	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

Although	O
the	O
joint	O
modeling	O
of	O
focus	B-TaskName
prediction	I-TaskName
and	I-TaskName
question	I-TaskName
generation	I-TaskName
has	O
been	O
studied	O
before	O
,	O
our	O
design	O
differs	O
by	O
using	O
shared	O
representations	O
consisting	O
of	O
the	O
input	O
text	O
and	O
semantic	O
graph	O
,	O
and	O
the	O
prediction	O
of	O
focuses	O
are	O
included	O
through	O
gating	O
mechanisms	O
,	O
whereas	O
previous	O
work	O
,	O
e.g.	O
Pan	O
et	O
al	O
.	O
(	O

Below	O
,	O
we	O
first	O
describe	O
constructing	O
the	O
semantic	B-MethodName
graph	I-MethodName
-	I-MethodName
augmented	I-MethodName
encoder	I-MethodName
,	O
followed	O
by	O
the	O
joint	O
modeling	O
of	O
two	O
tasks	O
.	O

Improving	O
Long	O
Text	O
Comprehension	O
with	O
Semantic	B-MethodName
Graph	I-MethodName
.	O

To	O
construct	O
the	O
semantic	B-MethodName
graph	I-MethodName
,	O
for	O
each	O
sentence	O
,	O
we	O
start	O
with	O
obtaining	O
its	O
dependency	O
tree	O
using	O
Stanford	B-MethodName
CoreNLP	I-MethodName
(	O
Manning	O
et	O
al	O
.	O
,	O

2010	O
)	O
,	O
we	O
extract	O
semantic	O
roles	O
and	O
their	O
relations	O
with	B-MethodName
AllenNLP	I-MethodName
(	O
Shi	O
and	O
Lin	O
,	O
2019	O
)	O
.	O

To	O
merge	O
the	O
two	O
sources	O
of	O
information	O
,	O
we	O
add	O
an	O
edge	O
in	O
the	O
dependency	O
tree	O
to	O
connect	O
the	O
head	O
word	O
of	O
the	O
predicate	O
and	O
the	O
head	O
word	O
of	O
each	O
semantic	O
role	O
.	O

Joint	B-MethodName
Modeling	I-MethodName
with	O
Cross	O
-	O
attentions	O
.	O

Given	O
a	O
predicted	O
question	O
type	O
t	O
and	O
a	O
multi	O
-	O
sentence	O
textx	O
=	O
{	O
x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
n	O
}	O
,	O
the	B-MethodName
BART	I-MethodName
encoder	O
builds	O
the	O
contextual	O
representation	O
H	O
=	O
{	O
h	O
0	O
,	O
h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
n	O
}	O
at	O
the	O
last	O
layer	O
,	O
where	O
h	O
0	O
is	O
for	O
t.	O
To	O
encode	O
the	O
semantic	O
graph	O
,	O
we	O
initialize	O
the	O
node	O
representation	O
for	O
node	O
v	O
i	O
by	O
taking	O
the	O
average	O
contextual	O
representations	O
of	O
its	O
tokens	O
and	O
appending	O
four	O
bits	O
encoding	O
the	O
number	O
of	O
nodes	O
(	O
capped	O
at	O
10	O
)	O
that	O
are	O
merged	O
into	O
v	O
i	O
,	O
to	O
add	O
frequency	O
information	O
.	O

We	O
then	O
apply	O
graph	B-MethodName
attention	I-MethodName
networks	I-MethodName
(	O
GATs	O
)	O
(	O
Veličković	O
et	O
al	O
.	O
,	O

2018	O
)	O
of	O
L	O
layers	O
to	O
update	O
the	O
representations	O
as	O
follows	O
:	O
v	O
(	O
l	O
)	O
i	O
=	O
j∈Ni	O
a	O
i	O
,	O
j	O
W	O
(	O
l	O
)	O
v	O
(	O
l−1	O
)	O
j	O
(	O
1)where	O
W	O
(	O
l	O
)	O
is	O
a	O
learnable	O
parameter	O
for	O
the	O
l	O
-	O
th	O
layer	O
,	O
and	O
N	O
i	O
denotes	O
the	O
neighbors	O
of	O
v	O
i	O
.	O

The	O
attention	O
score	O
a	O
i	O
,	O
j	O
is	O
calculated	O
as	O
in	O
GATs	B-MethodName
.	O

To	O
predict	O
focuses	O
,	O
the	O
final	O
node	O
representation	O
v	O
(	O
L	O
)	O
i	O
is	O
fed	O
into	O
the	O
following	O
feedforward	B-MethodName
network	I-MethodName
,	O
yielding	O
the	O
probability	O
of	O
v	O
i	O
being	O
a	O
focus	O
as	O
:p	O
f	O
ocus	O
(	O
v	O
i	O
=	O
1	O
)	O
=	O
σ(W	O
1	O
tanh(W	O
2	O
v	O
(	O
L	O
)	O
i	O
)	O
)	O
(	O
2)where	O
W	O
1	O
and	O
W	O
2	O
are	O
learnable	O
parameters	O
.	O

To	O
generate	O
the	O
question	O
,	O
we	O
use	O
the	O
gating	O
mechanism	O
to	O
inform	O
the	O
focus	O
prediction	O
results	O
,	O
where	O
new	O
node	O
representations	O
after	O
being	O
weighted	O
by	O
the	O
focus	O
probability	O
are	O
:	O
v	O
(	O
L	O
)	O
i	O
=	O
g	O
i	O
v	O
(	O
L	O
)	O
i	O
g	O
i	O
=	O
p	O
f	O
ocus	O
(	O
v	O
i	O
=	O
1)(3)Our	O
model	O
benefits	O
from	O
both	O
large	O
pre	O
-	O
training	O
and	O
hybrid	O
semantic	O
graphs	O
by	O
adding	O
a	O
separate	O
cross	O
attention	O
for	O
node	O
presentations	O
in	O
each	O
BART	B-MethodName
decoder	O
layer	O
.	O

We	O
then	O
design	O
separate	O
cross	O
attentions	O
to	O
attend	O
(	O
1	O
)	O
the	O
output	O
of	O
the	O
BART	B-MethodName
encoder	O
,	O
yielding	O
z	O
e	O
,	O
and	O
(	O
2	O
)	O
the	O
node	O
representations	O
V	O
(	O
L	O
)	O
,	O
producing	O
z	O
v	O
,	O
which	O
are	O
formulated	O
as	O
:	O
z	O
e	O
=	O
LN(z	O
s	O
+	O
Attn(z	O
s	O
,	O
H	O
)	O
)	O
(	O
4)z	O
v	O
=	O
LN(z	O
e	O
+	O
Attn(z	O
e	O
,	O
V	O
(	O
L	O
)	O
)	O
)	O
(	O
5	O
)	O
z	O
=	O
LN(z	O
v	O
+	O
FFN(z	O
v	O
)	O
)	O
(	O
6)where	O
z	O
s	O
denotes	O
the	O
output	O
of	O
self	O
attentions	O
for	O
the	O
current	O
layer	O
,	O
and	O
z	O
is	O
the	O
output	O
for	O
the	O
layer	O
.	O

We	O
thus	O
propose	O
to	O
leverage	O
question	O
templates	O
to	O
gain	O
stronger	O
controllability	O
.	O

Below	O
we	O
first	O
present	O
how	O
to	O
automatically	O
extract	O
templates	O
from	O
the	O
training	O
set	O
,	O
and	O
then	O
introduce	O
two	O
model	O
variants	O
that	O
are	O
built	O
on	O
the	O
JOINTGEN	B-MethodName
framework	O
:	O
EXPLGEN	B-MethodName
uses	O
exemplar	O
templates	O
to	O
guide	O
the	O
model	O
to	O
generate	O
questions	O
of	O
selected	O
types	O
,	O
and	O
TPLGEN	B-MethodName
adds	O
an	O
extra	O
step	O
to	O
first	O
generate	O
type	O
-	O
specific	O
templates	O
.	O

We	O
further	O
consider	O
topically	O
related	O
words	O
in	O
the	O
questions	O
,	O
by	O
calculating	O
word	O
-	O
level	O
semantic	O
similarities	O
based	O
on	O
Numberbatch	B-MethodName
word	I-MethodName
embeddings	I-MethodName
(	O
Speer	O
et	O
al	O
.	O
,	O

For	O
instance	O
,	O
a	O
question	O
"	O
What	O
are	O
the	O
differences	O
between	O
global	O
warming	O
and	O
climate	O
change	O
?	O
"	O
becomes	O
"	O
What	O
are	O
the	O
differences	O
between	O
[	O
NP	O
]	O
and	O
[	O
NP]?"Exemplars	B-MethodName
for	I-MethodName
Guidance	I-MethodName
(	I-MethodName
EXPLGEN	I-MethodName
)	I-MethodName
.	O

They	O
are	O
listed	O
in	O
Table	O
10	O
in	O
Appendix	O
D.During	O
training	O
,	O
we	O
choose	O
the	O
exemplar	O
that	O
has	O
the	O
lowest	O
edit	O
distance	O
with	O
the	O
question	O
,	O
which	O
is	O
also	O
used	O
for	O
training	O
an	O
exemplar	O
selector	O
based	O
on	O
RoBERTa	B-MethodName
.	O

During	O
testing	O
,	O
the	O
exemplar	O
with	O
the	O
highest	O
selector	O
score	O
is	O
used	O
.	O

Specifically	O
,	O
we	O
reuse	O
EXPLGEN	B-MethodName
to	O
learn	O
to	O
generate	O
a	O
target	O
template	O
,	O
as	O
derived	O
from	O
the	O
template	O
extraction	O
procedure	O
.	O

During	O
question	O
realization	O
,	O
TPLGEN	B-MethodName
uses	O
a	O
BART	B-MethodName
-	I-MethodName
based	I-MethodName
generator	I-MethodName
that	O
takes	O
as	O
input	O
the	O
question	O
type	O
,	O
the	O
input	O
text	O
,	O
the	O
generated	O
template	O
,	O
and	O
the	O
words	O
that	O
are	O
predicted	O
as	O
focuses	O
.	O

We	O
use	O
separate	O
cross	O
attentions	O
to	O
attend	O
the	O
representations	O
of	O
the	O
focused	O
words	O
,	O
similar	O
to	O
how	O
node	O
representations	O
are	O
attended	O
in	O
JOINTGEN.We	B-MethodName
recognize	O
that	O
having	O
separate	O
stages	O
of	O
exemplar	O
selection	O
and	O
template	O
generation	O
introduces	O
extra	O
model	O
training	O
cost	O
and	O
potential	O
errors	O
in	O
the	O
pipeline	O
.	O

Comparisons	O
and	O
Metrics	O
.	O

We	O
compare	O
with	O
DEEPQG	B-MethodName
(	O
Pan	O
et	O
al	O
.	O
,	O

We	O
also	O
compare	O
with	O
BART	O
models	O
that	O
are	O
finetuned	O
on	O
the	O
same	O
datasets	O
as	O
in	O
our	O
models	O
,	O
by	O
using	O
inputs	O
of	O
(	O
1	O
)	O
the	O
answer	O
(	B-MethodName
BART	I-MethodName
)	O
,	O
(	O
2	O
)	O
the	O
answer	O
and	O
a	O
predicted	O
question	O
word	O
(	O
BART+QWORD	B-MethodName
)	O
,	O
and	O
(	O
3	O
)	O
the	O
answer	O
and	O
a	O
predicted	O
question	O
type	O
(	O
BART+QTYPE	B-MethodName
)	O
.	O

For	O
BART+QWORD	B-MethodName
,	O
the	O
question	O
word	O
is	O
predicted	O
by	O
a	O
RoBERTa	B-MethodName
classifier	O
that	O
considers	O
the	O
answer	O
and	O
is	O
trained	O
on	O
our	O
training	O
sets	O
.	O

For	O
both	O
our	O
models	O
and	O
BART+QTYPE	B-MethodName
,	O
the	O
most	O
confident	O
type	O
predicted	O
by	O
the	O
classifier	O
γ	O
a	O
(	O
described	O
in	O
§	O
3.2	O
)	O
,	O
which	O
reads	O
in	O
the	O
answer	O
,	O
is	O
used	O
as	O
input	O
.	O

To	O
test	O
the	O
efficacy	O
of	O
semantic	B-MethodName
graphs	I-MethodName
,	O
we	O
further	O
compare	O
with	O
a	O
variant	O
of	O
JOINTGEN	B-MethodName
that	O
only	O
uses	O
the	O
flat	O
Transformer	B-MethodName
for	O
focus	O
prediction	O
and	O
question	O
generation	O
,	O
denoted	O
as	O
JOINTGEN	B-MethodName
w/o	O
graph	O
.	O

We	O
evaluate	O
the	O
generated	O
questions	O
with	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O

2002	O
)	O
,	O
METEOR	B-MetricName
(	O
Lavie	O
and	O
Agarwal	O
,	O
2007	O
)	O
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
having	O
structured	O
representation	O
is	O
useful	O
for	O
focus	O
detection	O
and	O
the	O
final	O
question	O
generation	O
task	O
.	O

We	O
also	O
observe	O
a	O
huge	O
performance	O
gap	O
between	O
DEEPQG	B-MethodName
and	O
systems	O
based	O
on	O
BART	B-MethodName
,	O
signifying	O
the	O
importance	O
of	O
leveraging	O
pre	O
-	O
trained	O
models	O
for	O
open	O
-	O
ended	O
question	O
generation	O
.	O

Meanwhile	O
,	O
adding	O
question	O
types	O
helps	O
BART	B-MethodName
generate	O
more	O
relevant	O
questions	O
than	O
using	O
question	O
words	O
,	O
indicating	O
the	O
value	O
of	O
our	O
new	O
question	O
type	O
ontology	O
.	O

Notably	O
,	O
our	O
template	O
-	O
based	O
generators	O
,	O
EX	B-MethodName
-	I-MethodName
PLGEN	I-MethodName
and	O
TPLGEN	B-MethodName
,	O
which	O
are	O
trained	O
to	O
comply	O
with	O
the	O
given	O
templates	O
,	O
still	O
produce	O
comparable	O
scores	O
.	O

For	B-MethodName
BART	I-MethodName
,	O
we	O
use	B-HyperparameterName
nucleus	I-HyperparameterName
sampling	I-HyperparameterName
(	O
Holtzman	O
et	O
al	O
.	O
,	O

2020	O
)	O
with	B-HyperparameterName
k	I-HyperparameterName
=	O
10	B-HyperparameterValue
and	O
p	B-HyperparameterName
=	O
0.7	B-HyperparameterValue
to	O
sample	O
diverse	O
questions	O
.	O

To	O
evaluate	O
,	O
we	O
first	O
calculate	O
the	O
question	B-MetricName
type	I-MetricName
accuracy	I-MetricName
by	O
comparing	O
whether	O
the	O
types	O
of	O
the	O
generated	O
questions	O
match	O
the	O
specified	O
ones	O
,	O
with	O
types	O
labeled	O
by	O
our	O
classifier	O
γ	O
q	O
(	O
§	O
3.2	O
)	O
.	O

Finally	O
,	O
we	O
consider	O
pairwise	B-MetricName
BLEU-4	I-MetricName
(	O
Cho	O
et	O
al	O
.	O
,	O

2019	O
)	O
by	O
computing	O
the	O
BLEU-4	B-MetricName
between	O
pairwise	O
generated	O
questions	O
per	O
sample	O
,	O
where	O
lower	O
values	O
suggest	O
higher	O
content	O
diversity	O
.	O

First	O
,	O
our	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
can	O
generate	O
questions	O
with	O
diverse	O
types	O
and	O
content	O
,	O
as	O
shown	O
by	O
the	O
significantly	O
higher	O
numbers	O
of	O
unique	O
types	O
than	O
all	O
comparisons	O
and	O
lower	O
pairwise	O
BLEU	B-MetricName
scores	O
than	O
comparisons	O
except	O
for	O
BART	B-MethodName
with	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
in	O
Table	O
3	O
.	O

This	O
implies	O
stronger	O
type	O
control	O
by	O
template	O
-	O
based	O
generators	O
,	O
compared	O
to	O
BART+QTYPE	B-MethodName
and	O
JOINTGEN	B-MethodName
which	O
only	O
use	O
the	O
question	O
type	O
token	O
as	O
input	O
.	O

Results	O
on	O
numbers	O
of	O
unique	O
types	O
by	O
varying	O
numbers	O
of	O
question	O
types	O
specified	O
in	O
the	O
input	O
are	O
displayed	O
in	O
Figure	O
3	O
,	O
where	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
maintain	O
steady	O
controllability	O
.	O

Among	O
the	O
comparisons	O
,	O
although	O
BART	B-MethodName
with	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
and	O
BART+QWORD	B-MethodName
both	O
have	O
low	O
pairwise	B-MetricName
BLEU	I-MetricName
,	O
the	O
types	O
of	O
questions	O
they	O
can	O
generate	O
are	O
limited	O
.	O

We	O
hire	O
three	O
annotators	O
who	O
have	O
participated	O
in	O
our	O
question	O
type	O
annotation	O
study	O
to	O
evaluate	O
80	O
groups	O
of	O
questions	O
generated	O
by	O
four	O
selected	O
models	O
on	O
each	O
dataset	O
.	O

We	O
find	O
that	O
human	O
judges	O
rate	O
questions	O
generated	O
by	O
our	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
as	O
having	O
greater	O
diversities	O
over	O
all	O
aspects	O
,	O
except	O
for	O
syntax	O
diversity	O
on	O
Reddit	O
,	O
as	O
shown	O
in	O
Table	O
4	O
.	O

Among	O
the	O
two	O
model	O
variants	O
,	O
questions	O
by	O
TPLGEN	B-MethodName
yield	O
more	O
diverse	O
answers	O
.	O

Based	O
on	O
our	O
observation	O
,	B-MethodName
TPLGEN	I-MethodName
uses	O
automatically	O
generated	O
templates	O
to	O
produce	O
more	O
focused	O
questions	O
with	O
different	O
answers	O
,	O
compared	O
to	O
EXPLGEN	B-MethodName
which	O
employs	O
exemplars	O
.	O

Besides	O
Figure	O
1	O
,	O
we	O
show	O
more	O
sample	O
outputs	O
in	O
Figure	O
4	O
,	O
where	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
exhibit	O
stronger	O
controllability	O
than	O
JOINTGEN	B-MethodName
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
our	O
JOINTGEN	B-MethodName
model	O
produces	O
questions	O
with	O
better	O
answerability	O
and	O
that	O
cover	O
broader	O
content	O
in	O
the	O
answers	O
.	O

Between	O
BART+QWORD	B-MethodName
and	O
BART+QTYPE	B-MethodName
,	O
human	O
judges	O
rate	O
the	O
system	O
outputs	O
that	O
conditioned	O
on	O
our	O
question	O
types	O
to	O
have	O
better	O
overall	O
quality	O
.	O

Does	O
focus	O
prediction	O
correlate	O
with	O
question	O
quality	O
?	O
We	O
first	O
investigate	O
the	O
relationship	O
between	O
focus	O
prediction	O
and	O
question	O
generation	O
by	O
using	O
our	O
joint	O
model	O
JOINTGEN	B-MethodName
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
5	O
,	O
there	O
is	O
a	O
strong	O
correlation	O
between	O
F1	B-MetricName
scores	O
of	O
focus	O
prediction	O
and	O
BLEU-4	B-MetricName
as	O
well	O
We	O
also	O
show	O
the	O
F1	B-MetricName
scores	O
and	O
BLEU-4	B-MetricName
for	O
selected	O
question	O
types	O
on	O
the	O
right	O
of	O
Figure	O
5	O
,	O
again	O
demonstrating	O
the	O
effect	O
of	O
focus	O
detection	O
on	O
question	O
quality	O
.	O

When	O
do	O
our	O
models	O
fail	O
to	O
respect	O
the	O
given	O
types	O
?	O
Next	O
,	O
we	O
provide	O
insights	O
into	O
which	O
types	O
of	O
questions	O
are	O
challenging	O
to	O
generate	O
by	O
using	O
our	O
template	O
-	O
based	O
models	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
.	O

We	O
describe	O
a	O
joint	O
question	O
focus	O
detection	O
and	O
question	O
generation	O
framework	O
with	O
a	O
novel	O
semantic	B-TaskName
graphaugmented	I-TaskName
representation	I-TaskName
,	O
which	O
is	O
directly	O
built	O
on	O
large	O
pre	O
-	O
trained	O
models	O
.	O

We	O
discard	O
secondary	O
dependency	O
relations	O
for	O
graph	O
construction	O
,	O
including	O
case	O
,	O
mark	O
,	O
cc	O
,	O
cc	O
:	O
preconj	O
,	O
aux	O
,	O
aux	O
:	O
pass	O
,	O
cop	O
,	O
det	O
,	O
discourse	O
,	O
expl	O
,	O
det	O
:	O
predet	O
,	O
punct	O
,	O
ref	O
.	O

The	O
definition	O
for	O
each	O
dependency	O
can	O
be	O
found	O
in	O
Universal	B-MethodName
Dependency	I-MethodName
.	O

For	O
the	O
Graph	B-MethodName
Attention	I-MethodName
Networks	I-MethodName
(	I-MethodName
GATs	I-MethodName
)	O
in	O
our	O
focus	O
predictor	O
,	O
we	O
adopt	O
the	O
implementation	O
by	O
PyTorch	O
Geometric	O
(	O
Fey	O
and	O
Lenssen	O
,	O
2019	O
)	O
.	O

We	O
use	O
Adam	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
for	O
the	O
training	O
of	O
all	O
our	O
models	O
.	O

Our	O
question	O
type	O
classifiers	O
and	O
template	O
exemplar	O
classifiers	O
are	O
trained	O
with	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
.	O

For	O
training	O
generation	O
models	O
,	O
the	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
3	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
each	O
batch	B-HyperparameterName
contains	O
at	O
most	O
32,768	B-HyperparameterValue
models	O
except	O
for	O
models	O
with	O
GATs	B-MethodName
.	O

We	O
use	B-HyperparameterName
beam	I-HyperparameterName
search	I-HyperparameterName
for	O
decoding	O
.	O

A	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
1.5	B-HyperparameterValue
are	O
used	O
for	O
all	O
models	O
.	O

Repeated	B-HyperparameterName
trigram	I-HyperparameterName
blocking	I-HyperparameterName
is	O
applied	O
to	O
question	O
generation	O
.	O

The	O
minimum	B-HyperparameterName
and	I-HyperparameterName
maximum	I-HyperparameterName
lengths	I-HyperparameterName
for	O
generation	O
are	O
set	O
to	O
1	B-HyperparameterValue
and	O
100	B-HyperparameterValue
,	O
respectively	O
.	O

It	O
is	O
not	O
determined	O
by	O
the	O
interrogative	O
word	O
of	O
the	O
question	O
.	O

We	O
build	O
models	O
with	O
different	O
textual	O
representations	O
,	O
and	O
show	O
that	O
the	O
identified	O
features	O
are	O
highly	O
predictive	O
of	O
engagement	O
.	O

Our	O
metric	O
of	O
engagement	O
is	O
stream	B-DatasetName
rate	I-DatasetName
,	O
which	O
we	O
define	O
as	O
the	O
proportion	O
of	O
first	O
-	O
time	O
listeners	O
-of	O
those	O
who	O
have	O
begun	O
streaming	O
the	O
episode	O
-who	O
listen	O
for	O
at	O
least	O
five	O
minutes	O
.	O

Notably	O
,	O
stream	O
rate	O
is	O
different	O
from	O
the	O
metric	O
of	O
popularity	B-MetricName
as	O
given	O
by	O
the	O
raw	O
number	O
of	O
streams	O
;	O
the	O
latter	O
is	O
inevitably	O
influenced	O
by	O
factors	O
unrelated	O
to	O
the	O
content	O
,	O
such	O
as	O
the	O
host	O
or	O
publisher	O
reputation	O
,	O
publicity	O
,	O
expo	O
-	O
sure	O
in	O
recommendations	O
and	O
search	O
engines	O
,	O
and	O
time	O
of	O
publication	O
,	O
whereas	O
a	O
listener	O
's	O
decision	O
to	O
continue	O
listening	O
for	O
as	O
long	O
as	O
five	O
minutes	O
is	O
likely	O
to	O
be	O
influenced	O
by	O
the	O
content	O
.	O

Our	O
predictive	O
models	O
prove	O
that	O
stylistic	O
factors	O
alone	O
play	O
a	O
significant	O
role	O
in	O
determining	O
if	O
a	O
podcast	O
has	O
high	O
or	O
low	O
engagement	O
,	O
achieving	O
an	O
accuracy	B-MetricName
of	O
72	B-MetricValue
%	I-MetricValue
in	O
distinguishing	O
between	O
very	O
high	O
engagement	O
(	O
top	O
25	B-MetricValue
%	I-MetricValue
of	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
in	O
the	O
corpus	O
)	O
and	O
very	O
low	O
engagement	O
(	O
bottom	O
25	B-MetricValue
%	I-MetricValue
)	O
examples	O
.	O

We	O
also	O
show	O
that	O
the	O
overall	O
textual	O
information	O
in	O
podcasts	O
is	O
highly	O
predictive	O
of	O
engagement	O
in	O
this	O
experiment	O
,	O
with	O
an	O
accuracy	B-MetricName
as	O
high	O
as	O
81	B-MetricValue
%	I-MetricValue
.	O

To	O
understand	O
how	O
style	O
in	O
podcasts	O
compares	O
to	O
other	O
spoken	O
media	O
,	O
we	O
apply	O
our	O
analysis	O
to	O
a	O
corpus	B-TaskName
of	I-TaskName
TED	I-TaskName
talks	I-TaskName
.	O

Finally	O
,	O
we	O
manually	O
examine	O
the	O
highest	O
engagement	O
podcasts	O
in	O
our	O
dataset	O
to	O
characterize	O
their	O
content	O
.	B-MethodName

Content	I-MethodName
-	I-MethodName
Based	I-MethodName
Podcast	I-MethodName
Recommendations	I-MethodName
Yang	O
et	O
al	O
.	O
(	O

2019	O
)	O
model	O
transcripts	O
with	O
a	O
topic	B-MethodName
model	I-MethodName
,	O
and	O
the	O
audio	O
with	O
a	O
representation	O
they	O
trained	O
to	O
predict	O
the	O
non	O
-	O
textual	O
attributes	O
of	O
seriousness	O
and	O
energy	O
.	O

They	O
find	O
that	O
combining	O
these	O
representations	O
improves	O
over	O
the	O
purely	O
topic	O
based	O
model	O
on	O
popularity	B-TaskName
prediction	I-TaskName
.	O

Predicting	B-TaskName
Performance	I-TaskName
from	I-TaskName
Language	I-TaskName
Previous	O
research	O
in	O
natural	O
language	O
processing	O
has	O
explored	O
the	O
connections	O
between	O
textual	O
features	O
and	O
audience	O
engagement	O
in	O
books	O
(	O
Ganjigunte	O
Ashok	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
YouTube	O
(	O
Kleinberg	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
in	O
addition	O
to	O
the	O
entire	O
field	O
of	O
sentiment	B-TaskName
and	I-TaskName
opinion	I-TaskName
mining	I-TaskName
of	O
data	O
such	O
as	O
user	O
reviews	O
(	O
Pang	O
et	O
al	O
.	O
,	O

The	O
Spotify	B-DatasetName
Podcast	I-DatasetName
Dataset	O
is	O
a	O
recently	O
released	O
corpus	O
of	O
over	O
100	O
,	O
000	O
podcast	O
episodes	O
,	O
mostly	O
in	O
English	O
,	O
that	O
are	O
transcribed	O
with	O
Google	B-MethodName
's	I-MethodName
Speech	I-MethodName
to	I-MethodName
Text	I-MethodName
commercial	I-MethodName
speech	I-MethodName
recognition	I-MethodName
,	O
reported	O
in	O
the	O
paper	O
to	O
have	O
an	O
18	B-MetricValue
%	I-MetricValue
word	B-MetricName
error	I-MetricName
on	O
podcasts	O
.	O

A	O
podcast	O
,	O
also	O
known	O
as	O
a	O
'	O
show	O
'	O
in	O
the	O
dataset	O
,	O
is	O
a	O
collection	O
of	O
episodes	O
.	O

All	O
textual	O
data	O
was	O
normalized	O
and	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagged	I-TaskName
with	O
spacy	O
.	O

2	O
Promotional	O
and	O
extraneous	O
material	O
was	O
detected	O
by	O
the	O
classifier	O
described	O
by	O
,	O
a	O
model	O
using	O
BERT	B-MethodName
with	O
a	O
classification	O
head	O
,	O
trained	O
on	O
a	O
manually	O
annotated	O
set	O
of	O
episode	O
descriptions	O
.	O

This	O
classifier	O
is	O
reported	O
to	O
have	O
a	O
sentence	B-MetricName
classification	I-MetricName
accuracy	I-MetricName
of	O
95	B-MetricValue
%	I-MetricValue
on	O
episode	O
descriptions	O
.	O

As	O
described	O
in	O
the	O
introduction	O
,	O
we	O
use	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
engagement	O
metric	O
,	O
defined	O
as	O
the	O
proportion	O
of	O
the	O
show	O
's	O
first	O
-	O
time	O
listeners	O
who	O
stream	O
at	O
least	O
five	O
minutes	O
of	O
the	O
episode	O
.	O

Stream	B-MetricName
rate	I-MetricName
in	O
the	O
dataset	O
shows	O
a	O
weak	O
but	O
statistically	O
significant	O
inverse	O
rank	O
correlation	O
with	O
popularity	B-MetricName
(	O
Spearman	B-HyperparameterName
's	I-HyperparameterName
ρ	I-HyperparameterName
=	O
−0.12	B-HyperparameterValue
,	O
p	B-HyperparameterName
<	O
0.001	B-HyperparameterValue
)	O
.	O

70	B-MetricValue
%	I-MetricValue
stream	B-MetricName
rate	I-MetricName
in	O
a	O
well	O
-	O
known	O
podcast	O
which	O
A	O
weekly	O
podcast	O
covering	O
all	O
things	O
witchcraft	O
in	O
the	O
modern	O
world	O
.	O

Join	O
us	O
,	O
two	O
best	O
friends	O
and	O
Midwestern	O
witches	O
(	O
one	O
Wiccan	O
,	O
one	O
not	O
)	O
,	O
as	O
we	O
dive	O
into	O
all	O
things	O
witchy	O
.	O

We	O
're	O
starting	O
at	O
the	O
beginning	O
,	O
making	O
this	O
podcast	O
a	O
great	O
resource	O
for	O
newbies	O
...	O
would	O
have	O
attracted	O
a	O
broad	O
array	O
of	O
listeners	O
is	O
not	O
comparable	O
to	O
70	B-MetricValue
%	I-MetricValue
stream	B-MetricName
rate	I-MetricName
in	O
a	O
relatively	O
unknown	O
podcast	O
.	O

Therefore	O
,	O
we	O
bin	O
the	O
dataset	O
into	O
popularity	O
quartiles	O
for	O
analysis	O
on	O
stream	B-MetricName
rate	I-MetricName
,	O
which	O
is	O
found	O
to	O
be	O
uncorrelated	O
with	O
popularity	O
within	O
each	O
quartile	O
.	O

Stream	B-MetricName
rate	I-MetricName
is	O
uncorrelated	O
with	O
the	O
time	O
of	O
publication	O
.	O

To	O
control	O
for	O
duration	O
effects	O
in	O
the	O
analysis	O
of	O
transcripts	O
,	O
we	O
truncate	O
transcripts	O
at	O
ten	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

We	O
select	O
the	O
moststreamed	O
episode	O
from	O
each	O
show	O
as	O
its	O
representative	O
,	O
thereby	O
ensuring	O
that	O
every	O
show	O
is	O
represented	O
by	O
a	O
single	O
episode	O
in	O
the	O
data	O
.	O

Since	O
the	O
original	O
corpus	O
is	O
an	O
English	O
-	O
language	O
collection	O
,	O
all	O
of	O
our	O
analysis	O
is	O
constrained	O
to	O
English	O
,	O
and	O
we	O
filter	O
out	O
any	O
stray	O
examples	O
in	O
the	O
corpus	O
that	O
are	O
detected	O
as	O
non	O
-	O
English	O
after	O
running	B-TaskName
language	I-TaskName
identification	I-TaskName
(	O
Lui	O
and	O
Baldwin	O
,	O
2011	O
)	O
on	O
the	O
descriptions	O
.	O

The	O
resulting	O
dataset	O
has	O
5371	B-HyperparameterValue
episodes	O
.	O

For	O
example	O
,	O
technical	O
podcasts	O
are	O
expected	O
to	O
contain	O
more	O
complex	O
language	O
compared	O
to	O
chit	O
-	O
chat	O
,	O
crime	O
podcasts	O
to	O
contain	O
words	O
with	O
negative	O
sentiments	O
as	O
opposed	O
to	O
motivational	O
podcasts	O
,	O
and	O
so	O
on	O
.	O

Instead	O
,	O
we	O
fit	O
an	O
LDA	B-MethodName
topic	I-MethodName
model	O
(	O
Blei	O
et	O
al	O
.	O
,	O

2003	O
)	O
with	O
100	B-HyperparameterValue
topics	B-HyperparameterName
3	O
to	O
transcripts	O
of	O
the	O
entire	O
100k	B-HyperparameterValue
podcast	O
corpus	O
as	O
in	O
previous	O
works	O
Yang	O
et	O
al	O
.	O
,	O

Table	O
2	O
shows	O
a	O
sample	O
of	O
the	O
inferred	O
topics	O
.	O

Length	B-HyperparameterName
Descriptions	I-HyperparameterName
are	O
known	O
to	O
be	O
important	O
for	O
listeners	O
on	O
their	O
first	O
encounter	O
with	O
the	O
pod	O
-	O
cast	O
.	O

We	O
also	O
measure	O
audio	B-HyperparameterName
duration	I-HyperparameterName
,	O
since	O
surveys	O
show	O
it	O
is	O
a	O
consideration	O
(	O
McLean	O
,	O
2020).Proportion	O
of	O
ads	O
and	O
show	O
notes	O
Descriptions	O
of	O
well	O
-	O
known	O
podcasts	O
tend	O
to	O
contain	O
advertisements	O
of	O
other	O
podcasts	O
made	O
by	O
the	O
same	O
network	O
,	O
links	O
to	O
the	O
hosts	O
'	O
or	O
guests	O
'	O
social	O
media	O
presence	O
and	O
websites	O
,	O
or	O
show	O
notes	O
and	O
transcripts	O
,	O
and	O
podcast	O
creators	O
are	O
often	O
advised	O
to	O
include	O
such	O
information	O
(	O
Dennis	O
,	O
2020	O
)	O
,	O
and	O
surveys	O
have	O
shown	O
that	O
the	O
majority	O
of	O
podcast	O
listeners	O
do	O
not	O
mind	O
sponsor	O
ads	O
in	O
the	O
content	O
(	O
McLean	O
,	O
2020	O
)	O
.	O

The	O
proportion	O
of	O
ads	O
in	O
transcripts	O
is	O
given	O
by	O
a	O
manually	O
identified	B-MethodName
LDA	I-MethodName
topic	O
that	O
corresponds	O
to	O
words	O
indicative	O
of	O
ads	O
.	O

Do	O
listeners	O
seem	O
to	O
prefer	O
descriptions	O
that	O
accurately	O
convey	O
the	O
topics	O
and	O
synopsis	O
of	O
the	O
episode	O
?	O
We	O
measure	O
faithfulness	O
of	O
the	O
episode	O
description	O
to	O
the	O
first	O
ten	O
minutes	O
of	O
the	O
transcript	O
as	O
the	O
cosine	B-MethodName
similarity	I-MethodName
between	O
the	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
bag	I-MethodName
of	I-MethodName
words	I-MethodName
representation	O
of	O
both	O
texts	O
.	O

4	O
Distinctiveness	O
Podcast	O
creators	O
are	O
often	O
encouraged	O
to	O
develop	O
a	O
distinctive	O
style	O
(	O
Gray	O
,	O
2021a	O
)	O
.	O

We	O
define	O
distinctiveness	O
as	O
the	O
perplexity	B-MetricName
of	O
the	O
given	O
text	O
under	O
a	O
unigram	B-MethodName
language	I-MethodName
model	I-MethodName
trained	O
over	O
all	O
the	O
episodes	O
in	O
the	O
dataset	O
.	O

2019	O
)	O
of	O
randomly	O
sampling	O
a	O
constant	O
number	O
of	O
words	O
from	O
each	O
text	O
and	O
taking	O
the	O
mean	B-MetricName
cross	I-MetricName
entropy	I-MetricName
over	O
a	O
few	O
samples	O
.	O

2020	O
)	O
,	O
we	O
make	O
two	O
measurements	O
:	O
the	O
Flesch	B-MetricName
-	I-MetricName
Kincaid	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Flesch	O
,	O
1948	O
)	O
that	O
measures	O
the	O
number	O
of	O
syllables	O
per	O
word	O
and	O
the	O
number	O
of	O
words	O
per	O
sentence	O
,	O
and	O
the	O
Dale	B-MetricName
-	I-MetricName
Chall	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Chall	O
and	O
Dale	O
,	O
1948	O
)	O
which	O
measures	O
word	O
'	O
difficulty	O
'	O
using	O
a	O
lookup	O
table	O
.	O

While	O
caution	O
must	O
be	O
taken	O
on	O
interpreting	O
reading	O
grade	B-MetricName
level	I-MetricName
for	O
transcribed	O
speech	O
,	O
these	O
measures	O
have	O
been	O
explored	O
for	O
speech	O
in	O
prior	O
work	O
(	O
Schumacher	O
and	O
Eskenazi	O
,	O
2016).Vocabulary	O
Diversity	O
We	O
examine	O
whether	O
podcast	O
creators	O
of	O
high	O
engagement	O
podcasts	O
use	O
more	O
diverse	O
vocabularies	O
,	O
quantified	O
by	O
the	O
entropy	B-MetricName
of	O
the	O
unigram	O
words	O
in	O
the	O
text	O
,	O
motivated	O
by	O
advice	O
to	O
avoid	O
word	O
repetition	O
(	O
Bellis	O
,	O
2017	O
)	O
.	O

Popular	O
advice	O
often	O
encourages	O
podcast	O
creators	O
to	O
be	O
upbeat	O
and	O
positive	O
(	O
Briggman	O
,	O
2020	O
)	O
.	O

The	O
NRC	B-DatasetName
Emotion	I-DatasetName
Lexicon	I-DatasetName
(	O
Mohammad	O
and	O
Turney	O
,	O
2013	O
)	O
contains	O
positive	O
and	O
negative	O
sentiment	O
assignments	O
,	O
as	O
well	O
as	O
emotions	O
such	O
as	O
anger	O
,	O
trust	O
,	O
and	O
fear	O
,	O
for	O
14182	O
words	O
.	O

Since	O
a	O
lexicon	O
lookup	O
for	O
sentiment	O
is	O
naturally	O
limited	O
in	O
that	O
it	O
does	O
not	O
account	O
for	O
compositionality	O
and	O
can	O
not	O
model	O
words	O
and	O
variants	O
that	O
are	O
missing	O
in	O
the	O
lexicon	O
,	O
we	O
also	O
apply	O
a	O
fullsentence	O
classifier	O
,	O
the	O
sentiment	O
model	O
from	O
the	O
Google	B-MethodName
Natural	I-MethodName
Language	I-MethodName
API	I-MethodName
7	I-MethodName
.	O

Swearing	O
and	O
fillers	O
We	O
conjecture	O
that	O
podcasts	O
with	O
swearing	O
and	O
adult	O
language	O
may	O
not	O
have	O
broad	O
appeal	O
.	O

Public	O
speaking	O
recommendations	O
in	O
podcasting	O
guides	O
(	O
Coips	O
and	O
Kramer	O
,	O
2020	O
)	O
emphasize	O
the	O
reduction	O
of	O
filler	O
words	O
like	O
'	O
yeah	O
'	O
or	O
'	O
okay	O
'	O
,	O
and	O
the	O
use	O
of	O
professional	O
speech	O
.	O

Instead	O
,	O
we	O
take	O
advantage	O
of	O
the	O
observation	O
that	O
some	O
of	O
the	O
topics	O
inferred	O
by	O
the	B-MethodName
LDA	I-MethodName
model	O
correspond	O
to	O
swear	O
words	O
and	O
filler	O
terms	O
,	O
and	O
measure	O
the	O
proportions	O
of	O
these	O
topics	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
different	O
linguistic	O
features	O
by	O
comparing	O
group	O
means	O
between	O
the	O
top	O
and	O
bottom	O
25	O
%	O
of	O
podcasts	O
by	O
engagement	O
within	O
each	O
popularity	O
quartile	O
(	O
approximately	O
335	O
podcasts	O
per	O
group	O
)	O
with	O
bootstrapped	O
Welch	B-MethodName
's	I-MethodName
ttests	I-MethodName
.	O

We	O
report	O
the	O
group	O
mean	O
differences	O
of	B-MethodName
LDA	I-MethodName
topic	O
proportions	O
in	O
order	O
to	O
contextualize	O
results	O
on	O
the	O
other	O
features	O
.	O

For	O
LDA	B-MethodName
features	O
,	O
we	O
note	O
significance	O
after	O
a	O
Bonferroni	B-MetricName
correction	I-MetricName
of	I-MetricName
α	I-MetricName
=	O
0.05/100	B-MetricValue
,	O
and	O
for	O
the	O
other	O
linguistic	O
features	O
,	O
a	O
Bonferroni	B-HyperparameterName
correction	I-HyperparameterName
of	I-HyperparameterName
α	I-HyperparameterName
=	O
0.05/30.In	B-HyperparameterValue
the	O
results	O
,	O
'	O
description	O
'	O
refers	O
to	O
the	O
concatenation	O
of	O
the	O
show	O
description	O
and	O
the	O
representative	O
episode	O
's	O
description	O
.	O

Similarly	O
,	O
swearing	O
is	O
associated	O
with	O
low	O
engagement	O
.	O

Filler	O
words	O
are	O
only	O
negatively	O
associated	O
with	O
engagement	O
in	O
the	O
lowest	O
popularity	O
quartile	O
,	O
though	O
the	O
lack	O
of	O
correlation	O
in	O
other	O
quartiles	O
could	O
be	O
because	O
the	B-MethodName
LDA	I-MethodName
topics	O
representing	O
fillers	O
do	O
n't	O
model	O
context	O
,	O
and	O
therefore	O
do	O
not	O
capture	O
their	O
discourse	O
function	O
in	O
the	O
way	O
the	O
tagger	O
does	O
for	O
interjections	O
.	O

Next	O
,	O
we	O
build	O
classifiers	O
to	O
automatically	O
distinguish	O
high	O
and	O
low	O
engagement	O
podcasts	O
.	O

We	O
make	O
a	O
single	O
dataset	O
for	O
podcasts	O
across	O
all	O
quartiles	O
by	O
aggregating	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
within	O
each	O
quartile	O
.	O

This	O
aggregation	O
is	O
to	O
ensure	O
fair	O
comparisons	O
of	O
podcasts	O
in	O
different	O
quartiles	O
,	O
since	O
a	O
stream	B-MetricName
rate	I-MetricName
value	O
that	O
is	O
considered	O
high	O
for	O
a	O
popular	O
podcast	O
,	O
for	O
example	O
,	O
may	O
not	O
be	O
so	O
in	O
the	O
low	O
quartiles	O
.	O

We	O
train	O
logistic	B-MethodName
regression	I-MethodName
classifiers	O
using	O
different	O
representations	O
of	O
the	O
content	O
:	O
the	O
linguistic	O
features	O
listed	O
previously	O
,	O
the	O
non	O
-	O
stylistic	O
LDA	B-MethodName
topic	O
proportions	O
,	O
and	O
bag	B-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
ngrams	I-MethodName
(	O
unigram	O
and	O
bigram	O
words	O
)	O
with	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
scoring	I-MethodName
.	O

In	O
addition	O
,	O
we	O
train	O
two	O
neural	O
classifiers	O
-a	O
feedforward	B-MethodName
neural	I-MethodName
network	I-MethodName
with	I-MethodName
a	I-MethodName
single	I-MethodName
hidden	I-MethodName
layer	I-MethodName
,	O
using	O
a	O
paragraph	O
vector	O
representation	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
of	O
the	O
document	O
as	O
input	O
8	O
,	O
and	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Prediction	O
accuracies	B-MetricName
(	O
Table	O
4	O
)	O
are	O
over	O
70	B-MetricValue
%	I-MetricValue
with	O
linguistic	O
features	O
only	O
,	O
indicating	O
that	O
the	O
features	O
that	O
we	O
have	O
identified	O
are	O
relatively	O
strong	O
predictors	O
of	O
engagement	O
.	O

Analysis	O
of	O
the	O
weights	O
of	O
the	O
bag	B-MethodName
of	I-MethodName
n	I-MethodName
-	I-MethodName
grams	I-MethodName
models	O
surface	O
patterns	O
in	O
language	O
usage	O
that	O
corroborate	O
our	O
analysis	O
on	O
linguistic	O
features	O
-swearing	O
and	O
negative	O
sentiment	O
is	O
predictive	O
of	O
low	O
engagement	O
,	O
for	O
example	O
.	O

The	O
BERT	B-MethodName
classifiers	O
achieve	O
nearly	O
81	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
indicating	O
that	O
podcast	O
content	O
is	O
highly	O
predictive	O
of	O
engagement	O
.	O

Table	O
6	O
shows	O
how	O
classification	O
accuracies	O
change	O
when	O
the	O
task	O
is	O
to	O
distinguish	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
,	O
with	O
K	B-HyperparameterName
ranging	O
from	O
10	B-HyperparameterValue
to	I-HyperparameterValue
50	I-HyperparameterValue
(	O
all	O
reports	O
thus	O
far	O
have	O
been	O
with	O
K	B-HyperparameterName
=	O
25	B-HyperparameterValue
)	O
.	O

Performance	O
drops	O
as	O
K	B-HyperparameterName
increases	O
(	O
and	O
the	O
gap	O
between	O
the	O
two	O
sets	O
thereby	O
decreases	O
)	O
although	O
the	O
amount	O
of	O
training	O
data	O
goes	O
up	O
,	O
showing	O
that	O
the	O
differences	O
in	O
language	O
usage	O
are	O
more	O
predictable	O
at	O
the	O
extremes	O
of	O
engagement	O
.	O

2018;Acharyya	O
et	O
al	O
.	O
,	O

While	O
we	O
do	O
n't	O
have	O
access	O
to	O
the	O
stream	B-MetricName
rate	I-MetricName
of	O
the	O
lectures	O
,	O
the	O
data	O
includes	O
the	O
total	O
view	O
count	O
and	O
ratings	O
.	O

We	O
test	O
the	O
same	O
features	O
that	O
we	O
formulated	O
for	O
podcasts	O
,	O
except	O
for	B-MethodName
LDA	I-MethodName
topic	O
distributions	O
(	O
due	O
to	O
the	O
small	O
size	O
of	O
the	O
TED	B-DatasetName
corpus	I-DatasetName
relative	O
to	O
the	O
full	O
100k+	O
podcast	O
data	O
)	O
,	O
and	O
ads	O
and	O
swear	O
words	O
since	O
these	O
occur	O
rarely	O
if	O
at	O
all	O
in	O
TED	O
talks	O
.	O

On	O
the	O
prediction	O
task	O
,	O
we	O
achieve	O
up	O
to	O
71.15	B-MetricValue
%	I-MetricValue
(	O
Table	O
8)	O
accuracy	B-MetricName
using	O
only	O
linguistic	O
features	O
,	O
similar	O
to	O
the	O
performance	O
on	O
podcasts	O
.	O

However	O
,	O
the	O
bag	B-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
ngrams	I-MethodName
features	O
are	O
less	O
predictive	O
than	O
linguistic	O
features	O
,	O
and	O
the	O
BERT	B-MethodName
model	O
only	O
matches	O
the	O
classifier	O
with	O
linguistic	O
features	O
rather	O
than	O
exceeding	O
it	O
.	O

Our	O
paper	O
centers	O
five	O
minute	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
target	O
metric	O
for	O
analysis	O
and	O
prediction	O
.	O

2018).Aggregate	O
stream	B-MetricName
rate	I-MetricName
in	O
podcasts	O
is	O
a	O
specific	O
engagement	O
metric	O
distinct	O
from	O
metrics	O
and	O
media	O
in	O
previous	O
studies	O
.	O

2020	O
)	O
find	O
that	O
algorithms	O
driven	O
by	O
engagement	O
lead	O
to	O
less	O
diverse	O
recommendations	O
;	O
however	O
,	O
that	O
work	O
does	O
not	O
study	O
the	O
relationship	O
between	O
the	O
type	O
of	O
content	O
that	O
is	O
favored	O
by	O
the	O
engagement	O
metric	O
.	O

While	O
a	O
comprehensive	O
analysis	O
of	O
podcast	O
engagement	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
work	O
,	O
we	O
manually	O
examine	O
the	O
top	O
10	O
%	O
of	O
podcast	O
episodes	O
by	O
engagement	O
in	O
our	O
collection	O
,	O
a	O
total	O
of	O
537	O
episodes	O
.	O

As	O
we	O
noted	O
in	O
§	O
5.1.1	O
,	O
the	O
LDA	B-MethodName
topics	O
associated	O
with	O
high	O
engagement	O
are	O
broad	O
:	O
lifestyle	O
,	O
mental	O
health	O
,	O
spirituality	O
,	O
crime	O
,	O
investing	O
,	O
working	O
out	O
,	O
careers	O
,	O
business	O
,	O
parenting	O
,	O
health	O
,	O
art	O
,	O
and	O
relationships	O
.	O

Our	O
predictive	O
models	O
perform	O
well	O
at	O
distinguishing	O
high	O
and	O
low	O
engagement	O
podcasts	O
using	O
only	O
textual	O
information	O
.	O

Our	O
comparison	O
with	O
a	O
similar	O
task	O
on	O
TED	B-DatasetName
data	O
shows	O
similarities	O
and	O
differences	O
between	O
podcasts	O
and	O
public	O
lectures	O
vis	O
a	O
vis	O
engagement	O
.	O

As	O
with	O
all	O
user	O
data	O
,	O
the	O
engagement	O
metric	O
is	O
influenced	O
by	O
the	O
interface	O
and	O
recommendations	O
of	O
the	O
streaming	O
platform	O
from	O
which	O
the	O
data	O
was	O
collected	O
,	O
and	O
may	O
not	O
translate	O
to	O
other	O
platforms	O
,	O
nor	O
reflect	O
an	O
objective	O
notion	O
of	O
listener	O
engagement	O
.	O

Automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
is	O
interesting	O
,	O
owing	O
to	O
the	O
various	O
linguistic	O
peculiarities	O
present	O
in	O
the	O
language	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
the	O
first	O
large	O
scale	O
study	O
of	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
,	O
with	O
an	O
emphasis	O
on	O
the	O
impact	O
of	O
unit	O
selection	O
in	O
Sanskrit	O
ASR	B-TaskName
.	O

In	O
this	O
work	O
,	O
we	O
release	O
a	O
78	O
hour	O
ASR	B-TaskName
dataset	O
for	O
Sanskrit	O
,	O
which	O
faithfully	O
captures	O
several	O
of	O
the	O
linguistic	O
characteristics	O
expressed	O
by	O
the	O
language	O
.	O

We	O
investigate	O
the	O
role	O
of	O
different	O
acoustic	O
model	O
and	O
language	O
model	O
units	O
in	O
ASR	B-TaskName
systems	O
for	O
Sanskrit	O
.	O

We	O
also	O
propose	O
a	O
new	O
modelling	O
unit	O
,	O
inspired	O
by	O
the	O
syllable	O
level	O
unit	O
selection	O
,	O
that	O
captures	O
character	O
sequences	O
from	O
one	O
vowel	O
in	O
the	O
word	O
to	O
the	O
next	O
vowel	O
.	O

We	O
also	O
highlight	O
the	O
importance	O
of	O
choosing	O
graphemic	O
representations	O
for	O
Sanskrit	O
and	O
show	O
the	O
impact	O
of	O
this	O
choice	O
on	O
word	B-MetricName
error	I-MetricName
rates	I-MetricName
(	O
WER	B-MetricName
)	O
.	O

Finally	O
,	O
we	O
extend	O
these	O
insights	O
from	O
Sanskrit	O
ASR	B-TaskName
for	O
building	O
ASR	B-TaskName
systems	O
in	O
two	O
other	O
Indic	O
languages	O
,	O
Gujarati	O
and	O
Telugu	O
.	O

For	O
both	O
these	O
languages	O
,	O
our	O
experimental	O
results	O
show	O
that	O
the	O
use	O
of	O
phonetic	O
based	O
graphemic	O
representations	O
in	O
ASR	B-TaskName
results	O
in	O
performance	O
improvements	O
as	O
compared	O
to	O
ASR	B-TaskName
systems	O
that	O
use	O
native	O
scripts	O
.	O

Phonemic	O
orthography	O
is	O
beneficial	O
for	O
a	O
language	O
,	O
when	O
it	O
comes	O
to	O
designing	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
Systems	O
(	O
ASR	B-TaskName
)	O
,	O
specifically	O
for	O
unit	O
selection	O
at	O
both	O
the	O
Acoustic	O
Model	O
(	O
AM	O
)	O
and	O
Language	O
Model	O
(	O
LM	O
)	O
levels	O
.	O

Regardless	O
of	O
the	O
aforementioned	O
commonalities	O
preserved	O
in	O
both	O
the	O
speech	O
and	O
text	O
in	O
Sanskrit	O
,	O
designing	O
a	O
large	O
scale	O
ASR	B-TaskName
system	O
raises	O
several	O
challenges	O
.	O

The	O
language	O
is	O
lexically	O
productive	O
,	O
which	O
results	O
in	O
long	O
compound	O
words	O
with	O
multiple	O
components	O
in	O
usage	O
.	O

This	O
makes	O
the	O
ASR	B-TaskName
task	O
further	O
challenging	O
,	O
as	O
the	O
speakers	O
are	O
prone	O
to	O
carry	O
their	O
influence	O
from	O
their	O
corresponding	O
mother	O
tongues	O
into	O
the	O
Sanskrit	O
utterances	O
as	O
well	O
.	O

2010;Kulkarni	O
et	O
al	O
.	O
,	O

2012;Kulkarni	O
et	O
al	O
.	O
,	O

2021	O
)	O
,	O
large	O
scale	O
systems	O
for	O
processing	O
of	O
speech	O
in	O
Sanskrit	O
,	O
are	O
almost	O
non	O
-	O
existent	O
.	O

First	O
,	O
we	O
present	O
a	O
new	O
dataset	O
,	O
with	O
78	O
hours	O
of	O
speech	O
covering	O
about	O
46,000	O
sentences	O
,	O
for	O
ASR	B-TaskName
in	O
Sanskrit	O
.	O

Using	O
this	O
dataset	O
,	O
we	O
propose	O
a	O
new	O
,	O
largevocabulary	O
Sanskrit	O
ASR	B-TaskName
system	O
,	O
which	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
is	O
the	O
first	O
such	O
system	O
for	O
Sanskrit	O
.	O

native	O
script	O
(	O
Devanagari	O
)	O
and	O
SLP1.Finally	O
,	O
we	O
extend	O
our	O
insights	O
to	O
model	O
ASR	B-TaskName
systems	O
for	O
two	O
more	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

We	O
report	O
the	O
performance	O
of	O
these	O
ASR	B-TaskName
systems	O
on	O
two	O
publicly	O
available	O
ASR	B-TaskName
datasets	O
.	O

Our	O
main	O
contributions	O
in	O
this	O
work	O
are	O
:	O
1	O
)	O
We	O
present	O
(	O
in	O
Section	O
2	O
)	O
a	O
new	O
,	O
large	O
vocabulary	O
Sanskrit	O
ASR	B-TaskName
system	O
and	O
the	O
first	O
ever	O
ASRbased	B-TaskName
study	O
for	O
Sanskrit	O
using	O
a	O
new	O
,	O
large	O
and	O
diverse	O
,	O
labeled	O
speech	O
corpus	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañ	O
cayah	O
̣/	O
)	O
.	O

2	O
)	O
We	O
investigate	O
(	O
in	O
Sections	O
3	O
and	O
4	O
)	O
different	O
modeling	O
choices	O
for	O
both	O
acoustic	O
models	O
and	O
language	O
models	O
in	O
Sanskrit	O
ASR	B-TaskName
systems	O
,	O
along	O
with	O
different	O
graphemic	O
representations	O
.	O

3	O
)	O
We	O
also	O
contextualize	O
our	O
findings	O
for	O
Sanskrit	O
by	O
providing	O
comparisons	O
on	O
ASR	B-TaskName
systems	O
built	O
for	O
two	O
other	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

The	O
recordings	O
were	O
primarily	O
collected	O
with	O
the	O
help	O
of	O
volunteers	O
,	O
recording	O
their	O
speech	O
by	O
using	O
the	O
Recorder	O
app	O
on	O
Android	O
phones	O
and	O
the	O
Audacity	O
platform	O
,	O
and	O
from	O
various	O
sources	O
available	O
online	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
various	O
linguistic	O
phenomena	O
that	O
are	O
important	O
to	O
consider	O
when	O
preparing	O
datasets	O
and	O
building	O
ASR	B-TaskName
systems	O
for	O
Sanskrit	O
with	O
the	O
help	O
of	O
illustrative	O
examples	O
.	O

However	O
,	O
this	O
makes	O
it	O
challenging	O
for	O
an	O
ASR	B-TaskName
system	O
.	O

The	O
Unicode	O
encoding	O
for	O
the	O
native	O
scripts	O
in	O
Sanskrit	O
,	O
similar	O
to	O
several	O
indian	O
languages	O
,	O
does	O
not	O
preserve	O
the	O
correspondence	O
with	O
the	O
phonemic	O
encoding	O
.	O

However	O
,	O
Sandhi	O
splitting	O
can	O
change	O
some	O
phonemes	O
corresponding	O
to	O
the	O
words	O
in	O
almost	O
all	O
cases	O
.	O

This	O
leads	O
to	O
a	O
mismatch	O
between	O
the	O
speech	O
transcript	O
and	O
the	O
speech	O
audio	O
,	O
potentially	O
creating	O
further	O
complications	O
for	O
ASR	B-TaskName
.	O

We	O
consider	O
the	O
benefits	O
of	O
using	O
BPE	O
as	O
a	O
subword	O
unit	O
for	O
Sanskrit	O
ASR.While	B-TaskName
BPE	O
is	O
a	O
purely	O
data	O
-	O
driven	O
segmentation	O
strategy	O
,	O
we	O
next	O
present	O
a	O
linguistically	O
motivated	O
segmentation	O
approach	O
that	O
might	O
be	O
aligned	O
with	O
finding	O
syllable	O
units	O
for	O
ASR	B-TaskName
that	O
are	O
more	O
phonetically	O
compliant	O
.	O

We	O
propose	O
segmenting	O
words	O
at	O
vowel	O
boundaries	O
to	O
extract	O
the	O
units	O
for	O
which	O
alignment	O
with	O
speech	O
is	O
learnt	O
within	O
the	O
ASR	B-TaskName
system	O
.	O

For	O
acoustic	O
models	O
,	O
an	O
effective	O
unit	O
of	O
a	O
word	O
for	O
ASR	B-TaskName
would	O
arguably	O
be	O
the	O
syllable	O
(	O
Lee	O
et	O
al	O
.	O
,	O

To	O
create	O
syllable	O
units	O
,	O
phonemes	O
are	O
then	O
combined	O
together	O
based	O
on	O
the	O
sonority	O
sequencing	O
principle	O
(	O
Clements	O
,	O
1990	O
)	O
.	O

native	O
script	O
and	O
SLP1	O
)	O
described	O
in	O
Section	O
3.1	O
,	O
we	O
study	O
three	O
different	O
units	O
for	O
the	O
acoustic	O
modeling	O
(	O
AM	O
)	O
in	O
ASR	B-TaskName
,	O
viz	O
.	O
,	O

Whereas	O
,	O
for	O
language	O
modeling	O
(	O
LM	O
)	O
,	O
we	O
study	O
word	O
,	O
BPE	O
and	O
VS	O
based	O
units	O
.	O

In	O
Figure	O
3	O
,	O
we	O
report	O
the	O
vocabulary	O
size	O
based	O
on	O
each	O
of	O
these	O
three	O
different	O
unit	O
selections	O
and	O
contrast	O
the	O
sizes	O
with	O
that	O
of	O
two	O
extreme	O
hypothetical	O
systems	O
-one	O
that	O
considers	O
the	O
entire	O
word	O
as	O
a	O
single	O
unit	O
for	O
AM	O
and	O
the	O
other	O
that	O
treats	O
the	O
phoneme	O
as	O
a	O
single	O
unit	O
for	O
AM	O
.	O

Description	O
of	O
Datasets	O
:	O
In	O
addition	O
to	O
reporting	O
ASR	B-TaskName
results	O
on	O
the	O
carefully	O
created	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañcayah	O
̣/	O
)	O
dataset	O
(	O
described	O
in	O
Section	O
2	O
)	O
,	O
we	O
also	O
contrast	O
through	O
experimental	O
analysis	O
on	O
two	O
other	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

Corpora	O
in	O
these	O
two	O
languages	O
were	O
accompanied	O
by	O
pronunciation	O
lexicons	O
,	O
which	O
we	O
used	O
to	O
build	O
phoneme	O
-	O
based	O
ASR	B-TaskName
systems	O
to	O
compare	O
against	O
our	O
grapheme	O
-	O
based	O
systems	O
.	O

2011	O
)	O
for	O
all	O
our	O
ASR	B-TaskName
experiments	O
.	O

Our	O
acoustic	O
model	O
is	O
implemented	O
using	O
Time	B-MethodName
Delay	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
TDNNs	B-MethodName
)	O
(	O
Peddinti	O
et	O
al	O
.	O
,	O

2015	O
)	O
containing	O
14	B-HyperparameterValue
layers	B-HyperparameterName
.	O

We	O
use	O
40dimensional	O
MFCCs	O
as	O
our	O
input	O
features	O
along	O
with	O
100	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
i	B-HyperparameterName
-	I-HyperparameterName
vector	I-HyperparameterName
based	I-HyperparameterName
speaker	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Saon	O
et	O
al	O
.	O
,	O

The	O
language	O
models	O
were	O
trained	O
using	O
both	O
training	O
transcripts	O
from	O
the	O
speech	O
data	O
,	O
as	O
well	O
as	O
additional	O
textual	O
data	O
derived	O
from	O
the	O
Leipzig	B-DatasetName
Corpora	I-DatasetName
Collection	I-DatasetName
for	O
Gujarati	O
and	O
Telugu	O
(	O
Goldhahn	O
et	O
al	O
.	O
,	O

2012	O
)	O
and	O
the	O
Digital	B-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Sanskrit	I-DatasetName
(	O
Hellwig	O
,	O
2010	O
)	O
for	O
Sanskrit	O
.	O

The	O
word	O
vocabulary	O
sizes	O
in	O
the	O
lexicons	O
for	O
Sanskrit	O
,	O
Telugu	O
and	O
Gujarati	O
are	O
76	O
K	O
,	O
43	O
K	O
and	O
48	O
K	O
,	O
respectively	O
.	O

Results	O
:	O
Tables	O
3	O
,	O
4	O
and	O
5	O
,	O
present	O
the	O
WERs	B-MetricName
from	O
ASR	B-TaskName
systems	O
built	O
using	O
different	O
choices	O
of	O
AM	O
and	O
LM	O
units	O
using	O
both	O
the	O
graphemic	O
representations	O
(	O
Native	O
and	O
SLP1	O
)	O
for	O
Sanskrit	O
,	O
Gujarati	O
and	O
Telugu	O
,	O
respectively	O
.	O

Gujarati	O
and	O
Telugu	O
have	O
lower	O
OOV	O
rates	O
of	O
18.63	O
%	O
and	O
15.26	O
%	O
.Table	O
6	O
shows	O
the	O
distribution	O
of	O
words	O
with	O
1	O
-	O
4	O
continuous	O
consonants	O
in	O
all	O
three	O
languages	O
.	O

In	O
the	O
training	O
dataset	O
used	O
in	O
the	O
Sanskrit	O
ASR	B-TaskName
experiments	O
with	O
the	O
vocab	O
size	O
of	O
70.5	O
K	O
,	O
more	O
than	O
87.25	O
%	O
words	O
have	O
a	O
frequency	O
less	O
than	O
3	O
,	O
where	O
as	O
in	O
Telugu	O
and	O
Gujarati	O
training	O
dataset	O
,	O
this	O
is	O
76.76	O
%	O
and	O
77.26	O
%	O
,	O
respectively	O
.	O

We	O
also	O
find	O
that	O
ASR	B-TaskName
performance	O
using	O
phonemes	O
is	O
comparable	O
to	O
graphemes	O
for	O
Gujarati	O
and	O
Telugu	O
.	O

With	O
the	O
consistent	O
mapping	O
between	O
graphemes	O
and	O
phonemes	O
and	O
the	O
absence	O
of	O
schwa	O
deletion	O
,	O
it	O
is	O
intuitive	O
that	O
grapheme	O
-	O
based	O
models	O
would	O
be	O
most	O
appropriate	O
for	O
Sanskrit	O
.	O

In	O
Sanskrit	O
the	O
pause	O
given	O
between	O
the	O
subwords	O
of	O
a	O
compound	O
word	O
and	O
in	O
between	O
two	O
words	O
varies	O
depending	O
on	O
the	O
fluency	O
of	O
the	O
speaker	O
and	O
the	O
complexity	O
of	O
the	O
text	O
,	O
which	O
can	O
deteriorate	O
the	O
WER	B-MetricName
.	O

After	O
negating	O
these	O
two	O
particular	O
errors	O
,	O
we	O
will	O
get	O
17.79	B-MetricValue
%	I-MetricValue
as	O
the	O
modulo	B-MetricName
substitution	I-MetricName
deletion	I-MetricName
WER	I-MetricName
for	O
our	O
best	O
model	O
of	O
Sanskrit	O
(	O
Sr	O
.	O

The	O
character	B-MetricName
error	I-MetricName
rate	I-MetricName
3.10	B-MetricValue
%	I-MetricValue
for	O
the	O
best	O
model	O
in	O
Sanskrit	O
also	O
ensures	O
the	O
performance	O
of	O
the	O
model	O
and	O
the	O
quality	O
of	O
the	O
dataset	O
,	O
where	O
as	O
the	O
CER	B-MetricName
for	O
the	O
best	O
model	O
of	O
Gujarati	O
and	O
Telugu	O
are	O
5.49	B-MetricValue
%	I-MetricValue
and	O
5.60	B-MetricValue
%	I-MetricValue
respectively	O
,	O
much	O
higher	O
than	O
Sanskrit	O
.	O

It	O
shows	O
the	O
WERs	B-MetricName
we	O
can	O
expect	O
from	O
our	O
models	O
when	O
the	O
speakers	O
and	O
content	O
largely	O
vary	O
in	O
domain	O
from	O
our	O
dataset	O
.	O

These	O
test	O
utterances	O
were	O
evaluated	O
using	O
our	O
best	O
performing	O
Sanskrit	O
ASR	B-TaskName
models	O
.	O

We	O
would	O
like	O
to	O
thank	O
Prof.	O
K.	O
Ramasubramanian	O
,	O
IIT	O
Bombay	O
,	O
for	O
supporting	O
the	O
creation	O
of	O
Sanskrit	O
speech	O
corpus	O
.	O

A	O
Differences	O
between	O
Sanskrit	O
and	O
other	O
Indic	O
languages	O
for	O
ASR	B-TaskName
Many	O
Indian	O
languages	O
are	O
known	O
to	O
be	O
derived	O
from	O
Sanskrit	O
(	O
Kulkarni	O
et	O
al	O
.	O
,	O

This	O
phenomenon	O
is	O
not	O
observed	O
in	O
the	O
South	O
Indian	O
languages	O
.	O

For	O
example	O
,	O
the	O
word	O
'	O
गलती	O
'	O
(	O
/galtī/	O
meaning	O
mistake	O
)	O
in	O
Hindi	O
observes	O
implicit	O
schwa	O
deletion	O
after	O
the	O
consonant	O
'	O
ल'(/la/	O
)	O
.	O

ASR	B-TaskName
becomes	O
challenging	O
because	O
of	O
this	O
phenomenon	O
since	O
the	O
occurrence	O
of	O
schwa	O
deletion	O
is	O
not	O
always	O
explicitly	O
specified	O
in	O
the	O
orthography	O
.	O

While	O
constructing	O
phonetic	O
representations	O
for	O
ASR	B-TaskName
,	O
such	O
deletions	O
introduce	O
ambiguities	O
in	O
pronunciation	O
which	O
could	O
be	O
alleviated	O
by	O
enforcing	O
more	O
consistency	O
between	O
graphemes	O
and	O
phonemes	O
.	O

In	O
contrast	O
,	O
in	O
the	O
case	O
of	O
Sanskrit	O
,	O
since	O
pronunciation	O
is	O
strictly	O
governed	O
by	O
the	O
शक्षा(/śiks	O
̣ā/	O
)	O
(	O
Manomohan	O
and	O
Pān	O
̣ini	O
,	O
1938	O
)	O
,	O
a	O
treatise	O
on	O
phonetics	O
,	O
schwa	O
deletion	O
is	O
not	O
observed	O
.	O

Therefore	O
we	O
experimented	O
varying	O
number	O
of	O
subword	O
unit	O
with	O
vocabulary	O
sizes	O
of	O
2	O
K	O
,	O
4	O
K	O
,	O
8	O
K	O
,	O
16	O
K	O
,	O
32	O
K	O
and	O
64	O
K	O
(	O
K=1000	O
)	O
.	O

Even	O
in	O
this	O
configuration	O
,	O
BPE	O
outperforms	O
VS	O
,	O
as	O
BPE	O
reports	O
a	O
WER	B-MetricName
of	O
21.94	B-MetricValue
as	O

To	O
address	O
the	O
gaps	O
at	O
both	O
ends	O
of	O
the	O
spectrum	O
,	O
we	O
propose	O
MERGEDISTILL	B-MethodName
,	O
a	O
framework	O
to	O
merge	O
pre	O
-	O
trained	O
LMs	O
in	O
a	O
way	O
that	O
can	O
best	O
leverage	O
their	O
assets	O
with	O
minimal	O
dependencies	O
,	O
using	O
task	O
-	O
agnostic	O
knowledge	O
distillation	O
.	O

2020;Artetxe	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
pre	O
-	O
training	O
data	O
volume	O
(	O
Liu	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
an	O
ability	O
to	O
handle	O
codemixed	O
text	O
(	O
Pires	O
et	O
al	O
.	O
,	O

While	O
with	O
a	O
BERT	B-MethodName
-	O
compatible	O
tokenization	O
tokens	O
will	O
appear	O
twice	O
,	O
once	O
with	O
"	O
Al-	O
"	O
and	O
once	O
without	O
it	O
,	O
AraBERT	B-MethodName
first	O
segments	O
the	O
words	O
using	O
Farasa	O
(	O
Abdelali	O
et	O
al	O
.	O
,	O

Figure	O
2	O
:	O
Overview	O
of	O
MERGEDISTILL	B-MethodName
:	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pretraining	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
effort	O
of	O
its	O
kind	O
,	O
and	O
makes	O
the	O
following	O
contributions:•	O
We	O
propose	O
MERGEDISTILL	B-MethodName
,	O
a	O
task	O
-	O
agnostic	O
distillation	O
approach	O
to	O
merge	O
multiple	O
teacher	O
LMs	O
at	O
the	O
pre	O
-	O
training	O
stage	O
,	O
to	O
train	O
a	O
strong	O
multilingual	O
student	O
LM	O
that	O
can	O
then	O
be	O
finetuned	O
for	O
any	O
task	O
on	O
all	O
languages	O
in	O
the	O
student	O
LM	O
.	O

Our	O
approach	O
is	O
more	O
maintainable	O
(	O
fewer	O
models	O
)	O
,	O
compute	O
efficient	O
and	O
teacherarchitecture	O
agnostic	O
(	O
since	O
we	O
obtain	O
offline	O
predictions).•	O
We	O
use	O
MERGEDISTILL	B-MethodName
to	O
i	O
)	O
combine	O
monolingual	O
teacher	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
that	O
is	O
competitive	O
with	O
or	O
outperforms	O
individual	O
teachers	O
,	O
ii	O
)	O
combine	O
multilingual	O
teacher	O
LMs	O
,	O
such	O
that	O
the	O
overlapping	O
languages	O
can	O
learn	O
from	O
multiple	O
teachers.•	O
Through	O
extensive	O
experiments	O
and	O
analysis	O
,	O
we	O
study	O
the	O
importance	O
of	O
typological	O
similarity	O
in	O
building	O
multilingual	O
models	O
,	O
and	O
the	O
impact	O
of	O
strong	O
teacher	O
LM	O
vocabularies	O
and	O
predictions	O
in	O
our	O
framework	O
.	O

2021	O
)	O
,	O
trained	O
on	O
massive	O
amounts	O
of	O
multilingual	O
data	O
,	O
have	O
surpassed	O
cross	O
-	O
lingual	O
word	O
embedding	O
spaces	O
(	O
Glavaš	O
et	O
al	O
.	O
,	O

2019	O
)	O
;	O
Wu	O
and	O
Dredze	O
(	O
2019	O
)	O
highlight	O
their	O
cross	O
-	O
lingual	O
ability	O
,	O
several	O
limitations	O
have	O
been	O
studied	O
.	O

2020	O
)	O
highlight	O
that	O
even	O
the	O
best	O
multilingual	O
models	O
do	O
not	O
yield	O
satisfactory	O
transfer	O
performance	O
on	O
the	O
XTREME	B-DatasetName
bechmark	O
covering	O
9	O
tasks	O
and	O
40	O
languages	O
.	O

A	O
few	O
examples	O
of	O
these	O
are	O
AraBERT	B-MethodName
(	O
Antoun	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
CamemBERT	B-MethodName
(	O
Martin	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
FinBERT	B-MethodName
(	O
Virtanen	O
et	O
al	O
.	O
,	O

2020	O
)	O
maintain	O
an	O
ever	O
-	O
growing	O
list	O
of	O
BERT	B-MethodName
models	O
here	O
most	O
commonly	O
been	O
used	O
for	O
task	O
-	O
specific	O
model	O
compression	O
of	O
a	O
teacher	O
into	O
a	O
single	O
-	O
task	O
student	O
(	O
Tang	O
et	O
al	O
.	O
,	O

In	O
the	O
context	O
of	O
neural	O
machine	O
translation	O
,	O
Tan	O
et	O
al	O
.	O
(	O

Notations	O
:	O
Let	O
K	O
denote	O
the	O
set	O
of	O
languages	O
we	O
train	O
our	O
student	O
LM	O
on	O
and	O
T	O
denote	O
the	O
set	O
of	O
teacher	O
LMs	O
input	O
to	O
MERGEDISTILL	B-MethodName
3	O
.	O

Consequently	O
,	O
T	O
k	O
denotes	O
the	O
set	O
of	O
teacher	O
LMs	O
trained	O
on	O
language	O
k	O
,	O
where|T	O
k	O
|	O
≥	O
1	O
∀	O
k	O
∈	O
K.	O
An	O
overview	O
of	O
MERGEDISTILL	B-MethodName
is	O
presented	O
in	O
Figure	O
2	O
.	O

Step	O
1	O
:	O
Input	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pre	O
-	O
training	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

Hence	O
,	O
we	O
first	O
store	O
the	O
top	O
-	O
k	B-HyperparameterName
logits	O
for	O
each	O
masked	O
word	O
offline	O
,	O
loading	O
and	O
normalizing	O
them	O
during	O
student	O
LM	O
training	O
,	O
similar	O
to	O
(	O
Tan	O
et	O
al	O
.	O
,	O

This	O
converts	O
each	O
teacher	O
token	O
index	O
to	O
its	O
corresponding	O
student	O
token	O
index	O
,	O
ready	O
for	O
consumption	O
by	O
the	O
student	O
model	O
.	O

Step	O
L	O
MLM	O
(	O
x	O
m	O
|x	O
−m	O
)	O
=	O
−	O
1	O
n	O
n	O
i=1	O
|v|	O
j=1	O
P(x	O
m	O
i	O
,	O
v	O
j	O
)	O
In	O
addition	O
to	O
learning	O
from	O
gold	O
labels	O
,	O
we	O
use	O
teacher	O
predictions	O
as	O
soft	O
labels	O
and	O
minimize	O
the	O
cross	O
entropy	O
between	O
student	O
and	O
teacher	O
distributions	O
.	O

In	O
this	O
section	O
,	O
we	O
aim	O
to	O
answer	O
the	O
following	O
questions	O
:	O
Distillation	O
Parameters	O
:	O
We	O
have	O
two	O
hyperparameter	O
choices	O
here	O
:	O
1	O
)	O
k	B-HyperparameterName
in	O
top	O
-	O
k	B-HyperparameterName
logits	O
-as	O
it	O
increases	O
,	O
we	O
observe	O
that	O
while	O
performances	O
remain	O
similar	O
,	O
storing	O
k>8	B-HyperparameterName
number	O
of	O
predictions	O
for	O
each	O
masked	O
word	O
offline	O
significantly	O
increases	O
resource	O
requirements	O
4	O
.	O

Hence	O
,	O
we	O
set	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

2	O
)	O
the	O
value	O
of	O
λ	B-HyperparameterName
in	O
the	O
loss	O
function	O
,	O
which	O
decides	O
the	O
proportion	O
of	O
teacher	O
loss	O
,	O
is	O
annealed	O
through	O
training	O
similar	O
to	O
Clark	O
et	O
al	O
.	O
(	O

2019).Evaluation	O
Metrics	O
:	O
We	O
report	O
F1	B-MetricName
scores	O
for	O
structured	O
prediction	O
tasks	O
(	O
NER	O
,	O
POS	O
)	O
,	O
accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O

scores	O
for	O
sentence	O
classification	O
tasks	O
(	O
XNLI	O
,	O
PAWS	O
-	O
X	O
)	O
,	O
and	O
F1	B-MetricName
/	O
Exact	B-MetricName
Match	I-MetricName
(	O
F1	B-MetricName
/	O
EM	B-MetricName
)	O
scores	O
for	O
question	O
answering	O
tasks	O
(	O
XQuAD	O
,	O
MLQA	O
,	O
TyDiQA	O
)	O
.	O

We	O
also	O
report	O
a	O
task	O
-	O
specific	O
relative	B-MetricName
deviation	I-MetricName
from	O
teachers	O
(	O
RDT	B-MetricName
)	O
(	O
in	O
%	O
)	O
averaged	O
across	O
all	O
languages	O
(	O
n	O
)	O
.	O

For	O
each	O
task	O
,	O
RDT	B-MetricName
is	O
calculated	O
as	O
:	O
RDT(S	B-MetricName
,	O
{	O
T	O
1	O
,	O
...	O
,	O
T	O
n	O
}	O
)	O
=	O
100	O
n	O
n	O
i=1	O
(	O
P	O
T	O
i	O
−	O
P	O
S	O
)	O
P	O
T	O
i(3)where	O
P	O
T	O
i	O
and	O
P	O
S	O
are	O
performances	O
of	O
the	O
i	O
th	O
teacher	O
and	O
student	O
LMs	O
,	O
respectively	O
.	O

In	O
this	O
setup,|T	O
k	O
|	O
=	O
1	O
∀	O
k	O
∈	O
K	O
,	O
i.e.	O
,	O
each	O
language	O
can	O
learn	O
from	O
its	O
respective	O
monolingual	O
teacher	O
LM	O
only	O
.	O

Our	O
teacher	O
selection	O
and	O
setup	O
follows	O
a	O
two	O
-	O
step	O
process	O
.	O

This	O
makes	O
us	O
choose	O
teacher	O
LMs	O
for	O
:	O
Arabic	O
(	O
ar	O
)	O
,	O
Chinese	O
(	O
zh	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Finnish	O
(	O
fi	O
)	O
,	O
Table	O
4	O
:	O
Results	O
for	O
multilingual	O
teacher	O
and	O
student	O
LMs	O
on	O
the	O
XTREME	B-MetricName
benchmark	O
.	O

We	O
compare	O
performances	O
of	O
three	O
student	O
LM	O
variants	O
as	O
described	O
in	O
Section	O
4.3	O
to	O
the	O
two	O
teachers	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

Relative	B-MetricName
deviations	I-MetricName
of	O
5	B-MetricValue
%	I-MetricValue
or	O
less	O
from	O
teacher	O
(	O
i.e.	O
,	O
RDT	B-MetricName
≥	O
−5	B-MetricValue
%	I-MetricValue
)	O
are	O
marked	O
in	O
bold	O
.	O

Overall	O
,	O
we	O
find	O
that	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
among	O
all	O
student	O
variants	O
and	O
report	O
its	O
RDT	B-MetricName
(	O
in	O
%	O
)	O
(	O
Equation	O
3	O
)	O
from	O
the	O
two	O
teachers	O
.	O

This	O
encouraging	O
result	O
proves	O
that	O
even	O
with	O
very	O
limited	O
data	O
,	O
MERGEDISTILL	B-MethodName
enables	O
one	O
to	O
combine	O
strong	O
monolingual	O
teacher	O
LMs	O
to	O
train	O
competitive	O
student	O
LMs	O
that	O
can	O
leverage	O
the	O
benefits	O
of	O
multilinguality	O
.	O

Pre	O
-	O
training	O
:	O
In	O
this	O
experiment	O
,	O
we	O
make	O
use	O
of	O
pre	O
-	O
existing	O
multilingual	O
models	O
:	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

mBERT	B-MethodName
is	O
trained	O
on	O
104	O
languages	O
and	O
MuRIL	B-MethodName
covers	O
12	O
of	O
these	O
(	O
11	O
Indian	O
languages	O
+	O
English	O
):	O
Bengali	O
(	O
bn	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Gujarati	O
(	O
gu	O
)	O
,	O
Hindi	O
(	O
hi	O
)	O
,	O
Kannada	O
(	O
kn	O
)	O
,	O
Malayalam	O
(	O
ml	O
)	O
,	O
Marathi	O
(	O
mr	O
)	O
,	O
Nepali	O
(	O
ne	O
)	O
,	O
Punjabi	O
(	O
pa	O
)	O
,	O
Tamil	O
(	O
ta	O
)	O
,	O
Telugu	O
(	O
te	O
)	O
,	O
and	O
Urdu	O
(	O
ur	O
)	O
,	O
with	O
higher	O
performance	O
for	O
these	O
languages	O
on	O
the	O
XTREME	B-DatasetName
benchmark	O
.	O

In	O
this	O
case	O
,	O
the	O
MuRIL	B-MethodName
Languages	O
(	O
MuL	O
)	O
have	O
two	O
as	O
shown	O
in	O
Table	O
3	O
teachers	O
(	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
)	O
and	O
the	O
Non	O
-	O
MuRIL	B-MethodName
Languages	O
(	O
Non	O
-	O
MuL	O
)	O
can	O
learn	O
from	O
mBERT	B-MethodName
only	O
.	O

Therefore	O
,	O
while	O
we	O
only	O
use	O
mBERT	B-MethodName
as	O
the	O
teacher	O
LM	O
for	O
Non	O
-	O
MuL	O
across	O
all	O
experiments	O
,	O
we	O
consider	O
three	O
possibilities	O
for	O
MuL	O
:	O
•	O
Student	O
Both	O
all	O
:	O
Tokenize	O
each	O
input	O
example	O
using	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
separately	O
and	O
include	O
both	O
copies	O
in	O
training.•	O
Student	O
Both	O
best	O
:	O
Tokenize	O
each	O
input	O
example	O
using	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
separately	O
and	O
include	O
only	O
the	O
best	O
copy	O
in	O
training	O
.	O

All	O
the	O
student	O
LMs	O
use	O
a	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
architecture	O
and	O
have	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
288,973	B-HyperparameterValue
.	O

We	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512.Finetuning	B-HyperparameterValue
:	O
We	O
report	O
zero	O
-	O
shot	O
performance	O
for	O
all	O
languages	O
in	O
the	O
XTREME	B-DatasetName
(	O
Hu	O
et	O
al	O
.	O
,	O

Overall	O
,	O
we	O
find	O
that	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
among	O
all	O
student	O
variants	O
.	O

For	O
Non	O
-	O
MuL	O
,	O
Student	O
MuRIL	B-MethodName
beats	O
the	O
teacher	O
(	O
mBERT	B-MethodName
)	O
by	O
an	O
average	B-MetricName
relative	I-MetricName
score	I-MetricName
of	O
3.8	B-MetricValue
%	I-MetricValue
.	O

For	O
MuL	O
,	O
Student	O
MuRIL	B-MethodName
beats	O
one	O
teacher	O
(	O
mBERT	B-MethodName
)	O
by	O
8.8	B-MetricValue
%	I-MetricValue
,	O
but	O
underperforms	O
the	O
other	O
teacher	O
(	O
MuRIL	B-MethodName
)	O
by	O
3.8	B-MetricValue
%	I-MetricValue
.	O

MuRIL	B-MethodName
is	O
trained	O
on	O
monolingual	O
and	O
parallel	O
data	O
9	O
while	O
the	O
student	O
LMs	O
only	O
see	O
∼22	O
%	O
of	O
unique	O
tokens	O
in	O
comparison	O
.	O

MuRIL	B-MethodName
also	O
has	O
different	O
language	O
sampling	O
strategies	O
(	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
as	O
opposed	O
to	O
0.7	B-HyperparameterValue
in	O
our	O
setting	O
,	O
where	O
a	O
lower	O
α	B-HyperparameterName
value	O
upsamples	O
more	O
rigorously	O
from	O
the	O
tail	O
languages	O
)	O
,	O
which	O
have	O
a	O
significant	O
role	O
to	O
play	O
in	O
multilingual	O
model	O
performances	O
(	O
Conneau	O
et	O
al	O
.	O
,	O

We	O
also	O
observe	O
a	O
significant	O
drop	O
in	O
Student	O
mBERT	B-MethodName
's	O
performance	O
for	O
MuL	O
when	O
compared	O
to	O
the	O
other	O
student	O
LM	O
variants	O
.	O

This	O
might	O
be	O
because	O
the	O
input	O
is	O
tokenized	O
using	O
the	O
mBERT	B-MethodName
tokenizer	O
which	O
prevents	O
learning	O
from	O
MuRIL	B-MethodName
tokens	O
in	O
the	O
student	O
vocabulary	O
.	O

In	O
our	O
case	O
,	O
we	O
do	O
n't	O
observe	O
much	O
of	O
a	O
difference	O
in	O
incorporating	O
mBERT	B-MethodName
predictions	O
for	O
MuL.8	O
More	O
details	O
in	O
Appendix	O
A.3	O
9	O
More	O
details	O
in	O
Appendix	O
A.2	O
The	O
importance	O
of	O
vocabulary	O
and	O
teacher	O
LM	O
preditions	O
:	O
In	O
Furthermore	O
,	O
we	O
also	O
observe	O
that	O
SM2	B-MethodName
and	O
SM3	B-MethodName
achieve	O
competitive	O
performances	O
despite	O
SM3	B-MethodName
being	O
additionally	O
trained	O
on	O
teacher	O
LM	O
labels	O
.	O

In	O
our	O
case	O
,	O
we	O
hypothesize	O
that	O
training	O
on	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
exposes	O
the	O
model	O
to	O
sufficient	O
data	O
for	O
it	O
to	O
generalize	O
well	O
enough	O
and	O
mask	O
the	O
benefits	O
of	O
teacher	O
LM	O
predictions	O
.	O

To	O
validate	O
this	O
,	O
we	O
evaluate	O
the	O
performances	O
of	O
SM2	B-MethodName
and	O
SM3	B-MethodName
,	O
20	O
%	O
into	O
training	O
(	O
i.e.	O
100,000	O
steps	O
/	O
500,000	O
total	O
steps	O
)	O
as	O
shown	O
in	O
Table	O
5	O
.	O

We	O
observe	O
a	O
∼2.9	B-MetricValue
%	I-MetricValue
gain	B-MetricName
in	O
average	O
performance	O
for	O
SM3	B-MethodName
over	O
SM2	B-MethodName
,	O
clearly	O
highlighting	O
the	O
importance	O
of	O
teacher	O
LM	O
predictions	O
in	O
a	O
limited	O
data	O
scenario	O
.	O

Pre	O
-	O
trained	O
zero	O
-	O
shot	O
transfer	O
:	O
Interestingly	O
,	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
on	O
almost	O
all	O
tasks	O
for	O
Non	O
-	O
MuL.	O
This	O
hints	O
at	O
positive	O
transfer	O
from	O
strong	O
teachers	O
to	O
languages	O
that	O
the	O
teacher	O
does	O
not	O
cover	O
at	O
all	O
,	O
due	O
to	O
the	O
shared	O
multilingual	O
representations	O
.	O

This	O
would	O
make	O
MERGEDISTILL	B-MethodName
highly	O
beneficial	O
for	O
low	O
-	O
resource	O
languages	O
that	O
do	O
not	O
have	O
a	O
strong	O
teacher	O
or	O
limited	O
gold	O
data	O
.	O

In	O
this	O
paper	O
we	O
address	O
the	O
problem	O
of	O
merging	O
multiple	O
pre	O
-	O
trained	O
teacher	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
by	O
proposing	O
MERGEDIS	B-MethodName
-	I-MethodName
TILL	I-MethodName
,	O
a	O
task	O
-	O
agnostic	O
distillation	O
method	O
.	O

The	O
student	O
LM	O
learned	O
by	O
MERGEDISTILL	B-MethodName
may	O
be	O
further	O
fine	O
-	O
tuned	O
for	O
any	O
task	O
across	O
all	O
of	O
the	O
languages	O
covered	O
by	O
the	O
teacher	O
LMs	O
.	O

We	O
use	O
MERGEDISTILL	B-MethodName
to	O
i	O
)	O
combine	O
monolingual	O
teacher	O
LMs	O
into	O
one	O
student	O
multilingual	O
LM	O
which	O
is	O
competitive	O
with	O
the	O
teachers	O
,	O
thereby	O
demonstrating	O
positive	O
crosslingual	O
transfer	O
,	O
and	O
ii	O
)	O
combine	O
multilingual	O
LMs	O
to	O
train	O
student	O
LMs	O
that	O
learn	O
from	O
multiple	O
teachers	O
.	O

Through	O
experiments	O
on	O
multiple	O
benchmark	O
datasets	O
,	O
we	O
show	O
that	O
student	O
LMs	O
learned	O
by	O
MERGEDISTILL	B-MethodName
perform	O
competitively	O
or	O
even	O
outperform	O
teacher	O
LMs	O
trained	O
on	O
orders	O
of	O
magnitude	O
more	O
data	O
.	O

We	O
also	O
find	O
that	O
MERGEDIS	B-MethodName
-	I-MethodName
TILL	I-MethodName
enables	O
positive	O
transfer	O
from	O
strong	O
teachers	O
to	O
languages	O
not	O
covered	O
by	O
them	O
(	O
i.e.	O
zero	O
-	O
shot	O
transfer	O
)	O
.	O

We	O
pre	O
-	O
train	O
our	O
student	O
models	O
using	O
the	O
BERT	B-MethodName
base	I-MethodName
architecture	O
.	O

Student	O
similar	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
99112	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
162	O
M	O
parameters	O
.	O

Student	O
different	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
180996	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
225	O
M	O
parameters	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
250k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

We	O
pre	O
-	O
train	O
our	O
student	O
models	O
using	O
the	O
BERT	B-MethodName
base	I-MethodName
architecture	O
.	O

All	O
student	O
LMs	O
have	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
288973	B-HyperparameterName
.	O

Hence	O
,	O
we	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

We	O
use	O
TPUs	O
,	O
and	O
it	O
takes	O
around	O
3	O
days	O
to	O
pre	O
-	O
train	O
each	O
student	O
LM.We	O
present	O
pre	O
-	O
training	O
data	O
statistics	O
for	O
MuRIL	B-MethodName
and	O
the	O
student	O
LMs	O
in	O
Table	O
6	O
.	O

Here	O
we	O
only	O
include	O
the	O
monolingual	O
data	O
statistics	O
,	O
but	O
MuRIL	B-MethodName
is	O
additionally	O
trained	O
on	O
parallel	O
translated	O
and	O
transliterated	O
data	O
.	O

We	O
report	O
results	O
on	O
the	O
best	O
-	O
performing	O
checkpoint	O
for	O
the	O
We	O
present	O
results	O
for	O
Student	O
MuRIL	B-MethodName
trained	O
with	O
different	O
top	O
-	O
k	B-HyperparameterName
values	O
from	O
teacher	O
predictions	O
in	O
Table	O
8	O
.	O

Hence	O
,	O
we	O
stick	O
to	O
a	O
value	O
of	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

The	O
dataset	O
was	O
annotated	O
using	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
a	O
form	O
of	O
comparative	O
annotation	O
that	O
has	O
been	O
shown	O
to	O
alleviate	O
known	O
biases	O
of	O
using	O
rating	O
scales	O
.	O

Automated	B-TaskName
offensive	I-TaskName
language	I-TaskName
detection	I-TaskName
has	O
thus	O
been	O
gaining	O
interest	O
in	O
the	O
NLP	O
community	O
,	O
as	O
a	O
promising	O
direction	O
to	O
better	O
understand	O
the	O
nature	O
and	O
spread	O
of	O
such	O
content	O
.	O

There	O
are	O
several	O
challenges	O
in	O
the	O
automatic	B-TaskName
detection	I-TaskName
of	I-TaskName
offensive	I-TaskName
language	I-TaskName
(	O
Wiedemann	O
et	O
al	O
.	O
,	O

Hovy	O
(	O
2016	O
)	O
classified	O
comments	O
as	O
racist	O
,	O
sexist	O
,	O
neither	O
;	O
as	O
hate	O
-	O
speech	O
,	O
offensive	O
but	O
not	O
hate	O
-	O
speech	O
,	O
neither	O
offensive	O
nor	O
hate	O
-	O
speech	O
and	O
Founta	O
et	O
al	O
.	O
(	O

However	O
,	O
these	O
categories	O
have	O
significant	O
overlaps	O
with	O
each	O
other	O
,	O
creating	O
ill	O
-	O
defined	O
boundaries	O
,	O
thus	O
introducing	O
ambiguity	O
and	O
annotation	O
inconsistency	O
(	O
Founta	O
et	O
al	O
.	O
,	O

2020;Soral	O
et	O
al	O
.	O
,	O

The	O
most	O
common	O
strategy	O
used	O
is	O
key	B-MethodName
-	I-MethodName
word	I-MethodName
based	I-MethodName
sampling	I-MethodName
.	O

This	O
results	O
in	O
datasets	O
that	O
are	O
rich	O
in	O
explicit	O
offensive	O
language	O
(	O
language	O
that	O
is	O
unambiguous	O
in	O
its	O
potential	O
to	O
be	O
offensive	O
,	O
such	O
as	O
those	O
using	O
slurs	O
or	O
swear	O
words	O
(	O
Waseem	O
et	O
al	O
.	O
,	O

2017;Wiegand	O
et	O
al	O
.	O
,	O

key	B-MethodName
-	I-MethodName
word	I-MethodName
based	I-MethodName
sampling	I-MethodName
often	O
results	O
in	O
spurious	O
correlations	O
(	O
e.g.	O
,	O
sports	O
-	O
related	O
expressions	O
such	O
as	O
announcer	O
and	O
sport	O
occur	O
very	O
frequently	O
in	O
offensive	O
tweets	O
)	O
.	O

Thus	O
,	O
we	O
annotate	O
our	O
dataset	O
using	O
an	O
efficient	O
form	O
of	O
comparative	O
annotation	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
(	I-MethodName
BWS	I-MethodName
)	I-MethodName
(	O
Louviere	O
,	O
1991;Louviere	O
et	O
al	O
.	O
,	O

By	O
obtaining	O
real	O
-	O
valued	O
offensiveness	O
scores	O
,	O
different	O
thresholds	O
can	O
be	O
used	O
in	O
downstream	O
applications	O
to	O
handle	O
varying	O
degrees	O
of	O
offensiveness	O
appropriately	O
.	O

We	O
also	O
greatly	O
mitigate	O
issues	O
of	O
annotator	O
de	O
-	O
sensitization	O
as	O
one	O
will	O
still	O
be	O
able	O
to	O
recognize	O
if	O
one	O
comment	O
is	O
more	O
offensive	O
than	O
another	O
,	O
even	O
if	O
they	O
think	O
both	O
comments	O
are	O
not	O
that	O
offensive	O
.	O

Finally	O
,	O
we	O
benchmark	O
several	O
widely	O
-	O
used	O
neural	O
models	O
in	O
their	O
ability	O
to	O
predict	O
offensiveness	O
scores	O
on	O
this	O
new	O
dataset	O
.	O

Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
used	O
terms	O
frequently	O
occurring	O
in	O
offensive	O
tweets	O
,	O
while	O
used	O
a	O
list	O
of	O
hate	O
-	O
related	O
terms	O
to	O
extract	O
offensive	O
tweets	O
from	O
the	O
Twitter	O
search	O
API	O
.	O

2018	O
)	O
,	O
Wiegand	O
et	O
al	O
.	O
(	O

2021	O
)	O
.	O

The	O
comments	O
were	O
sampled	O
at	O
random	O
from	O
a	O
large	O
dump	O
of	O
English	O
Wikipedia	O
,	O
and	O
boosted	O
by	O
including	O
comments	O
from	O
blocked	O
users	O
.	O

While	O
these	O
labels	O
were	O
introduced	O
to	O
create	O
a	O
separation	O
between	O
the	O
nature	O
of	O
comments	O
with	O
a	O
score	O
of	O
1.0	O
and	O
those	O
with	O
a	O
score	O
of	O
0.6	O
(	O
which	O
would	O
otherwise	O
be	O
classified	O
as	O
attacks	O
)	O
,	O
they	O
are	O
discrete	O
.	O

In	O
our	O
work	O
,	O
using	O
the	O
BWS	B-MethodName
comparative	O
annotation	O
setup	O
,	O
we	O
assign	O
fine	O
-	O
grained	O
continuous	O
scores	O
to	O
comments	O
to	O
denote	O
their	O
degree	O
of	O
offensiveness	O
.	O

BWS	B-MethodName
was	O
proposed	O
by	O
Louviere	O
(	O
1991	O
)	O
.	O

Kiritchenko	O
and	O
Mohammad	O
(	O
2017	O
)	O
have	O
experimentally	O
shown	O
that	O
BWS	B-MethodName
produces	O
more	O
reliable	O
finegrained	O
scores	O
than	O
the	O
scores	O
acquired	O
utilizing	O
rating	O
scales	O
.	O

In	O
the	O
BWS	B-MethodName
annotation	O
setup	O
,	O
the	O
annotators	O
are	O
given	O
an	O
n	O
-	O
tuple	O
(	O
where	O
n	O
>	O
1	O
,	O
and	O
commonly	O
n	O
=	O
4	O
)	O
,	O
and	O
asked	O
which	O
item	O
is	O
the	O
best	O
and	O
which	O
is	O
the	O
worst	O
(	O
best	O
and	O
worst	O
correspond	O
to	O
the	O
highest	O
and	O
the	O
lowest	O
with	O
respect	O
to	O
a	O
property	O
of	O
interest	O
)	O
.	O

Within	O
the	O
NLP	O
community	O
,	O
BWS	B-MethodName
has	O
thus	O
far	O
been	O
used	O
only	O
for	O
creating	O
datasets	O
for	O
relational	O
similarity	O
(	O
Jurgens	O
et	O
al	O
.	O
,	O

2012	O
)	O
,	O
word	O
-	O
sense	O
disambiguation	O
(	O
Jurgens	O
,	O
2013	O
)	O
,	O
word	O
-	O
sentiment	O
intensity	O
(	O
Kiritchenko	O
et	O
al	O
.	O
,	O

Using	O
BWS	B-MethodName
,	O
we	O
create	O
the	O
first	O
dataset	O
with	O
degree	O
of	O
offensiveness	O
scores	O
for	O
social	O
media	O
comments	O
.	O

As	O
users	O
can	O
also	O
reply	O
to	O
a	O
comment	O
,	O
the	O
entire	O
discussion	O
has	O
a	O
hierarchical	O
structure	O
called	O
the	O
comment	O
thread	O
.	O

These	O
subreddits	O
were	O
chosen	O
to	O
cover	O
a	O
diverse	O
range	O
of	O
topics	O
.	O

The	O
CMV	O
subreddit	O
(	O
with	O
over	O
a	O
million	O
users	O
)	O
has	O
posts	O
and	O
comments	O
on	O
controversial	O
topics.3	O
.	O

For	O
example	O
,	O
Jay	O
and	O
Janschewitz	O
(	O
2008	O
)	O
show	O
that	O
people	O
tend	O
to	O
swear	O
when	O
they	O
are	O
angry	O
,	O
frustrated	O
or	O
anxious	O
.	O

We	O
followed	O
the	O
procedure	O
described	O
in	O
Kiritchenko	O
and	O
Mohammad	O
(	O
2016	O
)	O
to	O
obtain	O
BWS	B-MethodName
annotations	O
.	O

The	O
BWS	B-MethodName
responses	O
were	O
converted	O
to	O
scores	O
using	O
a	O
simple	O
counting	O
procedure	O
(	O
Orme	O
,	O
2009;Flynn	O
and	O
Marley	O
,	O
2014	O
)	O
.	O

We	O
release	O
the	O
aggregated	O
annotations	O
as	O
well	O
as	O
the	O
individual	O
annotations	O
of	O
Ruddit	B-DatasetName
,	O
to	O
allow	O
further	O
work	O
on	O
examining	O
and	O
understanding	O
the	O
variability	O
.	O

5	O
We	O
can	O
not	O
use	O
standard	O
inter	O
-	O
annotator	O
agreement	O
measures	O
to	O
ascertain	O
the	O
quality	O
of	O
comparative	O
annotations	O
.	O

The	O
disagreement	O
that	O
arises	O
in	O
tuples	O
having	O
two	O
items	O
that	O
are	O
close	O
together	O
in	O
their	O
degree	O
of	O
offensiveness	O
is	O
a	O
useful	O
signal	O
for	O
BWS	B-MethodName
(	O
helping	O
it	O
give	O
similar	O
scores	O
to	O
the	O
two	O
items	O
)	O
.	O

To	O
assess	O
this	O
reproducibility	O
,	O
we	O
computed	O
average	O
split	B-MetricName
-	I-MetricName
half	I-MetricName
reliability	I-MetricName
(	I-MetricName
SHR	I-MetricName
)	I-MetricName
values	O
over	O
100	O
trials	O
.	O

SHR	B-MetricName
is	O
a	O
commonly	O
used	O
approach	O
to	O
determine	O
consistency	O
in	O
psychological	O
studies	O
.	O

For	O
computing	O
SHR	B-MetricName
values	O
,	O
the	O
annotations	O
for	O
each	O
4	O
-	O
tuple	O
were	O
randomly	O
split	O
in	O
two	O
halves	O
.	O

Table	O
1	O
shows	O
the	O
SHR	B-MetricName
for	O
our	O
annotations	O
.	O

SHR	B-MetricName
scores	O
of	O
over	O
0.8	B-MetricValue
indicate	O
substantial	O
reliability	O
.	O

As	O
mentioned	O
earlier	O
,	O
in	O
Ruddit	B-DatasetName
,	O
certain	O
words	O
such	O
as	O
gay	O
,	O
trans	O
,	O
male	O
,	O
female	O
,	O
black	O
,	O
white	O
were	O
found	O
to	O
exhibit	O
a	O
relatively	O
higher	O
association	O
with	O
the	O
offensive	O
bins	O
than	O
with	O
the	O
supportive	O
bins	O
.	O

In	O
order	O
to	O
probe	O
the	O
effect	O
of	O
this	O
on	O
the	O
computational	O
models	O
,	O
we	O
created	O
a	O
variant	O
of	O
Ruddit	B-DatasetName
by	O
replacing	O
all	O
the	O
identity	O
terms	O
(	O
from	O
the	O
list	O
given	O
in	O
Appendix	O
A.4	O
)	O
in	O
the	O
comments	O
with	O
the	O
[	O
group	O
]	O
token	O
and	O
observed	O
the	O
effect	O
on	O
the	O
models	O
'	O
per	O
-	O
formance	O
.	O

We	O
refer	O
to	O
this	O
variant	O
of	O
the	O
dataset	O
as	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

We	O
examined	O
this	O
relationship	O
quantitatively	O
using	O
Ruddit	B-DatasetName
and	O
the	O
NRC	B-DatasetName
VAD	I-DatasetName
Lexicon	I-DatasetName
(	O
which	O
has	O
intensity	O
scores	O
along	O
the	O
valence	O
,	O
arousal	O
,	O
and	O
dominance	O
dimensions	O
)	O
.	O

For	O
each	O
comment	O
in	O
Ruddit	B-DatasetName
,	O
we	O
calculated	O
three	O
scores	O
that	O
captured	O
the	O
intensities	O
of	O
the	O
V	O
,	O
A	O
,	O
D	O
words	O
(	O
the	O
averages	O
of	O
the	O
intensities	O
of	O
the	O
V	O
/	O
A	O
/	O
D	O
words	O
in	O
the	O
comment	O
)	O
,	O
using	O
the	O
entire	O
lexicon	O
.	O

Only	O
comments	O
containing	O
at	O
least	O
4	O
words	O
from	O
the	O
VAD	O
lexicon	O
were	O
considered	O
for	O
the	O
score	O
and	O
correlation	O
calculation	O
.	O

two	O
sources	O
,	O
comments	O
are	O
more	O
prevalent	O
in	O
the	O
supportive	O
bins	O
.	O

The	O
higher	O
representation	O
of	O
comments	O
from	O
Topics	O
than	O
the	O
other	O
two	O
sources	O
in	O
the	O
offensive	O
bins	O
,	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
the	O
Topics	O
category	O
includes	O
subreddits	O
such	O
as	O
worldnews	O
and	O
worldpolitics	O
.	O

To	O
study	O
the	O
impact	O
of	O
comments	O
containing	O
swear	O
words	O
on	O
computational	O
models	O
,	O
we	O
created	O
another	O
variant	O
of	O
Ruddit	B-DatasetName
in	O
which	O
we	O
removed	O
all	O
the	O
comments	O
containing	O
at	O
least	O
one	O
swear	O
word	O
.	O

We	O
refer	O
to	O
this	O
variant	O
as	O
the	O
no	O
-	O
swearing	O
dataset	O
.	O

Thus	O
,	O
we	O
created	O
a	O
subset	O
of	O
Ruddit	B-DatasetName
containing	O
comments	O
with	O
scores	O
from	O
−0.5	O
to	O
0.5	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
benchmark	O
experiments	O
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
by	O
implementing	O
some	O
commonly	O
used	O
model	O
architectures	O
.	O

We	O
performed	O
5	B-HyperparameterValue
-	O
fold	O
crossvalidation	B-MetricName
for	O
each	O
of	O
the	O
models	O
.	O

6	O
Bidirectional	O
LSTM	O
We	O
fed	O
pre	O
-	O
trained	O
300	B-HyperparameterValue
dimensional	O
GloVe	B-HyperparameterName
word	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Pennington	O
et	O
al	O
.	O
,	O

2014	O
)	O
to	O
a	O
2	B-HyperparameterValue
-	O
layered	O
BiLSTM	B-MethodName
to	O
obtain	O
a	O
sentence	O
representation	O
(	O
using	O
a	O
concatenation	O
of	O
the	O
last	O
hidden	O
state	O
from	O
the	O
forward	O
and	O
backward	O
direction	O
)	O
.	O

We	O
used	O
Mean	O
Squared	O
Error	O
(	O
MSE	O
)	O
loss	O
as	O
the	O
objective	O
function	O
,	O
Adam	B-HyperparameterValue
with	O
0.001	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
the	O
optimizer	B-HyperparameterName
,	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
,	O
and	O
a	O
dropout	B-HyperparameterName
of	O
0.5	B-HyperparameterValue
.	O

The	O
model	O
was	O
trained	O
for	O
7	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

Dataset	O
HateBERT	B-MethodName
BERT	B-MethodName
BiLSTM	B-MethodName
r	B-MetricName
MSE	B-MetricName
r	B-MetricName
MSE	B-MetricName
r	B-MetricName
MSEa	B-MetricName
.	O

Ruddit	B-DatasetName
0.886	O
±	O
0.003	O
0.025	O
±	O
0.001	O
0.873	O
±	O
0.005	O
0.027	O
±	O
0.001	O
0.831	O
±	O
0.005	O
0.035	O
±	O
0.001	O
b.	O
Identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
0.883	O
±	O
0.006	O
0.025	O
±	O
0.001	O
0.869	O
±	O
0.007	O
0.027	O
±	O
0.001	O
0.824	O
±	O
0.007	O
0.036	O
±	O
0.001	O
c.	O
No	O
-	O
swearing	O
0.808	O
±	O
0.013	O
0.023	O
±	O
0.001	O
0.783	O
±	O
0.012	O
0.027	O
±	O
0.001	O
0.704	O
±	O
0.014	O
0.036	O
±	O
0.002	O
d.	O
Reduced	O
-	O
range	O
0.781	O
±	O
0.014	O
0.022	O
±	O
0.001	O
0.757	O
±	O
0.011	O
0.025	O
±	O
0.001	O
0.659	O
±	O
0.008	O
0.033	O
±	O
0.001	O
BERT	B-MethodName
We	O
fine	O
-	O
tuned	O
BERT	B-MethodName
base	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

More	O
details	O
in	O
Appendix	O
A.5.)HateBERT	B-MethodName
HateBERT	B-MethodName
(	O
Caselli	O
et	O
al	O
.	O
,	O

2020b	O
)	O
is	O
a	O
version	O
of	O
BERT	B-MethodName
pretrained	O
for	O
abusive	O
language	O
detection	O
in	O
English	O
.	O

HateBERT	B-MethodName
was	O
trained	O
on	O
RAL	O
-	O
E	O
,	O
a	O
large	O
dataset	O
of	O
English	O
language	O
Reddit	O
comments	O
from	O
communities	O
banned	O
for	O
being	O
offensive	O
or	O
hateful	O
.	O

2019).We	O
fine	O
-	O
tuned	O
HateBERT	B-MethodName
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
.	O

The	O
experimental	O
setup	O
for	O
this	O
model	O
is	O
the	O
same	O
as	O
that	O
described	O
for	O
the	O
BERT	B-MethodName
model	O
.	O

We	O
report	O
Pearson	B-MetricName
correlation	I-MetricName
(	I-MetricName
r	I-MetricName
)	I-MetricName
and	O
MSE	B-MetricName
,	O
averaged	O
over	O
all	O
folds	O
.	O

The	O
performance	O
of	O
the	O
models	O
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
is	O
shown	O
in	O
the	O
Table	O
5	O
.	O

Note	O
that	O
the	O
performance	O
values	O
on	O
the	O
noswearing	O
and	O
the	O
reduced	O
-	O
range	O
datasets	O
are	O
not	O
directly	O
comparable	O
to	O
the	O
performance	O
values	O
on	O
the	O
full	O
Ruddit	B-DatasetName
as	O
their	O
score	O
range	O
is	O
different	O
.	O

We	O
can	O
see	O
that	O
on	O
all	O
the	O
datasets	O
,	O
the	O
HateBERT	B-MethodName
model	O
performs	O
the	O
best	O
,	O
followed	O
by	O
the	O
BERT	B-MethodName
model	O
.	O

Interestingly	O
,	O
the	O
model	O
performance	O
(	O
for	O
all	O
models	O
)	O
does	O
not	O
change	O
substantially	O
when	O
trained	O
on	O
Ruddit	O
or	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

This	O
indicates	O
that	O
the	O
computational	O
models	O
are	O
not	O
learning	O
to	O
benefit	O
from	O
the	O
association	O
of	O
certain	O
identity	O
terms	O
with	O
a	O
specific	O
range	O
of	O
scores	O
on	O
the	O
offensiveness	O
scale	O
.	O

Yet	O
,	O
the	O
fact	O
that	O
the	O
models	O
still	O
obtain	O
performance	O
of	O
up	O
to	O
0.8	B-MetricValue
(	O
r	B-MetricName
)	O
demonstrates	O
that	O
they	O
necessitate	O
and	O
are	O
able	O
to	O
learn	O
other	O
types	O
of	O
offensiveness	O
features	O
.	O

It	O
is	O
also	O
worth	O
mentioning	O
that	O
even	O
if	O
they	O
encounter	O
swear	O
words	O
in	O
a	O
comment	O
,	O
the	O
task	O
is	O
not	O
simply	O
to	O
label	O
the	O
comment	O
as	O
offensive	O
but	O
to	O
provide	O
a	O
suitable	O
score	O
.	O

Finally	O
,	O
the	O
models	O
obtained	O
the	O
performance	O
of	O
up	O
to	O
0.78	B-MetricValue
(	O
r	B-MetricName
)	O
on	O
the	O
reduced	O
-	O
range	O
dataset	O
,	O
which	O
shows	O
that	O
even	O
if	O
the	O
comments	O
from	O
the	O
extreme	O
ends	O
of	O
the	O
offensiveness	O
scale	O
are	O
removed	O
,	O
Ruddit	O
still	O
presents	O
an	O
interesting	O
and	O
feasible	O
offensiveness	O
scoring	O
task	O
.	O

Error	O
Analysis	O
Figure	O
4	O
shows	O
the	O
squared	O
error	O
values	O
of	O
the	O
3	O
models	O
over	O
the	O
offensiveness	O
score	O
range	O
in	O
Ruddit	B-DatasetName
.	O

It	O
is	O
interesting	O
to	O
observe	O
that	O
HateBERT	B-MethodName
,	O
unlike	O
the	O
other	O
two	O
models	O
,	O
does	O
not	O
have	O
high	O
error	O
values	O
for	O
samples	O
within	O
the	O
score	O
range	O
0.25	O
-	O
0.75	O
.	O

This	O
indicates	O
that	O
HateBERT	B-MethodName
is	O
efficient	O
in	O
dealing	O
with	O
offensive	O
language	O
that	O
does	O
not	O
lie	O
in	O
the	O
extreme	O
offensive	O
end	O
.	O

BiLSTM	B-MethodName
seems	O
relatively	O
less	O
accurate	O
for	O
samples	O
in	O
the	O
supportive	O
range	O
(	O
−0.75	O
to	O
−0.25	O
)	O
.	O

We	O
used	O
a	O
comparative	O
annotation	O
technique	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
which	O
addresses	O
the	O
limitations	O
of	O
traditional	O
rating	O
scales	O
.	O

We	O
found	O
that	O
computational	O
models	O
are	O
not	O
benefiting	O
from	O
the	O
association	O
of	O
identity	O
terms	O
with	O
specific	O
range	O
of	O
scores	O
on	O
the	O
offensiveness	O
scale	O
.	O

We	O
create	O
Ruddit	B-DatasetName
to	O
study	O
,	O
understand	O
and	O
explore	O
the	O
nature	O
of	O
offensive	O
language	O
.	O

This	O
includes	O
several	O
aspects	O
such	O
as	O
mitigating	O
harm	O
to	O
people	O
involved	O
,	O
improving	O
data	O
privacy	O
,	O
and	O
informed	O
consent	O
.	O

We	O
release	O
data	O
in	O
a	O
manner	O
that	O
is	O
GDPR	O
compliant	O
.	O

8	O
The	O
researchers	O
using	O
the	O
dataset	O
need	O
to	O
retrieve	O
the	O
data	O
using	O
the	O
Reddit	O
API.Speaker	O
and	O
Annotator	O
Demographic	O
:	O
No	O
specific	O
speaker	O
demographic	O
information	O
is	O
available	O
for	O
the	O
comments	O
included	O
in	O
Ruddit	B-DatasetName
.	O

In	O
this	O
survey	O
,	O
3370	O
workers	O
participated	O
.	O

Finally	O
,	O
Ruddit	B-DatasetName
was	O
created	O
with	O
the	O
intention	O
to	O
look	O
at	O
wide	O
ranging	O
offensive	O
language	O
of	O
various	O
degrees	O
as	O
opposed	O
to	O
detecting	O
offensive	O
language	O
towards	O
specific	O
target	O
groups	O
.	O

Annotation	O
Guidelines	O
:	O
We	O
created	O
our	O
annotation	O
guidelines	O
drawing	O
inspiration	O
from	O
the	O
community	O
standards	O
set	O
for	O
offensive	O
language	O
on	O
several	O
social	O
media	O
platforms	O
.	O

AMT	O
provides	O
a	O
checkbox	O
where	O
requesters	O
can	O
indicate	O
that	O
some	O
content	O
in	O
the	O
task	O
may	O
be	O
offensive	O
.	O

These	O
tasks	O
are	O
not	O
shown	O
to	O
annotators	O
who	O
have	O
specified	O
so	O
in	O
their	O
profile	O
.	O

We	O
call	O
this	O
variant	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

2018	O
)	O
with	O
a	O
few	O
of	O
our	O
own	O
additions	O
.	O

Hyperparameter	O
Tuning	O
We	O
tuned	O
hyperparameters	O
for	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
.	O

All	O
experiments	O
were	O
performed	O
on	O
a	O
fixed	O
seed	O
value	O
of	O
12.For	O
the	O
BiLSTM	B-MethodName
model	O
,	O
the	O
batch	O
size	O
was	O
fixed	O
at	O
32	O
and	O
the	O
number	O
of	O
epochs	O
was	O
set	O
to	O
7	O
.	O

The	O
hyperparameter	O
search	O
space	O
is	O
as	O
follows	O
:	O
For	O
the	O
BERT	B-MethodName
model	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
fixed	O
at	O
16	B-HyperparameterValue
and	O
BERT	B-MethodName
tokenizer	B-HyperparameterName
's	I-HyperparameterName
maximum	I-HyperparameterName
length	I-HyperparameterName
was	O
set	O
to	O
200	B-HyperparameterValue
.	O

2019	O
)	O
found	O
to	O
work	O
best	O
on	O
all	O
tasks	O
.	O

The	O
search	O
space	O
is	O
as	O
follows:2714•	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
:	O
2e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
,	O
3e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
,	O
5e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
We	O
reported	O
the	O
best	O
setting	O
for	O
the	O
models	O
in	O
section	O
6.1	O
.	O

The	O
average	B-MetricName
r	I-MetricName
of	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
across	O
all	O
hyperparameter	O
search	O
This	O
research	O
was	O
funded	O
by	O
the	O
Facebook	O
Online	O
Safety	O
Benchmark	O
Research	O
award	O
for	O
the	O
project	O
"	O
A	O
Benchmark	O
and	O
Evaluation	O
Framework	O
for	O
Abusive	O
Language	O
Detection	O
.	O
"	O

With	O
the	O
increasing	O
interest	O
in	O
low	O
-	O
resource	O
languages	O
,	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
has	O
become	O
an	O
active	O
area	O
of	O
research	O
,	O
where	O
approaches	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

We	O
demonstrate	O
the	O
power	O
of	O
harnessing	O
linguistic	O
knowledge	O
as	O
priors	O
within	O
Adaptor	B-MethodName
Grammars	I-MethodName
in	O
a	O
minimally	O
-	O
supervised	O
learning	O
fashion	O
.	O

We	O
introduce	O
two	O
types	O
of	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
language	O
-	O
specific	O
grammars	O
;	O
and	O
2	O
)	O
linguistprovided	B-MethodName
affixes	I-MethodName
,	O
collected	O
by	O
an	O
expert	O
in	O
the	O
language	O
and	O
seeded	O
into	O
the	O
grammars	O
.	O

We	O
use	O
Japanese	O
and	O
Georgian	O
as	O
respective	O
case	O
studies	O
for	O
the	O
two	O
types	O
of	O
priors	O
and	O
introduce	O
new	O
datasets	O
for	O
these	O
languages	O
,	O
with	O
gold	O
morphological	B-TaskName
segmentation	I-TaskName
for	O
evaluation	O
.	O

We	O
show	O
that	O
the	O
use	O
of	O
priors	O
results	O
in	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
and	O
34.2	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
over	O
the	O
equivalent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
system	O
.	O

Morphological	B-TaskName
segmentation	I-TaskName
is	O
an	O
essential	O
subtask	O
in	O
many	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
applications	O
,	O
especially	O
in	O
the	O
case	O
of	O
morphologically	O
complex	O
languages	O
.	O

With	O
the	O
need	O
to	O
develop	O
NLP	O
tools	O
for	O
low	O
-	O
resource	O
languages	O
,	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
has	O
been	O
receiving	O
increasing	O
interest	O
over	O
the	O
last	O
two	O
decades	O
(	O
Goldsmith	O
,	O
2001;Creutz	O
and	O
Lagus	O
,	O
2007a;Poon	O
et	O
al	O
.	O
,	O

2019.In	O
this	O
work	O
,	O
we	O
show	O
how	O
linguistic	O
priors	O
effectively	O
boost	O
morphological	O
-	O
segmentation	O
performance	O
in	O
a	O
minimally	O
-	O
supervised	O
manner	O
that	O
does	O
not	O
require	O
segmented	O
words	O
for	O
training	O
.	O

We	O
integrate	O
our	O
priors	O
within	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
Johnson	O
et	O
al	O
.	O
,	O

2007	O
)	O
,	O
a	O
type	O
of	O
nonparametric	O
Bayesian	O
models	O
that	O
generalize	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammars	I-MethodName
(	O
PCFGs	O
)	O
.	O

Adaptor	B-MethodName
Grammars	I-MethodName
have	O
proved	O
successful	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
a	O
variety	O
of	O
typologically	O
diverse	O
languages	O
(	O
Eskander	O
et	O
al	O
.	O
,	O

2020).We	O
introduce	O
two	O
types	O
of	O
linguistic	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
a	O
languagespecific	O
grammar	O
that	O
is	O
tailored	O
for	O
the	O
language	O
of	O
interest	O
by	O
modeling	O
specific	O
morphological	O
phenomena	O
,	O
and	O
2	O
)	O
linguist	B-MethodName
-	I-MethodName
provided	I-MethodName
affixes	I-MethodName
,	O
where	O
an	O
expert	O
in	O
the	O
underlying	O
language	O
compiles	O
a	O
list	O
of	O
carefully	O
selected	O
affixes	O
and	O
seeds	O
it	O
into	O
the	O
grammars	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
.	O

1	O
We	O
utilize	O
MorphAGram	B-MethodName
(	O
Eskander	O
et	O
al	O
.	O
,	O

2020	O
)	O
2	O
,	O
an	O
open	O
-	O
source	O
morphologicalsegmentation	B-TaskName
framework	O
that	O
is	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
AGs	O
)	O
(	O
Johnson	O
et	O
al	O
.	O
,	O

2020.Adaptor	O
Grammars	O
are	O
non	O
-	O
parametric	O
Bayesian	O
models	O
that	O
are	O
composed	O
of	O
two	O
main	O
components	O
:	O
1	O
)	O
a	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammar	I-MethodName
(	I-MethodName
PCFG	I-MethodName
)	I-MethodName
whose	O
definition	O
relies	O
on	O
the	O
underlying	O
task	O
(	O
in	O
the	O
case	O
of	O
morphological	O
segmentation	O
,	O
a	O
PCFG	O
models	O
word	O
structure	O
)	O
;	O
and	O
2	O
)	O
an	O
adaptor	B-MethodName
that	O
is	O
based	O
on	O
the	O
Pitman	B-MethodName
-	I-MethodName
Yor	I-MethodName
process	I-MethodName
(	O
Pitman	O
,	O
1995	O
)	O
.	O

The	O
adaptor	B-MethodName
keeps	O
the	O
posterior	O
probability	O
of	O
a	O
subtree	O
proportional	O
to	O
the	O
number	O
of	O
times	O
that	O
subtree	O
is	O
utilized	O
to	O
parse	O
the	O
input	O
data	O
and	O
manages	O
the	O
caching	O
of	O
the	O
subtrees	O
.	O

The	O
learning	O
process	O
is	B-MethodName
Markov	I-MethodName
Chain	I-MethodName
Monte	I-MethodName
Carlo	I-MethodName
sampling	I-MethodName
(	I-MethodName
MCMC	I-MethodName
)	I-MethodName
(	O
Andrieu	O
et	O
al	O
.	O
,	O

2016	O
)	O
define	O
a	O
set	O
of	O
languageindependent	O
grammars	O
and	O
three	O
learning	O
settings	O
for	B-MethodName
Adaptor	I-MethodName
Grammars	I-MethodName
:	O
1	O
)	O
Standard	O
,	O
fully	O
unsupervised	O
;	O
2	O
)	O
Scholar	O
-	O
Seeded	O
,	O
minimally	O
-	O
supervised	O
by	O
manually	O
seeding	O
affixes	O
into	O
the	O
grammar	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
,	O
and	O
3	O
)	O
Cascaded	O
,	O
fully	O
unsupervised	O
by	O
approximating	O
the	O
Scholar	O
-	O
Seeded	O
setting	O
using	O
automatically	O
generated	O
af	O
-	O
fixes	O
from	O
an	O
initial	O
round	O
of	O
learning	O
.	O

We	O
consider	O
their	O
PrStSu+SM	B-MethodName
grammar	I-MethodName
in	O
the	O
current	O
study	O
as	O
it	O
is	O
the	O
grammar	O
that	O
performed	O
best	O
on	O
average	O
across	O
different	O
languages	O
.	O

Similar	O
to	O
the	O
Scholar	O
-	O
Seeded	O
setting	O
,	O
we	O
compile	O
a	O
list	O
of	O
affixes	O
and	O
seed	O
it	O
into	O
the	O
grammar	O
trees	O
before	O
learning	O
the	O
segmentation	O
model	O
.	O

We	O
annotate	O
two	O
datasets	O
with	O
morphological	B-TaskName
segmentation	I-TaskName
that	O
we	O
use	O
as	O
the	O
gold	O
standard	O
to	O
evaluate	O
our	O
segmentation	B-TaskName
models	O
for	O
Japanese	O
and	O
Georgian	O
.	O

For	O
Georgian	O
,	O
which	O
has	O
highly	O
complex	O
morphology	O
,	O
we	O
started	O
with	O
the	O
gold	O
-	O
standard	O
dataset	O
of	O
1000	O
words	O
introduced	O
by	O
Eskander	O
We	O
evaluate	O
our	O
morphological	O
-	O
segmentation	O
models	O
for	O
Japanese	O
in	O
the	O
Standard	O
(	O
STD	O
)	O
and	O
Cascaded	O
(	O
CAS	O
)	O
5	O
settings	O
,	O
both	O
with	O
generic	O
and	O
language	O
-	O
specific	O
(	O
LS	O
)	O
grammar	O
definitions	O
.	O

For	O
Georgian	O
,	O
we	O
evaluate	O
our	O
morphologicalsegmentation	O
models	O
in	O
the	O
Standard	O
(	O
STD	O
)	O
,	O
Cascaded	O
(	O
CAS	O
)	O
and	O
Scholar	O
-	O
Seeded	O
(	O
SS	O
)	O
settings	O
,	O
in	O
addition	O
to	O
the	O
proposed	O
Scholar	O
-	O
Seeded	O
setting	O
with	O
linguist	O
-	O
provided	O
affixes	O
(	O
SS	O
-	O
Ling).We	O
perform	O
the	O
evaluation	O
in	O
a	O
transductive	O
manner	O
,	O
where	O
the	O
unsegmented	O
words	O
in	O
the	O
gold	O
standard	O
are	O
part	O
of	O
the	O
training	O
sets	O
;	O
this	O
is	O
common	O
in	O
evaluating	O
unsupervised	O
and	O
minimally	B-TaskName
-	I-TaskName
supervised	I-TaskName
morphological	I-TaskName
segmentation	I-TaskName
(	O
Poon	O
et	O
al	O
.	O
,	O

BPR	B-MetricName
is	O
the	O
classical	O
metric	O
for	O
evaluating	O
morphological	B-TaskName
segmentation	I-TaskName
;	O
it	O
compares	O
the	O
boundaries	O
in	O
the	O
proposed	O
segmentation	O
to	O
those	O
in	O
the	O
reference	O
.	O

We	O
evaluate	O
our	O
system	O
versus	O
two	O
state	O
-	O
of	O
-	O
theart	O
unsupervised	O
baselines	O
:	O
MorphAGram	B-MethodName
without	O
the	O
use	O
of	O
linguistic	O
priors	O
and	O
Morfessor	B-MethodName
(	O
Virpioja	O
et	O
al	O
.	O
,	O

Morfessor	B-MethodName
is	O
a	O
commonly	O
-	O
used	O
framework	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
.	O

It	O
is	O
based	O
on	O
an	O
HMM	B-MethodName
model	O
that	O
relies	O
on	O
the	O
Minimum	B-MethodName
Description	I-MethodName
Length	I-MethodName
(	O
MDL	O
)	O
concept	O
for	O
deriving	O
the	O
optimal	O
segmentation	O
(	O
Creutz	O
and	O
Lagus	O
,	O
2007b	O
)	O
.	O

Finally	O
,	O
we	O
report	O
all	O
the	O
Adaptor	B-MethodName
-	I-MethodName
Grammar	I-MethodName
results	O
as	O
the	O
average	O
over	O
three	O
runs	O
of	O
different	O
randomization	O
parameters	O
.	O

Table	O
2	O
reports	O
the	O
overall	O
performance	O
of	O
our	O
models	O
for	O
both	O
Japanese	O
and	O
Georgian	O
,	O
while	O
Table	O
3	O
shows	O
the	O
results	O
per	O
part	O
-	O
of	O
-	O
speech	O
category	O
for	O
Georgian	O
.	O

For	O
Japanese	O
,	O
the	O
use	O
of	O
a	O
language	O
-	O
specific	O
grammar	O
definition	O
improves	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
resulting	O
in	O
BPR	B-MetricName
F1	I-MetricName
-	O
score	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
and	O
7.1	B-MetricValue
%	I-MetricValue
over	O
the	O
generic	O
Standard	O
and	O
Cascaded	O
settings	O
,	O
respectively	O
,	O
and	O
a	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reduction	O
of	B-MetricValue
9.8	I-MetricValue
%	I-MetricValue
over	O
Morfessor	B-MethodName
.	O

For	O
Georgian	O
,	O
the	O
use	O
of	O
linguist	O
-	O
provided	O
seeded	O
affixes	O
improves	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
where	O
the	O
recall	B-MetricName
significantly	O
increases	O
by	O
absolute	O
13.3	B-MetricValue
%	I-MetricValue
over	O
using	O
an	O
affix	O
list	O
of	O
lower	O
quality	O
.	O

In	O
addition	O
,	O
the	O
proposed	O
linguistic	O
priors	O
result	O
in	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reductions	O
of	O
34.2	B-MetricValue
%	I-MetricValue
,	O
30.0	B-MetricValue
%	I-MetricValue
and	O
31.1	B-MetricValue
%	I-MetricValue
over	O
the	O
Standard	O
,	O
Cascaded	O
and	O
regular	O
Scholar	O
-	O
Seeded	O
settings	O
,	O
respectively	O
,	O
and	O
a	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reduction	O
of	O
53.3	B-MetricValue
%	I-MetricValue
over	O
Morfessor	B-MethodName
.	O

Analysing	O
results	O
per	O
category	O
,	O
verbs	O
and	O
nouns	O
receive	O
the	O
biggest	O
F1	B-MetricName
-	O
score	O
improvements	O
of	O
absolute	O
14.3	B-MetricValue
%	I-MetricValue
and	O
4.9	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
with	O
the	O
use	O
of	O
linguist	O
-	O
provided	O
affixes	O
.	O

A	O
similar	O
pattern	O
of	O
results	O
is	O
found	O
with	O
EMMA-2	B-MethodName
.	O

Finally	O
,	O
all	O
the	O
improvements	O
due	O
to	O
the	O
use	O
of	O
linguistic	O
priors	O
are	O
statistically	O
significant	O
(	O
P	B-HyperparameterName
<	O
0.01	B-HyperparameterValue
)	O
on	O
both	O
metrics	O
.	O

Georgian	B-MethodName
segmentation	I-MethodName
models	I-MethodName
.	O

We	O
discuss	O
the	O
most	O
prominent	O
observations	O
below	O
.	O

Japanese	O
:	O
Both	O
the	O
STD	B-MethodName
and	B-MethodName
STD	I-MethodName
-	I-MethodName
LS	I-MethodName
models	O
perform	O
well	O
on	B-MethodName
prefix	I-MethodName
segmentation	I-MethodName
,	O
achieving	B-MetricName
F1	I-MetricName
-	O
scores	O
of	O
more	O
than	O
90	B-MetricValue
%	I-MetricValue
in	O
the	O
detection	O
of	O
several	O
one	O
-	O
character	O
prefixes	O
,	O
such	O
as	O
お	O
and	O
ご	O
.	O

However	O
,	O
STD	B-MethodName
-	I-MethodName
LS	I-MethodName
outperforms	O
its	O
languageindependent	O
counterpart	O
in	O
the	O
detection	O
of	O
stems	O
,	O
where	O
compounding	O
is	O
explicitly	O
modeled	O
.	O

For	O
instance	O
,	O
STD	B-MethodName
and	O
STD	B-MethodName
-	I-MethodName
LS	I-MethodName
achieve	O
F1	B-MetricName
-	O
scores	O
of	O
15.8	B-MetricValue
%	I-MetricValue
and	O
98.6	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
in	O
the	O
detection	O
of	O
the	O
common	O
stem	O
られ	O
(	O
be	O
)	O
.	O

Georgian	O
:	O
SS	O
-	O
Ling	O
outperforms	O
both	O
STD	B-MethodName
and	O
SS	B-MethodName
at	O
discovering	O
the	O
top	O
most	O
frequent	O
one	O
-	O
letter	O
morphemes	O
,	O
such	O
as	O
i	O
,	O
a	O
,	O
s	O
,	O
e	O
,	O
m	O
,	O
o	O
and	O
v	O
,	O
achieving	O
an	O
average	B-MetricName
F1	I-MetricName
-	O
score	O
of	O
76.0	B-MetricValue
%	I-MetricValue
,	O
compared	O
to	O
57.7	B-MetricValue
%	I-MetricValue
and	O
57.3	B-MetricValue
%	I-MetricValue
by	O
STD	B-MethodName
and	O
SS	B-MethodName
,	O
respectively	O
.	O

In	O
addition	O
,	O
SS	B-MethodName
and	O
STD	B-MethodName
suffer	O
lower	O
precision	B-MetricName
as	O
they	O
tend	O
to	O
oversegment	O
the	O
morphemes	O
represented	O
by	O
a	O
single	O
letter	O
.	O

Similarly	O
,	B-MethodName
SS	I-MethodName
-	I-MethodName
Ling	I-MethodName
can	O
recognize	O
the	O
most	O
frequent	O
two	O
-	O
letter	O
morphemes	O
,	O
namely	O
eb	O
and	O
da	O
,	O
with	O
absolute	O
increases	O
in	O
precision	B-MetricName
of	B-MetricValue
59.0	I-MetricValue
%	I-MetricValue
and	O
62.0	B-MetricValue
%	I-MetricValue
over	O
STD	B-MethodName
and	O
SS	B-MethodName
,	O
respectively	O
;	O
both	O
morphemes	O
are	O
explicitly	O
seeded	O
into	O
the	O
SS	O
-	O
Ling	O
grammar	O
prior	O
to	O
training	O
the	O
model	O
.	O

We	O
proposed	O
two	O
types	O
of	O
linguistic	O
priors	O
for	O
minimally	B-TaskName
-	I-TaskName
supervised	I-TaskName
morphological	I-TaskName
segmentation	I-TaskName
using	O
Adaptor	B-MethodName
Grammars	I-MethodName
.	O

Our	O
approaches	O
result	O
in	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
,	O
for	O
Japanese	O
,	O
and	O
34.2	B-MetricValue
%	I-MetricValue
,	O
for	O
Georgian	O
,	O
as	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
.	O

In	O
this	O
article	O
,	O
we	O
present	O
our	O
methodologies	O
for	O
SemEval-2021	B-DatasetName
Task-4	I-DatasetName
:	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
.	O

There	O
are	O
three	O
sub	O
-	O
tasks	O
within	O
this	O
task	O
:	O
Imperceptibility	B-TaskName
(	O
subtask	O
-	O
I	O
)	O
,	O
Non	B-TaskName
-	I-TaskName
Specificity	I-TaskName
(	O
subtask	O
-	O
II	O
)	O
,	O
and	O
Intersection	B-TaskName
(	O
subtask	O
-	O
III	O
)	O
.	O

We	O
use	O
encoders	B-MethodName
of	I-MethodName
transformers	I-MethodName
-	I-MethodName
based	I-MethodName
models	I-MethodName
pre	O
-	O
trained	O
on	O
the	O
masked	O
language	O
modelling	O
(	O
MLM	O
)	O
task	O
to	O
build	O
our	O
Fill	B-MethodName
-	I-MethodName
in	I-MethodName
-	I-MethodName
the	I-MethodName
-	I-MethodName
blank	I-MethodName
(	I-MethodName
FitB	I-MethodName
)	I-MethodName
models	O
.	O

Moreover	O
,	O
to	O
model	O
imperceptibility	B-MetricValue
,	O
we	O
define	O
certain	O
linguistic	O
features	O
,	O
and	O
to	O
model	O
non	B-MetricValue
-	I-MetricValue
specificity	I-MetricValue
,	O
we	O
leverage	O
information	O
from	O
hypernyms	O
and	O
hyponyms	O
provided	O
by	O
a	O
lexical	O
database	O
.	O

Specifically	O
,	O
for	B-MetricValue
non	I-MetricValue
-	I-MetricValue
specificity	I-MetricValue
,	O
we	O
try	O
out	O
augmentation	O
techniques	O
,	O
and	O
other	O
statistical	O
techniques	O
.	O

We	O
also	O
propose	O
variants	O
,	O
namely	O
Chunk	B-MethodName
Voting	I-MethodName
and	O
Max	B-MethodName
Context	I-MethodName
,	O
to	O
take	O
care	O
of	O
input	O
length	O
restrictions	O
for	O
BERT	B-MethodName
,	O
etc	O
.	O

Additionally	O
,	O
we	O
perform	O
a	O
thorough	O
ablation	O
study	O
,	O
and	O
use	O
Integrated	B-MethodName
Gradients	I-MethodName
to	O
explain	O
our	O
predictions	O
on	O
a	O
few	O
samples	O
.	O

Our	O
best	O
submissions	O
achieve	O
accuracies	B-MetricName
of	O
75.31	B-MetricValue
%	I-MetricValue
and	O
77.84	B-MetricValue
%	I-MetricValue
,	O
on	O
the	O
test	O
sets	O
for	O
subtask	O
-	O
I	O
and	O
subtask	O
-	O
II	O
,	O
respectively	O
.	O

For	O
subtask	O
-	O
III	O
,	O
we	O
achieve	O
accuracies	B-MetricName
of	O
65.64	B-MetricValue
%	I-MetricValue
and	O
62.27	B-MetricValue
%	I-MetricValue
.	O

One	O
of	O
the	O
earlier	O
pretraining	O
tasks	O
was	O
"	O
Masked	O
Language	O
Modelling	O
(	O
MLM	O
)	O
"	O
,	O
one	O
of	O
the	O
two	O
pretraining	O
tasks	O
of	O
the	O
breakthrough	O
model	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

SemEval-2021	B-DatasetName
Task-4	I-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

The	O
novelty	O
in	O
the	O
task	O
lies	O
in	O
its	O
3	O
subtasks	O
:	O
Imperceptibility	B-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
I	I-TaskName
)	I-TaskName
,	I-TaskName
Non	I-TaskName
-	I-TaskName
Specificity	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
II	I-TaskName
)	I-TaskName
,	I-TaskName
and	I-TaskName
Intersection	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
III	I-TaskName
)	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
using	O
BERT	B-MethodName
and	O
its	O
derivative	O
models	O
such	O
as	O
DistilBERT	B-MethodName
,	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

Further	O
,	O
we	O
propose	O
2	O
BERT	B-MethodName
variants	O
:	O
(	O
1	O
)	O
BERT	B-MethodName
Voting	I-MethodName
;	O
(	O
2	O
)	O
BERT	B-MethodName
Max	I-MethodName
.	O

Most	O
importantly	O
,	O
we	O
also	O
model	O
the	O
concepts	O
of	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

For	O
imperceptibility	B-TaskName
,	O
we	O
create	O
statistical	O
embeddings	O
using	O
features	O
that	O
have	O
a	O
high	O
correlation	O
with	O
concreteness	O
.	O

For	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
,	O
we	O
propose	O
two	O
approaches	O
:	O
(	O
1	O
)	O
we	O
augment	O
the	O
dataset	O
by	O
replacing	O
some	O
nouns	O
in	O
the	O
article	O
by	O
their	O
hypernyms	O
;	O
and	O
(	O
2	O
)	O
we	O
use	O
the	O
options	O
'	O
hyponyms	O
to	O
decide	O
the	O
most	O
appropriate	O
option	O
.	O

2019	O
)	O
by	O
trying	O
out	O
their	O
various	O
combinations	O
with	O
BERT.In	B-MethodName
Section	O
2	O
,	O
we	O
perform	O
a	O
succinct	O
literature	O
survey	O
.	O

Section	O
3	O
elucidates	O
our	O
approach	O
,	O
including	O
the	O
modelling	O
aspect	O
,	O
the	O
various	O
variants	O
of	O
the	O
base	O
model	O
,	O
and	O
the	O
different	O
ways	O
we	O
model	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

In	O
Section	O
4	O
,	O
we	O
perform	O
an	O
extensive	O
ablation	O
and	O
comparative	O
study	O
.	O

The	O
advent	O
of	O
large	O
-	O
scale	O
question	O
answering	O
systems	O
began	O
with	O
straightforward	O
tasks	O
,	O
like	O
the	O
one	O
introduced	O
by	O
the	O
SimpleQuestions	B-DatasetName
Dataset	O
(	O
Bordes	O
et	O
al	O
.	O
,	O

The	O
purpose	O
of	O
NLP	O
research	O
is	O
to	O
be	O
able	O
to	O
create	O
a	O
generalised	O
model	O
that	O
may	O
answer	O
questions	O
based	O
on	O
any	O
context	O
,	O
thus	O
datasets	O
like	O
the	O
CNN	B-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
(	O
Hermann	O
et	O
al	O
.	O
,	O

2015	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

Before	O
transformers	O
,	O
methods	O
consisting	O
of	O
LSTM	B-MethodName
/	I-MethodName
GRUs	I-MethodName
were	O
used	O
to	O
achieve	O
good	O
results	O
on	O
the	O
aforementioned	O
tasks	O
.	O

The	O
CLOTH	B-DatasetName
(	O
Xie	O
et	O
al	O
.	O
,	O

The	O
ReCAM	B-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021	O
)	O
dataset	O
puts	O
a	O
twist	O
to	O
archetypal	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
datasets	O
by	O
providing	O
answer	O
choices	O
that	O
are	O
abstract	O
in	O
some	O
form	O
and	O
which	O
are	O
not	O
available	O
in	O
the	O
passage	O
itself	O
.	O

The	O
models	O
created	O
for	O
the	O
QA	O
task	O
have	O
to	O
take	O
into	O
account	O
semantic	O
relations	O
between	O
the	O
options	O
and	O
the	O
context	O
.	B-MethodName

GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

Cloze	O
-	O
Style	O
QAThe	O
first	O
model	O
we	O
employ	O
follows	O
a	O
cloze	B-MethodName
-	I-MethodName
style	I-MethodName
question	I-MethodName
answering	I-MethodName
approach	O
,	O
in	O
which	O
we	O
use	O
various	O
pretrained	O
transformer	O
models	O
as	O
encoders	O
,	O
followed	O
by	O
a	O
decoder	O
layer	O
,	O
which	O
helps	O
us	O
to	O
select	O
the	O
correct	O
answer	O
.	O

Specifically	O
,	O
we	O
leverage	O
BERT	B-MethodName
along	O
with	O
some	O
of	O
its	O
popular	O
and	O
successful	O
variants	O
such	O
as	O
:	B-MethodName
Dis	I-MethodName
-	I-MethodName
tilBERT	I-MethodName
,	I-MethodName
ALBERT	I-MethodName
,	I-MethodName
and	I-MethodName
RoBERTa	I-MethodName
.	O

Each	O
candidate	O
option	O
is	O
first	O
tokenised	O
using	O
WordPiece	B-MethodName
tokeniser	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
mapped	O
to	O
the	O
vector	O
in	O
the	O
output	O
vocabulary	O
space	O
.	O

For	O
example	O
,	O
in	O
Word	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
the	O
word	O
"	O
dog	O
"	O
has	O
8	O
senses	O
,	O
while	O
the	O
word	O
"	O
love	O
"	O
has	O
10	O
senses	O
.	O

SentiWordNet	B-MethodName
(	O
Baccianella	O
et	O
al	O
.	O
,	O

2010	O
)	O
,	O
another	O
lexical	O
database	O
like	O
Word	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
,	O
gives	O
scores	O
based	O
on	O
the	O
how	O
positive	O
,	O
negative	O
or	O
objective	O
they	O
are	O
.	O

The	O
large	O
value	O
chosen	O
was	O
the	O
same	O
for	O
all	O
features	O
which	O
are	O
indirectly	O
proportional	O
to	O
concreteness	O
.	O

Towards	O
improving	O
the	O
trained	O
model	O
,	O
we	O
use	O
a	O
method	O
which	O
we	O
term	O
as	O
the	O
Difference	B-MethodName
Method	I-MethodName
.	O

Furthermore	O
,	O
we	O
use	O
a	O
Threshold	B-MethodName
Method	I-MethodName
towards	O
improving	O
the	O
model	O
performance	O
.	O

For	O
each	O
noun	O
,	O
we	O
use	O
the	O
Lesk	B-MethodName
algorithm	I-MethodName
(	O
Lesk	O
,	O
1986	O
)	O
to	O
find	O
the	O
most	O
appropriate	O
sense	O
of	O
the	O
word	O
based	O
on	O
its	O
context	O
.	O

Furthermore	O
,	O
we	O
randomly	O
mask	O
tokens	O
in	O
this	O
dataset	O
and	O
train	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
on	O
this	O
dataset	O
.	O

Firstly	O
,	O
it	O
serves	O
as	O
a	O
sort	O
of	O
domain	B-MethodName
adaptation	I-MethodName
,	O
and	O
secondly	O
,	O
it	O
infuses	O
a	O
sense	O
of	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
in	O
the	O
model	O
.	O

While	O
finetuning	O
BERT	B-MethodName
MLM	O
on	O
the	O
augmented	O
dataset	O
,	O
we	O
freeze	O
two	O
layers	O
,	O
due	O
to	O
time	O
and	O
computational	O
constraints	O
.	O

We	O
replace	O
the	O
normal	O
BERT	B-MethodName
Encoder	O
in	O
our	O
BERT	B-MethodName
FitB	I-MethodName
model	O
with	O
the	O
BERT	B-MethodName
Encoder	I-MethodName
fine	O
-	O
tuned	O
on	O
the	O
augmented	O
dataset	O
.	O

Hyponyms	B-MethodName
Options	I-MethodName
Method	I-MethodName
Here	O
,	O
we	O
use	O
the	O
Difference	B-MethodName
Method	I-MethodName
/	I-MethodName
Threshold	I-MethodName
Method	I-MethodName
.	O

However	O
,	O
"	O
drink	O
"	O
is	O
more	O
non	O
-	O
specific	O
than	O
"	O
beer	O
"	O
.	O

To	O
address	O
the	O
limitations	O
of	O
the	O
vanilla	O
transformer	O
-	O
based	O
models	O
,	O
we	O
attempt	O
multiple	O
modifications	O
to	O
the	O
proposed	O
baseline	O
transformer	O
models	O
,	O
specifically	O
for	O
BERT	B-MethodName
.	O

The	O
major	O
limitation	O
of	O
the	O
pretrained	O
BERT	B-MethodName
model	O
that	O
we	O
'	O
ve	O
used	O
,	O
is	O
the	O
restriction	O
on	O
the	O
length	O
of	O
the	O
tokenised	O
inputs	O
.	O

Only	O
512	B-HyperparameterValue
tokens	B-HyperparameterName
from	O
a	O
sample	O
can	O
be	O
processed	O
by	O
BERT	B-MethodName
in	O
one	O
parse	O
and	O
hence	O
,	O
some	O
articles	O
end	O
up	O
getting	O
truncated	O
and	O
context	O
is	O
lost	O
.	O

We	O
split	O
the	O
article	O
into	O
chunks	O
and	O
pair	O
each	O
chunk	O
with	O
the	O
question	O
such	O
that	O
the	O
length	O
of	O
the	O
tokenised	O
(	O
chunk	O
,	O
question	O
)	O
pair	O
is	O
512	B-HyperparameterValue
.	O

While	O
splitting	O
the	O
article	O
into	O
chunks	O
,	O
we	O
keep	O
a	O
maxoverlap	B-HyperparameterName
stride	I-HyperparameterName
of	O
128	B-HyperparameterValue
so	O
that	O
the	O
context	O
of	O
the	O
previous	O
chunk	O
is	O
not	O
lost	O
.	O

For	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Similarity	I-MethodName
)	I-MethodName
,	O
the	O
weights	O
are	O
calculated	O
as	O
:	O
weight	O
ij	O
=	O
u	O
i	O
.v	O
j	O
||u	O
i	O
||||v	O
j	O
||	O
(	O
1)where	O
u	O
i	O
is	O
the	O
embedding	O
of	O
the	O
question	O
in	O
the	O
i	O
th	O
sample	O
,	O
and	O
v	O
j	O
is	O
the	O
embedding	O
of	O
the	O
j	O
th	O
chunk	O
of	O
the	O
sample	O
's	O
article	O
.	O

To	O
find	O
the	O
embeddings	O
,	O
we	O
extract	O
the	O
[	O
CLS	O
]	O
embedding	O
from	O
a	O
pretrained	O
BERT	B-MethodName
encoder	O
.	O

We	O
call	O
the	O
method	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching).We	I-MethodName
normalise	O
the	O
computed	O
weights	O
:	O
norm	O
weight	O
ij	O
=	O
weight	O
ij	O
n	O
i	O
j=1	O
weight	O
ij	O
(	O
3)where	O
n	O
i	O
is	O
the	O
number	O
of	O
chunks	O
in	O
the	O
i	O
th	O
sample	O
.	O

Max	B-MethodName
Context	I-MethodName
This	O
method	O
is	O
a	O
slight	O
modification	O
of	O
the	O
Voting	B-MethodName
Method	I-MethodName
.	O

We	O
propose	O
a	O
few	O
modifications	O
to	O
the	O
baseline	O
,	O
namely	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

2017a	O
)	O
provided	O
by	O
the	O
organisers	O
.	O

GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
BERT	I-MethodName
We	O
use	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
on	O
top	O
of	O
BERT	B-MethodName
embeddings	O
.	O

This	O
could	O
lead	O
to	O
potential	O
improvement	O
in	O
performance	O
for	O
subtask	O
-	O
I	O
as	O
BERT	B-MethodName
embeddings	O
are	O
more	O
feature	O
-	O
rich	O
than	O
GloVe	B-MethodName
embeddings	O
.	O

Reader	O
,	O
we	O
came	O
up	O
with	O
an	O
approach	O
that	O
uses	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
across	O
two	O
-	O
BERT	B-MethodName
streams	O
.	O

The	O
first	O
stream	O
takes	O
in	O
the	O
question	O
input	O
,	O
and	O
works	O
like	O
the	O
regular	O
BERT	O
model	O
.	O

Then	O
,	O
to	O
the	O
layer	O
L	O
+	O
1	O
for	O
question	O
stream	O
,	O
Q	O
L	O
is	O
passed	O
as	O
input	O
,	O
while	O
to	O
layer	O
L	O
+	O
1	O
for	O
article	O
stream	O
,	O
GA(Q	O
L	O
,	O
A	O
L	O
)	O
is	O
passed	O
,	O
where	O
GA	O
is	O
the	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
function	O
.	O

This	O
is	O
done	O
for	O
all	O
12	O
layers	O
of	O
BERT	B-MethodName
-	I-MethodName
BASE	I-MethodName
.	O

Finally	O
,	O
on	O
this	O
model	O
,	O
two	O
types	O
of	O
heads	O
are	O
attached	O
-Selection	B-MethodName
and	I-MethodName
Pooling	I-MethodName
(	I-MethodName
similar	I-MethodName
to	I-MethodName
BERT	I-MethodName
FitB	I-MethodName
)	I-MethodName
,	O
and	O
Attention	B-MethodName
Classification	I-MethodName
(	I-MethodName
similar	I-MethodName
to	I-MethodName
GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
)	I-MethodName
.	O

Since	O
this	O
is	O
a	O
major	O
change	O
in	O
the	O
architecture	O
of	O
BERT	B-MethodName
,	O
this	O
model	O
needs	O
a	O
significant	O
amount	O
of	O
pretraining	O
.	O

Answer	O
-	O
Attention	O
Since	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
also	O
attends	O
to	O
the	O
candidate	O
answer	O
embeddings	O
,	O
we	O
also	O
attempt	O
an	O
approach	O
where	O
we	O
pass	O
the	O
options	O
to	O
the	O
BERT	B-MethodName
model	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
Lai	O
et	O
al	O
.	O
(	O

2019	O
)	O
propose	O
a	O
combination	O
of	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
and	I-MethodName
Self	I-MethodName
-	I-MethodName
Attention	I-MethodName
-Gated	I-MethodName
Self	I-MethodName
-	I-MethodName
Attention	I-MethodName
(	I-MethodName
GSA	I-MethodName
)	I-MethodName
.	O

They	O
show	O
improvements	O
on	O
smaller	O
datasets	O
compared	O
to	O
Compare	B-MethodName
-	I-MethodName
Aggregate	I-MethodName
Approaches	O
.	O

We	O
use	O
two	O
GSA	B-MethodName
layers	O
on	O
top	O
of	O
BERT	B-MethodName
Embeddings	O
,	O
and	O
use	O
the	O
same	O
decoder	O
and	O
selection	O
method	O
as	O
BERT	B-MethodName
FitB.	I-MethodName
In	O
all	O
our	O
experiments	O
,	O
we	O
use	O
the	O
PyTorch	O
implementations	O
of	O
the	O
transformers	O
-	O
based	O
models	O
provided	O
by	O
the	O
HuggingFace	O
.The	O
metric	O
for	O
all	O
the	O
3	O
subtasks	O
is	O
accuracy	B-MetricName
.	O

For	O
subtask	O
-	O
I	O
,	O
to	O
obtain	O
the	O
linguistic	O
features	O
mentioned	O
in	O
3.2	O
,	O
and	O
to	O
obtain	O
the	O
hypernyms	O
and	O
hyponyms	O
for	O
subtask	O
-	O
II	O
,	O
we	O
use	O
the	O
lexical	O
database	O
,	O
WordNet	B-DatasetName
provided	O
by	O
NLTK	O
(	O
Bird	O
and	O
Loper	O
,	O
2004	O
)	O
,	O
a	O
library	O
in	O
Python	O
.	O

The	O
training	O
and	O
the	O
evaluation	O
of	O
systems	O
was	O
on	O
Google	O
Colaboratory	O
's	O
free	O
GPU	O
(	O
NVIDIA	O
K80	O
/	O
P100	O
)	O
.	O

DistilBERT	B-MethodName
took	O
about	O
half	O
an	O
hour	O
for	O
training	O
.	O

For	O
finetuning	O
the	O
BERT	B-MethodName
FitB	I-MethodName
Hypr	I-MethodName
Aug	I-MethodName
Model	O
on	O
the	O
augmented	O
dataset	O
on	O
the	O
MLM	O
task	O
,	O
we	O
use	O
Nvidia	O
-	O
DGX	O
Station	O
with	O
the	O
following	O
specifications	O
:	O
four	O
32	O
GB	O
Tesla	O
V100	O
GPUs	O
,	O
256	O
GB	O
RAM	O
and	O
forty	O
Intel	O
Xeon	O
2.20GHz	O
processors	O
since	O
it	O
is	O
a	O
computationally	O
intensive	O
task	O
.	O

For	O
all	O
our	O
experiments	O
,	O
we	O
use	O
Adam	B-HyperparameterName
Optimiser	I-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2017	O
)	O
and	O
Cross	B-HyperparameterName
Entropy	I-HyperparameterName
Loss	I-HyperparameterName
.	O

For	O
choosing	O
the	O
optimal	O
set	O
of	O
hyperparameters	O
,	O
we	O
run	O
a	O
Grid	B-MethodName
Search	I-MethodName
on	O
our	O
models	O
.	O

We	O
zero	O
in	O
on	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e-5	B-MetricValue
.	O

Schedulers	O
such	O
as	O
Linear	B-MetricName
Scheduler	I-MetricName
,	O
Cosine	B-MetricName
Annealing	I-MetricName
Scheduler	I-MetricName
,	O
etc	O
.	O

For	O
the	O
FitB	B-HyperparameterName
models	O
,	O
we	O
keep	O
all	O
the	O
layers	O
unfrozen	O
.	O

Additionally	O
,	O
the	O
maximum	B-HyperparameterName
input	I-HyperparameterName
length	I-HyperparameterName
is	O
kept	O
as	O
512	B-HyperparameterValue
.	O

We	O
train	O
our	O
models	O
for	O
4	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
keeping	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
2	B-HyperparameterValue
.	O

Among	O
the	O
vanilla	O
models	O
,	O
BERT	B-MethodName
FitB	I-MethodName
Large	I-MethodName
performs	O
the	O
best	O
.	O

This	O
is	O
understandable	O
when	O
it	O
comes	O
to	O
DistilBERT	B-MethodName
and	O
ALBERT	B-MethodName
,	O
since	O
these	O
models	O
are	O
pruned	O
and	O
distilled	O
for	O
faster	O
computation	O
.	O

Notably	O
,	O
DistilBERT	B-MethodName
gives	O
comparable	O
performance	O
to	O
BERT	B-MethodName
FitB	I-MethodName
Base	I-MethodName
.	O

A	O
slightly	O
surprising	O
observation	O
was	O
that	O
there	O
is	O
a	O
degradation	O
in	O
accuracy	O
on	O
using	O
RoBERTa	B-MethodName
.	O

This	O
could	O
be	O
because	O
even	O
though	O
it	O
was	O
pretrained	O
more	O
robustly	O
than	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
it	O
was	O
not	O
pretrained	O
on	O
the	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
Task	I-TaskName
,	O
and	O
hence	O
,	O
might	O
perform	O
worse	O
on	O
Textual	B-TaskName
Entailment	I-TaskName
tasks	I-TaskName
.	O

A	O
peculiar	O
observation	O
is	O
that	O
the	O
large	O
variants	O
of	O
ALBERT	B-MethodName
FitB	I-MethodName
and	O
RoBERTa	B-MethodName
FitB	I-MethodName
models	O
perform	O
worse	O
than	O
their	O
base	O
variants	O
.	O

For	O
subtask	O
-	O
I	O
,	O
in	O
table	O
2	O
,	O
we	O
also	O
demonstrate	O
the	O
results	O
of	O
BERT	B-MethodName
Ensemble	I-MethodName
,	O
in	O
which	O
we	O
ensemble	O
(	O
i.e.	O
,	O
averaging	O
over	O
the	O
predictions	O
)	O
two	O
checkpoints	O
saved	O
during	O
the	O
training	O
process	O
.	O

When	O
it	O
comes	O
to	O
the	O
Difference	B-MethodName
Method	I-MethodName
using	I-MethodName
Linguistic	I-MethodName
Features	I-MethodName
for	O
imperceptibility	B-TaskName
,	O
we	O
observe	O
an	O
improvement	O
on	O
the	O
dev	O
set	O
,	O
but	O
a	O
slight	O
fall	O
is	O
observed	O
while	O
evaluating	O
it	O
on	O
the	O
test	O
set	O
.	O

No	O
new	O
date	O
has	O
been	O
set	O
,	O
but	O
the	O
statement	O
said	O
that	O
"	O
President	O
Michel	O
Martelly	O
,	O
in	O
his	O
constant	O
concern	O
to	O
guarantee	O
political	O
stability	O
,	O
promises	O
to	O
pursue	O
consultations	O
with	O
the	O
different	O
sectors	O
of	O
national	O
life	O
in	O
order	O
to	O
hold	O
the	O
elections	O
as	O
soon	O
as	O
possible	O
"	O
.	O

2013).For	O
non	O
-	O
specificity	O
,	O
with	O
the	O
hypernym	B-MethodName
augmentation	I-MethodName
method	I-MethodName
,	O
BERT	B-MethodName
FitB	I-MethodName
achieves	O
lower	O
accuracy	O
.	O

A	O
possible	O
reason	O
for	O
this	O
could	O
be	O
that	O
replacing	O
the	O
nouns	O
with	O
their	O
hypernyms	O
in	O
some	O
contexts	O
changes	O
the	O
meaning	O
of	O
the	O
sentence	O
(	O
even	O
though	O
we	O
use	B-MethodName
Lesk	I-MethodName
Algorithm	I-MethodName
for	O
WSD	O
,	O
not	O
all	O
hypernyms	O
make	O
sense	O
)	O
.	O

For	O
the	O
hyponyms	B-MethodName
method	I-MethodName
,	O
we	O
can	O
improve	O
our	O
results	O
by	O
recursively	O
generating	O
hyponyms	O
for	O
a	O
particular	O
option	O
,	O
instead	O
of	O
taking	O
the	O
immediate	O
hyponyms	O
.	O

In	O
Table	O
3	O
,	O
a	O
positive	O
sign	O
for	O
the	O
Difference	B-MethodName
Method	I-MethodName
or	O
the	O
Threshold	B-MethodName
Method	I-MethodName
is	O
the	O
improvement	O
in	O
the	O
results	O
of	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching	I-MethodName
)	I-MethodName
when	O
we	O
consider	O
the	O
hyponyms	O
.	O

The	O
accuracy	B-MetricName
jumps	O
from	O
72.86	B-MetricValue
%	I-MetricValue
to	I-MetricValue
75.79	I-MetricValue
%	I-MetricValue
on	O
the	O
dev	O
set	O
and	O
from	O
77.83	B-MetricValue
%	I-MetricValue
to	I-MetricValue
78.98	I-MetricValue
%	I-MetricValue
on	O
the	O
test	O
set	O
.	O

BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
performs	O
better	O
than	O
vanilla	B-MethodName
BERT	I-MethodName
FitB	I-MethodName
on	O
both	O
subtasks	O
.	O

This	O
is	O
intuitive	O
since	O
in	O
the	O
latter	O
,	O
we	O
truncate	O
the	O
article	O
to	O
512	B-HyperparameterValue
tokens	I-HyperparameterValue
without	O
any	O
consideration	O
of	O
how	O
much	O
context	O
is	O
lost	O
.	O

For	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
when	O
compared	O
with	O
the	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
baseline	O
,	O
the	B-MetricName
accuracy	I-MetricName
improves	O
from	O
21	B-MetricValue
%	I-MetricValue
to	O
39	B-MetricValue
%	I-MetricValue
on	O
subtask	O
-	O
I	O
dev	O
set	O
.	O

Due	O
to	O
computational	O
restrictions	O
,	O
we	O
could	O
n't	O
pretrain	O
GA	O
-	O
BERT	O
,	O
and	O
only	O
fine	O
-	O
tuned	O
it	O
for	O
subtask	O
-	O
I	O
to	O
get	O
an	O
idea	O
about	O
its	O
performance	O
,	O
which	O
was	O
sub	O
-	O
optimal	O
(	B-MetricValue
19	I-MetricValue
%	I-MetricValue
)	O
.	O

The	O
Answer	B-MethodName
-	I-MethodName
Attention	I-MethodName
system	I-MethodName
gave	O
us	O
a	O
dev	O
score	O
of	O
≈61	B-MetricName
%	I-MetricName
on	O
subtask	O
-	O
I	O
,	O
which	O
is	O
much	O
higher	O
than	O
the	O
baseline	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
achieves	O
≈31	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
on	O
subtask	O
-	O
I	O
dev	O
set	O
.	O

We	O
see	O
improvement	O
as	O
we	O
reduced	O
number	O
of	O
layers	O
to	O
1(≈38	B-MetricValue
%	I-MetricValue
)	O
and	O
to	O
0(≈73	B-MetricValue
%	I-MetricValue
)	O
.	O

It	O
cited	O
research	O
which	O
suggested	O
a	O
20	O
%	O
tax	O
would	O
save	O
just	O
four	O
calories	O
per	O
day	O
.	O

Professor	O
Capewell	O
will	O
cite	O
Mexico	O
as	O
one	O
example	O
where	O
a	O
10	O
%	O
sugary	O
drinks	O
tax	O
is	O
believed	O
to	O
have	O
contributed	O
to	O
a	O
10	O
%	O
reduction	O
in	O
the	O
consumption	O
of	O
such	O
beverages	O
while	O
Finland	O
,	O
France	O
,	O
Hungary	O
,	O
Latvia	O
and	O
the	O
USA	O
have	O
also	O
introduced	O
sugar	O
taxes	O
.	O

2017	O
)	O
.	O

2020	O
)	O
to	O
compute	O
the	O
word	O
-	O
wise	O
attribution	O
scores	O
for	O
BERT	B-MethodName
FitB	I-MethodName
for	O
both	O
subtasks	O
.	O

We	O
compute	O
the	O
Integrated	B-MethodName
Gradients	I-MethodName
of	O
the	O
target	O
with	O
respect	O
to	O
the	O
embedding	O
outputs	O
.	O

The	O
Riemann	B-MethodName
Right	I-MethodName
Approximation	I-MethodName
Method	I-MethodName
with	O
n	B-HyperparameterName
steps	I-HyperparameterName
=	O
25	B-HyperparameterValue
is	O
used	O
.	O

After	O
obtaining	O
the	O
token	O
-	O
wise	O
attribution	O
scores	O
,	O
we	O
obtain	O
the	O
word	O
-	O
wise	O
attribution	O
scores	O
by	O
using	O
token	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
word	I-MethodName
offset	I-MethodName
mapping	I-MethodName
.	O

The	O
word	O
"	O
legislative	O
"	O
is	O
,	O
in	O
a	O
sense	O
,	O
more	O
imperceptible	O
than	O
any	O
of	O
the	O
words	O
mentioned	O
above	O
.	O

These	O
words	O
are	O
related	O
to	O
"	O
legislative	O
"	O
which	O
exhibits	O
the	O
fact	O
that	O
BERT	B-MethodName
FitB	I-MethodName
is	O
not	O
only	O
able	O
to	O
learn	O
the	O
concept	O
of	O
imperceptibility	B-TaskName
,	O
but	O
is	O
also	O
able	O
to	O
predict	O
a	O
suitable	O
word	O
.	O

Note	O
that	O
"	O
snacks	O
"	O
is	O
also	O
an	O
option	O
;	O
however	O
,	O
food	O
is	O
more	O
non	B-TaskName
-	I-TaskName
specific	I-TaskName
than	O
"	O
snacks	O
"	O
and	O
hence	O
,	O
food	O
is	O
the	O
correct	O
option	O
.	O

We	O
reckon	O
that	O
with	O
more	O
careful	O
tuning	O
of	O
parameters	O
such	O
as	O
the	O
threshold	O
in	O
the	O
Difference	B-MethodName
Method	I-MethodName
,	O
we	O
will	O
be	O
able	O
to	O
achieve	O
these	O
gains	O
on	O
the	O
test	O
set	O
.	O

We	O
further	O
interpreted	O
the	O
outputs	O
of	O
transformers	O
-	O
based	O
models	O
using	O
Integrated	B-MethodName
Gradients	I-MethodName
,	O
and	O
demonstrated	O
that	O
transformer	O
models	O
are	O
able	O
to	O
learn	O
the	O
concepts	O
of	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

This	O
motivates	O
the	O
study	O
of	O
claim	B-TaskName
provenance	I-TaskName
,	O
which	O
seeks	O
to	O
trace	O
and	O
explain	O
the	O
origins	O
of	O
claims	O
.	O

This	O
establishes	O
relevant	O
search	O
queries	O
,	O
and	O
it	O
allows	O
us	O
to	O
obtain	O
source	O
article	O
candidates	O
for	O
each	O
identified	O
sentence	O
and	O
propose	O
an	O
ILP	B-MethodName
based	I-MethodName
algorithm	I-MethodName
to	O
infer	O
the	O
best	O
sources	O
.	O

We	O
experiment	O
with	O
a	O
newly	O
created	O
evaluation	O
dataset	O
1	O
,	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
,	O
based	O
on	O
fact	O
-	O
checking	O
articles	O
from	O
www.politifa	O
ct.com	O
;	O
our	O
experimental	O
results	O
show	O
that	O
our	O
solution	O
leads	O
to	O
a	O
significant	O
improvement	O
over	O
baselines	O
.	O

Misinformation	O
is	O
on	O
the	O
rise	O
,	O
and	O
people	O
are	O
fighting	O
it	O
with	O
fact	O
checking	O
.	O

2020	O
)	O
focuses	O
on	O
automating	O
factchecking	O
for	O
a	O
single	O
claim	O
.	O

In	O
reality	O
,	O
a	O
claim	O
can	O
be	O
complex	O
,	O
and	O
proposed	O
as	O
a	O
conclusion	O
of	O
an	O
article	O
.	O

This	O
motivates	O
the	O
study	O
of	O
provenance	B-TaskName
for	I-TaskName
natural	I-TaskName
language	I-TaskName
claims	I-TaskName
,	O
which	O
describes	O
where	O
a	O
specific	O
claim	O
may	O
have	O
come	O
from	O
and	O
how	O
it	O
has	O
spread	O
.	O

Early	O
work	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

However	O
,	O
that	O
model	O
is	O
insufficient	O
to	O
capture	O
the	O
provenance	O
of	O
an	O
article	O
,	O
because	O
(	O
1	O
)	O
an	O
article	O
consists	O
of	O
multiple	O
claims	O
,	O
and	O
it	O
leverages	O
information	O
from	O
other	O
sources	O
,	O
therefore	O
the	O
provenance	O
of	O
all	O
claims	O
should	O
be	O
included	O
in	O
the	O
article	O
's	O
provenance	O
;	O
(	O
2	O
)	O
the	O
inference	O
solution	O
they	O
proposed	O
can	O
only	O
extract	O
domain	O
-	O
level	O
provenance	O
information	O
,	O
e.g.	O
,	O
cbsnews.com	O
,	O
while	O
it	O
can	O
not	O
directly	O
link	O
the	O
claim	O
to	O
its	O
source	O
article	O
,	O
e.g.	O
,	O
https://www.cbsnews.com/news/preventingcoronavirus-facemask-60-minutes-2020-03-08/.	O
Such	O
fine	O
-	O
grained	O
provenance	O
information	O
is	O
important	O
because	O
it	O
can	O
help	O
people	O
understand	O
the	O
original	O
context	O
that	O
influenced	O
the	O
information	O
they	O
read	O
.	O

Technically	O
,	O
capturing	O
fine	O
-	O
grained	O
provenance	O
for	O
an	O
article	O
is	O
challenging	O
because	O
(	O
1	O
)	O
there	O
may	O
be	O
large	O
numbers	O
of	O
sentences	O
in	O
an	O
article	O
,	O
and	O
not	O
all	O
are	O
from	O
external	O
sources	O
nor	O
important	O
(	O
thus	O
,	O
their	O
provenance	O
may	O
not	O
be	O
worth	O
considering);(2	O
)	O
a	O
sentence	O
in	O
an	O
article	O
is	O
usually	O
just	O
a	O
textual	O
fragment	O
of	O
its	O
source	O
article	O
,	O
and	O
simply	O
looking	O
for	O
other	O
articles	O
with	O
related	O
content	O
may	O
result	O
in	O
low	O
precision	B-MetricName
with	O
regards	O
to	O
finding	O
the	O
correct	O
original	O
article	O
.	O

The	O
key	O
contributions	O
of	O
this	O
paper	O
are	O
(	O
1	O
)	O
we	O
introduce	O
and	O
formalize	O
the	O
problem	O
of	O
inferring	B-TaskName
finegrained	I-TaskName
provenance	I-TaskName
for	O
an	O
article	O
;	O
(	O
2	O
)	O
we	O
propose	O
a	O
general	O
framework	O
to	O
infer	O
the	O
source	O
articles	O
that	O
have	O
provided	O
important	O
information	O
for	O
the	O
given	O
article	O
,	O
including	O
(	O
a	O
)	O
a	O
ranking	O
module	O
that	O
can	O
identify	O
sentences	O
that	O
contain	O
important	O
external	O
information	O
based	O
on	O
the	O
main	O
topic	O
and	O
the	O
main	O
entities	O
in	O
the	O
article	O
;	O
(	O
b	O
)	O
a	O
query	O
generator	O
that	O
can	O
generate	O
possible	O
metadata	O
for	O
the	O
source	O
article	O
,	O
e.g.	O
,	O
the	O
title	O
,	O
the	O
published	O
date	O
,	O
the	O
source	O
website	O
,	O
based	O
on	O
the	O
context	O
of	O
the	O
selected	O
sentences	O
;	O
(	O
c	O
)	O
an	O
integer	O
linear	O
program	O
(	O
ILP	O
)	O
based	O
algorithm	O
to	O
jointly	O
identify	O
the	O
source	O
articles	O
from	O
all	O
of	O
the	O
candidates	O
.	O
(	O

3	O
)	O
to	O
evaluate	O
our	O
solutions	O
,	O
we	O
collect	O
a	O
new	O
dataset	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
from	O
politifact.com	O
,	O
and	O
our	O
experimental	O
results	O
show	O
that	O
the	O
solution	O
we	O
proposed	O
can	O
lead	O
to	O
a	O
significant	O
improvement	O
compared	O
with	O
baselines	O
.	O

Given	O
an	O
article	O
d	O
,	O
we	O
are	O
to	O
capture	O
its	O
finegrained	B-TaskName
provenance	I-TaskName
,	O
by	O
inferring	O
k	O
source	O
articles	O
SA	O
k	O
(	O
d	O
)	O
that	O
provide	O
the	O
most	O
important	O
information	O
for	O
d.	O
We	O
adopt	O
the	O
notion	O
of	O
provenance	O
from	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Then	O
we	O
will	O
choose	O
top	O
-	O
k	O
sentences	O
based	O
on	O
their	O
score	O
,	O
and	O
try	O
to	O
find	O
source	O
articles	O
for	O
those	O
sentences	O
.	O

As	O
we	O
have	O
discussed	O
in	O
Section	O
1	O
,	O
directly	O
searching	O
the	O
identified	O
sentence	O
on	O
a	O
search	O
engine	O
may	O
result	O
in	O
a	O
low	O
precision	B-MetricName
of	O
finding	O
the	O
correct	O
source	O
article	O
.	O

Figure	O
2	O
depicts	O
the	O
three	O
steps	O
we	O
need	O
to	O
conduct	O
to	O
infer	O
the	O
fine	B-TaskName
-	I-TaskName
grained	I-TaskName
provenance	I-TaskName
,	O
which	O
correspond	O
to	O
the	O
three	O
subproblems	O
listed	O
above	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
is	O
no	O
existing	O
dataset	O
that	O
can	O
support	O
inferring	O
fine	O
-	O
grained	O
provenance	O
for	O
an	O
article	O
,	O
therefore	O
we	O
create	O
a	O
new	O
dataset	O
based	O
on	O
the	O
fact	O
-	O
checks	O
from	O
politifact	O
.com	O
to	O
support	O
the	O
training	O
and	O
the	O
evaluation	O
of	O
this	O
problem	O
.	O

We	O
want	O
to	O
note	O
it	O
is	O
possible	O
that	O
there	O
may	O
be	O
some	O
sources	O
missing	O
in	O
the	O
ground	O
truth	O
we	O
can	O
obtain	O
,	O
therefore	O
,	O
we	O
focus	O
more	O
on	O
the	O
recall	O
in	O
the	O
evaluation	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
elaborate	O
how	O
we	O
solve	O
the	O
problems	O
proposed	O
in	O
Section	O
2	O
.	O

We	O
build	O
our	O
model	O
by	O
leveraging	O
Roberta	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

Using	O
the	O
same	O
notation	O
in	O
the	O
paper	O
,	O
we	O
concatenate	O
t	O
d	O
and	O
each	O
e	O
∈	O
E	O
d	O
,	O
feeding	O
it	O
to	O
the	O
model	O
as	O
sentence	O
A	O
,	O
and	O
s	O
∈	O
P	O
(	O
d	O
)	O
or	O
N	O
(	O
d	O
)	O
as	O
sentence	O
B	O
,	O
as	O
the	O
input	O
of	O
Roberta	B-MethodName
.	O

We	O
then	O
use	O
Roberta	B-MethodName
as	O
a	O
binary	O
classification	O
model	O
,	O
that	O
is	O
,	O
we	O
use	O
its	O
[	O
CLS	O
]	O
vector	O
as	O
input	O
to	O
a	O
two	B-HyperparameterValue
layer	B-HyperparameterName
neural	O
network	O
to	O
obtain	O
the	O
probability	O
of	O
s	O
referring	O
to	O
important	O
external	O
information	O
.	O

We	O
start	O
training	O
from	O
a	O
pre	O
-	O
trained	O
Roberta	B-MethodName
model	O
and	O
fine	O
-	O
tune	O
it	O
to	O
our	O
ranking	O
task	O
using	O
the	O
following	O
loss	O
,	O
given	O
s	O
i	O
∈	O
P	O
(	O
d	O
)	O
and	O
s	O
j	O
∈	O
N	O
(	O
d):Li	O
,	O
j	O
=	O
−	O
log	O
σi	O
−	O
log	O
(	O
1	O
−	O
σj	O
)	O
+	O
max	O
0	O
,	O
τ	O
(	O
sj	O
)	O
−	O
τ	O
(	O
si	O
)	O
+	O
(	O
1)where	O
τ	O
(	O
s	O
i	O
)	O
and	O
τ	O
(	O
s	O
j	O
)	O
are	O
the	O
representations	O
,	O
obtained	O
by	O
the	O
output	O
of	O
a	O
single	B-HyperparameterValue
layer	B-HyperparameterName
neural	O
network	O
τ	O
on	O
top	O
of	O
the	O
[	O
CLS	O
]	O
vector	O
of	O
Roberta	B-MethodName
.	O

The	O
next	O
step	O
is	O
to	O
find	O
candidate	O
articles	O
that	O
can	O
be	O
the	O
source	O
articles	O
based	O
on	O
the	O
identified	O
sentences	O
.	O

To	O
generate	O
a	O
query	O
that	O
can	O
improve	O
the	O
recall	B-MetricName
,	O
the	O
question	O
here	O
is	O
what	O
search	O
keywords	O
are	O
good	O
for	O
finding	O
the	O
source	O
articles	O
besides	O
the	O
identified	O
sentences	O
themselves	O
?	O
In	O
this	O
work	O
,	O
we	O
argue	O
that	O
the	O
metadata	O
of	O
the	O
target	O
article	O
,	O
including	O
its	O
source	O
domain	O
,	O
title	O
and	O
published	O
date	O
is	O
a	O
good	O
choice	O
.	O

As	O
a	O
baseline	O
,	O
we	O
train	O
this	O
model	O
via	O
fine	O
-	O
tuning	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
a	O
pretrained	O
text	O
generation	O
model	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
extend	O
the	O
BART	B-MethodName
baseline	O
to	O
incorporate	O
two	O
sources	O
of	O
inputs	O
,	O
by	O
first	O
feeding	O
the	O
text	O
inputs	O
independently	O
to	O
the	O
BART	B-MethodName
's	O
encoders	O
,	O
then	O
concatenating	O
the	O
outputs	O
of	O
the	O
encoders	O
together	O
,	O
and	O
finally	O
feeding	O
the	O
unified	O
representations	O
to	O
the	O
BART	B-MethodName
's	O
decoder	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
rank	O
-	O
aware	O
multi	O
-	O
head	O
cross	O
-	O
attention	O
to	O
relieve	O
this	O
problem	O
.	O

The	O
basic	O
idea	O
is	O
when	O
BART	B-MethodName
's	O
decoders	O
are	O
performing	O
cross	O
-	O
attention	O
over	O
the	O
text	O
input	O
of	O
the	O
sentences	O
and	O
the	O
possible	O
metadata	O
,	O
we	O
require	O
that	O
each	O
set	O
of	O
attention	O
heads	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

Note	O
that	O
the	O
candidate	O
metadata	O
from	O
the	O
urls	O
ranked	O
higher	O
will	O
always	O
receive	O
more	O
attention	O
than	O
the	O
others	O
in	O
this	O
case	O
.	O

The	O
model	O
extends	O
(	O
1	O
)	O
BART	B-MethodName
's	O
encoders	O
to	O
incorporate	O
two	O
types	O
of	O
input	O
,	O
one	O
is	O
the	O
context	O
of	O
the	O
selected	O
sentence	O
,	O
and	O
the	O
other	O
one	O
is	O
possible	O
metadata	O
collected	O
from	O
a	O
search	O
engine	O
,	O
(	O
2	O
)	O
BART	B-MethodName
's	O
decoders	O
with	O
a	O
rankaware	B-HyperparameterValue
multi	B-HyperparameterName
-	I-HyperparameterName
head	I-HyperparameterName
cross	I-HyperparameterName
attention	I-HyperparameterName
to	O
generate	O
the	O
gold	O
metadata	O
.	O

Based	O
on	O
our	O
observations	O
,	O
the	O
author	O
is	O
very	O
likely	O
to	O
leverage	O
the	O
external	O
information	O
coming	O
from	O
the	O
same	O
source	O
websites	O
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
the	O
last	O
hidden	O
layer	O
of	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Setup	O
We	O
use	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
dataset	O
introduced	O
in	O
Section	O
3	O
.	O

To	O
compare	O
the	O
performance	O
,	O
we	O
implement	O
our	O
solution	O
(	O
SR	B-MethodName
-	I-MethodName
TE	I-MethodName
)	O
as	O
described	O
in	O
Section	O
4.1	O
,	O
and	O
compare	O
it	O
with	O
(	O
1	O
)	O
a	O
retrieval	O
baseline	O
that	O
simply	O
computes	O
the	O
cosine	O
similarity	O
between	O
the	O
embedding	O
vectors	O
(	O
using	O
Roberta	B-MethodName
)	O
of	O
the	O
title	O
and	O
the	O
sentence	O
in	O
the	O
article	O
(	O
SR	B-MethodName
)	O
.	O

This	O
retrieval	O
baseline	O
only	O
captures	O
the	O
relatedness	O
between	O
the	O
sentence	O
and	O
the	O
main	O
topic	O
of	O
the	O
article;(2	O
)	O
a	O
retrieval	O
baseline	O
similar	O
to	O
SR	B-MethodName
,	O
but	O
computing	O
the	O
cosine	O
similarity	O
between	O
the	O
embedding	O
vectors	O
of	O
the	O
concatenation	O
of	O
the	O
title	O
and	O
the	O
most	O
important	O
entities	O
(	O
top-50	O
)	O
and	O
the	O
sentence	O
in	O
the	O
article	O
(	O
SR	B-MethodName
-	I-MethodName
E	I-MethodName
)	O
,	O
where	O
we	O
want	O
to	O
show	O
the	O
effect	O
of	O
considering	O
important	O
entities	O
;	O
(	O
3	O
)	O
our	O
learning	O
solution	O
without	O
considering	O
entities	O
(	O
SR	B-MethodName
-	I-MethodName
T	I-MethodName
)	O
.	O

We	O
report	O
the	O
mean	B-MetricName
precision	I-MetricName
and	O
recall	B-MetricName
of	I-MetricName
the	I-MetricName
top	I-MetricName
-	I-MetricName
k	I-MetricName
results	I-MetricName
respectively	O
.	O

The	O
gaps	O
between	O
SR	B-MethodName
,	O
SR	B-MethodName
-	I-MethodName
E	I-MethodName
,	O
and	O
SR	B-MethodName
-	I-MethodName
T	I-MethodName
,	O
SR	B-MethodName
-	I-MethodName
TE	I-MethodName
show	O
that	O
considering	O
important	O
entities	O
always	O
results	O
in	O
an	O
improvement	O
on	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
which	O
reveals	O
that	O
the	O
sentences	O
can	O
not	O
be	O
identified	O
based	O
on	O
their	O
relatedness	O
to	O
the	O
title	O
(	O
the	O
main	O
topic	O
)	O
only	O
,	O
but	O
also	O
requires	O
other	O
important	O
information	O
in	O
the	O
article	O
.	O

Setup	O
We	O
collect	O
all	O
of	O
the	O
sentences	O
that	O
correspond	O
to	O
the	O
source	O
articles	O
in	O
training	O
,	O
validation	O
and	O
test	O
set	O
of	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
serving	O
as	O
training	O
,	O
validation	O
and	O
testing	O
respectively	O
.	O

To	O
evaluate	O
the	O
performance	O
,	O
we	O
report	O
Rouge	B-MetricName
1	I-MetricName
,	O
Rouge	B-MetricName
2	I-MetricName
and	O
Rouge	B-MetricName
L	I-MetricName
score	O
of	O
the	O
text	O
generated	O
,	O
and	O
compare	O
with	O