Lifelong	B-TaskName
learning	I-TaskName
(	I-TaskName
LL	I-TaskName
)	I-TaskName
aims	O
to	O
train	O
a	O
neural	O
network	O
on	O
a	O
stream	O
of	O
tasks	O
while	O
retaining	O
knowledge	O
from	O
previous	O
tasks	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
LL	B-TaskName
framework	O
for	O
language	O
models	O
.	O

In	O
order	O
to	O
alleviate	O
catastrophic	O
forgetting	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
enhances	O
LAMOL	B-MethodName
,	O
a	O
recent	O
LL	B-TaskName
model	O
,	O
by	O
applying	O
critical	O
freezing	O
guided	O
by	O
human	O
rationales	O
.	O

In	O
the	O
experiment	O
,	O
we	O
tested	O
Rational	B-MethodName
LAMOL	I-MethodName
on	O
permutations	O
of	O
three	O
datasets	O
from	O
the	O
ERASER	O
benchmark	O
.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
framework	O
outperformed	O
vanilla	O
LAMOL	B-MethodName
on	O
most	O
permutations	O
.	O

Furthermore	O
,	O
unsupervised	O
rationale	O
generation	O
was	O
able	O
to	O
consistently	O
improve	O
the	O
overall	O
LL	B-TaskName
performance	O
from	O
the	O
baseline	O
without	O
relying	O
on	O
human	O
-	O
annotated	O
rationales	O
.	O

The	O
grounds	O
of	O
lifelong	B-TaskName
learning	I-TaskName
(	O
LL	B-TaskName
)	O
stem	O
from	O
the	O
ability	O
of	O
humans	O
to	O
continually	O
acquire	O
,	O
consolidate	O
,	O
and	O
transfer	O
knowledge	O
and	O
skills	O
throughout	O
their	O
lifespan	O
.	O

We	O
focus	O
on	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
(	O
LLL	B-TaskName
)	O
,	O
which	O
is	O
lifelong	B-TaskName
learning	I-TaskName
on	O
a	O
stream	O
of	O
NLP	O
tasks	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
the	O
grounds	O
of	O
LLL	B-TaskName
are	O
left	O
largely	O
underexplored	O
.	O

LAMOL	B-MethodName
is	O
an	O
LLL	B-TaskName
general	O
framework	O
that	O
has	O
garnered	O
recent	O
interest	O
due	O
to	O
its	O
simplicity	O
(	O
Sun	O
et	O
al	O
.	O
,	O

In	O
particular	O
,	O
LAMOL	B-MethodName
transforms	O
all	O
NLP	O
tasks	O
into	O
the	O
question	O
answering	O
(	O
QA	O
)	O
format	O
according	O
to	O
McCann	O
et	O
al	O
.	O
(	O

However	O
,	O
there	O
is	O
still	O
a	O
gap	O
between	O
the	O
performance	O
of	O
LAMOL	B-MethodName
and	O
the	O
result	O
of	O
multi	B-TaskName
-	I-TaskName
task	I-TaskName
learning	I-TaskName
which	O
is	O
generally	O
considered	O
as	O
the	O
upper	O
bound	O
of	O
LLL	B-TaskName
performance	O
.	O

In	O
this	O
paper	O
,	O
we	O
improve	O
existing	O
LLL	B-TaskName
strategies	O
by	O
proposing	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
rationale	O
-	O
based	O
lifelong	O
learning	O
framework	O
which	O
equips	O
the	O
original	O
LAMOL	B-MethodName
with	O
critical	O
freezing	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O

Particularly	O
,	O
we	O
devise	O
an	O
algorithm	O
to	O
identify	O
critical	O
components	O
in	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
language	I-MethodName
models	I-MethodName
using	O
rationales	O
,	O
and	O
the	O
selected	O
compo	O
-	O
nents	O
will	O
be	O
frozen	O
to	O
maintain	O
learned	O
knowledge	O
while	O
being	O
trained	O
on	O
a	O
new	O
task	O
.	O

The	O
contributions	O
of	O
our	O
paper	O
are	O
listed	O
below:•	O
We	O
demonstrate	O
the	O
importance	O
of	O
freezing	O
plastic	O
components	O
(	O
i.e.	O
,	O
components	O
that	O
are	O
most	O
susceptible	O
to	O
change	O
)	O
in	O
transformerbased	B-MethodName
models	I-MethodName
to	O
strengthen	O
memories	O
of	O
the	O
previously	O
learned	O
tasks	O
in	O
the	O
LLL	B-TaskName
setting.•	O
We	O
propose	O
critical	O
component	O
identification	O
algorithm	O
which	O
analyzes	O
the	O
transformerbased	B-MethodName
LLL	I-MethodName
model	I-MethodName
with	O
rationales	O
so	O
as	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
freeze	O
.	O

2020	O
)	O
but	O
we	O
adapted	O
it	O
to	O
NLP.•	O
We	O
propose	O
that	O
unsupervised	O
generated	O
rationales	O
by	O
InvRat	B-MethodName
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
can	O
be	O
effectively	O
used	O
as	O
substitutions	O
of	O
human	O
rationales	O
,	O
allowing	O
our	O
framework	O
to	O
be	O
applied	O
to	O
generic	O
NLP	O
datasets	O
.	O

We	O
evaluated	O
Rational	B-MethodName
LAMOL	I-MethodName
on	O
six	O
task	O
order	O
permutations	O
of	O
three	O
datasets	O
from	O
the	O
ERASER	O
benchmark	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
results	O
show	O
that	O
our	O
proposed	O
framework	O
outperformed	O
the	O
original	O
LAMOL	B-MethodName
on	O
five	O
out	O
of	O
the	O
six	O
permutations	O
,	O
achieving	O
average	O
improvements	O
of	O
1.83	B-MetricValue
%	I-MetricValue
with	O
a	O
lower	O
standard	O
deviation	O
of	O
4.57	B-MetricValue
%	I-MetricValue
.	O

Moreover	O
,	O
using	O
unsupervised	O
rationale	O
generation	O
instead	O
of	O
human	O
rationales	O
also	O
yielded	O
competitive	O
performance	O
,	O
achieving	O
average	O
improvements	O
of	O
2.67	B-MetricValue
%	I-MetricValue
from	O
original	O
LAMOL	B-MethodName
.	O

In	O
this	O
section	O
,	O
we	O
briefly	O
introduce	O
the	O
concept	O
of	O
lifelong	B-TaskName
learning	I-TaskName
,	O
catastrophic	O
forgetting	O
,	O
and	O
component	O
freezing	O
which	O
are	O
relevant	O
to	O
the	O
core	O
idea	O
of	O
Rational	B-MethodName
LAMOL	I-MethodName
.	O

We	O
also	O
briefly	O
summarize	O
prominent	O
researches	O
related	O
to	O
rationales	O
.	O

While	O
people	O
fine	O
tune	O
a	O
pre	O
-	O
trained	O
model	O
to	O
perform	O
a	O
single	O
task	O
,	O
lifelong	B-TaskName
learning	I-TaskName
(	O
LL	B-TaskName
)	O
is	O
a	O
setting	O
in	O
which	O
a	O
learner	O
performs	O
sequential	O
learning	O
of	O
infinitely	O
incoming	O
tasks	O
τ	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
...	O
,	O
τ	O
i	O
,	O
...	O
,	O
}	O
,	O
where	O
τ	O
i	O
is	O
the	O
i	O
-	O
th	O
task	O
to	O
learn	O
at	O
a	O
particular	O
point	O
in	O
time	O
.	O

The	O
objective	O
of	O
the	O
LL	B-TaskName
learner	O
is	O
to	O
ideally	O
both	O
optimize	O
the	O
performance	O
on	O
the	O
new	O
task	O
and	O
maintain	O
optimal	O
performance	O
on	O
previous	O
tasks	O
τ	O
t	O
for	O
t	O
=	O
0	O
,	O
1	O
,	O
...	O
,	O
i.	O
Moreover	O
,	O
the	O
ability	O
to	O
transfer	O
knowledge	O
across	O
different	O
tasks	O
is	O
also	O
desired	O
.	O

There	O
are	O
multiple	O
existing	O
works	O
that	O
aim	O
to	O
mitigate	O
catastrophic	O
forgetting	O
in	O
LL	B-TaskName
.	O

2016;.Lifelong	B-TaskName
Language	I-TaskName
Learning	I-TaskName
or	O
LLL	B-TaskName
is	O
a	O
scenario	O
where	O
a	O
model	O
sequentially	O
learns	O
from	O
a	O
stream	O
of	O
NLP	O
tasks	O
in	O
an	O
LL	B-TaskName
manner	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
LLL	B-TaskName
has	O
rarely	O
been	O
studied	O
and	O
previous	O
works	O
usually	O
target	O
a	O
single	O
type	O
of	O
NLP	O
tasks	O
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
proposed	O
LAMOL	B-MethodName
,	O
a	O
learning	O
framework	O
that	O
utilizes	O
a	O
language	O
model	O
to	O
simultaneously	O
predict	O
outputs	O
and	O
learn	O
to	O
generate	O
pseudo	O
-	O
training	O
examples	O
,	O
which	O
are	O
exploited	O
to	O
alleviate	O
catastrophic	O
forgetting	O
.	O

Hence	O
,	O
LAMOL	B-MethodName
,	O
as	O
well	O
as	O
our	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
naturally	O
falls	O
into	O
the	O
data	O
-	O
based	O
LL	B-TaskName
approach	O
since	O
data	O
from	O
previous	O
tasks	O
,	O
albeit	O
generated	O
,	O
is	O
utilized	O
to	O
constrain	O
a	O
model	O
.	O

Component	O
Freezing	O
While	O
component	O
freezing	O
is	O
also	O
a	O
common	O
practice	O
in	O
the	O
fine	O
-	O
tuning	O
process	O
,	O
it	O
is	O
done	O
to	O
prevent	O
loss	O
in	O
general	O
knowledge	O
in	O
lower	O
layers	O
of	O
the	O
model	O
(	O
Raganato	O
and	O
Tiedemann	O
,	O
2018).By	O
contrast	O
,	O
many	O
architecture	O
-	O
based	O
LL	B-TaskName
methods	O
,	O
for	O
example	O
Rusu	O
et	O
al	O
.	O
(	O

Our	O
Rational	B-MethodName
LAMOL	I-MethodName
also	O
uses	O
component	O
freezing	O
,	O
but	O
unlike	O
architecturebased	O
methods	O
,	O
only	O
a	O
small	O
part	O
of	O
the	O
model	O
is	O
frozen	O
and	O
its	O
size	O
is	O
constant	O
throughout	O
the	O
learning	O
process	O
.	O

Rationales	O
could	O
be	O
either	O
annotated	O
by	O
humans	O
or	O
generated	O
by	O
machine	O
learning	O
models	O
.	O

In	O
the	O
experiment	O
,	O
we	O
used	O
human	O
rationales	O
from	O
ERASER	O
in	O
the	O
critical	O
component	O
identification	O
step	O
to	O
find	O
the	O
most	O
plastic	O
component	O
to	O
be	O
frozen	O
.	O

In	O
order	O
to	O
allow	O
Rational	B-MethodName
LAMOL	I-MethodName
to	O
be	O
applied	O
to	O
any	O
NLP	O
dataset	O
,	O
we	O
choose	O
to	O
leverage	O
InvRat	O
to	O
automatically	O
produce	O
rationales	O
due	O
to	O
its	O
superior	O
performance	O
and	O
straightforward	O
application	O
,	O
removing	O
the	O
need	O
for	O
human	O
rationales	O
.	O

We	O
introduce	O
Rational	B-MethodName
LAMOL	I-MethodName
and	O
its	O
detailed	O
implementation	O
in	O
this	O
section	O
.	O

As	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
based	O
from	O
LAMOL	B-MethodName
(	O
Sun	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
briefly	O
explain	O
LAMOL	B-MethodName
in	O
Section	O
3.1	O
.	O

Then	O
we	O
introduce	O
the	O
core	O
lifelong	B-TaskName
learning	I-TaskName
framework	O
of	O
Rational	B-MethodName
LAMOL	I-MethodName
in	O
Section	O
3.2	O
.	O

Language	B-MethodName
Modeling	I-MethodName
for	I-MethodName
Lifelong	I-MethodName
Language	I-MethodName
Learning	I-MethodName
(	O
LAMOL	B-MethodName
)	O
(	O
Sun	O
et	O
al	O
.	O
,	O

In	O
addition	O
,	O
LAMOL	B-MethodName
trains	O
the	O
LM	O
as	O
a	O
generative	O
model	O
upon	O
receiving	O
a	O
special	O
generation	O
token	O
.	O

Using	O
a	O
single	O
model	O
for	O
both	O
providing	O
answers	O
and	O
generating	O
pseudo	O
-	O
samples	O
,	O
LAMOL	B-MethodName
truly	O
exhibits	O
a	O
model	O
of	O
LM	O
and	O
QA	O
duality	O
.	O

The	O
benefit	O
that	O
comes	O
with	O
the	O
generative	O
part	O
of	O
the	O
model	O
tackles	O
the	O
long	O
-	O
standing	O
issue	O
of	O
LLcatastrophic	B-TaskName
forgetting	O
.	O

2017;Kemker	O
and	O
Kanan	O
,	O
2017	O
)	O
,	O
LAMOL	B-MethodName
transfers	O
all	O
the	O
responsibilities	O
into	O
a	O
single	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
exploiting	O
rationales	O
with	O
LAMOL	B-MethodName
to	O
further	O
improve	O
the	O
LLL	B-TaskName
performance	O
,	O
discussed	O
next	O
.	O

Rational	B-MethodName
LAMOL	I-MethodName
,	O
illustrated	O
in	O
Figure	O
1	O
(	O
right	O
)	O
,	O
is	O
a	O
learning	O
framework	O
revolving	O
around	O
the	O
original	O
methodologies	O
of	O
LAMOL	B-MethodName
.	O

We	O
consider	O
an	O
LL	B-TaskName
setting	O
where	O
τ	O
=	O
{	O
τ	O
1	O
,	O
τ	O
2	O
,	O
...	O
,	O
τ	O
i	O
,	O
...	O
}	O
is	O
a	O
stream	O
of	O
learning	O
tasks	O
and	O
τ	O
i	O
is	O
the	O
i	O
-	O
th	O
task	O
to	O
train	O
at	O
a	O
particular	O
point	O
in	O
time	O
.	O

Using	O
these	O
notations	O
and	O
starting	O
from	O
M	O
0	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
works	O
iteratively	O
in	O
four	O
steps	O
as	O
follows	O
.	O

First	O
,	O
given	O
a	O
model	O
M	O
i	O
,	O
it	O
trains	O
M	O
i	O
with	O
the	O
task	O
τ	O
i+1	O
using	O
LAMOL	B-MethodName
's	O
training	O
procedure	O
to	O
obtainM	O
i+1	O
.	O

Note	O
that	O
despite	O
the	O
unique	O
nature	O
of	O
LAMOL	B-MethodName
,	O
our	O
Rational	B-MethodName
LAMOL	I-MethodName
does	O
not	O
limit	O
its	O
usage	O
to	O
a	O
single	O
model	O
architecture	O
.	O

We	O
propose	O
the	O
Critical	O
Component	O
Identification	O
(	O
CCI	O
)	O
algorithm	O
,	O
pointing	O
out	O
the	O
most	O
plastic	O
block	O
of	O
our	O
transformer	B-MethodName
-	I-MethodName
based	I-MethodName
LL	I-MethodName
model	I-MethodName
before	O
moving	O
on	O
to	O
a	O
new	O
task	O
completely	O
.	O
(	O

The	O
chosen	O
block	O
is	O
the	O
one	O
that	O
forgets	O
what	O
it	O
has	O
learned	O
from	O
the	O
recent	O
task	O
the	O
most	O
when	O
being	O
introduced	O
a	O
new	O
task	O
,	O
so	O
we	O
will	O
freeze	O
the	O
block	O
to	O
prevent	O
catastrophic	O
forgetting	O
in	O
Rational	B-MethodName
LAMOL.As	I-MethodName
shown	O
in	O
Algorithm	O
1	O
,	O
for	O
each	O
validation	O
sample	O
x	O
∈	O
X	O
of	O
task	O
i	O
,	O
the	O
CCI	O
compares	O
the	O
attention	O
maps	O
AT	O
produced	O
by	O
the	O
model	O
M	O
i	O
(	O
i.e.	O
,	O
the	O
old	O
model	O
M	O
O	O
in	O
Algorithm	O
1	O
)	O
andM	O
i+1	O
(	O
i.e.	O
,	O
the	O
new	O
model	O
M	O
N	O
in	O
Algorithm	O
1	O
)	O
to	O
find	O
the	O
most	O
plastic	O
block	O
b	O
with	O
respect	O
to	O
this	O
sample	O
.	O

Then	O
it	O
returns	O
the	O
block	O
F	O
which	O
is	O
the	O
mode	O
of	O
all	O
b	O
,	O
voted	O
by	O
most	O
of	O
the	O
samples	O
in	O
X.	O
Note	O
that	O
most	O
of	O
the	O
variable	O
names	O
are	O
preserved	O
similar	O
to	O
Nguyen	O
et	O
al	O
.	O
(	O

In	O
particular	O
,	O
to	O
find	O
b	O
for	O
the	O
sample	O
x	O
,	O
we	O
iterate	O
over	O
all	O
blocks	O
j	O
=	O
1	O
,	O
...	O
,	O
K	O
and	O
perform	O
two	O
steps	O
.	O

2020	O
)	O
,	O
elementary	O
visualization	O
of	O
attentions	O
are	O
possible	O
in	O
Transformers	B-MethodName
(	O
Vig	O
,	O
2019;Hoover	O
et	O
al	O
.	O
,	O

We	O
hypothesize	O
that	O
the	O
semantic	O
nature	O
of	O
the	O
self	O
-	O
attention	O
mechanisms	O
would	O
opt	O
for	O
tokens	O
most	O
relating	O
to	O
positive	O
evidence	O
vital	O
for	O
predictions	O
,	O
being	O
analogous	O
to	O
rationales	O
-	O
snippets	O
thatAlgorithm	O
1	O
Critical	O
Component	O
Identification	O
Input	O
:	O
Validation	O
set	O
X	O
,	O
ground	O
truth	O
rationale	O
GT	O
,	O
old	O
model	O
M	O
O	O
,	O
new	O
model	O
M	O
N	O
,	O
number	O
of	O
blocks	O
K	O
Output	O
:	O
Critical	O
block	O
F	O
Ł←	O
∅	O
for	O
all	O
validation	O
sample	O
x	O
∈	O
X	O
do	O
:	O
IoUs	O
←	O
∅	O
AT	O
O	O
,	O
AT	O
N	O
←	O
[	O
M	O
O	O
(	O
x	O
)	O
,	O
M	O
N	O
(	O
x	O
)	O
]	O
for	O
j	O
=	O
1	O
,	O
K	O
do	O
:	O
RM	O
M	O
O	O
,	O
GT	O
←	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
with	O
highest	O
IoU	O
M	O
O	O
,	O
GT	O
RM	O
M	O
N	O
,	O
M	O
O	O
←	O
AT	O
j	O
,	O
a	O
*	O
,	O
s	O
*	O
with	O
highest	O
IoU	O
M	O
N	O
,	O
M	O
O	O
APPEND(IoUs	O
,	O
max(IoU	O
M	O
N	O
,	O
M	O
O	O
)	O
)	O
end	O
for	O
b	O
←	O
arg	O
min	O
j	O
IoUs	O
APPEND(Ł	O
,	O
b	O
)	O
end	O
for	O
F	O
=	O
MODE(Ł	O
)	O
return	O
Fsupport	O
outputs	O
.	O

Hence	O
,	O
we	O
propose	O
another	O
algorithm	O
,	O
applying	O
to	O
heads	O
.	O

To	O
overcome	O
the	O
limitation	O
,	O
we	O
leverage	O
a	O
recent	O
unsupervised	O
rationale	O
generation	O
framework	O
,	O
In	B-MethodName
-	I-MethodName
vRat	I-MethodName
(	O
Chang	O
et	O
al	O
.	O
,	O

2020	O
)	O
to	O
generate	O
rationales	O
as	O
substitutions	O
.	O

Originally	O
,	O
InvRat	B-MethodName
was	O
designed	O
for	O
single	O
-	O
input	O
tasks	O
such	O
as	O
sentiment	O
analysis	O
.	O

Table	O
1	O
contains	O
a	O
summary	O
of	O
the	O
datasets	O
,	O
dataset	O
sizes	O
,	O
and	O
metrics.•	O
BoolQ	B-DatasetName
(	O
Clark	O
et	O
al	O
.	O
,	O

2019	O
):	O
a	O
dataset	O
comprises	O
selected	O
passages	O
from	O
Wikipedia	O
and	O
naturally	O
occurring	O
yes	O
/	O
no	O
questions	O
to	O
be	O
answered	O
by	O
the	O
model.•	O
Movie	B-DatasetName
Reviews	I-DatasetName
(	O
Zaidan	O
and	O
Eisner	O
,	O
2008	O
):	O
a	O
dataset	O
composed	O
of	O
movie	O
reviews	O
.	O

It	O
contains	O
positive	O
and	O
negative	O
sentiment	O
labels	O
to	O
be	O
predicted	O
by	O
the	O
model.•	O
SciFact	B-DatasetName
(	O
Wadden	O
et	O
al	O
.	O
,	O

We	O
followed	O
the	O
best	O
LAMOL	B-MethodName
configuration	O
from	O
Sun	O
et	O
al	O
.	O
(	O

All	O
parameters	O
were	O
kept	O
at	O
the	O
default	O
values	O
.	O

2019	O
)	O
as	O
the	O
language	O
model	O
.	O

Each	O
task	O
was	O
trained	O
for	O
five	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

We	O
used	O
β	B-HyperparameterName
=	O
80	B-HyperparameterValue
,	O
i.e.	O
,	O
selecting	O
the	O
top	O
20	O
percentile	O
of	O
attention	O
scores	O
to	O
compare	O
with	O
ground	O
truth	O
rationales	O
.	O

As	O
the	O
ERASER	O
benchmark	O
has	O
an	O
average	O
ratio	O
of	O
rationale	O
tokens	O
to	O
document	O
tokens	O
of	O
around	O
9.4	O
%	O
,	O
we	O
allowed	O
rationale	B-HyperparameterName
selection	I-HyperparameterName
to	O
be	O
two	O
times	O
the	O
average	O
ratio	O
(	O
i.e.	O
,	O
20%).For	B-HyperparameterValue
InvRat	B-MethodName
,	O
we	O
opted	O
for	O
300	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
GloVe	B-HyperparameterName
embeddings	I-HyperparameterName
(	O
Pennington	O
et	O
al	O
.	O
,	O

The	O
generator	O
and	O
the	O
predictor	O
modules	O
of	O
InvRat	B-MethodName
were	O
based	O
on	O
1	B-HyperparameterValue
-	I-HyperparameterValue
layer	I-HyperparameterValue
bidirectional	B-HyperparameterName
gated	I-HyperparameterName
recurrent	I-HyperparameterName
units	I-HyperparameterName
(	O
Chung	O
et	O
al	O
.	O
,	O

2014	O
)	O
with	O
256	B-HyperparameterValue
hidden	B-HyperparameterName
units	I-HyperparameterName
as	O
in	O
Chang	O
et	O
al	O
.	O
(	O

Maximum	B-HyperparameterName
model	I-HyperparameterName
input	I-HyperparameterName
was	O
set	O
to	O
1,024	B-HyperparameterValue
tokens	I-HyperparameterValue
.	O

This	O
section	O
reports	O
the	O
performance	O
of	O
Rationale	B-MethodName
LAMOL	I-MethodName
and	O
compares	O
it	O
with	O
LAMOL	B-MethodName
as	O
the	O
baseline	O
as	O
well	O
as	O
multitask	B-TaskName
learning	I-TaskName
,	O
which	O
is	O
considered	O
as	O
the	O
upper	O
bound	O
of	O
LL	B-TaskName
.	O

In	O
order	O
to	O
validate	O
if	O
component	O
freezing	O
truly	O
helps	O
reduce	O
catastrophic	O
forgetting	O
,	O
we	O
performed	O
partial	O
brute	O
force	O
block	O
-	O
level	O
freezing	O
on	O
each	O
task	O
permutation	O
to	O
approximately	O
determine	O
the	O
upper	O
bound	O
of	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
block	O
.	O

Brute	O
force	O
was	O
able	O
to	O
outperform	O
vanilla	O
LAMOL	B-MethodName
by	O
a	O
substantial	O
margin	O
of	O
3.68	B-MetricValue
%	I-MetricValue
,	O
only	O
1.36	B-MetricValue
%	I-MetricValue
from	O
the	O
multitask	O
upper	O
bound	O
.	O

This	O
suggests	O
that	O
component	O
freezing	O
is	O
able	O
to	O
further	O
nullify	O
the	O
effect	O
of	O
catastrophic	O
forgetting	O
from	O
LAMOL	B-MethodName
.	O

It	O
also	O
achieved	O
a	O
standard	O
deviation	O
of	O
only	O
2.3	B-MetricValue
%	I-MetricValue
compared	O
with	O
LAMOL	B-MethodName
's	O
5.28	B-MetricValue
%	I-MetricValue
.	O

A	O
sample	O
of	O
accuracy	B-MetricName
graphs	O
(	O
as	O
the	O
learning	O
progressed	O
)	O
of	O
the	O
compared	O
methods	O
,	O
with	O
the	O
BoolQ	B-DatasetName
→	O
SciFact	B-DatasetName
→	O
Movies	B-DatasetName
(	O
BSM	O
)	O
task	O
order	O
is	O
shown	O
in	O
Figure	O
4	O
from	O
top	O
to	O
bottom	O
,	O
respectively	O
.	O

As	O
the	O
first	O
task	O
,	O
BoolQ	B-DatasetName
was	O
not	O
really	O
affected	O
by	O
SciFact	B-DatasetName
,	O
but	O
encountered	O
a	O
heavy	O
drop	O
during	O
the	O
third	O
task	O
of	O
Movies	B-DatasetName
.	O

In	O
the	O
baseline	O
,	O
BoolQ	B-DatasetName
dropped	O
from	O
61	B-MetricValue
%	I-MetricValue
to	O
a	O
mere	O
6	B-MetricValue
%	I-MetricValue
,	O
while	O
only	O
rebounding	O
up	O
to	O
26	B-MetricValue
%	I-MetricValue
at	O
the	O
end	O
.	O

However	O
,	O
after	O
freezing	O
the	O
most	O
plastic	O
block	O
identified	O
by	O
partial	O
brute	O
forcing	O
,	O
BoolQ	B-DatasetName
dropped	O
from	O
62	B-MetricValue
%	I-MetricValue
to	O
15	B-MetricValue
%	I-MetricValue
,	O
and	O
rebounding	O
up	O
to	O
47	B-MetricValue
%	I-MetricValue
.	O

Comparatively	O
,	O
in	O
the	O
second	O
task	O
,	O
SciFact	B-DatasetName
encountered	O
a	O
smaller	O
drop	O
during	O
the	O
third	O
task	O
from	O
63	B-MetricValue
%	I-MetricValue
to	O
55	B-MetricValue
%	I-MetricValue
,	O
and	O
then	O
Green	O
background	O
refers	O
to	O
the	O
epochs	O
on	O
which	O
the	O
model	O
is	O
first	O
introduced	O
with	O
a	O
particular	O
task	O
.	O

In	O
this	O
figure	O
,	O
for	O
example	O
,	O
the	O
model	O
is	O
trained	O
on	O
Bool	B-DatasetName
-	I-DatasetName
Q	I-DatasetName
and	O
evaluated	O
on	O
all	O
the	O
three	O
tasks	O
during	O
epoch	O
1	O
-	O
5	O
.	O

rebounded	O
back	O
to	O
65	B-MetricValue
%	I-MetricValue
.	O

As	O
the	O
last	O
task	O
,	O
movies	B-DatasetName
was	O
not	O
affected	O
by	O
catastrophic	O
forgetting	O
.	O

Accuracy	B-MetricName
graphs	O
for	O
all	O
permutation	O
of	O
tasks	O
is	O
available	O
in	O
Appendix	O
6	O
from	O
which	O
we	O
make	O
several	O
observations	O
concerning	O
the	O
effect	O
of	O
task	O
orders	O
on	O
the	O
overall	O
performance:•	O
There	O
is	O
evidence	O
that	O
Movies	B-DatasetName
accelerate	O
the	O
forgetting	O
process	O
of	O
first	O
task	O
due	O
to	O
the	O
abrupt	O
change	O
in	O
data	O
distribution.•	O
However	O
,	O
the	O
performance	O
on	O
the	O
task	O
Movies	B-DatasetName
itself	O
is	O
barely	O
affected	O
by	O
the	O
task	O
order	O
.	O

We	O
attribute	O
it	O
to	O
the	O
low	O
difficulty	O
of	O
the	O
task.•	O
There	O
is	O
usually	O
no	O
interference	O
between	O
the	O
tasks	O
Bool	B-DatasetName
-	I-DatasetName
Q	I-DatasetName
and	O
SciFact	B-DatasetName
when	O
these	O
tasks	O
are	O
trained	O
in	O
adjacency	O
since	O
they	O
are	O
similar	O
.	O

Combined	O
with	O
time	O
required	O
for	O
CCI	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
required	O
approximately	O
2.4	O
times	O
more	O
time	O
than	O
vanilla	O
LAMOL	B-MethodName
to	O
completely	O
train	O
a	O
model	O
as	O
shown	O
in	O
Figure	O
3	O
.	O

From	O
Table	O
2	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
block	O
outperformed	O
LAMOL	B-MethodName
by	O
1.83	B-MetricValue
%	I-MetricValue
average	O
accuracy	B-MetricName
(	O
0.97	B-MetricValue
%	I-MetricValue
average	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
over	O
all	O
permutations	O
while	O
having	O
smaller	O
standard	O
deviation	O
,	O
indicating	O
that	O
it	O
is	O
also	O
more	O
robust	O
to	O
task	O
orders	O
.	O

Rational	B-MethodName
LAMOL	I-MethodName
head	O
was	O
able	O
to	O
match	O
or	O
outperform	O
LAMOL	B-MethodName
in	O
five	O
out	O
of	O
six	O
task	O
orders	O
,	O
but	O
the	O
significant	O
decrease	O
in	O
the	O
SBM	O
order	O
lowered	O
the	O
average	O
to	O
a	O
0.43	B-MetricValue
%	I-MetricValue
gain	O
(	O
and	O
a	O
slight	O
decrease	O
in	O
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
from	O
the	O
baseline	O
.	O

Upon	O
further	O
inspection	O
,	O
we	O
found	O
that	O
the	O
pseudo	O
-	O
samples	O
of	O
SciFact	B-DatasetName
contained	O
high	O
variance	O
in	O
quality	O
during	O
pseudodata	O
replay	O
.	O

In	O
addition	O
to	O
generation	O
token	O
mismatch	O
,	O
i.e.	O
,	O
a	O
situation	O
where	O
a	O
pseudo	O
-	O
sample	O
has	O
an	O
answer	O
token	O
from	O
a	O
wrong	O
task	O
,	O
the	O
low	O
volume	O
of	O
SciFact	B-DatasetName
training	O
data	O
affected	O
the	O
quality	O
of	O
the	O
pseudo	O
-	O
samples	O
generated	O
.	O

Without	O
the	O
SBM	O
drop	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
head	O
performed	O
comparatively	O
well	O
or	O
slightly	O
higher	O
with	O
the	O
block	O
-	O
level	O
.	O

Performing	O
a	O
one	O
-	O
tailed	O
paired	O
ttest	O
on	O
all	O
data	O
points	O
of	O
the	O
total	O
3	O
random	O
seeds	O
,	O
we	O
observed	O
that	O
block	O
-	O
level	O
freezing	O
is	O
able	O
to	O
win	O
against	O
the	O
original	O
LAMOL	B-MethodName
with	O
statistical	O
significance	O
(	O
p	O
-	O
value	O
of	O
0.023	O
and	O
0.042	O
for	O
block	O
-	O
level	O
and	O
generated	O
block	O
-	O
level	O
respectively	O
)	O
.	O

With	O
the	O
SBM	O
result	O
neglected	O
as	O
an	O
outlier	O
,	O
both	O
block	O
-	O
level	O
and	O
head	O
-	O
level	O
significantly	O
improved	O
the	O
results	O
compared	O
with	O
the	O
original	O
LAMOL	B-MethodName
(	O
p	O
-	O
value	O
of	O
0.015	O
,	O
0.014	O
,	O
0.010	O
,	O
0.049	O
for	O
block	O
-	O
level	O
,	O
generated	O
block	O
-	O
level	O
,	O
head	O
-	O
level	O
,	O
and	O
generated	O
headlevel	O
respectively	O
)	O
.	O

Even	O
though	O
our	O
Rationale	B-MethodName
LAMOL	I-MethodName
outperformed	O
the	O
baseline	O
,	O
there	O
was	O
still	O
a	O
gap	O
from	O
the	O
brute	O
force	O
upper	O
bound	O
.	O

Due	O
to	O
the	O
difference	O
in	O
focus	O
between	O
human	O
and	O
machines	O
,	O
it	O
is	O
conceivable	O
that	O
the	O
rationales	O
generated	O
by	O
InvRat	B-MethodName
would	O
be	O
mostly	O
misaligned	O
with	O
human	O
rationales	O
.	O

This	O
is	O
shown	O
in	O
Table	O
3	O
,	O
where	O
the	O
F1	B-MetricName
scores	O
of	O
InvRat	B-MethodName
are	O
quite	O
low	O
when	O
compared	O
with	O
human	O
rationales	O
.	O

Figure	O
5	O
shows	O
an	O
example	O
of	O
generated	O
rationales	O
output	O
by	B-MethodName
InvRat	I-MethodName
compared	O
with	O
human	O
rationales	O
.	O

Despite	O
that	O
,	O
Generated	O
Rational	B-MethodName
LAMOL	I-MethodName
block	O
outperformed	O
both	O
Rational	B-MethodName
LAMOL	I-MethodName
and	O
LAMOL	B-MethodName
baseline	O
by	O
0.84	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
(	O
0.31	B-MetricValue
%	I-MetricValue
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
and	O
2.67	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
(	O
1.27	B-MetricValue
%	I-MetricValue
Macro	B-MetricName
-	I-MetricName
F1	I-MetricName
)	O
respectively	O
,	O
further	O
reducing	O
the	O
gap	O
to	O
Brute	O
Force	O
,	O
the	O
approximate	O
upper	O
bound	O
of	O
the	O
proposed	O
CCI	O
.	O

This	O
suggests	O
that	O
rationales	O
chosen	O
by	O
InvRat	B-MethodName
,	O
regardless	O
of	O
how	O
nonsensical	O
they	O
appear	O
,	O
still	O
carry	O
information	O
that	O
eliminates	O
the	O
need	O
for	O
human	O
rationales	O
.	O

The	O
results	O
are	O
consistent	O
with	O
Bao	O
et	O
al	O
.	O
(	O

Last	O
but	O
not	O
least	O
,	O
Figure	O
3	O
shows	O
that	O
the	O
process	O
of	O
generating	O
rationales	O
using	O
InvRat	B-MethodName
,	O
including	O
training	O
and	O
inference	O
,	O
contributed	O
only	O
marginally	O
,	O
about	O
15	O
minutes	O
,	O
to	O
the	O
total	O
time	O
used	O
in	O
the	O
training	O
process	O
.	O

To	O
effectively	O
retain	O
learned	O
knowledge	O
in	O
LL	B-TaskName
for	O
NLP	O
tasks	O
,	O
we	O
proposed	O
Rational	B-MethodName
LAMOL	I-MethodName
,	O
a	O
learning	O
framework	O
that	O
uses	O
rationales	O
to	O
identify	O
and	O
freeze	O
the	O
most	O
critical	O
components	O
of	O
the	O
model	O
while	O
being	O
trained	O
on	O
a	O
new	O
task	O
.	O

We	O
showed	O
that	O
Rational	B-MethodName
LAMOL	I-MethodName
is	O
able	O
to	O
outperform	O
LAMOL	B-MethodName
by	O
a	O
significant	O
margin	O
.	O

Overall	O
,	O
Rational	B-MethodName
LAMOL	I-MethodName
bridges	O
the	O
gap	O
between	O
LL	B-TaskName
in	O
NLP	O
with	O
model	O
understanding	O
through	O
rationales	O
,	O
exhibiting	O
potential	O
for	O
a	O
true	O
lifelong	B-TaskName
language	I-TaskName
learning	I-TaskName
as	O
well	O
as	O
limiting	O
catastrophic	O
forgetting	O
.	O

However	O
,	O
we	O
uncover	O
a	O
striking	O
contrast	O
to	O
this	O
promise	O
:	O
across	O
5	O
models	O
and	O
4	O
datasets	O
on	O
the	O
task	O
of	O
visual	B-TaskName
question	I-TaskName
answering	I-TaskName
,	O
a	O
wide	O
variety	O
of	O
active	O
learning	O
approaches	O
fail	O
to	O
outperform	O
random	O
selection	O
.	O

Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
VQA	B-TaskName
)	O
,	O
the	O
task	O
of	O
answering	O
questions	O
about	O
Figure	O
1	O
:	O
We	O
systematically	O
evaluate	O
active	O
learning	O
on	O
VQA	B-TaskName
datasets	O
and	O
isolate	O
their	O
inability	O
to	O
perform	O
better	O
than	O
random	O
sampling	O
due	O
to	O
the	O
presence	O
of	O
collective	O
outliers	O
.	O

Unfortunately	O
,	O
today	O
's	O
VQA	B-TaskName
models	O
are	O
data	O
hungry	O
:	O
Their	O
performance	O
scales	O
monotonically	O
with	O
more	O
train	O
-	O
ing	O
data	O
(	O
Lu	O
et	O
al	O
.	O
,	O

2019	O
)	O
show	O
little	O
to	O
no	O
improvement	O
in	O
sample	O
efficiency	O
across	O
5	O
models	O
on	O
4	O
VQA	B-TaskName
datasets	O
-indeed	O
,	O
in	O
some	O
cases	O
performing	O
worse	O
than	O
randomly	O
selecting	O
data	O
to	O
label	O
.	O

We	O
mitigate	O
the	O
cold	O
start	O
challenge	O
of	O
needing	O
a	O
representative	O
initial	O
dataset	O
by	O
varying	O
the	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
seed	I-HyperparameterName
set	I-HyperparameterName
in	O
our	O
experiments	O
.	O

Finally	O
,	O
we	O
use	O
deep	B-MethodName
Bayesian	I-MethodName
active	I-MethodName
learning	I-MethodName
to	O
calibrate	O
model	O
uncertainty	O
to	O
high	O
-	O
dimensional	O
data	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011;Gal	O
and	O
Ghahramani	O
,	O
2016;.After	O
concluding	O
that	O
negative	O
results	O
are	O
consistent	O
across	O
all	O
experimental	O
conditions	O
,	O
we	O
investigate	O
active	O
learning	O
's	O
ineffectiveness	O
on	O
VQA	B-TaskName
as	O
a	O
data	O
problem	O
and	O
identify	O
the	O
existence	O
of	O
collective	O
outliers	O
(	O
Han	O
and	O
Kamber	O
,	O
2000	O
)	O
as	O
the	O
source	O
of	O
the	O
problem	O
.	O

While	O
global	O
outliers	O
deviate	O
from	O
the	O
rest	O
of	O
the	O
data	O
and	O
are	O
often	O
a	O
consequence	O
of	O
labeling	O
error	O
,	O
collective	O
outliers	O
cluster	O
together	O
;	O
they	O
may	O
not	O
individually	O
be	O
identifiable	O
as	O
outliers	O
but	O
collectively	O
deviate	O
from	O
other	O
examples	O
in	O
the	O
dataset	O
.	O

For	O
instance	O
,	O
VQA-2	B-DatasetName
(	O
Goyal	O
et	O
al	O
.	O
,	O

Similarly	O
,	O
GQA	B-DatasetName
(	O
Hudson	O
and	O
Manning	O
,	O
2019	O
)	O
asks	O
underspecified	O
questions	O
(	O
e.g.	O
,	O
"	O
what	O
is	O
the	O
person	O
wearing	O
?	O
"	O
which	O
can	O
have	O
multiple	O
correct	O
answers	O
)	O
.	O

Collective	O
outliers	O
are	O
not	O
specific	O
to	O
VQA	B-TaskName
,	O
but	O
can	O
similarly	O
be	O
found	O
in	O
many	O
open	O
-	O
ended	O
tasks	O
,	O
including	O
visual	O
navigation	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

This	O
allows	O
us	O
to	O
conclude	O
that	O
collective	O
outliers	O
are	O
,	O
indeed	O
,	O
responsible	O
for	O
the	O
ineffectiveness	O
of	O
active	O
learning	O
for	O
VQA	B-TaskName
.	O

Our	O
work	O
tests	O
the	O
utility	O
of	O
multiple	O
recent	O
active	O
learning	O
methods	O
on	O
the	O
open	O
-	O
ended	O
understanding	O
task	O
of	O
VQA	B-TaskName
.	O

We	O
draw	O
on	O
the	O
dataset	O
analysis	O
literature	O
to	O
identify	O
collective	O
outliers	O
as	O
the	O
bottleneck	O
hindering	O
active	O
learning	O
methods	O
in	O
this	O
setting	O
.	O

2003;Culotta	O
and	O
McCallum	O
,	O
2005	O
)	O
,	O
named	O
entity	O
recognition	O
(	O
Hachey	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
semantic	O
parsing	O
(	O
Dong	O
et	O
al	O
.	O
,	O

However	O
,	O
these	O
same	O
methods	O
struggle	O
to	O
outperform	O
a	O
random	O
baseline	O
when	O
applied	O
to	O
the	O
task	O
of	O
VQA	B-TaskName
(	O
Lin	O
and	O
Parikh	O
,	O
2017;Jedoui	O
et	O
al	O
.	O
,	O

To	O
study	O
this	O
discrepancy	O
,	O
we	O
systematically	O
apply	O
8	O
diverse	O
active	O
learning	O
methods	O
to	O
VQA	B-TaskName
,	O
including	O
methods	O
that	O
use	O
model	B-MethodName
uncertainty	I-MethodName
(	O
Abramson	O
and	O
Freund	O
,	O
2004;Collins	O
et	O
al	O
.	O
,	O

2009	O
)	O
,	O
Bayesian	B-MethodName
uncertainty	I-MethodName
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016;Kendall	O
and	O
Gal	O
,	O
2017	O
)	O
,	O
disagreement	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

2011	O
;	O
,	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018).Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
.	O

Progress	O
on	O
VQA	B-TaskName
has	O
been	O
heralded	O
as	O
a	O
marker	O
for	O
progress	O
on	O
general	O
open	O
-	O
ended	O
understanding	O
tasks	O
,	O
resulting	O
in	O
several	O
benchmarks	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2015;Ren	O
et	O
al	O
.	O
,	O

2019	O
)	O
versus	O
those	O
that	O
are	O
easy	O
to	O
learn	O
(	O
Bras	O
et	O
al	O
.	O
,	O

Unlike	O
prior	O
datasets	O
analyzed	O
by	O
Dataset	O
Maps	O
that	O
have	O
a	O
small	O
number	O
of	O
global	O
outliers	O
as	O
hard	O
examples	O
,	O
we	O
discover	O
that	O
VQA	B-TaskName
datasets	O
contain	O
copious	O
amounts	O
of	O
collective	O
outliers	O
,	O
which	O
are	O
difficult	O
or	O
even	O
impossible	O
for	O
models	O
to	O
learn	O
.	O

We	O
adopt	O
the	O
standard	O
pool	O
-	O
based	O
active	O
learning	O
setup	O
from	O
prior	O
work	O
(	O
Lewis	O
and	O
Gale	O
,	O
1994;Settles	O
,	O
2009;Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
consisting	O
of	O
a	O
model	O
M	O
,	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
of	O
labeled	O
examples	O
(	O
x	O
i	O
,	O
y	O
i	O
)	O
∈	O
D	O
seed	O
used	O
to	O
initialize	O
M	O
,	O
an	O
unlabeled	O
pool	O
of	O
data	O
D	O
pool	O
,	O
and	O
an	O
acquisition	O
function	O
A(x	O
,	O
M	O
)	O
.	O

We	O
follow	O
prior	O
work	O
to	O
simulate	O
an	O
oracle	O
using	O
existing	O
datasets	O
,	O
forming	O
D	O
seed	O
from	O
a	O
fixed	O
percentage	O
of	O
the	O
full	O
dataset	O
,	O
and	O
using	O
the	O
remainder	O
as	O
D	O
pool	O
Lin	O
and	O
Parikh	O
,	O
2017;Siddhant	O
and	O
Lipton	O
,	O
2018	O
)	O
.	O

We	O
re	O
-	O
train	O
M	O
after	O
each	O
acquisition	O
iteration	O
.	O

Prior	O
work	O
has	O
noted	O
the	O
impact	O
of	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
on	O
active	O
learning	O
performance	O
(	O
Lin	O
and	O
Parikh	O
,	O
2017;Misra	O
et	O
al	O
.	O
,	O

We	O
run	O
multiple	O
active	O
learning	O
evaluations	O
with	O
varying	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
(	O
ranging	O
from	O
5	B-HyperparameterValue
%	I-HyperparameterValue
to	I-HyperparameterValue
50	I-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
size	I-HyperparameterValue
)	O
.	O

We	O
keep	O
the	O
size	O
of	O
each	O
acquisition	O
batch	B-HyperparameterName
B	I-HyperparameterName
to	O
a	O
constant	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
overall	I-HyperparameterValue
pool	I-HyperparameterValue
size	I-HyperparameterValue
.	O

Visual	B-TaskName
Question	I-TaskName
Answering	I-TaskName
(	O
VQA	B-TaskName
)	O
requires	O
reasoning	O
over	O
two	O
modalities	O
:	O
images	O
and	O
text	O
.	O

We	O
evaluate	O
with	O
a	O
representative	O
sample	O
of	O
existing	O
VQA	B-TaskName
models	O
,	O
including	O
the	O
following	O
:	O
2LogReg	B-MethodName
is	O
a	O
logistic	O
regression	O
model	O
that	O
uses	O
either	O
ResNet-101	O
or	O
Faster	O
R	O
-	O
CNN	O
image	O
features	O
with	O
mean	O
-	O
pooled	O
GloVe	O
question	O
embeddings	O
(	O
Pennington	O
et	O
al	O
.	O
,	O

Although	O
these	O
models	O
are	O
not	O
as	O
performant	O
as	O
the	O
subsequent	O
models	O
,	O
logistic	O
regression	O
has	O
been	O
effective	O
on	O
VQA	B-TaskName
(	O
Suhr	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
and	O
is	O
pervasive	O
in	O
the	O
active	O
learning	O
literature	O
(	O
Schein	O
and	O
Ungar	O
,	O
2007;Yang	O
and	O
Loog	O
,	O
2018;Mussmann	O
and	O
Liang	O
,	O
2018).LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
is	O
a	O
standard	O
model	O
introduced	O
with	O
VQA-1	B-TaskName
(	O
Agrawal	O
et	O
al	O
.	O
,	O

We	O
use	O
more	O
performant	O
ResNet-101	O
features	O
instead	O
of	O
the	O
original	O
VGGNet	O
features	O
as	O
our	O
visual	O
backbone	O
.	O

BUTD	B-MethodName
(	B-MethodName
Bottom	I-MethodName
-	I-MethodName
Up	I-MethodName
Top	I-MethodName
-	I-MethodName
Down	I-MethodName
Attention	I-MethodName
)	O
uses	O
object	O
-	O
based	O
features	O
in	O
tandem	O
with	O
attention	O
over	O
objects	O
(	O
Anderson	O
et	O
al	O
.	O
,	O

BUTD	B-MethodName
won	O
the	O
2017	O
VQA	B-TaskName
Challenge	O
(	O
Teney	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
and	O
has	O
been	O
a	O
consistent	O
baseline	O
for	O
recent	O
work	O
in	O
VQA.LXMERT	B-TaskName
is	O
a	O
large	O
multi	O
-	O
modal	O
transformer	O
model	O
that	O
uses	O
BUTD	B-MethodName
's	O
object	O
features	O
and	O
contextualized	O
BERT	O
language	O
features	O
(	O
Tan	O
and	O
Bansal	O
,	O
2019	O
)	O
.	O

LXMERT	B-MethodName
is	O
pretrained	O
on	O
a	O
corpus	O
of	O
aligned	O
image	O
-	O
and	O
-	O
textual	O
data	O
spanning	O
MS	O
COCO	O
,	O
Visual	O
Genome	O
,	O
VQA-2	B-DatasetName
,	O
NLVR-2	O
,	O
and	O
GQA	B-DatasetName
(	O
Lin	O
et	O
al	O
.	O
,	O

3	O
Several	O
active	B-MethodName
learning	I-MethodName
methods	O
have	O
been	O
developed	O
to	O
account	O
for	O
different	O
aspects	O
of	O
the	O
machine	O
learning	O
training	O
pipeline	O
:	O
while	O
some	O
acquire	O
examples	O
with	O
high	O
aleotoric	O
uncertainty	O
(	O
Settles	O
,	O
2009	O
)	O
(	O
having	O
to	O
do	O
with	O
the	O
natural	O
uncertainty	O
in	O
the	O
data	O
)	O
or	O
epistemic	O
uncertainty	O
(	O
having	O
to	O
do	O
with	O
the	O
uncertainty	O
in	O
the	O
modeling	O
/	O
learning	O
process	O
)	O
,	O
others	O
attempt	O
to	O
acquire	O
examples	O
that	O
reflect	O
the	O
distribution	O
of	O
data	O
in	O
the	O
pool	O
(	O
Sener	O
and	O
Savarese	O
,	O
2018	O
)	O
.	O

We	O
sample	O
a	O
diverse	O
set	O
of	O
these	O
methods	O
:	O
Random	B-MethodName
Sampling	I-MethodName
serves	O
as	O
our	O
baseline	O
passive	O
approach	O
for	O
acquiring	O
examples	O
.	O

Least	B-MethodName
Confidence	I-MethodName
acquires	O
examples	O
with	O
lowest	O
model	O
prediction	O
probability	O
(	O
Settles	O
,	O
2009).Entropy	B-MethodName
acquires	O
examples	O
with	O
the	O
highest	O
entropy	O
in	O
the	O
model	O
's	O
output	O
(	O
Settles	O
,	O
2009	O
)	O
.	O

MC	B-MethodName
-	I-MethodName
Dropout	I-MethodName
Entropy	I-MethodName
(	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
with	I-MethodName
Entropy	I-MethodName
acquisition	I-MethodName
)	O
acquires	O
examples	O
with	O
high	O
entropy	O
in	O
the	O
model	O
's	O
output	O
averaged	O
over	O
multiple	O
passes	O
through	O
a	O
neural	O
network	O
with	O
different	O
dropout	O
masks	O
(	O
Gal	O
and	O
Ghahramani	O
,	O
2016	O
)	O
.	O

BALD	B-MethodName
(	O
Bayesian	B-MethodName
Active	I-MethodName
Learning	I-MethodName
by	I-MethodName
Disagreement	I-MethodName
)	O
builds	O
upon	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
by	O
proposing	O
a	O
decision	O
theoretic	O
objective	O
;	O
it	O
acquires	O
examples	O
that	O
maximise	O
the	O
decrease	O
in	O
expected	O
posterior	O
entropy	O
(	O
Houlsby	O
et	O
al	O
.	O
,	O

Since	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
operates	O
over	O
a	O
representation	O
space	O
(	O
and	O
not	O
an	O
output	O
distribution	O
,	O
like	O
prior	O
strategies	O
)	O
and	O
VQA	B-TaskName
models	O
operate	O
over	O
two	O
modalities	O
,	O
we	O
employ	O
three	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
variants	O
:	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Language	O
)	O
and	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Vision	O
)	O
operate	O
over	O
their	O
respective	O
representation	O
spaces	O
while	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
(	O
Fused	O
)	O
operates	O
over	O
the	O
"	O
fused	O
"	O
vision	O
and	O
language	O
representation	O
space	O
.	O

Due	O
to	O
space	O
constraints	O
,	O
we	O
only	O
visualize	O
4	O
active	O
learning	O
strategies	O
-Least	B-MethodName
-	I-MethodName
Confidence	I-MethodName
,	O
BALD	B-MethodName
,	O
CoreSet	B-MethodName
-	O
Fused	O
,	O
and	O
the	O
Random	B-MethodName
Baseline	I-MethodName
-using	O
3	O
models	O
(	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
BUTD	B-MethodName
,	O
LXMERT	B-MethodName
)	O
.	O

4	O
Results	O
and	O
trends	O
are	O
consistent	O
across	O
the	O
different	O
acquisition	O
functions	O
,	O
models	O
and	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
(	O
see	O
the	O
appendix	O
for	O
results	O
with	O
other	O
models	O
,	O
acquisition	O
functions	O
,	O
and	O
seed	B-HyperparameterName
set	I-HyperparameterName
sizes	I-HyperparameterName
)	O
.	O

Strategies	O
perform	O
on	O
par	O
with	O
or	O
worse	O
than	O
the	O
random	O
baseline	O
,	O
when	O
using	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
dataset	I-HyperparameterValue
as	O
the	O
seed	B-HyperparameterName
set.4	I-HyperparameterName
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-VQA-2	B-DatasetName
Random	O
Baseline	O
Least	O
-	O
Confidence	O
BALD	O
Core	O
-	O
Set	O
(	O
Fused	O
)	O
4	O
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
BUTD	B-MethodName
-VQA-2	B-DatasetName
4	O
0	O
K	O
8	O
0	O
K	O
1	O
2	O
0	O
K	O
1	O
6	O
0	O
K	O
2	O
0	O
0	O
K	O
2	O
4	O
0	O
K	O
2	O
8	O
0	O
K	O
3	O
2	O
0	O
K	O
3	O
6	O
0	O
K	O
4	O
0	O
0	O
K	O
Number	O
of	O
Training	O
Examples	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
Validation	O
Accuracy	B-MetricName
LXMERT	B-MethodName
-VQA-2Figure	B-DatasetName
3	O
:	O
Results	O
for	O
the	O
full	O
VQA-2	B-DatasetName
dataset	O
,	O
also	O
using	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
dataset	I-HyperparameterValue
as	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
.	O

One	O
complexity	O
of	O
VQA	B-TaskName
is	O
the	O
size	O
of	O
the	O
output	O
space	O
and	O
the	O
number	O
of	O
examples	O
present	O
(	O
Agrawal	O
et	O
al	O
.	O
,	O

2017	O
)	O
;	O
VQA-2	B-DatasetName
has	O
400k	O
training	O
examples	O
,	O
and	O
in	O
excess	O
of	O
3k	O
possible	O
answers	O
(	O
see	O
Table	O
1	O
)	O
.	O

To	O
ensure	O
our	O
results	O
and	O
conclusions	O
are	O
not	O
due	O
to	O
the	O
size	O
of	O
the	O
output	O
space	O
,	O
we	O
build	O
two	O
meaningful	O
,	O
but	O
narrow	O
-	O
domain	O
VQA	B-TaskName
datasets	O
from	O
subsets	O
of	O
VQA-2	B-DatasetName
.	O

VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
.	O

We	O
generate	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
by	O
compiling	O
a	O
list	O
of	O
20	O
popular	O
sports	O
(	O
e.g.	O
,	O
soccer	O
,	O
football	O
,	O
tennis	O
,	O
etc	O
.	O
)	O

in	O
VQA-2	B-DatasetName
,	O
and	O
restricting	O
the	O
set	O
of	O
questions	O
to	O
those	O
with	O
answers	O
in	O
this	O
list	O
.	O

We	O
picked	O
the	O
sports	O
categories	O
by	O
ranking	O
the	O
GloVe	O
vector	O
similarity	O
between	O
the	O
word	O
"	O
sports	O
"	O
to	O
answers	O
in	O
VQA-2	B-DatasetName
,	O
and	O
selected	O
the	O
20	O
most	O
commonly	O
occurring	O
answers	O
.	O

VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

We	O
generate	O
the	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
dataset	O
similarly	O
,	O
compiling	O
a	O
list	O
of	O
the	O
20	O
commonly	O
occurring	O
food	O
categories	O
by	O
GloVe	O
vector	O
similarity	O
to	O
the	O
word	O
"	O
food	O
.	O
"	O

Results	O
.	O

Figure	O
2	O
presents	O
results	O
for	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
with	O
an	O
initial	O
seed	B-HyperparameterName
set	I-HyperparameterName
restricted	O
to	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
500	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

The	O
appendix	O
reports	O
similar	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
.	O

For	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
Least	O
-	O
Confidence	O
appears	O
to	O
be	O
slightly	O
more	O
sample	O
efficient	O
,	O
while	O
all	O
other	O
strategies	O
perform	O
on	O
par	O
with	O
or	O
worse	O
than	O
random	O
.	O

For	O
BUTD	B-MethodName
,	O
all	O
methods	O
are	O
on	O
par	O
with	O
random	O
;	O
for	O
LXMERT	B-MethodName
,	O
they	O
perform	O
worse	O
than	O
random	O
.	O

Generally	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
active	O
learning	O
performance	O
varies	O
,	O
but	O
fails	O
to	O
outperform	O
random	O
acquisition	O
.	O

VQA-2	B-DatasetName
is	O
the	O
canonical	O
dataset	O
for	O
evaluating	O
VQA	O
models	O
(	O
Goyal	O
et	O
al	O
.	O
,	O

Unlike	O
traditional	O
VQA-2	B-DatasetName
evaluation	O
,	O
which	O
treats	O
the	O
task	O
as	O
a	O
multi	O
-	O
label	O
binary	O
classification	O
problem	O
,	O
we	O
follow	O
prior	O
active	O
learning	O
work	O
on	O
VQA	O
(	O
Lin	O
and	O
Parikh	O
,	O
2017	O
)	O
,	O
which	O
formulates	O
it	O
as	O
a	O
multi	O
-	O
class	O
classification	O
problem	O
,	O
enabling	O
the	O
use	O
of	O
acquisition	O
functions	O
such	O
as	O
uncertainty	O
sampling	O
and	O
BALD	O
.	O

We	O
use	O
the	O
standard	O
GQA	B-DatasetName
training	O
set	O
of	O
943k	O
questions	O
,	O
900k	O
of	O
which	O
we	O
use	O
for	O
the	O
active	O
learning	O
pool	O
.	O

Figure	O
5	O
shows	O
results	O
on	O
GQA	B-DatasetName
using	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
of	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
90k	B-HyperparameterValue
examples	I-HyperparameterValue
)	O
.	O

Despite	O
its	O
notable	O
differences	O
in	O
question	O
structure	O
to	O
VQA-2	B-DatasetName
,	O
active	O
learning	O
still	O
performs	O
on	O
par	O
with	O
or	O
slightly	O
worse	O
than	O
random	O
.	O

A	O
simple	O
question	O
remains	O
-why	O
?	O
One	O
hypothesis	O
is	O
that	O
sample	O
inefficiency	O
stems	O
from	O
the	O
data	O
itself	O
:	O
there	O
is	O
only	O
a	O
2	B-MetricValue
%	I-MetricValue
gain	O
in	O
validation	O
accuracy	B-MetricName
when	O
training	O
on	O
half	O
versus	O
the	O
whole	O
dataset	O
.	O

For	O
instance	O
(	O
Figure	O
7	O
)	O
,	O
in	O
VQA-2	B-DatasetName
,	O
we	O
identify	O
clusters	O
of	O
hard	O
-	O
to	O
-	O
learn	O
examples	O
that	O
require	O
optical	O
character	O
recognition	O
(	O
OCR	O
)	O
for	O
reasoning	O
about	O
text	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
first	O
word	O
on	O
the	O
black	O
car	O
?	O
"	O
)	O
;	O
another	O
cluster	O
requires	O
external	O
knowledge	O
to	O
answer	O
(	O
"	O
What	O
is	O
the	O
symbol	O
on	O
the	O
hood	O
often	O
associated	O
with	O
?	O
"	O
)	O
.	O

In	O
GQA	B-DatasetName
,	O
we	O
identify	O
different	O
clusters	O
of	O
collective	O
outliers	O
;	O
one	O
cluster	O
stems	O
from	O
innate	O
underspecification	O
(	O
e.g.	O
,	O
"	O
what	O
is	O
on	O
the	O
shelf	O
?	O
"	O
with	O
multiple	O
objects	O
present	O
on	O
the	O
shelf	O
)	O
;	O
another	O
cluster	O
requires	O
multiple	O
reasoning	O
hops	O
difficult	O
for	O
current	O
models	O
(	O
e.g.	O
,	O
"	O
What	O
is	O
the	O
vehicle	O
that	O
is	O
driving	O
down	O
the	O
road	O
the	O
box	O
is	O
on	O
the	O
side	O
of	O
?	O
"	O
)	O
.	O

We	O
sample	O
100	O
random	O
"	O
hard	O
-	O
to	O
-	O
learn	O
"	O
examples	O
from	O
both	O
VQA-2	B-DatasetName
and	O
GQA	B-DatasetName
and	O
find	O
that	O
100	O
%	O
Ablating	O
Outliers	O
.	O

To	O
verify	O
that	O
collective	O
outliers	O
are	O
responsible	O
for	O
the	O
degradation	O
of	O
active	O
learning	O
performance	O
,	O
we	O
re	O
-	O
run	O
our	O
experiments	O
using	O
active	O
learning	O
pools	O
with	O
varying	O
numbers	B-HyperparameterName
of	I-HyperparameterName
outliers	I-HyperparameterName
removed	I-HyperparameterName
.	O

We	O
systematically	O
remove	O
examples	O
with	O
a	O
low	O
product	O
value	O
and	O
observe	O
how	O
active	O
learning	O
performance	O
changes	O
(	O
see	O
Figure	O
8).We	O
observe	O
a	O
2	O
-	O
3x	O
improvement	O
in	O
sample	O
efficiency	O
when	O
removing	O
50	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
entire	I-HyperparameterValue
data	I-HyperparameterValue
pool	I-HyperparameterValue
,	O
consisting	O
mainly	O
of	O
collective	O
outliers	O
(	O
Figure	O
8c	O
)	O
.	O

This	O
improvement	O
decreases	O
if	O
we	O
only	O
remove	O
25	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
full	I-HyperparameterValue
pool	I-HyperparameterValue
(	O
Figure	O
8b	O
)	O
,	O
and	O
further	O
degrades	O
if	O
we	O
remove	O
only	O
10	B-HyperparameterValue
%	I-HyperparameterValue
(	O
Figure	O
8a	O
)	O
.	O

This	O
paper	O
asks	O
a	O
simple	O
question	O
-why	O
does	O
the	O
modern	O
neural	O
active	O
learning	O
toolkit	O
fail	O
when	O
applied	O
to	O
complex	O
,	O
open	O
ended	O
tasks	O
?	O
While	O
we	O
focus	O
on	O
VQA	B-TaskName
,	O
collective	O
outliers	O
are	O
abundant	O
in	O
tasks	O
such	O
as	O
natural	O
language	O
inference	O
(	O
Bowman	O
et	O
al	O
.	O
,	O

Other	O
work	O
learns	O
to	O
identify	O
novel	O
utterances	O
by	O
learning	O
to	O
intelligently	O
set	O
thresholds	O
in	O
representation	O
space	O
(	O
Karamcheti	O
et	O
al	O
.	O
,	O

Overall	O
,	O
we	O
hope	O
that	O
our	O
experiments	O
serve	O
as	O
a	O
catalyst	O
for	O
future	O
work	O
on	O
evaluating	O
active	O
learning	O
methods	O
with	O
inputs	O
drawn	O
from	O
open	O
-	O
world	O
datasets	O
.	O

Generally	O
,	O
any	O
combination	O
of	O
{	O
active	O
learning	O
strategy	O
×	O
model	O
×	O
seed	B-HyperparameterName
set	I-HyperparameterName
size	I-HyperparameterName
×	O
analysis	O
/	O
acquisition	O
plot	O
}	O
is	O
present	O
in	O
this	O
paper	O
,	O
and	O
is	O
available	O
in	O
the	O
public	O
code	O
repository	O
.	O

For	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
,	O
we	O
base	O
our	O
implementation	O
off	O
of	O
this	O
repository	O
:	O
https://github.com/	O
Shivanshu	O
-	O
Gupta	O
/	O
Visual	B-TaskName
-	I-TaskName
Question	I-TaskName
-	I-TaskName
Answering	I-TaskName
,	O
while	O
for	O
the	O
Bottom	O
-	O
Up	O
Top	O
-	O
Down	O
Attention	O
Model	O
,	O
we	O
use	O
this	O
repository	O
:	O
https://github.com/	O
hengyuan	O
-	O
hu	O
/	O
bottom	O
-	O
up	O
-	O
attention	O
-	O
vqa	O
,	O
keeping	O
default	O
hyperparameters	O
the	O
same	O
.	O

Logistic	O
Regression	O
.	O

LXMERT	B-MethodName
.	O

As	O
mentioned	O
in	O
Section	O
3	O
,	O
the	O
default	O
LXMERT	B-MethodName
checkpoint	O
and	O
fine	O
-	O
tuning	O
code	O
made	O
publicly	O
available	O
in	O
Tan	O
and	O
Bansal	O
(	O
2019	O
)	O
(	O
associated	O
code	O
repository	O
:	O
https://github.com/	O
airsplay	O
/	O
lxmert	O
)	O
is	O
pretrained	O
on	O
data	O
from	O
VQA-2	B-DatasetName
and	O
GQA	B-DatasetName
,	O
leaking	O
information	O
that	O
could	O
substantially	O
affect	O
our	O
active	O
learning	O
results	O
.	O

To	O
mitigate	O
this	O
,	O
we	O
contacted	O
the	O
authors	O
,	O
who	O
kindly	O
provided	O
us	O
with	O
a	O
checkpoint	O
of	O
the	O
model	O
without	O
VQA	B-TaskName
pretraining	O
.	O

We	O
perform	O
a	O
coarse	O
grid	O
search	O
over	O
hyperparameters	O
,	O
using	O
the	O
LXMERT	B-MethodName
implementation	O
provided	O
by	O
HuggingFace	O
Transformers	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

For	O
our	O
implementations	O
of	O
the	O
deep	O
Bayesian	O
active	O
learning	O
methods	O
(	O
Monte	B-MethodName
-	I-MethodName
Carlo	I-MethodName
Dropout	I-MethodName
w/	I-MethodName
Entropy	I-MethodName
,	O
BALD	B-MethodName
)	O
,	O
we	O
follow	O
Gal	O
and	O
Ghahramani	O
(	O
2016	O
)	O
and	O
estimate	O
a	O
Dropout	O
distribution	O
via	O
test	O
-	O
time	O
dropout	O
,	O
running	O
multiple	O
forward	O
passes	O
through	O
our	O
neural	O
networks	O
,	O
with	O
different	O
,	O
randomly	O
sampled	O
Dropout	O
masks	O
.	O

In	O
the	O
original	O
Core	B-MethodName
-	I-MethodName
Set	I-MethodName
selection	O
active	O
learning	O
work	O
introduced	O
by	O
Sener	O
and	O
Savarese	O
(	O
2018	O
)	O
,	O
it	O
is	O
shown	O
that	O
Core	O
-	O
Set	O
selection	O
for	O
active	O
learning	O
can	O
be	O
reduced	O
to	O
a	O
version	O
of	O
the	O
k	O
-	O
centers	O
problem	O
,	O
which	O
can	O
be	O
solved	O
approximately	O
(	O
2	O
-	O
OPT	O
)	O
with	O
a	O
greedy	O
algorithm	O
.	O

While	O
we	O
can	O
run	O
this	O
out	O
completely	O
for	O
smaller	O
datasets	O
(	O
and	O
indeed	O
,	O
this	O
is	O
what	O
we	O
do	O
for	O
our	O
small	O
datasets	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
and	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
)	O
,	O
a	O
single	O
acquisition	O
iteration	O
for	O
a	O
large	O
dataset	O
for	O
the	O
full	O
VQA-2	B-DatasetName
dataset	O
takes	O
approximately	O
20	O
GPU	O
-	O
hours	O
on	O
the	O
resources	O
we	O
have	O
available	O
,	O
or	O
up	O
to	O
9	O
days	O
for	O
a	O
single	O
Core	O
-	O
Set	O
selection	O
run	O
.	O

For	O
GQA	B-DatasetName
,	O
performing	O
exact	O
Core	O
-	O
Set	O
selection	O
takes	O
at	O
least	O
twice	O
as	O
long	O
.	O

Then	O
,	O
rather	O
than	O
updating	O
distances	O
from	O
examples	O
in	O
our	O
acquired	O
set	O
to	O
points	O
in	O
our	O
pool	O
after	O
each	O
acquisitionx	O
,	O
we	O
delay	O
updates	O
,	O
instead	O
only	O
refreshing	O
the	O
distance	O
computation	O
every	O
2000	O
acquisitions	O
(	O
roughly	O
5	O
%	O
of	O
an	O
acquisition	O
batch	O
for	O
VQA-2	B-DatasetName
)	O
.	O

This	O
allows	O
us	O
to	O
report	O
results	O
for	O
Core	O
-	O
Set	O
selection	O
with	O
the	O
three	O
different	O
proposed	O
representations	O
(	O
Fused	O
,	O
Language	O
-	O
Only	O
,	O
Vision	O
-	O
Only	O
)	O
for	O
VQA-2	B-DatasetName
;	O
unfortunately	O
,	O
for	O
GQA	B-DatasetName
and	O
LXMERT	B-MethodName
(	O
due	O
to	O
the	O
high	O
cost	O
of	O
training	O
)	O
,	O
even	O
running	O
this	O
amortized	O
version	O
of	O
Core	O
-	O
Set	O
selection	O
is	O
prohibitive	O
,	O
so	O
we	O
report	O
a	O
subset	O
of	O
results	O
,	O
and	O
omit	O
the	O
rest	O
.	O

We	O
include	O
further	O
results	O
from	O
our	O
study	O
of	O
active	O
learning	O
applied	O
to	O
VQA	B-TaskName
,	O
including	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
(	O
not	O
included	O
in	O
the	O
main	O
body	O
)	O
,	O
active	O
learning	O
results	O
for	O
the	O
two	O
logistic	O
regression	O
models	O
-Log	O
-	O
Reg	O
(	O
ResNet-101	O
)	O
and	O
Log	O
-	O
Reg	O
(	O
Faster	O
R	O
-	O
CNN	O
)	O
,	O
as	O
well	O
as	O
with	O
the	O
4	O
acquisition	O
strategies	O
not	O
included	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
-Entropy	O
,	O
Monte	O
-	O
Carlo	O
Dropout	O
w/	O
Entropy	O
,	O
Core	O
-	O
Set	O
(	O
Language	O
)	O
,	O
and	O
Core	O
-	O
Set	O
(	O
Vision	O
)	O
.	O

Figure	O
9	O
shows	O
results	O
on	O
VQA	B-DatasetName
-	I-DatasetName
Food	I-DatasetName
with	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
,	O
BUTD	B-MethodName
,	O
and	O
LXMERT	B-MethodName
models	O
,	O
with	O
a	O
seed	B-HyperparameterName
set	I-HyperparameterName
comprised	O
of	O
10	B-HyperparameterValue
%	I-HyperparameterValue
of	I-HyperparameterValue
the	I-HyperparameterValue
total	I-HyperparameterValue
pool	I-HyperparameterValue
.	O

The	O
results	O
are	O
mostly	O
similar	O
to	O
those	O
reported	O
in	O
the	O
paper	O
;	O
strategies	O
track	O
or	O
underperform	O
random	O
sampling	O
,	O
with	O
the	O
exception	O
of	O
Least	O
-	O
Confidence	O
for	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
model	O
-however	O
,	O
this	O
is	O
the	O
sole	O
exception	O
,	O
and	O
the	O
LSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
has	O
the	O
highest	O
training	O
variance	O
of	O
all	O
the	O
models	O
we	O
try	O
.	O

Figure	O
10	O
shows	O
active	O
learning	O
results	O
for	O
the	O
Lo	O
-	O
gReg	O
(	O
ResNet-101	O
)	O
model	O
on	O
VQA	O
-	O
Sports	O
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
,	O
and	O
VQA-2	B-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
50	B-HyperparameterValue
%	I-HyperparameterValue
)	O
.	O

Figure	O
11	O
presents	O
the	O
same	O
set	O
of	O
experiments	O
as	O
the	O
prior	O
section	O
,	O
except	O
with	O
the	O
LogReg	B-MethodName
(	O
Faster	O
R	O
-	O
CNN	O
)	O
model	O
.	O

Figure	O
12	O
presents	O
results	O
for	O
the	O
four	O
other	O
active	O
learning	O
strategies	O
we	O
implement	O
-Entropy	O
,	O
Monte	O
Carlo	O
Dropout	O
w/	O
Entropy	O
,	O
Core	O
-	O
Set	O
(	O
Language	O
)	O
,	O
and	O
Core	O
-	O
Set	O
(	O
Vision	O
)	O
-for	O
the	O
BUTD	B-MethodName
model	O
.	O

Results	O
are	O
across	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
,	O
and	O
VQA-2	B-DatasetName
(	O
seed	B-HyperparameterName
set	I-HyperparameterName
=	O
10	B-HyperparameterValue
%	I-HyperparameterValue
,	O
50	B-HyperparameterValue
%	I-HyperparameterValue
)	O
-despite	O
the	O
unique	O
features	O
of	O
each	O
strategy	O
,	O
the	O
trends	O
remain	O
consistent	O
with	O
those	O
in	O
the	O
paper	O
.	O

Figure	O
12	O
:	O
Results	O
with	O
the	O
BUTD	B-MethodName
on	O
VQA	B-DatasetName
-	I-DatasetName
Sports	I-DatasetName
,	O
VQA-2	B-DatasetName
and	O
GQA	O
using	O
the	O
alternative	O
4	O
acquisition	O
strategies	O
not	O
included	O
in	O
the	O
main	O
body	O
of	O
the	O
paper	O
.	O

Unsurprisingly	O
,	O
results	O
are	O
consistent	O
with	O
those	O
reported	O
in	O
the	O
paper	O
.	O

Given	O
that	O
the	O
map	O
for	O
GQA	B-DatasetName
is	O
similar	O
to	O
the	O
map	O
for	O
VQA-2	B-DatasetName
,	O
it	O
is	O
not	O
surprising	O
that	O
the	O
active	O
learning	O
acquisitions	O
follow	O
a	O
similar	O
trend	O
,	O
preferring	O
to	O
select	O
"	O
hard	O
-	O
to	O
-	O
learn	O
"	O
examples	O
.	O

We	O
are	O
also	O
grateful	O
to	O
Hao	O
Tan	O
for	O
providing	O
us	O
with	O
the	O
LXMERT	B-MethodName
checkpoint	O
trained	O
without	O
access	O
to	O
VQA	B-TaskName
datasets	O
,	O
as	O
well	O
as	O
for	O
general	O
LXMERT	B-MethodName
fine	O
-	O
tuning	O
pointers	O
.	O

In	O
this	O
work	O
,	O
we	O
explore	O
the	O
method	O
of	O
employing	O
contrastive	O
learning	O
to	O
improve	O
the	O
text	O
representation	O
from	O
the	O
BERT	B-MethodName
model	O
for	O
relation	B-TaskName
extraction	I-TaskName
.	O

The	O
key	O
knob	O
of	O
our	O
framework	O
is	O
a	O
unique	O
contrastive	O
pre	O
-	O
training	O
step	O
tailored	O
for	O
the	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
by	O
seamlessly	O
integrating	O
linguistic	O
knowledge	O
into	O
the	O
data	O
augmentation	O
.	O

Furthermore	O
,	O
we	O
investigate	O
how	O
large	O
-	O
scale	O
data	O
constructed	O
from	O
the	O
external	O
knowledge	O
bases	O
can	O
enhance	O
the	O
generality	O
of	O
contrastive	O
pre	O
-	O
training	O
of	O
BERT	B-MethodName
.	O

The	O
experimental	O
results	O
on	O
three	O
relation	O
extraction	O
benchmark	O
datasets	O
demonstrate	O
that	O
our	O
method	O
can	O
improve	O
the	O
BERT	B-MethodName
model	O
representation	O
and	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O

In	O
addition	O
,	O
we	O
explore	O
the	O
interpretability	O
of	O
models	O
by	O
showing	O
that	O
BERT	B-MethodName
with	O
contrastive	O
pre	O
-	O
training	O
relies	O
more	O
on	O
rationales	O
for	O
prediction	O
.	O

Contrastive	O
learning	O
can	O
encode	O
general	O
properties	O
(	O
e.g.	O
invariance	O
)	O
in	O
the	O
learned	O
representation	O
while	O
it	O
is	O
relatively	O
hard	O
for	O
other	O
representation	O
learning	O
methods	O
to	O
achieve	O
(	O
Bengio	O
et	O
al	O
.	O
,	O

2020).Despite	O
its	O
advancement	O
,	O
contrastive	O
learning	O
has	O
not	O
been	O
well	O
studied	O
in	O
biomedical	O
natural	O
language	O
processing	O
(	O
BioNLP	O
)	O
,	O
especially	O
for	O
relation	B-TaskName
extraction	I-TaskName
(	O
RE	B-TaskName
)	O
tasks	O
.	O

Compared	O
to	O
computer	O
vision	O
,	O
it	O
is	O
more	O
challenging	O
to	O
design	O
a	O
general	O
and	O
efficient	O
data	O
augmentation	O
method	O
to	O
construct	O
positive	O
pairs	O
.	O

2019	O
)	O
.	O

Therefore	O
,	O
leveraging	O
contrastive	O
learning	O
in	O
the	O
large	O
pre	O
-	O
trained	O
language	O
models	O
to	O
learn	O
more	O
general	O
representation	O
for	O
RE	B-TaskName
tasks	O
remains	O
unexplored	O
.	O

To	O
bridge	O
this	O
gap	O
,	O
this	O
paper	O
presents	O
an	O
innovative	O
method	O
of	O
contrastive	O
pre	O
-	O
training	O
to	O
improve	O
the	O
language	O
model	O
representation	O
for	O
biomedical	B-TaskName
relation	I-TaskName
extraction	I-TaskName
.	O

As	O
the	O
main	O
difference	O
from	O
the	O
existing	O
contrastive	O
learning	O
framework	O
,	O
we	O
augment	O
the	O
datasets	O
for	O
RE	B-TaskName
tasks	O
by	O
randomly	O
changing	O
the	O
words	O
that	O
do	O
not	O
affect	O
the	O
relation	O
expression	O
.	O

We	O
hence	O
keep	O
words	O
on	O
SDP	O
fixed	O
during	O
the	O
data	O
augmentation	O
.	O

2016).To	O
verify	O
the	O
effectiveness	O
of	O
the	O
proposed	O
method	O
,	O
we	O
use	O
the	O
transformer	O
-	O
based	O
BERT	B-MethodName
model	O
as	O
a	O
backbone	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
evaluate	O
our	O
method	O
on	O
three	O
widely	O
studied	O
RE	B-TaskName
tasks	O
in	O
the	O
biomedical	O
domain	O
:	O
the	O
chemical	B-TaskName
-	I-TaskName
protein	I-TaskName
interactions	I-TaskName
(	O
ChemProt	B-TaskName
)	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
the	O
drug	B-TaskName
-	I-TaskName
drug	I-TaskName
interactions	I-TaskName
(	O
DDI	B-TaskName
)	O
(	O
Herrero	O
-	O
Zazo	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
and	O
the	O
protein	B-TaskName
-	I-TaskName
protein	I-TaskName
interactions	I-TaskName
(	O
PPI	B-TaskName
)	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2008	O
)	O
.	O

The	O
experimental	O
results	O
show	O
that	O
our	O
method	O
boosts	O
the	O
BERT	B-MethodName
model	O
performance	O
and	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
three	O
tasks	O
.	O

Interest	O
has	O
also	O
grown	O
in	O
designing	O
interpretable	O
BioNLP	O
models	O
that	O
are	O
both	O
plausible	O
(	O
accurate	O
)	O
and	O
rely	O
on	O
a	O
specific	O
part	O
of	O
the	O
input	O
(	O
faithful	O
rationales	O
)	O
(	O
DeYoung	O
et	O
al	O
.	O
,	O

1	O
)	O
We	O
propose	O
a	O
new	O
method	O
that	O
utilizes	O
contrastive	O
learning	O
to	O
improve	O
the	O
BERT	B-MethodName
model	O
on	O
biomedical	B-TaskName
relation	I-TaskName
extraction	I-TaskName
tasks	O
.	O
(	O

com	O
/	O
udel	O
-	O
biotm	O
-	O
lab	O
/	O
BERT	O
-	O
CLRE	O
.	O

The	O
contrastive	O
representation	O
has	O
all	O
the	O
properties	O
that	O
a	O
good	O
representation	O
should	O
have	O
:	O
1	O
)	O
Distributed	O
property	O
;	O
2	O
)	O
Abstraction	O
and	O
invariant	O
property	O
;	O
3	O
)	O
Disentangled	O
representation	O
(	O
Bengio	O
et	O
al	O
.	O
,	O

2020	O
)	O
propose	O
a	O
pre	O
-	O
trained	O
language	O
representation	O
model	O
(	O
CERT	B-MethodName
)	O
using	O
contrastive	O
learning	O
at	O
the	O
sentence	O
level	O
to	O
benefit	O
the	O
language	O
understanding	O
tasks	O
.	O

2020	O
)	O
propose	O
a	O
self	O
-	O
supervised	O
pretraining	O
framework	O
for	O
relation	O
extraction	O
to	O
explore	O
the	O
encoded	O
information	O
for	O
the	O
textual	O
context	O
and	O
entity	O
type	O
.	O

Relation	O
extraction	O
is	O
usually	O
seen	O
as	O
a	O
classification	O
problem	O
when	O
the	O
entity	O
mentions	O
are	O
given	O
in	O
the	O
text	O
.	O

Many	O
different	O
methods	O
have	O
been	O
proposed	O
to	O
solve	O
the	O
relation	B-TaskName
extraction	I-TaskName
problem	O
(	O
Culotta	O
and	O
Sorensen	O
,	O
2004;Sierra	O
et	O
al	O
.	O
,	O

2019;Radford	O
et	O
al	O
.	O
,	O

Among	O
all	O
the	O
language	O
models	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Several	O
BERT	B-MethodName
models	O
have	O
been	O
adapted	O
for	O
biomedical	O
domain	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
SciBERT	B-MethodName
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
Blue	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Peng	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

BioBERT	B-MethodName
,	O
SciBERT	B-MethodName
and	O
BlueBERT	B-MethodName
are	O
pre	O
-	O
trained	O
based	O
on	O
the	O
general	O
-	O
domain	O
BERT	B-MethodName
using	O
different	O
pre	O
-	O
training	O
data	O
.	O

In	O
contrast	O
,	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

employed	O
easy	O
data	O
augmentation	O
techniques	O
to	O
improve	O
model	O
performance	O
on	O
text	O
classification	O
tasks	O
.	O

As	O
the	O
preliminary	O
study	O
,	O
we	O
experiment	O
with	O
three	O
techniques	O
to	O
randomly	O
replace	O
the	O
tokens	O
to	O
generate	O
the	O
augmented	O
data	O
and	O
choose	O
the	O
best	O
one	O
for	O
our	O
contrastive	O
learning	O
method	O
:	O
1	O
)	O
Synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
,	O
2	O
)	O
Random	B-MethodName
swap	I-MethodName
(	O
RS	B-MethodName
)	O
,	O
and	O
3	O
)	O
Random	B-MethodName
deletion	I-MethodName
(	O
RD).Table	B-MethodName
1	O
gives	O
some	O
samples	O
after	O
applying	O
the	O
three	O
operations	O
on	O
a	O
sentence	O
from	O
the	O
PPI	B-TaskName
task	O
.	O

For	O
the	O
synonym	B-MethodName
replacement	I-MethodName
,	O
we	O
randomly	O
replace	O
n	O
words	O
with	O
their	O
synonyms	O
.	O

For	O
the	O
random	O
deletion	O
,	O
we	O
delete	O
some	O
words	O
with	O
the	O
probability	B-HyperparameterName
p.	I-HyperparameterName
The	O
probability	B-HyperparameterName
p	I-HyperparameterName
is	O
set	O
to	O
0.1	B-HyperparameterValue
in	O
our	O
experiments	O
and	O
the	O
parameter	O
n	O
for	O
SR	B-MethodName
and	O
RS	B-MethodName
is	O
calculated	O
by	O
p	O
×	O
l	O
,	O
where	O
l	O
is	O
the	O
length	O
of	O
the	O
sentence	O
.	O

To	O
examine	O
which	O
operation	O
performs	O
better	O
for	O
relation	B-TaskName
extraction	I-TaskName
tasks	O
,	O
we	O
train	O
three	O
BERT	B-MethodName
models	O
using	O
the	O
three	O
types	O
of	O
augmented	O
data	O
(	O
combined	O
with	O
the	O
original	O
training	O
data	O
)	O
.	O

Table	O
4	O
shows	O
that	O
the	O
synonym	B-TaskName
replacement	I-TaskName
(	O
SR	B-TaskName
)	O
operation	O
achieves	O
the	O
best	O
performance	O
on	O
all	O
three	O
tasks	O
and	O
we	O
will	O
employ	O
this	O
operation	O
in	O
our	O
data	O
augmentation	O
module	O
in	O
our	O
contrastive	O
learning	O
experiments	O
(	O
We	O
will	O
further	O
discuss	O
it	O
in	O
Section	O
5.2	O
)	O
.	O

In	O
this	O
work	O
,	O
we	O
employ	O
the	O
BERT	B-MethodName
model	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

As	O
demonstrated	O
in	O
(	O
Chen	O
et	O
al	O
.	O
,	O

We	O
follow	O
the	O
work	O
of	O
(	O
Chen	O
et	O
al	O
.	O
,	O

Therefore	O
,	O
we	O
have	O
2N	O
views	O
from	O
the	O
batch	O
.	O

Please	O
see	O
Algorithm	O
1	O
for	O
calculating	O
the	O
contrastive	O
loss	O
in	O
one	O
batch	O
.	O

Then	O
we	O
can	O
update	O
the	O
parameters	O
of	O
the	O
BERT	B-MethodName
model	O
and	O
projection	O
head	O
g	O
to	O
minimize	O
the	O
loss	O
L.	O
Input	O
:	O
encoder	O
f	O
(	O
BERT	B-MethodName
)	O
,	O
project	O
head	O
g	O
,	O
data	O
augmentation	O
module	O
,	O
data	O
batch	O
{	O
s	O
k	O
}	O
N	O
k=1	O
;	O
for	O
k=1	O
,	O
...	O
,	O
N	O
do	O
v	O
,	O
v	O
=	O
data_augment(s	O
k	O
)	O
;	O
z	O
2k−1	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
z	O
2k	O
=	O
g(f	O
(	O
v	O
)	O
)	O
;	O
end	O
L	O
=	O
1	O
2N	O
N	O
k=1	O
[	O
l(z	O
2k−1	O
,	O
z	O
2k	O
)	O
+	O
l(z	O
2k	O
,	O
z	O
2k−1	O
)	O
]	O
Figure	O
2	O
shows	O
the	O
training	O
procedure	O
of	O
our	O
framework	O
.	O

It	O
consists	O
of	O
three	O
stages	O
.	O

First	O
,	O
we	O
pretrain	O
the	O
BERT	B-MethodName
model	O
on	O
a	O
large	O
amount	O
of	O
unlabeled	O
data	O
from	O
a	O
specific	O
domain(e.g	O
.	O
,	O

Second	O
,	O
we	O
conduct	O
contrastive	O
pretraining	O
on	O
task	O
-	O
specific	O
data	O
as	O
a	O
continual	O
pretraining	O
step	O
after	O
the	O
domain	O
pre	O
-	O
training	O
of	O
BERT	B-MethodName
model	O
.	O

In	O
this	O
way	O
,	O
we	O
retain	O
the	O
learned	O
knowledge	O
from	O
general	O
pre	O
-	O
training	O
,	O
and	O
add	O
the	O
new	O
features	O
from	O
contrastive	O
learning	O
.	O

Finally	O
,	O
we	O
finetune	O
the	O
model	O
on	O
the	O
RE	B-TaskName
tasks	O
to	O
further	O
gain	O
taskspecific	O
knowledge	O
through	O
supervised	O
training	O
on	O
the	O
labeled	O
datasets	O
.	O

The	O
domain	O
pre	O
-	O
training	O
stage	O
follows	O
that	O
of	O
the	O
BERT	B-MethodName
using	O
the	O
masked	O
language	O
model	O
and	O
next	O
sentence	O
prediction	O
technique	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019).In	O
our	O
experiments	O
,	O
we	O
use	O
two	O
pre	O
-	O
trained	O
versions	O
for	O
the	O
biomedical	O
domain	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

Formally	O
,	O
assuming	O
a	O
curated	O
database	O
for	O
relation	O
r	O
contains	O
all	O
the	O
relevant	O
entities	O
and	O
text	O
,	O
we	O
consider	O
every	O
combination	O
of	O
the	O
entity	O
pairs	O
in	O
one	O
sentence	O
and	O
use	O
them	O
as	O
examples	O
for	O
this	O
relation	O
.	O

2006	O
)	O
are	O
utilized	O
for	O
DDI	B-TaskName
and	O
ChemProt	B-TaskName
,	O
respectively	O
.	O

As	O
discussed	O
before	O
,	O
we	O
will	O
utilize	O
the	O
BERT	B-MethodName
model	O
as	O
the	O
encoder	O
for	O
the	O
inputs	O
.	O

In	O
particular	O
,	O
we	O
will	O
employ	O
two	O
BERT	B-MethodName
models	O
pre	O
-	O
trained	O
for	O
the	O
biomedical	O
domain	O
in	O
our	O
experiments	O
:	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PubMedBERT	B-MethodName
(	O
Gu	O
et	O
al	O
.	O
,	O

The	O
statistics	O
of	O
these	O
datasets	O
is	O
shown	O
in	O
Table	O
2	O
.	O

For	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
tasks	O
,	O
we	O
employ	O
the	O
corpora	O
in	O
(	O
Krallinger	O
et	O
al	O
.	O
,	O

2013	O
)	O
PubMedBERT	B-MethodName
model	O
(	O
Gu	O
et	O
al	O
.	O
,	O

We	O
utilize	O
the	O
AIMed	B-DatasetName
corpus	O
for	O
the	O
PPI	B-TaskName
task	O
,	O
and	O
we	O
will	O
employ	O
10	B-HyperparameterValue
-	O
fold	O
cross	O
-	O
validation	O
on	O
it	O
since	O
there	O
is	O
no	O
standard	O
split	O
of	O
training	O
and	O
test	O
.	O

PPI	B-TaskName
is	O
a	O
binary	O
classification	O
problem	O
,	O
and	O
we	O
will	O
use	O
the	O
standard	O
precision	B-MetricName
(	O
P	B-MetricName
)	O
,	O
recall	B-MetricName
(	O
R	B-MetricName
)	O
and	O
F1	B-MetricName
-	I-MetricName
score	I-MetricName
(	O
F	B-MetricName
)	O
to	O
measure	O
the	O
model	O
performance	O
.	O

However	O
,	O
the	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
tasks	O
are	O
multiclass	O
classification	O
problems	O
.	O

The	O
ChemProt	B-DatasetName
corpus	O
is	O
labeled	O
with	O
five	O
positive	O
classes	O
and	O
the	O
negative	O
class	O
:	O
CPR:3	O
,	O
CPR:4	O
,	O
CPR:5	O
,	O
CPR:6	O
,	O
CPR:9	O
and	O
negative	O
.	O

Similar	O
to	O
the	O
DDI	B-DatasetName
corpus	O
,	O
there	O
are	O
four	O
positive	O
labels	O
and	O
one	O
negative	O
label	O
:	O
AD	O
-	O
VICE	O
,	O
EFFECT	O
,	O
INT	O
,	O
MECHANISM	O
and	O
negative	O
.	O

The	O
models	O
for	O
ChemProt	B-TaskName
and	O
DDI	B-TaskName
will	O
be	O
evaluated	O
utilizing	O
micro	B-MetricName
precision	I-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
score	I-MetricName
on	O
the	O
non	O
-	O
negative	O
classes	O
.	O

One	O
instance	O
of	O
relation	B-TaskName
extraction	I-TaskName
task	O
contains	O
two	O
parts	O
:	O
the	O
text	O
and	O
the	O
entity	O
mentions	O
.	O

In	O
order	O
to	O
make	O
the	O
BERT	B-MethodName
model	O
identify	O
the	O
positions	O
of	O
the	O
entities	O
,	O
we	O
replace	O
the	O
relevant	O
entity	O
names	O
with	O
predefined	O
tags	O
by	O
following	O
the	O
standard	O
pre	O
-	O
processing	O
step	O
for	O
relation	O
extraction	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

For	O
the	O
fine	O
-	O
tuning	O
of	O
the	O
BioBERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
16	B-HyperparameterValue
,	O
training	B-HyperparameterName
epoch	I-HyperparameterName
of	O
10	B-HyperparameterValue
,	O
and	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
128.During	B-HyperparameterValue
the	O
fine	O
-	O
tuning	O
of	O
PubMedBERT	B-MethodName
models	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
8	B-HyperparameterValue
,	O
training	B-HyperparameterName
epoch	I-HyperparameterName
of	O
10	B-HyperparameterValue
and	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
256	B-HyperparameterValue
are	O
utilized	O
.	O

In	O
the	O
contrastive	O
pre	O
-	O
training	O
step	O
of	O
the	O
BERT	B-MethodName
models	O
,	O
we	O
use	O
the	O
same	O
learning	B-HyperparameterName
rate	I-HyperparameterName
with	O
the	O
fine	O
-	O
tuning	O
,	O
and	O
the	O
training	B-HyperparameterName
epoch	I-HyperparameterName
is	O
selected	O
from	O
[	O
2,4,6,8,10	B-HyperparameterValue
]	O
based	O
on	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O

If	O
there	O
is	O
no	O
development	O
set	O
(	O
e.g.	O
,	O
PPI	B-TaskName
task	O
)	O
,	O
we	O
will	O
use	O
6	B-HyperparameterValue
as	O
the	O
default	O
training	O
epoch	B-HyperparameterName
.	O

2020	O
)	O
,	O
we	O
utilize	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
256	B-HyperparameterValue
and	O
128	B-HyperparameterValue
for	O
BioBERT	B-MethodName
and	O
Pub	B-MethodName
-	I-MethodName
MedBERT	I-MethodName
respectively	O
.	O

In	O
addition	O
,	O
the	O
temperature	B-HyperparameterName
parameter	I-HyperparameterName
τ	I-HyperparameterName
is	O
set	O
to	O
0.1	B-HyperparameterValue
during	O
the	O
training	O
.	O

5.1	O
BERT	B-MethodName
model	O
performance	O
with	O
contrastive	O
pre	O
-	O
training	O
ever	O
,	O
contrastive	O
pre	O
-	O
training	O
on	O
human	O
-	O
labeled	O
dataset	O
only	O
improves	O
the	O
model	O
with	O
a	O
small	O
margin	O
.	O

Compared	O
with	O
the	O
BERT	B-MethodName
models	O
without	O
contrastive	O
pre	O
-	O
training	O
,	O
we	O
observe	O
an	O
averaged	O
F1	B-MetricName
score	I-MetricName
improvement	O
(	O
on	O
the	O
two	O
BERT	B-MethodName
models	O
)	O
of	O
1.2	B-MetricValue
%	I-MetricValue
,	O
1.2	B-MetricValue
%	I-MetricValue
,	O
and	O
0.85	B-MetricValue
%	I-MetricValue
on	O
ChemProt	B-DatasetName
,	O
DDI	B-DatasetName
,	O
and	O
PPI	B-DatasetName
datasets	O
,	O
respectively	O
.	O

Since	O
PubMedBERT	B-MethodName
is	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
(	O
SOTA	O
)	O
model	O
on	O
these	O
three	O
tasks	O
,	O
we	O
further	O
improve	O
its	O
performance	O
by	O
adding	O
contrastive	O
pretraining	O
.	O

Table	O
4	O
shows	O
the	O
BERT	B-MethodName
model	O
performance	O
after	O
including	O
three	O
types	O
of	O
augmented	O
data	O
.	O

We	O
can	O
see	O
that	O
the	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
operation	O
yields	O
the	O
best	O
results	O
on	O
all	O
three	O
tasks	O
.	O

We	O
also	O
notice	O
that	O
the	O
augmented	O
data	O
from	O
the	O
random	B-MethodName
swap	I-MethodName
(	O
RS	B-MethodName
)	O
operation	O
hurt	O
the	O
model	O
performance	O
on	O
the	O
DDI	B-TaskName
and	O
PPI	B-TaskName
tasks	O
,	O
which	O
indicates	O
that	O
this	O
operation	O
might	O
change	O
the	O
relation	O
expression	O
in	O
the	O
sentence	O
.	O

Therefore	O
,	O
the	O
model	O
should	O
make	O
its	O
predictions	O
based	O
on	O
them	O
.	O

In	O
this	O
work	O
,	O
we	O
define	O
a	O
new	O
metric	O
to	O
measure	O
the	O
faithfulness	O
of	O
the	O
rationales	O
:	O
"	O
prediction	B-MetricName
shift	I-MetricName
"	O
.	O

If	O
the	O
model	O
predicts	O
one	O
test	O
example	O
(	O
nonnegative	O
)	O
with	O
label	O
L	O
t	O
,	O
but	O
changes	O
its	O
prediction	O
on	O
its	O
neighbor	O
(	O
the	O
augmented	O
data	O
point	O
)	O
with	O
another	O
label	O
L	O
t	O
,	O
we	O
will	O
say	O
a	O
"	O
prediction	O
shift	O
"	O
happens	O
(	O
In	O
Table	O
5	O
,	O
we	O
give	O
two	O
examples	O
of	O
pre	O
-	O
diction	O
shift	O
on	O
PubMedBERT	B-MethodName
model	O
)	O
.	O

To	O
generate	O
a	O
similar	O
set	O
(	O
with	O
test	O
set	O
)	O
for	O
the	O
measurement	O
of	O
"	O
prediction	O
shift	O
"	O
,	O
we	O
apply	O
the	O
same	O
synonym	B-MethodName
replacement	I-MethodName
(	O
SR	B-MethodName
)	O
technique	O
on	O
the	O
original	O
test	O
data	O
.	O

We	O
compare	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
on	O
two	O
types	O
of	O
BERT	B-MethodName
model	O
:	O
the	O
original	O
BERT	B-MethodName
and	O
the	O
BERT	B-MethodName
model	O
with	O
contrastive	O
pre	O
-	O
training	O
.	O

Table	O
6	O
illustrates	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
dramatically	O
reduce	O
the	O
number	O
of	O
"	O
prediction	O
shift	O
"	O
.	O

Those	O
results	O
indicate	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
rely	O
more	O
on	O
the	O
information	O
of	O
shortest	O
dependency	O
path	O
for	O
prediction	O
,	O
a.k.a	O
.	O
,	O

From	O
another	O
perspective	O
,	O
the	O
results	O
in	O
Table	O
6	O
also	O
demonstrate	O
that	O
the	O
BERT	B-MethodName
models	O
with	O
contrastive	O
pre	O
-	O
training	O
are	O
resilient	O
to	O
small	O
changes	O
of	O
the	O
inputs	O
,	O
which	O
means	O
the	O
models	O
are	O
more	O
robust	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
contrastive	O
pre	O
-	O
training	O
method	O
to	O
improve	O
the	O
text	O
representation	O
of	O
the	O
BERT	B-MethodName
model	O
.	O

The	O
experimental	O
results	O
demonstrate	O
that	O
our	O
method	O
outperforms	O
the	O
original	O
BERT	B-MethodName
model	O
on	O
three	O
relation	O
extraction	O
benchmarks	O
.	O

Additionally	O
,	O
our	O
method	O
shows	O
robustness	O
to	O
slightly	O
changed	O
inputs	O
over	O
the	O
BERT	B-MethodName
models	O
.	O

To	O
study	B-DatasetName
customer	I-DatasetName
service	I-DatasetName
dialogue	I-DatasetName
systems	I-DatasetName
in	I-DatasetName
more	I-DatasetName
realistic	I-DatasetName
settings	I-DatasetName
,	O
we	O
introduce	O
the	B-DatasetName
Action	I-DatasetName
-	I-DatasetName
Based	I-DatasetName
Conversations	I-DatasetName
Dataset	O
(	O
ABCD	B-DatasetName
)	O
,	O
a	O
fully	O
-	O
labeled	O
dataset	O
with	O
over	O
10	O
K	O
human	O
-	O
to	O
-	O
human	O
dialogues	O
containing	O
55	O
distinct	O
user	O
intents	O
requiring	O
unique	O
sequences	O
of	O
actions	O
constrained	O
by	O
policies	O
to	O
achieve	O
task	O
success	O
.	O

We	O
propose	O
two	O
additional	O
dialog	O
tasks	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
and	O
establish	O
a	O
series	O
of	O
baselines	O
involving	O
large	O
-	O
scale	O
,	O
pre	O
-	O
trained	O
language	O
models	O
on	O
this	O
dataset	O
.	O

Empirical	O
results	O
demonstrate	O
that	O
while	O
more	O
sophisticated	O
networks	O
outperform	O
simpler	O
models	O
,	O
a	O
considerable	O
gap	O
(	O
50.8	B-MetricValue
%	I-MetricValue
absolute	B-MetricName
accuracy	I-MetricName
)	O
still	O
exists	O
to	O
reach	O
human	O
-	O
level	O
performance	O
on	O
ABCD	O
.	O

2019;Rastogi	O
et	O
al	O
.	O
,	O

Figure	O
1	O
:	O
An	O
interaction	O
from	O
ABCD	B-DatasetName
(	O
left	O
)	O
starts	O
with	O
the	O
customer	O
receiving	O
a	O
prompt	O
(	O
top	O
right	O
)	O
to	O
ground	O
the	O
dialogue	O
.	O

See	O
Figure	O
1)To	O
more	O
closely	O
model	O
real	O
customer	O
service	O
agents	O
,	O
we	O
present	O
the	O
Action	B-DatasetName
-	I-DatasetName
Based	I-DatasetName
Conversations	I-DatasetName
Dataset	O
(	O
ABCD	B-DatasetName
)	O
consisting	O
of	O
10,042	O
conversations	O
containing	O
numerous	O
actions	O
with	O
precise	O
procedural	O
requirements	O
.	O

Thus	O
,	O
the	O
major	O
difference	O
between	O
ABCD	O
and	O
other	O
dialogue	O
datasets	O
,	O
such	O
as	O
Mul	B-DatasetName
-	I-DatasetName
tiWOZ	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

While	O
the	O
prevalent	O
data	O
collection	O
paradigm	O
involves	O
Wizard	O
-	O
of	O
-	O
Oz	O
techniques	O
,	O
our	O
situation	O
containing	O
asymmetric	O
speakers	O
compelled	O
the	O
design	B-TaskName
of	I-TaskName
a	I-TaskName
novel	I-TaskName
Expert	I-TaskName
Live	I-TaskName
Chat	I-TaskName
system	I-TaskName
.	O

Based	O
on	O
the	O
unique	O
aspects	O
of	O
ABCD	B-DatasetName
,	O
we	O
propose	O
two	O
new	O
tasks	O
.	O

To	O
start	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
(	O
AST	B-MetricName
)	O
closely	O
mirrors	O
the	O
format	O
of	O
Dialogue	O
State	O
Tracking	O
where	O
the	O
user	O
intent	O
is	O
inferred	O
from	O
the	O
dialogue	O
history	O
.	O

AST	B-MetricName
then	O
differs	O
since	O
the	O
correct	O
state	O
must	O
also	O
be	O
reconciled	O
with	O
the	O
requirements	O
outlined	O
in	O
the	O
Agent	O
Guidelines	O
.	O

As	O
a	O
second	O
task	O
,	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
(	O
CDS	B-MetricName
)	O
extends	O
this	O
notion	O
across	O
the	O
entire	O
conversation	O
.	O

Experiments	O
show	O
that	O
in	O
addition	O
to	O
conversation	O
history	O
,	O
conditioning	O
on	O
the	O
Agent	O
Guidelines	O
further	O
boosts	O
performance	O
,	O
with	O
top	O
models	O
relying	O
on	O
both	O
aspects	O
to	O
reach	O
31.9	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
.	O

Lastly	O
,	O
human	O
evaluation	O
reaches	O
82.7	B-MetricValue
%	I-MetricValue
,	O
demonstrating	O
ample	O
room	O
for	O
future	O
improvement	O
.	O

2	O
)	O
We	O
establish	O
a	O
new	O
technique	O
called	O
Expert	B-MethodName
Live	I-MethodName
Chat	I-MethodName
for	O
capturing	O
natural	O
dialogue	O
between	O
two	O
unequal	O
interlocutors	O
.	O
(	O

3	O
)	O
We	O
propose	O
two	O
metrics	O
,	O
Action	B-MetricName
State	I-MetricName
Tracking	I-MetricName
and	O
Cascading	B-MetricName
Dialogue	I-MetricName
Success	I-MetricName
,	O
for	O
measuring	O
dialogue	O
comprehension	O
with	O
policy	O
constraints	O
.	O

Unlike	O
opendomain	O
chatbots	O
often	O
built	O
for	O
entertainment	O
,	O
task	O
-	O
oriented	O
dialogue	O
systems	O
trained	O
on	O
such	O
datasets	O
are	O
intended	O
for	O
solving	O
user	O
issues	O
.	O

Rather	O
than	O
expanding	O
wider	O
,	O
ABCD	B-DatasetName
instead	O
focuses	O
deeper	O
by	O
increasing	O
the	O
count	O
and	O
diversity	O
of	O
actions	O
within	O
a	O
single	O
domain	O
.	O

All	O
actions	O
are	O
also	O
shown	O
.	O

The	O
closest	O
prior	O
work	O
to	O
ABCD	B-DatasetName
is	O
the	O
Schema	B-DatasetName
Guided	I-DatasetName
Dialogue	I-DatasetName
(	O
SGD	B-DatasetName
)	O
dataset	O
,	O
which	O
contains	O
dozens	O
of	O
API	O
calls	O
that	O
can	O
be	O
interpreted	O
as	O
individual	O
actions	O
sending	O
commands	O
to	O
a	O
SQL	O
engine	O
(	O
Rastogi	O
et	O
al	O
.	O
,	O

The	O
action	O
restrictions	O
within	O
ABCD	B-DatasetName
are	O
made	O
explicit	O
by	O
the	O
Agent	O
Guidelines	O
manual	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
task	O
setting	O
of	O
ABCD	B-DatasetName
by	O
following	O
along	O
with	O
the	O
example	O
dialog	O
shown	O
in	O
Figure	O
1	O
.	O

Accordingly	O
,	O
customers	O
within	O
ABCD	B-DatasetName
remain	O
oblivious	O
towards	O
what	O
values	O
apply	O
to	O
which	O
actions	O
,	O
nor	O
are	O
they	O
aware	O
that	O
actions	O
exist	O
in	O
first	O
place	O
.	O

ABCD	B-DatasetName
then	O
diverges	O
as	O
the	O
next	O
step	O
involves	O
interpreting	O
the	O
Agent	O
Guidelines	O
,	O
a	O
document	O
representing	O
the	O
internal	O
policies	O
of	O
a	O
company	O
in	O
the	O
online	O
retail	O
domain	O
(	O
See	O
Table	O
1	O
)	O
.	O

While	O
identifying	O
a	O
subflow	O
may	O
seem	O
straightforward	O
,	O
information	O
asymmetry	O
prevents	O
the	O
customers	O
from	O
directly	O
revealing	O
the	O
name	O
of	O
their	O
intent	O
.	O

In	O
our	O
case	O
,	O
the	O
agent	O
eventually	O
figures	O
out	O
the	O
correct	O
subflow	O
and	O
begins	O
to	O
execute	O
actions	O
,	O
which	O
consists	O
of	O
recording	O
values	O
given	O
by	O
the	O
customer	O
,	O
namely	O
the	O
customer	O
's	O
full	O
name	O
or	O
account	O
ID	O
in	O
order	O
to	O
[	O
Pull	O
up	O
Account	O
]	O
.	O

Dialogue	O
success	O
demands	O
that	O
agents	O
execute	O
a	O
chain	O
of	O
such	O
actions	O
in	O
the	O
right	O
order	O
with	O
the	O
right	O
values	O
,	O
while	O
simultaneously	O
engaging	O
the	O
customer	O
in	O
natural	O
language	O
conversation	O
.	O

To	O
start	O
,	O
the	O
permitted	O
actions	O
in	O
a	O
given	O
state	O
are	O
determined	O
not	O
only	O
by	O
Agent	O
Guidelines	O
,	O
but	O
also	O
by	O
the	O
user	O
's	O
desire	O
,	O
which	O
may	O
be	O
in	O
conflict	O
.	O

Keeping	O
the	O
bar	O
high	O
,	O
we	O
set	O
a	O
minimum	O
threshold	O
of	O
80	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
of	O
the	O
quiz	O
which	O
resulted	O
in	O
a	O
low	O
20	O
%	O
pass	O
rate	O
.	O

After	O
passing	O
the	O
exam	O
,	O
we	O
offered	O
the	O
answer	O
key	O
to	O
agents	O
which	O
further	O
improved	O
understanding	O
.	O

Rather	O
than	O
utilizing	O
Wizard	O
-	O
of	O
-	O
Oz	O
techniques	O
(	O
such	O
as	O
in	O
MultiWOZ	B-DatasetName
)	O
,	O
we	O
developed	O
Expert	O
Live	O
Chat	O
which	O
contains	O
three	O
unique	O
aspects:(1	O
)	O
Conversations	O
are	O
conducted	O
continuously	O
in	O
real	O
-	O
time	O
.	O
(	O

2	O
)	O
Users	O
involved	O
are	O
not	O
interchangeable	O
.	O
(	O

2018	O
)	O
to	O
produce	O
conversations	O
.	O

Despite	O
the	O
time	O
-	O
consuming	O
nature	O
,	O
some	O
datasets	O
have	O
produced	O
synchronous	O
dialogues	O
between	O
two	O
humans	O
(	O
Lewis	O
et	O
al	O
.	O
,	O

However	O
,	O
the	O
skill	O
sets	O
of	O
ABCD	B-DatasetName
workers	O
are	O
notably	O
unequal	O
,	O
exacerbating	O
the	O
matching	O
problem	O
.	O

With	O
these	O
changes	O
,	O
we	O
successfully	O
increased	O
the	O
pairing	O
rate	O
from	O
18	O
out	O
of	O
80	O
active	O
users	O
up	O
to	O
72	O
out	O
of	O
83	O
,	O
an	O
increase	O
of	O
nearly	O
400	O
%	O
,	O
while	O
maintaining	O
wait	B-HyperparameterName
times	I-HyperparameterName
under	O
10	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

In	O
particular	O
,	O
we	O
observed	O
the	O
greatest	O
gains	O
by	O
grounding	O
the	O
conversation	O
to	O
the	O
relatable	O
scenario	O
of	O
online	O
shopping	O
,	O
which	O
provided	O
immediate	O
context	O
to	O
participants	O
without	O
requiring	O
any	O
extra	O
training	O
.	O

We	O
validate	O
all	O
dialogues	O
to	O
pass	O
quality	O
thresholds	O
such	O
as	O
including	O
a	O
minimum	O
number	O
of	O
actions	O
and	O
avoiding	O
copy	O
/	O
paste	O
behavior	O
.	O

Unsurprisingly	O
,	O
ABCD	B-DatasetName
includes	O
more	O
actions	O
per	O
dialogue	O
than	O
other	O
datasets	O
,	O
by	O
at	O
least	O
a	O
factor	O
of	O
two	O
.	O

ABCD	B-DatasetName
also	O
contains	O
a	O
lower	O
absolute	O
number	O
of	O
tokens	O
,	O
but	O
also	O
has	O
the	O
highest	O
variance	O
in	O
the	O
number	O
of	O
tokens	O
per	O
turn	O
.	O
(	O

See	O
Table	O
2.)Since	O
each	O
subflow	O
represents	O
a	O
unique	O
customer	O
intent	O
,	O
ABCD	B-DatasetName
contains	O
55	O
user	O
intents	O
evenly	O
distributed	O
through	O
the	O
dataset	O
.	O

By	O
interpreting	O
buttons	O
as	O
domains	O
,	O
the	O
dataset	O
contains	O
30	O
domains	O
and	O
231	O
associated	O
slots	O
,	O
compared	O
to	O
7	O
domains	O
and	O
24	O
slots	O
within	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

Furthermore	O
,	O
the	O
unconstrained	O
nature	O
of	O
Expert	O
Live	O
Chat	O
allows	O
users	O
to	O
chat	O
with	O
each	O
other	O
in	O
a	O
free	O
-	O
form	O
style	O
.	O

Dialogues	O
exhibited	O
normal	O
texting	O
behavior	O
such	O
as	O
users	O
speaking	O
for	O
many	O
turns	O
in	O
a	O
row	O
or	O
fixing	O
typos	O
with	O
a	O
star	O
in	O
the	O
subsequent	O
line	O
.	O

Other	O
examples	O
of	O
linguistic	O
phenomenon	O
can	O
be	O
observed	O
in	O
Table	O
5	O
.	O

The	O
novel	O
features	O
in	O
ABCD	B-DatasetName
brings	O
two	O
new	O
dialog	O
tasks	O
,	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
and	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

We	O
also	O
build	O
baseline	O
systems	O
that	O
are	O
variants	O
of	O
standard	O
dialogue	O
models	O
and	O
report	O
their	O
results	O
on	O
ABCD	B-DatasetName
.	O

Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
(	O
AST	B-TaskName
)	O
aims	O
at	O
detecting	O
the	O
pertinent	O
intent	O
by	O
interpreting	O
customer	O
utterances	O
while	O
taking	O
into	O
account	O
constraints	O
from	O
the	O
Agent	O
Guidelines	O
,	O
an	O
aspect	O
not	O
considered	O
in	O
traditional	O
dialog	B-TaskName
state	I-TaskName
tracking	I-TaskName
(	O
DST	B-TaskName
)	O
.	O

In	O
contrast	O
,	O
the	O
appropriate	O
next	O
step	O
within	O
AST	B-TaskName
is	O
governed	O
by	O
the	O
Agent	O
Guidelines	O
,	O
which	O
might	O
require	O
[	O
Verify	O
Identity	O
]	O
of	O
the	O
customer	O
first	O
,	O
or	O
any	O
number	O
of	O
other	O
actions	O
,	O
before	O
executing	O
the	O
password	O
reset	O
.	O

Despite	O
the	O
similar	O
structure	O
,	O
AST	B-TaskName
deviates	O
from	O
DST	B-TaskName
since	O
predicting	O
the	O
right	O
action	O
requires	O
not	O
only	O
parsing	O
the	O
customer	O
utterance	O
,	O
but	O
also	O
adhering	O
to	O
Agent	O
Guidelines	O
.	O

To	O
measure	O
a	O
model	O
's	O
ability	O
to	O
comprehend	O
such	O
nuanced	O
situations	O
,	O
we	O
adopt	O
overall	O
accuracy	O
as	O
the	O
evaluation	O
metric	O
for	O
AST	B-TaskName
.	O

Since	O
the	O
appropriate	O
action	O
often	O
depends	O
on	O
the	O
situation	O
,	O
we	O
propose	O
the	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
(	O
CDS	B-TaskName
)	O
task	O
to	O
measure	O
a	O
model	O
's	O
ability	O
to	O
understand	O
actions	O
in	O
context	O
.	O

Whereas	O
AST	O
assumes	O
an	O
action	O
occurs	O
in	O
the	O
current	O
turn	O
,	O
CDS	B-TaskName
gives	O
an	O
agent	O
the	O
additional	O
options	O
of	O
responding	O
with	O
an	O
utterance	O
or	O
ending	O
the	O
conversation	O
.	O

Formally	O
,	O
given	O
C	O
t	O
=	O
[	O
x	O
1	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O

Therefore	O
,	O
CDS	B-TaskName
is	O
scored	O
using	O
a	O
variation	O
on	O
Cascading	O
Evaluation	O
(	O
Suhr	O
et	O
al	O
.	O
,	O

We	O
also	O
run	O
several	O
baselines	O
on	O
these	O
new	O
tasks	O
.	O

We	O
break	O
down	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
(	O
AST	B-TaskName
)	O
into	O
two	O
sub	O
-	O
problems	O
,	O
button	O
-	O
slot	O
prediction	O
and	O
value	O
-	O
filling	O
.	O

The	O
gate	O
is	O
conditioned	O
on	O
the	O
hidden	O
state	O
h	O
enc	O
as	O
well	O
as	O
a	O
learned	O
context	O
vector	O
c	O
i	O
.	O

When	O
the	O
next	O
step	O
is	O
to	O
take	O
action	O
,	O
the	O
AST	B-TaskName
model	O
is	O
reused	O
to	O
determine	O
the	O
button	O
-	O
slot	O
and	O
value	O
.	O

We	O
performed	O
experiments	O
on	O
the	O
two	O
newly	O
proposed	O
tasks	O
,	O
AST	B-TaskName
and	O
CDS	B-TaskName
.	O

AST	B-TaskName
consists	O
of	O
two	O
subtasks	O
,	O
button	O
-	O
slot	O
prediction	O
and	O
value	O
-	O
filling	O
,	O
while	O
CDS	O
builds	O
on	O
this	O
with	O
three	O
additional	O
subtasks	O
of	O
next	O
step	O
selection	O
,	O
utterance	O
ranking	O
,	O
and	O
intent	O
classification	O
.	O

For	O
both	O
tasks	O
,	O
we	O
experimented	O
with	O
two	O
types	O
of	O
frameworks	O
,	O
a	O
pipeline	O
version	O
and	O
an	O
end	O
-	O
to	O
-	O
end	O
version	O
.	O

2020).The	O
pipeline	O
model	O
uses	O
a	O
BERT	B-MethodName
model	O
trained	O
with	O
the	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
.	O

To	O
test	O
the	O
performance	O
of	O
different	O
pretrained	O
models	O
under	O
the	O
end	O
-	O
to	O
-	O
end	O
framework	O
,	O
we	O
experiment	O
with	O
three	O
additional	O
encoders	O
,	O
Al	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
RoBERTa	B-MethodName
-	I-MethodName
Large	I-MethodName
.	O

AlBERT	B-MethodName
model	O
has	O
an	O
inter	O
-	O
sentence	O
coherence	O
task	O
and	O
a	O
lighter	O
memory	O
footprint	O
compared	O
to	O
BERT	B-MethodName
,	O
while	O
RoBERTa	B-MethodName
model	O
has	O
substantially	O
more	O
data	O
and	O
hyper	O
-	O
parameter	O
tuning	O
in	O
pretraining	O
than	O
BERT.In	B-MethodName
the	O
future	O
,	O
we	O
also	O
plan	O
to	O
include	O
GPT	O
-	O
based	O
models	O
,	O
such	O
as	O
DialoGPT	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O

As	O
hinted	O
at	O
in	O
prior	O
works	O
(	O
Liang	O
et	O
al	O
.	O
,	O

In	O
the	O
AST	B-TaskName
task	O
,	O
we	O
found	O
steady	O
improvements	O
as	O
we	O
move	O
from	O
the	O
older	O
to	O
the	O
newer	O
models	O
with	O
vanilla	O
BERT	B-MethodName
at	O
59.5	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
and	O
RoBERTa	B-MethodName
doing	O
the	O
best	O
at	O
65.8	B-MetricValue
%	I-MetricValue
.	O

For	O
the	O
CDS	B-TaskName
task	O
,	O
we	O
found	O
a	O
similar	O
trend	O
where	O
RoBERTa	B-MethodName
-	I-MethodName
Large	I-MethodName
outperforms	O
BERT	B-MethodName
,	O
but	O
only	O
by	O
a	O
mere	O
0.6	B-MetricValue
%	I-MetricValue
.	O

Separately	O
,	O
we	O
evaluate	O
CDS	B-TaskName
subtask	O
difficulty	O
by	O
asking	O
human	O
volunteers	O
to	O
select	O
the	O
correct	O
label	O
from	O
a	O
list	O
of	O
possible	O
options	O
.	O

As	O
an	O
example	O
,	O
workers	O
would	O
be	O
presented	O
with	O
55	O
different	O
classes	O
for	O
Intent	B-TaskName
Classification	I-TaskName
and	O
asked	O
to	O
choose	O
the	O
right	O
one	O
.	O

On	O
the	O
other	O
hand	O
,	O
human	O
evaluation	O
for	O
the	O
overall	O
CDS	B-TaskName
task	O
was	O
judged	O
by	O
measuring	O
the	O
success	O
rate	O
in	O
a	O
standard	O
conversational	O
scenarios	O
where	O
behavioral	O
instincts	O
are	O
activated	O
,	O
so	O
humans	O
were	O
able	O
to	O
excel	O
on	O
this	O
environment	O
.	O

We	O
perform	O
an	O
ablation	O
study	O
to	O
test	O
the	O
significance	O
of	O
the	O
key	O
features	O
in	O
ABCD	B-DatasetName
.	O

See	O
Appendix	O
E	O
for	O
details.)We	O
observe	O
that	O
supplying	O
the	O
intent	O
information	O
to	O
the	O
BERT	B-MethodName
model	O
causes	O
a	O
noticeable	O
boost	O
in	O
dialog	O
success	O
,	O
bringing	O
the	O
score	O
to	O
32.3	B-MetricValue
%	I-MetricValue
.	O

However	O
,	O
augmenting	O
the	O
model	O
with	O
knowledge	O
of	O
the	O
guidelines	O
unexpectedly	O
dropped	O
performance	O
down	O
to	O
30.6	B-MetricValue
%	I-MetricValue
.	O

This	O
model	O
reached	O
the	O
peak	O
observed	O
performance	O
of	O
32.7	B-TaskName
%	I-TaskName
,	O
highlighting	O
the	O
importance	O
of	O
both	O
components	O
.	O

In	O
conclusion	O
,	O
we	O
have	O
presented	O
ABCD	B-DatasetName
which	O
includes	O
over	O
10	O
K	O
dialogues	O
that	O
incorporate	O
procedural	O
,	O
dual	O
-	O
constrained	O
actions	O
.	O

We	O
found	O
that	O
pre	O
-	O
trained	O
models	O
perform	O
decent	O
on	O
Action	B-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
but	O
there	O
is	O
a	O
large	O
gap	O
between	O
humans	O
agents	O
and	O
the	O
top	O
systems	O
for	B-TaskName
Cascading	I-TaskName
Dialogue	I-TaskName
Success	I-TaskName
.	O

By	O
grounding	O
dialogues	O
to	O
in	O
-	O
depth	O
scenarios	O
with	O
explicit	O
policies	O
,	O
we	O
hope	O
to	O
have	O
pushed	O
towards	O
a	O
better	O
understanding	O
of	O
dialogue	O
success	O
.	O

Optimizing	O
agents	O
performance	O
can	O
be	O
split	O
into	O
preparation	O
before	O
the	O
HIT	B-TaskName
(	O
Human	B-TaskName
Intelligence	I-TaskName
Task	I-TaskName
)	O
,	O
improving	O
HIT	B-TaskName
itself	O
,	O
and	O
ongoing	O
training	O
afterwards	O
.	O

Starting	O
with	O
the	O
pre	O
-	O
HIT	B-TaskName
phase	O
,	O
the	O
major	O
steps	O
largely	O
center	O
around	O
multiple	O
rounds	O
of	O
qualifications	O
to	O
filter	O
for	O
the	O
highest	O
quality	O
workers	O
available	O
.	O

During	O
the	O
post	O
-	O
HIT	B-TaskName
phase	O
,	O
effort	O
shifts	O
to	O
ensuring	O
that	O
each	O
worker	O
becomes	O
increasingly	O
comfortable	O
with	O
the	O
task	O
.	O

During	O
-	O
HIT	B-TaskName
Phase	O
The	O
HIT	B-TaskName
itself	O
was	O
priced	O
at	O
$	O
1.50	O
for	O
completing	O
the	O
conversation	O
with	O
an	O
extra	O
$	O
1.00	O
bonus	O
for	O
identifying	O
the	O
correct	O
customer	O
intent	O
at	O
the	O
end	O
-	O
of	O
-	O
chat	O
survey	O
.	O

However	O
,	O
by	O
encouraging	O
agents	O
to	O
focus	O
on	O
the	O
customer	O
intent	O
,	O
they	O
were	O
forced	O
to	O
peruse	O
the	O
Agent	O
Guidelines	O
for	O
the	O
associated	O
subflow	O
.	O

Post	O
-	O
HIT	B-TaskName
Phase	O
For	O
ongoing	O
training	O
,	O
we	O
began	O
producing	O
small	O
lists	O
of	O
bulletpoints	O
to	O
the	O
agents	O
on	O
areas	O
they	O
could	O
improve	O
on	O
.	O

Let	O
us	O
consider	O
the	O
number	O
of	O
agents	O
available	O
as	O
A	O
and	O
the	O
number	O
of	O
customers	O
available	O
as	O
C.	O
Given	O
budget	O
constraints	O
,	O
we	O
can	O
only	O
pay	O
some	O
maximum	O
number	O
of	O
workers	O
M	O
.	O

Although	O
the	O
customer	O
side	O
of	O
ABCD	B-DatasetName
is	O
a	O
simpler	O
task	O
,	O
there	O
is	O
still	O
a	O
minimum	O
bar	O
to	O
be	O
met	O
to	O
prevent	O
(	O
a	O
)	O
customers	O
who	O
spam	O
with	O
random	O
text	O
(	O
b	O
)	O
customers	O
who	O
fake	O
scenarios	O
or	O
(	O
c	O
)	O
customers	O
who	O
hoard	O
HITs	O
and	O
never	O
show	O
up	O
to	O
the	O
chat	O
.	O

In	O
a	O
typical	O
scenario	O
,	O
a	O
customer	O
might	O
leave	O
the	O
tab	O
open	O
to	O
work	O
on	O
other	O
tasks	O
,	O
but	O
when	O
they	O
are	O
eventually	O
paired	O
,	O
the	O
customer	O
is	O
often	O
busy	O
doing	O
something	O
else	O
,	O
leaving	O
the	O
chat	O
to	O
flounder	O
.	O

To	O
resolve	O
this	O
situation	O
,	O
we	O
begin	O
with	O
the	O
maximum	O
number	O
of	O
workers	O
M	O
as	O
the	O
starting	O
constraint	O
given	O
a	O
fixed	O
budget	O
.	O

If	O
we	O
qualify	O
too	O
many	O
workers	O
,	O
then	O
we	O
will	O
not	O
have	O
enough	O
budget	O
left	O
for	O
the	O
actual	O
conversations	O
,	O
so	O
instead	O
we	O
qualify	O
workers	O
in	O
mini	O
-	O
batches	O
.	O

We	O
also	O
establish	O
an	O
exam	O
that	O
is	O
purposely	O
very	O
easy	O
(	O
to	O
minimize	O
costs	O
)	O
,	O
but	O
just	O
hard	O
enough	O
to	O
deter	O
bots	O
and	O
spammers	O
.	O

To	O
raise	O
the	O
likelihood	O
that	O
the	O
customer	O
will	O
show	O
up	O
,	O
we	O
include	O
a	O
question	O
in	O
the	O
quiz	O
which	O
simply	O
asks	O
when	O
the	O
customer	O
is	O
available	O
to	O
perform	O
the	O
HIT	O
.	O

We	O
believe	O
our	O
modifications	O
have	O
only	O
scratched	O
the	O
surface	O
and	O
that	O
improving	O
the	O
user	O
experience	O
for	O
data	O
collection	O
offers	O
an	O
interesting	O
line	O
of	O
HCI	B-TaskName
research	O
to	O
explore	O
.	O

To	O
motivate	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
(	O
CDS	B-TaskName
)	O
over	O
typical	O
other	O
accuracy	O
metrics	O
,	O
consider	O
the	O
scenario	O
where	O
a	O
model	O
gets	O
80	B-MetricValue
%	I-MetricValue
of	O
turns	O
correct	O
,	O
while	O
still	O
achieving	O
0	O
%	O
accuracy	O
on	O
the	O
conversation	O
level	O
because	O
it	O
always	O
messes	O
up	O
somewhere	O
right	O
at	O
the	O
end	O
of	O
the	O
dialogue	O
.	O

Instead	O
,	O
cascading	B-TaskName
dialogue	I-TaskName
success	I-TaskName
creates	O
an	O
evaluation	O
example	O
for	O
the	O
remainder	O
of	O
each	O
conversation	O
starting	O
from	O
each	O
turn	O
.	O

Now	O
imagine	O
the	O
model	O
consistently	O
predicted	O
turn	O
C	O
incorrectly	O
,	O
and	O
everything	O
else	O
correct	O
.	O

Averaging	O
across	O
all	O
turns	O
would	O
yield	O
a	O
final	O
cascading	B-MetricName
success	I-MetricName
rate	I-MetricName
of	O
45.8	B-MetricValue
%	I-MetricValue
.	O

A	O
turn	O
-	O
based	O
metric	O
would	O
yield	O
75	B-MetricValue
%	I-MetricValue
while	O
a	O
conversation	O
-	O
based	O
metric	O
would	O
yield	O
0	B-MetricValue
%	I-MetricValue
.	O

Thus	O
,	O
CDS	B-TaskName
allows	O
a	O
model	O
to	O
earn	O
partial	O
credit	O
on	O
what	O
it	O
has	O
learned	O
without	O
severe	O
penalties	O
in	O
either	O
direction	O
.	O

When	O
training	O
the	O
best	O
model	O
for	B-TaskName
Action	I-TaskName
State	I-TaskName
Tracking	I-TaskName
,	O
we	O
ended	O
up	O
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-5	B-HyperparameterValue
,	B-HyperparameterName
hidden	I-HyperparameterName
dimension	I-HyperparameterName
of	O
1024	B-HyperparameterValue
,	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.05	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
10	B-HyperparameterValue
examples	O
.	O

Training	O
lasted	O
for	O
14	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
where	O
we	O
early	O
stopped	O
if	O
overall	O
accuracy	B-MetricName
failed	O
to	O
improve	O
for	O
three	O
epochs	O
in	O
a	O
row	O
.	O

The	O
RAdam	B-HyperparameterValue
optimizer	B-HyperparameterName
had	O
a	O
linear	O
warm	O
-	O
up	O
for	O
three	O
epochs	O
,	O
with	O
hyperparameters	O
kept	O
at	O
their	O
defaults	O
of	O
0.9	B-HyperparameterValue
and	O
0.999	B-HyperparameterValue
.	O

For	O
Cascading	B-TaskName
Dialogue	I-TaskName
Success	I-TaskName
,	O
our	O
best	O
model	O
had	O
a	O
1e-5	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
1024	B-HyperparameterValue
hidden	B-HyperparameterName
dimension	I-HyperparameterName
and	O
no	O
weight	O
decay	O
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
shrunk	O
to	O
3	B-HyperparameterValue
examples	O
,	O
but	O
this	O
was	O
due	O
purely	O
to	O
memory	O
rather	O
than	O
performance	O
reasons	O
.	O

Second	O
,	O
the	O
intent	O
classifier	O
is	O
directly	O
fed	O
the	O
solution	O
,	O
which	O
is	O
what	O
allows	O
it	O
to	O
trivially	O
reach	O
perfect	O
accuracy	B-MetricName
.	O

Since	O
ABCD	B-DatasetName
was	O
collected	O
using	O
Expert	O
Live	O
Chat	O
rather	O
than	O
templates	O
,	O
we	O
observe	O
various	O
linguistic	O
diversity	O
in	O
the	O
chats	O
.	O

The	O
authors	O
would	O
like	O
to	O
thank	O
Tao	O
Lei	O
,	O
Felix	O
Wu	O
and	O
Anmol	O
Kabra	O
for	O
their	O
feedback	O
and	O
support	O
.	O

This	O
paper	O
presents	O
a	O
new	O
dataset	O
which	O
was	O
collected	O
through	O
the	O
use	O
of	O
crowdworkers	O
.	O

Nowadays	O
,	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
which	O
aims	O
to	O
verify	O
whether	O
a	O
news	O
document	O
is	O
trusted	O
or	O
fake	O
,	O
has	O
become	O
urgent	O
and	O
important	O
.	O

Most	O
existing	O
methods	O
rely	O
heavily	O
on	O
linguistic	O
and	O
semantic	O
features	O
from	O
the	O
news	O
content	O
,	O
and	O
fail	O
to	O
effectively	O
exploit	O
external	O
knowledge	O
which	O
could	O
help	O
determine	O
whether	O
the	O
news	O
document	O
is	O
trusted	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
called	O
CompareNet	B-MethodName
,	O
which	O
compares	O
the	O
news	O
to	O
the	O
knowledge	O
base	O
(	O
KB	O
)	O
through	O
entities	O
for	B-TaskName
fake	I-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Considering	O
that	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
is	O
correlated	O
with	O
topics	O
,	O
we	O
also	O
incorporate	O
topics	O
to	O
enrich	O
the	O
news	O
representation	O
.	O

Specifically	O
,	O
we	O
first	O
construct	O
a	O
directed	B-MethodName
heterogeneous	I-MethodName
document	I-MethodName
graph	I-MethodName
for	O
each	O
news	O
incorporating	O
topics	O
and	O
entities	O
.	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
for	O
learning	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
content	O
.	O

Experimental	O
results	O
on	O
two	O
benchmark	O
datasets	O
demonstrate	O
that	O
CompareNet	B-MethodName
significantly	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

2017;Khurana	O
and	O
Intelligentie	O
,	O
2017;Shu	O
et	O
al	O
.	O
,	O

To	O
avoid	O
feature	O
engineering	O
,	O
deep	O
neural	O
models	O
such	O
as	O
Bi	B-MethodName
-	I-MethodName
LSTM	I-MethodName
and	I-MethodName
convolutional	I-MethodName
neural	I-MethodName
networks	I-MethodName
(	O
CNN	B-MethodName
)	O
have	O
been	O
employed	O
(	O
Oshikawa	O
et	O
al	O
.	O
,	O

They	O
modeled	O
a	O
news	O
document	O
as	O
a	O
fully	O
connected	O
sentence	O
graph	O
and	O
proposed	O
a	O
graph	B-MethodName
attention	I-MethodName
model	I-MethodName
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

External	O
KB	O
such	O
as	O
Wikipedia	B-DatasetName
contains	O
a	O
large	O
amount	O
of	O
high	O
-	O
quality	O
structured	O
subjectpredicate	O
-	O
object	O
triplets	O
and	O
unstructured	O
entity	O
descriptions	O
,	O
which	O
could	O
serve	O
as	O
evidence	O
for	O
detecting	O
fake	O
news	O
.	O

proposed	O
to	O
construct	O
knowledge	O
graphs	O
from	O
positive	O
and	O
negative	O
news	O
,	O
and	O
apply	O
TransE	B-MethodName
to	O
learn	O
triplet	O
scores	O
for	O
fake	O
news	O
detection	O
(	O
Pan	O
et	O
al	O
.	O
,	O

In	O
this	O
paper	O
,	O
to	O
take	O
full	O
advantage	O
of	O
the	O
external	O
knowledge	O
,	O
we	O
propose	O
a	O
novel	O
endto	B-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
CompareNet	I-MethodName
which	O
directly	O
compares	O
the	O
news	O
to	O
the	O
KB	O
through	O
entities	O
for	O
fake	O
news	O
detection	O
.	O

In	O
CompareNet	B-MethodName
,	O
we	O
also	O
consider	O
using	O
topics	O
to	O
enrich	O
the	O
news	O
document	O
representation	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
,	O
since	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
and	O
topics	O
are	O
highly	O
correlated	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Based	O
on	O
the	O
directed	O
heterogeneous	O
document	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
topic	O
-	O
enriched	O
news	O
representations	O
and	O
contextual	O
entity	O
representations	O
.	O

To	O
facilitate	O
related	O
researches	O
,	O
we	O
release	O
both	O
our	O
code	O
and	O
dataset	O
to	O
the	O
public	O
2	O
.1	O
https://en.wikipedia.org/wiki/Mammography	O
2	O
https://github.com/ytc272098215/FakeNewsDetection	O
In	O
summary	O
,	O
our	O
main	O
contributions	O
include	O
:	O
1	O
)	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
graph	I-MethodName
neural	I-MethodName
model	I-MethodName
CompareNet	I-MethodName
which	O
compares	O
the	O
news	O
to	O
the	O
external	O
knowledge	O
through	O
entities	O
for	O
fake	O
news	O
detection.2	O
)	O
In	O
CompareNet	B-MethodName
,	O
we	O
also	O
consider	O
the	O
useful	O
topic	O
information	O
.	O

Generally	O
,	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
usually	O
focuses	O
on	O
news	O
events	O
while	O
fact	O
-	O
checking	O
is	O
broader	O
(	O
Oshikawa	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

The	O
approaches	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
can	O
be	O
divided	O
into	O
two	O
categories	O
:	O
social	O
-	O
based	O
and	O
content	O
-	O
based	O
.	O

Social	B-MethodName
based	I-MethodName
models	O
basically	O
include	O
stance	B-MethodName
-	I-MethodName
based	I-MethodName
and	O
propagation	B-MethodName
-	I-MethodName
based	I-MethodName
.	O

Stance	B-MethodName
-	I-MethodName
based	I-MethodName
models	O
utilize	O
users	O
'	O
opinions	O
to	O
infer	O
news	O
veracity	O
(	O
Jin	O
et	O
al	O
.	O
,	O

Propagation	B-MethodName
-	I-MethodName
based	I-MethodName
approaches	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
are	O
based	O
on	O
the	O
basic	O
assumption	O
that	O
the	O
credibility	O
of	O
a	O
news	O
event	O
is	O
highly	O
related	O
to	O
the	O
credibilities	O
of	O
relevant	O
social	O
media	O
posts	O
.	O

Both	O
homogeneous	B-MethodName
(	O
Jin	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
heterogeneous	B-MethodName
credibility	I-MethodName
networks	I-MethodName
(	O
Gupta	O
et	O
al	O
.	O
,	O

2020	O
)	O
constructed	O
a	O
heterogeneous	O
network	O
of	O
news	O
articles	O
,	O
creators	O
and	O
news	O
subjects	O
,	O
and	O
proposed	O
a	O
deep	B-MethodName
diffusive	I-MethodName
network	I-MethodName
model	O
for	O
incorporating	O
the	O
network	O
structure	O
information	O
to	O
simultaneously	O
detect	B-TaskName
fake	I-TaskName
news	I-TaskName
articles	I-TaskName
,	O
creators	O
and	O
subjects	O
.	O

2015;Rubin	O
et	O
al	O
.	O
,	O

proposed	O
to	O
construct	O
knowledge	O
graphs	O
from	O
positive	O
and	O
negative	O
news	O
,	O
and	O
apply	O
TransE	B-MethodName
to	O
learn	O
triplet	O
scores	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
(	O
Pan	O
et	O
al	O
.	O
,	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
graph	O
neural	O
model	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
which	O
directly	O
compares	O
the	O
news	O
to	O
external	O
knowledge	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Considering	O
that	O
the	O
detection	B-TaskName
of	I-TaskName
fake	I-TaskName
news	I-TaskName
is	O
correlated	O
with	O
topics	O
,	O
we	O
also	O
use	O
topics	O
to	O
enrich	O
the	O
news	O
representation	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

2019	O
;)	O
also	O
consider	O
incorporating	O
multi	O
-	O
modal	O
features	O
such	O
as	O
images	O
for	O
improving	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

In	O
this	O
section	O
,	O
we	O
detail	O
our	O
proposed	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
model	O
CompareNet	B-MethodName
,	O
which	O
directly	O
compares	O
the	O
news	O
to	O
external	O
knowledge	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

As	O
shown	O
in	O
Figure	O
2	O
,	O
we	O
also	O
consider	O
topics	O
for	O
enriching	O
news	O
representation	O
since	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
is	O
highly	O
correlated	O
with	O
topics	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Based	O
on	O
the	O
graph	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
to	O
learn	O
the	O
topic	O
-	O
enriched	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
that	O
encode	O
the	O
semantics	O
of	O
the	O
news	O
document	O
.	O

Finally	O
,	O
the	O
obtained	O
entity	O
comparison	O
features	O
are	O
combined	O
with	O
the	O
topic	O
-	O
enriched	O
news	O
document	O
representation	O
for	O
fake	O
news	O
detection	O
.	O

There	O
are	O
three	O
kinds	O
of	O
nodes	O
in	O
the	O
graph	O
:	O
sentences	O
S	O
=	O
{	O
s	O
1	O
,	O
s	O
2	O
,	O
•	O
•	O
•	O
,	O
s	O
m	O
}	O
,	O
topics	O
T	O
=	O
{	O
t	O
1	O
,	O
t	O
2	O
,	O
•	O
•	O
•	O
,	O
t	O
K	O
}	O
and	O
entities	O
E	O
=	O
{	O
e	O
1	O
,	O
e	O
2	O
,	O
•	O
•	O
•	O
,	O
e	O
n	O
}	O
,	O
i.e.	O
,	O
V	O
=	O
S	O
∪	O
T	O
∪	O
E.The	O
set	O
of	O
edges	O
E	O
represent	O
the	O
relations	O
among	O
sentences	O
,	O
topics	O
and	O
entities	O
.	O

Since	O
topic	O
information	O
is	O
important	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
apply	O
the	O
unsupervised	B-MethodName
LDA	I-MethodName
(	O
Blei	O
et	O
al	O
.	O
,	O

2003	O
)	O
(	O
the	O
total	B-HyperparameterName
topic	I-HyperparameterName
number	I-HyperparameterName
K	B-HyperparameterName
is	O
set	O
as	O
100	B-HyperparameterValue
)	O
to	O
mine	O
the	O
latent	O
topics	O
T	O
from	O
all	O
the	O
sentences	O
of	O
all	O
the	O
documents	O
in	O
our	O
dataset	O
.	O

Specifically	O
,	O
each	O
sentence	O
is	O
taken	O
as	O
a	O
pseudo	O
-	O
document	O
and	O
is	O
assigned	O
to	O
the	O
top	B-HyperparameterName
P	I-HyperparameterName
relevant	I-HyperparameterName
topics	I-HyperparameterName
with	O
the	O
largest	O
probabilities	O
.	O

Thus	O
,	O
each	O
sentence	O
is	O
also	O
connected	O
with	O
its	O
top	B-HyperparameterName
P	I-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
in	O
bi	O
-	O
direction	O
,	O
allowing	O
the	O
useful	O
topic	O
information	O
to	O
propagate	O
among	O
the	O
sentences	O
.	O

Note	O
that	O
we	O
can	O
also	O
deal	O
with	O
new	O
coming	O
news	O
documents	O
by	O
inferring	O
the	O
topics	O
with	O
trained	O
LDA	B-MethodName
.	O

We	O
identify	O
the	O
entities	O
E	O
in	O
the	O
document	O
d	O
and	O
map	O
them	O
to	O
Wikipedia	B-DatasetName
using	O
the	O
entity	O
linking	O
tool	O
TAGME	O
3	O
.	O

Based	O
on	O
the	O
above	O
directed	O
heterogeneous	O
document	O
graph	O
G	O
,	O
we	O
develop	O
a	O
heterogeneous	B-MethodName
graph	I-MethodName
attention	I-MethodName
network	I-MethodName
for	O
learning	O
the	O
news	O
representation	O
as	O
well	O
as	O
the	O
contextual	O
entity	O
representations	O
.	O

The	O
heterogeneous	O
convolution	O
layer	O
updates	O
the	O
(	O
l	O
+	O
1)-th	O
layer	O
representation	O
of	O
the	O
nodes	O
H	O
(	O
l+1	O
)	O
by	O
aggregating	O
the	O
features	O
of	O
their	O
neighboring	O
nodes	O
H	O
(	O
l	O
)	O
τ	O
with	O
different	O
types	O
τ	O
.	O
(	O

We	O
believe	O
entity	O
comparison	O
features	O
could	O
improve	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
based	O
on	O
the	O
assumption	O
that	O
e	O
c	O
learned	O
from	O
trusted	O
news	O
document	O
can	O
be	O
better	O
aligned	O
with	O
the	O
corresponding	O
e	O
KB	O
;	O
while	O
inverse	O
for	O
fake	O
news	O
.	O

Due	O
to	O
the	O
simplicity	O
of	O
TransE	B-MethodName
(	O
Bordes	O
et	O
al	O
.	O
,	O

2013	O
)	O
,	O
we	O
adopted	O
TransE	B-MethodName
to	O
learn	O
entity	O
representations	O
e	O
s	O
∈	O
R	O
M	O
from	O
the	O
triplets	O
.	O

Formally	O
,	O
given	O
a	O
triplet	O
(	O
h	O
,	O
r	O
,	O
t	O
)	O
,	O
TransE	B-MethodName
regards	O
a	O
relationship	O
r	O
as	O
a	O
translation	O
vector	O
r	O
from	O
the	O
head	O
entity	O
h	O
to	O
the	O
tail	O
entity	O
t	O
,	O
namely	O
h	O
+	O
r	O
=	O
t.	O
Textual	O
Embedding	O
.	O

For	O
each	O
entity	O
,	O
we	O
take	O
the	O
first	O
paragraph	O
of	O
the	O
corresponding	O
Wikipedia	O
page	O
as	O
its	O
text	O
description	O
.	O

2019	O
)	O
,	O
we	O
use	O
SLN	B-DatasetName
:	O
Satirical	B-DatasetName
and	I-DatasetName
Legitimate	I-DatasetName
News	I-DatasetName
Database	I-DatasetName
(	O
Rubin	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
LUN	B-DatasetName
:	O
Labeled	B-DatasetName
Unreliable	I-DatasetName
News	I-DatasetName
Dataset	O
(	O
Rashkin	O
et	O
al	O
.	O
,	O

Our	O
baseline	O
models	O
include	O
deep	O
neural	O
models	O
:	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
,	O
CNN	B-MethodName
(	O
Kim	O
,	O
2014	O
)	O
,	O
BERT+LSTM	B-MethodName
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

2019	O
)	O
(	O
BERT	B-MethodName
for	O
sentence	O
encoder	O
and	O
then	O
LSTM	B-MethodName
for	O
document	O
encoder	O
)	O
and	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

We	O
also	O
compare	O
our	O
model	O
with	O
graph	O
neural	O
models	O
:	O
GCN	B-MethodName
and	O
GAT	B-MethodName
based	O
on	O
an	O
undirected	O
fully	O
-	O
connected	O
sentence	O
graph	O
,	O
which	O
use	O
attention	O
pooling	O
or	O
max	O
pooling	O
for	O
learning	O
news	O
document	O
representation	O
.	O

2019	O
)	O
,	O
we	O
use	O
LSTM	B-MethodName
to	O
encode	O
sentences	O
with	O
randomly	O
initialized	O
word	O
embeddings	O
,	O
which	O
is	O
the	O
same	O
as	O
all	O
the	O
graph	O
neural	O
baselines	O
.	O

We	O
run	O
our	O
model	O
5	O
times	O
and	O
report	O
the	O
micro	O
-	O
averaged	O
(	O
Precision	O
=	O
Recall	O
=	O
F1	O
)	O
and	O
macro	O
-	O
averaged	O
scores	O
(	O
Precision	O
,	O
Recall	O
,	O
F1	O
)	O
in	O
all	O
the	O
settings	O
including	O
2	O
-	O
way	O
and	O
4	O
-	O
way	O
classification.2	O
-	O
way	O
classification	O
:	O
We	O
use	O
the	O
satirical	O
and	O
trusted	O
news	O
articles	O
from	O
LUN	B-DatasetName
-	I-DatasetName
train	I-DatasetName
for	O
training	O
,	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
for	O
validation	O
and	O
evaluate	O
our	O
model	O
on	O
the	O
entire	O
SLN	B-DatasetName
dataset	O
.	O

This	O
is	O
done	O
to	O
emulate	O
a	O
real	O
-	O
world	O
scenario	O
where	O
we	O
want	O
to	O
see	O
the	O
performance	O
of	O
our	O
model	O
on	O
an	O
out	O
-	O
of	O
-	O
domain	O
dataset.4	O
-	O
way	O
classification	O
:	O
We	O
split	O
the	O
LUN	B-DatasetName
-	I-DatasetName
train	I-DatasetName
into	O
a	O
80:20	O
split	O
to	O
create	O
our	O
training	O
and	O
validation	O
set	O
.	O

We	O
use	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
as	O
our	O
in	O
-	O
domain	O
test	O
set	O
.	O

In	O
our	O
experiments	O
,	O
we	O
set	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
topics	I-HyperparameterName
K	I-HyperparameterName
=	O
100	B-HyperparameterValue
in	O
LDA	B-MethodName
.	O

Each	O
sentence	O
is	O
assigned	O
to	O
top	B-HyperparameterName
P	I-HyperparameterName
=	O
2	B-HyperparameterValue
topics	O
with	O
the	O
largest	O
probabilities	O
.	O

The	O
layer	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
our	I-HyperparameterName
heterogeneous	I-HyperparameterName
graph	I-HyperparameterName
convolution	I-HyperparameterName
is	O
set	O
as	B-HyperparameterName
L	I-HyperparameterName
=	O
1	B-HyperparameterValue
.	O

Specifically	O
,	O
all	O
the	O
hidden	B-HyperparameterName
dimensions	I-HyperparameterName
used	O
in	O
our	O
model	O
are	O
set	O
as	O
M	B-HyperparameterName
=	O
100	B-HyperparameterValue
.	O

The	O
node	B-HyperparameterName
embedding	I-HyperparameterName
dimension	I-HyperparameterName
N	B-HyperparameterName
=	O
32	B-HyperparameterValue
.	O

For	O
GCN	B-HyperparameterName
,	O
GAT	B-HyperparameterName
and	O
CompareNet	B-HyperparameterName
,	O
we	O
set	O
the	O
activation	B-HyperparameterValue
function	I-HyperparameterValue
as	O
LeakyRelU	B-HyperparameterValue
with	O
slope	B-HyperparameterName
0.2	B-HyperparameterValue
.	O

For	O
model	O
training	O
,	O
we	O
train	O
the	O
models	O
for	O
a	O
maximum	O
of	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.001	B-HyperparameterValue
.	O

We	O
set	O
L2	B-HyperparameterName
normalization	I-HyperparameterName
factor	I-HyperparameterName
η	B-HyperparameterName
as	O
1e-6	B-HyperparameterValue
.	O

We	O
report	O
only	O
micro	B-MetricName
F1	I-MetricName
since	O
micro	B-MetricName
Precision	I-MetricName
=	O
Recall	B-MetricName
=	O
F1	B-MetricName
.	O

As	O
we	O
can	O
see	O
,	O
our	O
proposed	O
model	O
CompareNet	B-HyperparameterName
significantly	O
outperforms	O
all	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
in	O
terms	O
of	O
all	O
the	O
metrics	O
.	O

Compared	O
to	O
the	O
best	O
baseline	O
model	O
,	O
CompareNet	B-HyperparameterName
improves	O
both	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
by	O
nearly	O
3	B-MetricValue
%	I-MetricValue
.	O

We	O
can	O
also	O
find	O
that	O
the	O
graph	O
neural	O
network	O
based	O
models	O
GCN	B-MethodName
and	O
GAT	B-MethodName
all	O
perform	O
better	O
than	O
the	O
deep	O
neural	O
models	O
including	O
CNN	B-MethodName
,	O
LSTM	B-MethodName
and	O
BERT	B-MethodName
.	O

Our	O
model	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
further	O
improves	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
by	O
effectively	O
exploiting	O
the	O
topics	O
as	O
well	O
as	O
the	O
external	O
KB	O
.	O

The	O
topics	O
enrich	O
the	O
news	O
representation	O
,	O
and	O
the	O
external	O
KB	O
offers	O
evidences	O
for	O
fake	B-TaskName
news	I-TaskName
detection	I-TaskName
.	O

Our	O
model	O
CompareNet	B-MethodName
achieves	O
the	O
best	O
performance	O
in	O
terms	O
of	O
all	O
metrics	O
.	O

We	O
believe	O
that	O
our	O
model	O
CompareNet	B-MethodName
benefits	O
from	O
the	O
topics	O
and	O
external	O
knowledge	O
.	O

In	O
this	O
subsection	O
,	O
we	O
conduct	O
experiments	O
to	O
study	O
the	O
effectiveness	O
of	O
each	O
module	O
in	O
CompareNet	B-MethodName
and	O
the	O
way	O
we	O
incorporate	O
external	O
knowledge	O
.	O

We	O
study	O
the	O
average	O
performance	O
of	O
5	O
runs	O
on	O
the	O
LUN	B-DatasetName
-	I-DatasetName
test	I-DatasetName
set	O
.	O

As	O
shown	O
in	O
Table	O
4	O
,	O
we	O
test	O
the	O
performance	O
of	O
CompareNet	B-MethodName
removing	O
structured	O
triplets	O
,	O
removing	O
the	O
entire	O
external	O
knowledge	O
,	O
removing	O
topics	O
,	O
and	O
removing	O
both	O
topics	O
and	O
external	O
knowledge	O
.	O

Removing	O
both	O
topics	O
and	O
external	O
knowledge	O
(	O
i.e.	O
,	O
w/o	O
Both	O
)	O
will	O
lead	O
to	O
substantial	O
performance	O
drop	O
(	O
4.0	B-MetricValue
-	I-MetricValue
5.0	I-MetricValue
%	I-MetricValue
)	O
.	O

The	O
variant	O
model	O
CompareNet	B-MethodName
(	I-MethodName
undirected	I-MethodName
)	I-MethodName
although	O
incorporating	O
both	O
topics	O
and	O
external	O
knowledge	O
achieves	O
lower	O
performance	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
and	O
CompareNet	B-MethodName
w/o	I-MethodName
Topics	I-MethodName
.	O

The	O
reason	O
could	O
be	O
that	O
CompareNet	B-MethodName
(	I-MethodName
undirected	I-MethodName
)	I-MethodName
directly	O
aggregates	O
the	O
true	O
entity	O
knowledge	O
into	O
the	O
news	O
representation	O
in	O
graph	O
convolution	O
without	O
considering	O
the	O
directed	O
edges	O
,	O
which	O
misleads	O
the	O
classifier	O
for	O
differentiating	O
fake	O
news	O
.	O

The	O
last	O
variant	O
Com	B-MethodName
-	I-MethodName
pareNet	I-MethodName
(	I-MethodName
concatenation	I-MethodName
)	I-MethodName
also	O
performs	O
lower	O
than	O
CompareNet	B-MethodName
w/o	I-MethodName
Entity	I-MethodName
Cmp	I-MethodName
,	O
further	O
indicating	O
that	O
directly	O
concatenating	O
true	O
entity	O
knowledge	O
is	O
not	O
a	O
good	O
way	O
for	O
incorporating	O
entity	O
knowledge	O
.	O

Its	O
performance	O
drops	O
by	O
around	O
2.0	B-MetricValue
%	I-MetricValue
compared	O
to	O
CompareNet	B-MethodName
.	O

These	O
demonstrate	O
the	O
effectiveness	O
of	O
the	O
carefully	O
designed	O
entity	O
comparison	O
network	O
in	O
CompareNet	B-MethodName
.	O

Figure	O
3	O
shows	O
the	O
performance	O
(	O
micro	B-MetricName
and	I-MetricName
macro	I-MetricName
F1	I-MetricName
)	O
of	O
our	O
model	O
CompareNet	B-MethodName
on	O
LUN	B-DatasetName
validation	I-DatasetName
set	O
with	O
different	O
number	O
of	O
top	B-HyperparameterName
assigned	I-HyperparameterName
topics	I-HyperparameterName
P	I-HyperparameterName
to	O
each	O
sentence	O
.	O

As	O
we	O
can	O
see	O
clearly	O
,	O
micro	B-MetricName
F1	I-MetricName
and	O
macro	B-MetricName
F1	I-MetricName
first	O
consistently	O
rises	O
with	O
the	O
increase	O
of	O
P	B-HyperparameterName
and	O
then	O
drops	O
when	O
P	B-HyperparameterName
is	O
larger	O
than	O
2	O
.	O

Thus	O
,	O
in	O
our	O
experiments	O
,	O
we	O
set	O
P	B-HyperparameterName
=	O
2	B-HyperparameterValue
.	O

To	O
further	O
illustrate	O
why	O
our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baseline	O
GAT+Attn	B-MethodName
(	O
Vaibhav	O
et	O
al	O
.	O
,	O

We	O
believe	O
that	O
our	O
model	O
CompareNet	B-MethodName
benefits	O
from	O
the	O
comparison	O
to	O
Wikipedia	B-DatasetName
knowledge	I-DatasetName
by	O
the	O
entity	O
comparison	O
network	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
graph	O
neural	O
model	O
CompareNet	B-MethodName
which	O
compares	O
the	O
news	O
to	O
the	O
external	O
knowledge	O
for	O
fake	O
news	O
detection	O
.	O

This	O
paper	O
describes	O
three	O
open	O
access	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
corpora	O
and	O
presents	O
the	O
results	O
and	O
implications	O
of	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
for	I-TaskName
endangered	I-TaskName
language	I-TaskName
documentation	I-TaskName
.	O

Two	O
issues	O
are	O
addressed	O
.	O

First	O
,	O
the	O
advantage	O
for	O
ASR	B-TaskName
accuracy	O
of	O
targeting	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
in	O
addition	O
to	O
,	O
or	O
in	O
substitution	O
of	O
,	O
linguistic	O
units	O
(	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
morae	B-MethodName
)	O
and	O
then	O
using	O
ROVER	B-MethodName
for	O
system	O
combination	O
.	O

BPE	B-MethodName
units	O
consistently	O
outperform	O
linguistic	O
units	O
although	O
the	O
best	O
results	O
are	O
obtained	O
by	O
system	O
combination	O
of	O
different	O
BPE	B-MethodName
targets	O
.	O

Second	O
,	O
a	O
case	O
is	O
made	O
that	O
for	O
endangered	O
language	O
documentation	O
,	O
ASR	B-TaskName
contributions	O
should	O
be	O
evaluated	O
according	O
to	O
extrinsic	O
criteria	O
(	O
e.g.	O
,	O
positive	B-MetricName
impact	I-MetricName
on	I-MetricName
downstream	I-MetricName
tasks	I-MetricName
)	O
and	O
not	O
simply	O
intrinsic	O
metrics	O
(	O
e.g.	O
,	O
CER	B-MetricName
and	O
WER	B-MetricName
)	O
.	O

2006	O
)	O
,	O
and	O
financial	O
support	O
for	O
endangered	O
language	O
documentation	O
(	O
the	O
Volkswagen	O
Foundation	O
,	O
the	O
NSF	O
Documenting	O
Endangered	O
Language	O
Program	O
,	O
and	O
the	O
SOAS	O
Endangered	O
Language	O
Documentation	O
Programme	O
)	O
.	O

1992	O
)	O
and	O
Himmelmann	O
(	O
1998	O
)	O
have	O
been	O
published	O
by	O
Seifart	O
et	O
al	O
.	O
(	O

Within	O
the	O
last	O
decade	O
,	O
the	O
National	O
Science	O
Foundation	O
supported	O
a	O
series	O
of	O
three	O
workshops	O
,	O
under	O
the	O
acronym	O
AARDVARC	O
(	O
Automatically	O
Annotated	O
Repository	O
of	O
Digital	O
Audio	O
and	O
Video	O
Resources	O
Community	O
)	O
to	O
bring	O
together	O
field	O
linguists	O
working	O
on	O
endangered	O
languages	O
and	O
computational	O
linguists	O
working	O
on	O
automatic	O
annotation	O
-	O
particularly	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR)-to	B-TaskName
address	O
the	O
impact	O
of	O
what	O
has	O
been	O
called	O
the	O
"	O
transcription	O
bottleneck	O
"	O
(	O
Whalen	O
and	O
Damir	O
,	O
2012	O
)	O
.	O

Finally	O
,	O
articles	O
directly	O
referencing	O
ASR	B-TaskName
of	O
endangered	O
languages	O
have	O
become	O
increasingly	O
common	O
over	O
the	O
last	O
five	O
years	O
(	O
Adams	O
et	O
al	O
.	O
,	O
,	O

2021).This	O
article	O
continues	O
work	O
on	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
ASR	B-TaskName
(	O
Mitra	O
et	O
al	O
.	O
,	O

The	O
most	O
recent	O
efforts	O
(	O
2020	O
and	O
2021	O
)	O
have	O
adopted	O
the	O
ESPNet	O
toolkit	O
for	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
automatic	I-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
E2E	B-TaskName
ASR	I-TaskName
)	O
.	O

This	O
approach	O
has	O
proven	O
to	O
be	O
very	O
efficient	O
in	O
terms	O
of	O
time	O
needed	O
to	O
develop	O
the	O
ASR	B-TaskName
recipe	O
(	O
Shi	O
et	O
al	O
.	O
,	O

2021	O
)	O
and	O
in	O
yielding	O
ASR	B-TaskName
hypotheses	O
of	O
an	O
accuracy	O
capable	O
of	O
significantly	O
reducing	O
the	O
extent	O
of	O
human	O
effort	O
needed	O
to	O
finalize	O
accurate	O
transcribed	O
audio	O
for	O
permanent	O
archiving	O
as	O
here	O
demonstrated	O
.	O

Section	O
2	O
discusses	O
the	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
corpora	O
,	O
and	O
Section	O
3	O
explores	O
the	O
general	O
goals	O
of	O
EL	O
documentation	O
.	O

Section	O
4	O
reviews	O
the	O
E2E	B-TaskName
ASR	I-TaskName
and	O
corresponding	O
results	O
using	O
ESPNet	O
.	O

The	O
conclusion	O
is	O
offered	O
in	O
Section	O
5.2	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
:	O
Corpus	O
characteristics	O
and	O
development	O
Much	O
work	O
on	O
computer	O
-	O
assisted	O
EL	O
documentation	O
is	O
closely	O
related	O
to	O
work	O
on	O
low	O
-	O
resource	O
languages	O
,	O
for	O
the	O
obvious	O
reason	O
that	O
most	O
ELs	O
have	O
limited	O
resources	O
,	O
be	O
they	O
time	O
-	O
coded	O
transcriptions	O
,	O
interlinearized	O
texts	O
,	O
or	O
corpora	O
in	O
parallel	O
translation	O
.	O

The	O
resources	O
for	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
,	O
the	O
language	O
targeted	O
in	O
this	O
present	O
study	O
,	O
are	O
,	O
however	O
,	O
relatively	O
abundant	O
by	O
EL	O
standards	O
(	O
119.32	O
hours	O
over	O
three	O
corpora	O
)	O
,	O
the	O
result	O
of	O
over	O
a	O
decade	O
of	O
linguistic	O
and	O
anthropological	O
research	O
by	O
Amith	O
and	O
Castillo	O
García	O
(	O
2020	O
)	O
.	O

Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
(	O
henceforth	O
YM	B-DatasetName
)	O
,	O
an	O
endangered	O
Mixtecan	O
language	O
spoken	O
in	O
the	O
municipality	O
of	O
San	O
Luis	O
Acatlán	O
,	O
Guerrero	O
,	O
Mexico	O
,	O
is	O
one	O
of	O
some	O
50	O
languages	O
in	O
the	O
Mixtec	O
language	O
family	O
,	O
which	O
is	O
within	O
a	O
larger	O
unit	O
,	O
Otomanguean	O
,	O
that	O
Suárez	O
(	O
1983	O
)	O
considers	O
a	O
hyper	O
-	O
family	O
or	O
stock	O
.	O

YM	B-DatasetName
is	O
spoken	O
in	O
four	O
communities	O
:	O
Yoloxóchitl	O
,	O
Cuanacaxtitlan	O
,	O
Arroyo	O
Cumiapa	O
,	O
and	O
Buena	O
Vista	O
.	O

Mutual	O
intelligibility	O
among	O
the	O
four	O
communities	O
is	O
high	O
despite	O
differences	O
in	O
phonology	O
,	O
morphology	O
,	O
and	O
syntax	O
.	O

YMC	B-DatasetName
(	O
referring	O
only	O
to	O
the	O
Mixtec	O
of	O
the	O
community	O
of	O
Yoloxóchitl	O
[	O
16.81602	O
,	O
-98.68597	O
]	O
)	O
manifests	O
28	O
distinct	O
tonal	O
patterns	O
on	O
1,451	O
to	O
-	O
date	O
identified	O
bimoraic	O
lexical	O
stems	O
.	O

For	O
example	O
,	O
24	O
distinct	O
tonal	O
patterns	O
on	O
the	O
bimoraic	O
segmental	O
sequence	O
[	O
nama	O
]	O
yield	O
30	O
words	O
(	O
including	O
five	O
homophones	O
)	O
.	O

In	O
a	O
not	O
-	O
insignificant	O
number	O
of	O
cases	O
,	O
suppletive	O
stems	O
exist	O
,	O
generally	O
manifesting	O
variation	O
in	O
a	O
stem	O
-	O
initial	O
consonant	O
and	O
often	O
the	O
stem	O
-	O
initial	O
vowel	O
.	O

The	O
ample	O
tonal	O
inventory	O
of	O
YMC	B-DatasetName
presents	O
obstacles	O
to	O
native	O
speaker	O
literacy	O
and	O
an	O
ASR	B-TaskName
system	O
learning	O
to	O
convert	O
an	O
acoustic	O
signal	O
to	O
text	O
.	O

It	O
also	O
complicates	O
the	O
construction	O
of	O
a	O
language	O
lexicon	O
for	O
HMM	B-MethodName
-	I-MethodName
based	I-MethodName
systems	I-MethodName
,	O
a	O
lexicon	O
that	O
is	O
not	O
required	O
in	O
E2E	B-TaskName
ASR	I-TaskName
.	O

The	O
phonological	O
and	O
morphological	O
differences	O
between	O
YMC	B-DatasetName
and	O
the	O
Mixtec	O
of	O
the	O
three	O
other	O
YM	B-DatasetName
communities	O
create	O
challenges	O
for	O
transcription	O
and	O
,	O
by	O
extension	O
,	O
for	O
applying	O
YMC	B-DatasetName
ASR	B-TaskName
to	O
speech	O
recordings	O
from	O
these	O
other	O
villages	O
.	O

This	O
ample	O
size	O
has	O
yielded	O
lower	O
character	B-MetricName
(	O
CER	B-MetricName
)	O
and	O
word	B-MetricName
(	O
WER	B-MetricName
)	O
error	O
rates	O
than	O
would	O
usually	O
occur	O
with	O
truly	O
low	O
-	O
resource	O
EL	O
documentation	O
projects	O
.	O

A	O
second	O
YMC	B-DatasetName
corpus	O
(	B-DatasetName
YMC	I-DatasetName
-	I-DatasetName
FB	I-DatasetName
;	O
for	O
'	O
field	O
botany	O
'	O
)	O
was	O
developed	O
during	O
ethno	O
-	O
botanical	O
fieldwork	O
.	O

Kenia	O
Velasco	O
Gutiérrez	O
(	O
a	O
Spanish	O
-	O
speaking	O
botanist	O
)	O
and	O
Esteban	O
Guadalupe	O
Sierra	O
(	O
a	O
native	O
speaker	O
from	O
Yoloxóchitl	O
)	O
led	O
105	O
days	O
of	O
fieldwork	O
that	O
yielded	O
888	O
distinct	O
plant	O
collections	O
.	O

A	O
total	O
of	O
584	O
recordings	O
were	O
made	O
in	O
all	O
four	O
YM	O
communities	O
;	O
only	O
452	O
were	O
in	O
Yoloxóchitl	O
,	O
and	O
of	O
these	O
,	O
435	O
,	O
totaling	O
15.17	O
hours	O
with	O
only	O
three	O
speakers	O
,	O
were	O
used	O
as	O
a	O
second	O
test	O
case	O
for	O
E2E	B-TaskName
ASR	I-TaskName
.	O

This	O
Spanish	O
section	O
has	O
not	O
been	O
factored	O
into	O
the	O
duration	O
of	O
the	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
,	O
nor	O
has	O
it	O
been	O
evaluated	O
for	O
character	O
and	O
word	O
error	O
rates	O
at	O
this	O
time	O
(	O
pending	O
future	O
implementation	O
of	O
a	O
multilingual	O
model	O
)	O
.	O

2021).•	O
178	O
recordings	O
(	O
6.81	O
hours	O
)	O
were	O
processed	O
by	O
E2E	B-TaskName
ASR	I-TaskName
,	O
then	O
corrected	O
by	O
Castillo	O
.	O

This	O
set	O
was	O
not	O
used	O
to	O
teach	O
or	O
evaluate	O
novice	O
trainee	O
transcription	O
skills	O
but	O
only	O
to	O
determine	O
CER	B-MetricName
and	O
WER	B-MetricName
for	O
E2E	B-TaskName
ASR	I-TaskName
with	O
the	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
.	O

No	O
training	O
or	O
validation	O
sets	O
were	O
created	O
from	O
this	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
corpus	O
,	O
which	O
for	O
this	O
present	O
paper	O
was	O
used	O
solely	O
to	O
test	O
E2E	B-TaskName
ASR	I-TaskName
efficiency	O
using	O
the	O
recipe	O
developed	O
from	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
scores	O
for	O
YMC	B-DatasetName
-	I-DatasetName
FB	I-DatasetName
were	O
only	O
produced	O
after	O
Castillo	O
used	O
the	O
ELAN	O
interface	O
to	O
correct	O
the	O
ASR	B-TaskName
hypotheses	O
for	O
this	O
corpus	O
(	O
see	O
Appendix	O
A	O
for	O
an	O
example	O
ASR	B-TaskName
output	O
)	O
.	O

The	O
recordings	O
involved	O
some	O
speakers	O
not	O
represented	O
in	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
.	O

This	O
environment	O
may	O
have	O
introduced	O
reverb	O
or	O
other	O
effects	O
that	O
might	O
have	O
negatively	O
affected	O
ASR	B-TaskName
CER	B-MetricName
and	O
WER.Accessibility	B-MetricName
:	O
All	O
three	O
corpora	O
(	O
119.32	O
hours	O
)	O
are	O
available	O
at	O
the	O
OpenSLR	O
data	O
portal	O
(	O
Amith	O
and	O
Castillo	O
García	O
,	O
2020	O
)	O
3	O
Goals	O
and	O
challenges	O
of	O
corpora	O
-	O
based	O
endangered	O
language	O
documentation	O
The	O
oft	O
-	O
cited	O
Boasian	O
trilogy	O
of	O
grammar	O
,	O
dictionaries	O
,	O
and	O
texts	O
is	O
a	O
common	O
foundation	O
for	O
EL	O
documentation	O
.	O

And	O
a	O
grammar	O
itself	O
would	O
benefit	O
greatly	O
from	O
a	O
large	O
set	O
of	O
annotated	O
natural	O
speech	O
recordings	O
not	O
simply	O
to	O
provide	O
examples	O
of	O
particular	O
structures	O
but	O
to	O
facilitate	O
a	O
statistical	O
analysis	O
of	O
speech	O
patterns	O
(	O
e.g.	O
,	O
for	O
YMC	B-DatasetName
,	O
the	O
relative	O
frequency	O
of	O
completive	O
verbs	O
marked	O
solely	O
by	O
tone	O
vs.	O
those	O
marked	O
by	O
the	O
prefix	O
ni	O
1	O
-	O
)	O
.	O

End	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
is	O
used	O
to	O
rapidly	O
increase	O
corpus	O
size	O
while	O
offering	O
the	O
opportunity	O
to	O
target	O
certain	O
genres	O
(	O
such	O
as	O
expert	O
conversations	O
on	O
the	O
nomenclature	O
,	O
classification	O
,	O
and	O
use	O
of	O
local	O
flora	O
and	O
fauna	O
;	O
ritual	O
discourse	O
;	O
material	O
cultural	O
production	O
;	O
techniques	O
for	O
fishing	O
and	O
hunting	O
)	O
that	O
are	O
of	O
ethnographic	O
interest	O
but	O
are	O
often	O
insufficiently	O
covered	O
in	O
EL	O
documentation	O
projects	O
that	O
struggle	O
to	O
produce	O
large	O
and	O
varied	O
corpora	O
.	O

With	O
the	O
human	O
effortreducing	O
advances	O
in	O
ASR	B-TaskName
for	O
YMC	B-DatasetName
presented	O
in	O
this	O
paper	O
,	O
such	O
extensive	O
targeted	O
recording	O
of	O
endangered	O
cultural	O
knowledge	O
can	O
now	O
easily	O
be	O
included	O
in	O
the	O
documentation	O
effort	O
.	O

The	O
evaluation	O
metric	O
,	O
therefore	O
,	O
is	O
not	O
intrinsic	O
(	O
e.g.	O
,	O
reduced	B-MetricName
CER	I-MetricName
and	O
WER	B-MetricName
)	O
but	O
rather	O
extrinsic	O
:	O
the	O
impact	B-MetricName
of	I-MetricName
ASR	I-MetricName
on	I-MetricName
the	I-MetricName
downstream	I-MetricName
task	I-MetricName
of	I-MetricName
creating	I-MetricName
a	I-MetricName
large	I-MetricName
and	I-MetricName
varied	I-MetricName
corpus	I-MetricName
of	O
Yoloxóchitl	B-DatasetName
Mixtec	I-DatasetName
.	O

ASR	B-TaskName
for	I-TaskName
endangered	I-TaskName
languages	I-TaskName
is	O
made	O
difficult	O
not	O
simply	O
because	O
of	O
limited	O
resources	O
for	O
training	O
a	O
robust	O
system	O
but	O
by	O
a	O
series	O
of	O
factors	O
briefly	O
discussed	O
in	O
this	O
section	O
.	O

Recording	O
conditions	O
:	O
Noisy	O
environments	O
,	O
including	O
overlapping	O
speech	O
,	O
reverberation	O
in	O
indoor	O
recordings	O
,	O
natural	O
sounds	O
in	O
outdoor	O
recordings	O
,	O
less	O
than	O
optimal	O
microphone	O
placement	O
(	O
e.g.	O
,	O
a	O
boom	O
mic	O
in	O
video	O
recordings	O
)	O
,	O
and	O
failure	O
to	O
separately	O
mike	O
speakers	O
for	O
multichannel	O
recordings	O
all	O
negatively	O
impact	O
the	O
accuracy	B-MetricName
of	O
ASR	B-TaskName
output	O
.	O

Also	O
to	O
the	O
point	O
,	O
field	O
recordings	O
are	O
seldom	O
made	O
with	O
an	O
eye	O
to	O
seeding	O
a	O
corpus	O
in	O
ways	O
that	O
would	O
specifically	O
benefit	O
ASR	B-TaskName
results	O
(	O
e.g.	O
,	O
recording	O
a	O
large	O
number	O
of	O
speakers	O
for	O
shorter	O
durations	O
,	O
rather	O
than	O
fewer	O
speakers	O
for	O
longer	O
times	O
)	O
.	O

To	O
date	O
,	O
then	O
,	O
processing	O
a	O
corpus	O
through	O
ASR	B-TaskName
techniques	O
of	O
any	O
nature	O
(	O
HMM	O
,	O
end	O
-	O
to	O
-	O
end	O
)	O
has	O
been	O
more	O
of	O
an	O
afterthought	O
than	O
planned	O
at	O
project	O
beginning	O
.	O

Development	O
of	O
a	O
corpus	O
from	O
the	O
beginning	O
with	O
an	O
eye	O
to	O
subsequent	O
ASR	B-TaskName
potential	O
would	O
be	O
immensely	O
helpful	O
to	O
these	O
computational	O
efforts	O
.	O

It	O
could	O
,	O
perhaps	O
should	O
,	O
be	O
increasingly	O
considered	O
in	O
the	O
initial	O
project	O
design	O
.	O

Indeed	O
,	O
just	O
as	O
funding	O
agencies	O
such	O
as	O
NSF	O
require	O
that	O
projects	O
address	O
data	O
management	O
issues	O
,	O
it	O
might	O
be	O
worth	O
considering	O
the	O
suggested	O
inclusion	O
of	O
how	O
to	O
make	O
documentation	O
materials	O
more	O
amenable	O
to	O
ASR	B-TaskName
and	O
NLP	O
processing	O
as	O
machine	O
learning	O
technologies	O
are	O
getting	O
more	O
robust	O
.	O

Preliminary	O
,	O
though	O
not	O
quantified	O
,	O
CER	O
analysis	O
for	O
YMC	B-DatasetName
ASR	B-TaskName
suggests	O
that	O
"	O
Spanish	O
-	O
origin	O
"	O
words	O
provoke	O
a	O
significantly	O
higher	O
error	O
rate	O
than	O
the	O
YMC	B-DatasetName
lexicon	O
uninfluenced	O
by	O
Spanish	O
.	O

Tone	O
-	O
based	O
inflectional	O
morphology	O
is	O
not	O
separated	O
in	O
any	O
YMC	B-DatasetName
transcriptions	O
.	O

2	O
The	O
transcription	O
strategy	O
for	O
YMC	B-DatasetName
was	O
unusual	O
in	O
that	O
the	O
practical	O
orthography	O
was	O
a	O
deep	O
,	O
underlying	O
system	O
that	O
represented	O
segmental	O
morpheme	O
boundaries	O
and	O
showed	O
elided	O
tones	O
in	O
parentheses	O
.	O

2021	O
,	O
§	O
2.3).Only	O
after	O
documentation	O
(	O
recording	O
and	O
timecoded	O
transcriptions	O
)	O
was	O
well	O
advanced	O
did	O
work	O
begin	O
on	O
a	O
finite	O
state	O
transducer	O
for	O
the	O
YMC	B-DatasetName
corpus	O
.	O

Note	O
,	O
then	O
,	O
that	O
the	O
surface	O
forms	O
in	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
corpus	O
are	O
based	O
on	O
FST	O
generation	O
from	O
an	O
underlying	O
transcription	O
as	O
input	O
and	O
not	O
from	O
the	O
direct	O
transcription	O
of	O
the	O
acoustic	O
signal	O
.	O

This	O
could	O
increase	O
the	O
CER	B-MetricName
and	O
WER	B-MetricName
for	O
ASR	B-TaskName
of	O
surface	O
forms	O
,	O
given	O
that	O
the	O
reference	O
for	O
evaluation	O
is	O
not	O
directly	O
derived	O
from	O
the	O
acoustic	O
signal	O
while	O
the	O
ASR	B-TaskName
hypothesis	O
is	O
so	O
derived	O
.	O

In	O
an	O
evaluation	O
across	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
development	O
and	O
test	O
sets	O
(	O
total	O
6.53	O
hours	O
)	O
of	O
the	O
relative	O
accuracy	O
of	O
ASR	B-TaskName
when	O
using	O
underlying	O
versus	O
surface	O
orthography	O
,	O
it	O
was	O
found	O
that	O
training	O
on	O
underlying	O
orthography	O
produced	O
slightly	O
greater	O
accuracy	O
than	O
training	O
on	O
surface	O
forms	O
:	O
Underlying	O
=	O
7.7/16.0	B-MetricValue
[	O
CER	B-MetricName
/	O
WER	B-MetricName
]	O
compared	O
to	O
Surface	O
=	O
7.8/16.5	B-MetricValue
[	O
CER	B-MetricName
/	O
WER	B-MetricName
]	O
(	O
Shi	O
et	O
al	O
.	O
,	O

The	O
decision	O
to	O
use	O
underlying	O
representations	O
in	O
ASR	B-TaskName
training	O
has	O
,	O
however	O
,	O
several	O
more	O
important	O
advantages	O
.	O

As	O
the	O
example	O
in	O
Appendix	O
A	O
demonstrates	O
,	O
ASR	O
output	O
includes	O
basic	O
segmentation	O
at	O
the	O
morphological	O
level	O
.	O

3.3	O
Intrinsic	O
metrics	O
:	O
CER	B-MetricName
,	O
WER	B-MetricName
,	O
and	O
consistency	B-MetricName
in	I-MetricName
transcriptions	I-MetricName
used	I-MetricName
as	I-MetricName
reference	I-MetricName
:	O
Although	O
both	O
CER	B-MetricName
and	O
WER	B-MetricName
reference	O
"	O
error	O
rate	O
"	O
in	O
regards	O
to	O
character	O
and	O
word	O
,	O
respectively	O
,	O
the	O
question	O
of	O
the	O
accuracy	O
of	O
the	O
reference	O
itself	O
is	O
rarely	O
explored	O
(	O
but	O
cf	O
.	O

For	O
YMC	B-DatasetName
,	O
only	O
one	O
speaker	O
,	O
Castillo	O
García	O
,	O
is	O
capable	O
of	O
accurate	O
transcription	O
,	O
which	O
in	O
YMC	B-DatasetName
is	O
the	O
sole	O
gold	O
standard	O
for	O
ASR	B-TaskName
training	O
,	O
validation	O
,	O
and	O
testing	O
.	O

Three	O
native	O
speaker	O
experts	O
have	O
worked	O
with	O
Amith	O
on	O
transcription	O
for	O
over	O
six	O
years	O
,	O
but	O
the	O
reference	O
for	O
ASR	B-TaskName
development	O
are	O
native	O
-	O
speaker	O
transcriptions	O
carefully	O
proofed	O
by	O
Amith	O
,	O
a	O
process	O
that	O
both	O
corrected	O
simple	O
errors	O
and	O
applied	O
a	O
single	O
standard	O
implemented	O
by	O
one	O
researcher	O
.	O

When	O
all	O
three	O
native	O
speaker	O
experts	O
were	O
asked	O
to	O
transcribe	O
the	O
same	O
90	O
minutes	O
or	O
recordings	O
,	O
and	O
the	O
results	O
were	O
compared	O
,	O
there	O
was	O
not	O
an	O
insignificant	O
level	O
of	O
variation	O
(	O
9%).The	O
aforementioned	O
scenario	O
suggests	O
the	O
impact	O
on	O
ASR	O
intrinsic	O
metrics	O
of	O
variation	O
in	O
transcriptions	O
across	O
multiple	O
annotators	O
,	O
or	O
even	O
inconsistencies	O
of	O
one	O
skilled	O
annotator	O
in	O
the	O
context	O
of	O
incipient	O
writing	O
systems	O
.	O

This	O
affects	O
not	O
only	O
ASR	B-TaskName
output	O
but	O
also	O
the	O
evaluation	O
of	O
ASR	B-TaskName
accuracy	O
via	O
character	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rates	I-MetricName
.	O

It	O
may	O
be	O
that	O
rather	O
than	O
character	B-MetricName
and	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
,	O
it	O
would	O
be	O
advisable	O
to	O
consider	O
the	O
character	O
and	O
word	O
discrepancy	O
rate	O
a	O
change	O
in	O
terminology	O
that	O
perhaps	O
better	O
communicates	O
the	O
idea	O
that	O
the	O
differences	O
between	O
REF	O
and	O
HYP	O
are	O
often	O
as	O
much	O
a	O
matter	O
of	O
opinion	O
as	O
fact	O
.	O

The	O
nature	O
and	O
value	O
of	O
utilizing	O
intrinsic	O
metrics	O
(	O
e.g.	O
,	O
CER	B-MetricName
and	O
WER	B-MetricName
)	O
for	O
evaluating	O
ASR	B-TaskName
effectiveness	I-TaskName
for	I-TaskName
endangered	I-TaskName
language	I-TaskName
documentation	I-TaskName
merits	O
rethinking	O
.	O

An	O
additional	O
factor	O
that	O
has	O
emerged	O
in	O
the	O
YMC	B-DatasetName
corpora	I-DatasetName
,	O
which	O
contains	O
very	O
rapid	O
speech	O
,	O
is	O
what	O
may	O
be	O
called	O
"	O
hypercorrection	O
"	O
.	O

In	O
both	O
cases	O
,	O
ASR	B-TaskName
"	O
errors	O
"	O
might	O
represent	O
a	O
more	O
accurate	O
representation	O
of	O
the	O
acoustic	O
signal	O
than	O
the	O
transcription	O
of	O
even	O
the	O
most	O
highly	O
capable	O
native	O
speakers	O
.	O

Parity	O
could	O
perhaps	O
best	O
be	O
considered	O
as	O
not	O
based	O
on	O
CER	B-MetricName
and	O
WER	B-MetricName
alone	O
but	O
on	O
whether	O
ASR	B-TaskName
output	O
achieves	O
a	O
lower	O
error	O
rate	O
in	O
these	O
two	O
measurements	O
as	O
compared	O
to	O
what	O
another	O
skilled	O
human	O
transcriber	O
might	O
achieve	O
.	O

Given	O
the	O
nature	O
of	O
EL	O
documentation	O
,	O
which	O
requires	O
high	O
levels	O
of	O
accuracy	O
if	O
the	O
corpus	O
is	O
to	O
be	O
easily	O
used	O
for	O
future	O
linguistic	O
research	O
,	O
it	O
is	O
essential	O
that	O
ASR	B-TaskName
-	O
generated	O
hypotheses	O
be	O
reviewed	O
by	O
an	O
expert	O
human	O
annotator	O
before	O
permanent	O
archiving	O
.	O

Certainly	O
,	O
audio	O
can	O
be	O
archived	O
with	O
metadata	O
alone	O
or	O
with	O
unchecked	O
ASR	B-TaskName
transcriptions	O
(	O
see	O
Michaud	O
et	O
al	O
.	O
,	O

2018	O
,	O
§	O
4.3	O
and	O
4.4	O
)	O
,	O
but	O
the	O
workflow	O
envisioned	O
for	O
YMC	B-DatasetName
is	O
to	O
use	O
ASR	O
to	O
reduce	O
human	O
effort	O
while	O
the	O
archived	O
corpus	O
of	O
audio	O
and	O
text	O
maintains	O
results	O
equivalent	O
to	O
those	O
that	O
would	O
be	O
obtained	O
by	O
careful	O
,	O
and	O
laborintensive	O
,	O
expert	O
transcription	O
.	O

CER	B-MetricName
and	O
WER	B-MetricName
were	O
measured	O
for	O
YMC	B-DatasetName
corpora	O
with	O
training	O
sets	O
of	O
10	O
,	O
20	O
,	O
50	O
,	O
and	O
92	O
hours	O
.	O

and	O
7.7/16.1	O
(	O
92	O
hrs	O
.	O
)	O
;	O

Measurement	O
of	O
human	O
effort	O
reduction	O
suggests	O
that	O
with	O
a	O
corpus	O
of	O
30	O
-	O
50	O
hours	O
,	O
even	O
for	O
a	O
relatively	O
challenging	O
language	O
such	O
as	O
YMC	B-DatasetName
,	O
E2E	B-TaskName
ASR	I-TaskName
can	O
achieve	O
the	O
level	O
of	O
accuracy	O
that	O
allows	O
a	O
reduction	O
of	O
human	O
effort	O
by	O
>	O
75	O
percent	O
(	O
e.g.	O
,	O
from	O
40	O
to	O
10	O
hours	O
,	O
approximately	O
)	O
.	O

These	O
totals	O
are	O
derived	O
from	O
measurements	O
with	O
the	O
FB	O
and	O
VN	O
corpora	O
,	O
the	O
two	O
corpora	O
for	O
which	O
ASR	B-TaskName
provided	O
the	O
initial	O
transcription	O
,	O
and	O
Castillo	O
subsequently	O
corrected	O
the	O
output	O
,	O
keeping	O
track	O
of	O
the	O
time	O
he	O
spent	O
.	O

For	O
the	O
first	O
corpus	O
,	O
Castillo	O
required	O
58.20	O
hours	O
to	O
correct	O
6.65	O
hours	O
of	O
audio	O
(	O
from	O
173	O
of	O
the	O
178	O
files	O
that	O
had	O
not	O
been	O
first	O
transcribed	O
by	O
a	O
speaker	O
trainee	O
)	O
.	O

Over	O
the	O
entire	O
set	O
of	O
197	O
files	O
(	O
11.81	O
hours	O
)	O
,	O
human	O
effort	O
was	O
111.27	O
hours	O
,	O
or	O
9.42	O
hours	O
to	O
correct	O
1	O
hour	O
of	O
audio	O
.	O

Given	O
that	O
the	O
ASR	B-TaskName
system	I-TaskName
was	O
trained	O
on	O
an	O
underlying	O
orthography	O
,	O
the	O
final	O
result	O
of	O
<	O
10	O
hours	O
of	O
human	O
effort	O
per	O
hour	O
of	O
audio	O
is	O
a	O
transcribed	O
and	O
partially	O
parsed	O
corpus	O
.	O

Table	O
3	O
presents	O
an	O
analysis	O
of	O
two	O
lines	O
of	O
a	O
recording	O
that	O
was	O
first	O
processed	O
by	O
E2E	B-TaskName
ASR	I-TaskName
and	O
corrected	O
by	O
Castillo	O
García	O
.	O

This	O
focus	O
on	O
extrinsic	O
metrics	O
reflects	O
the	O
realization	O
that	O
the	O
ultimate	O
goal	O
of	O
computational	O
systems	O
is	O
not	O
to	O
achieve	O
the	O
lowest	O
CER	B-MetricName
and	O
WER	B-MetricName
but	O
to	O
help	O
documentation	O
initiatives	O
more	O
efficiently	O
produce	O
results	O
that	O
will	O
benefit	O
future	O
stakeholders	O
.	O

Recently	O
,	O
E2E	B-TaskName
ASR	I-TaskName
has	O
reached	O
comparable	O
or	O
better	O
performances	O
than	O
conventional	O
Hidden	B-MethodName
-	I-MethodName
Markov	I-MethodName
-	I-MethodName
Model	I-MethodName
-	O
based	O
ASR	B-TaskName
(	O
Graves	O
and	O
Jaitly	O
,	O
2014;Chiu	O
et	O
al	O
.	O
,	O

In	O
practice	O
,	O
E2E	B-TaskName
ASR	I-TaskName
systems	I-TaskName
are	O
less	O
affected	O
by	O
linguistic	O
constraints	O
and	O
are	O
generally	O
easier	O
to	O
train	O
.	O

The	O
benefits	O
of	O
such	O
systems	O
are	O
reflected	O
in	O
the	O
recent	O
trends	O
of	O
using	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
for	O
EL	O
documentation	O
(	O
Adams	O
et	O
al	O
.	O
,	O

2020;Matsuura	O
et	O
al	O
.	O
,	O

2021).In	O
developing	O
E2E	B-TaskName
ASR	I-TaskName
recipes	O
for	O
YMC	B-DatasetName
,	O
we	O
have	O
adopted	O
transformer	B-MethodName
and	I-MethodName
conformerbased	I-MethodName
encoder	I-MethodName
-	I-MethodName
decoder	I-MethodName
networks	I-MethodName
with	I-MethodName
hybrid	I-MethodName
CTC	I-MethodName
/	I-MethodName
attention	I-MethodName
training	O
(	O
Karita	O
et	O
al	O
.	O
,	O

We	O
used	O
the	O
YMC	B-DatasetName
-	I-DatasetName
Exp	I-DatasetName
(	O
trainsplit	O
)	O
for	O
training	O
and	O
other	O
YMC	B-DatasetName
corpora	O
for	O
evaluation	O
.	O

Four	O
systems	O
employ	O
the	O
byte	B-MethodName
-	I-MethodName
pair	I-MethodName
encoding	I-MethodName
(	O
BPE	B-MethodName
)	O
method	O
trained	O
from	O
unigram	O
language	O
models	O
(	O
Kudo	O
and	O
Richardson	O
,	O
2018	O
)	O
,	O
with	O
transcription	B-HyperparameterName
alphabets	I-HyperparameterName
limited	O
to	O
the	O
150	B-HyperparameterName
,	O
500	B-HyperparameterName
,	O
1000	B-HyperparameterName
,	O
and	O
1500	B-HyperparameterName
most	O
frequent	O
byte	O
-	O
pairs	O
in	O
the	O
training	O
set	O
.	O

The	O
other	O
three	O
ASR	O
systems	O
adopt	O
linguistic	O
units	O
,	O
including	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
mora	B-MethodName
.	O

The	O
YM	B-DatasetName
word	O
is	O
defined	O
as	O
a	O
stem	O
with	O
all	O
prefixes	O
(	O
such	O
as	O
completetive	O
ni	O
1	O
-	O
,	O
causative	O
sa	O
4	O
-	O
,	O
and	O
iterative	O
nda	O
3	O
-	O
)	O
separated	O
from	O
the	O
stem	O
by	O
a	O
hyphen	O
;	O
and	O
all	O
enclitics	O
(	O
particularly	O
person	O
markers	O
for	O
subjects	O
,	O
objects	O
,	O
and	O
possessors	O
,	O
such	O
as	O
=	O
yu	O
3	O
,	O
1sg	O
;	O
=	O
un	O
4	O
,	O
2sg	O
;	O
=	O
an	O
4	O
,	O
3sgFem	O
;	O
=	O
o	O
4	O
,	O
1plIncl	O
;	O
as	O
well	O
as	O
=	O
lu	O
3	O
,	O
augmentive	O
)	O
.	O

ASR	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
coco	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
Exp	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
ko	O
4	O
ko	O
13	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
Note	O
ASR	B-TaskName
suggested	O
Spanish	O
'	O
coco	O
'	O
coconut	O
for	O
Mixtec	O
ko	O
4	O
ko	O
13	O
(	O
'	O
to	O
be	O
abundant[plants	O
]	O
'	O
)	O
itics	O
have	O
alternative	O
tones	O
,	O
depending	O
on	O
stemfinal	O
vowel	O
and	O
tone	O
,	O
respectively	O
.	O

Three	O
combinations	O
have	O
been	O
evaluated	O
:	O
(	O
1	O
)	O
ROVER	B-MethodName
among	O
only	O
linguistic	O
units	O
(	O
i.e.	O
,	O
morae	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
word	B-MethodName
)	O
,	O
(	O
2	O
)	O
ROVER	B-MethodName
among	O
only	O
sub	O
-	O
word	O
units	O
(	O
in	O
this	O
case	O
BPE	B-MethodName
)	O
;	O
and	O
(	O
3	O
)	O
ROVER	B-MethodName
combination	O
utilizing	O
all	O
seven	O
systems	O
.	O

The	O
first	O
addresses	O
the	O
performance	O
of	O
end	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
end	I-TaskName
ASR	I-TaskName
across	O
three	O
corpora	O
,	O
each	O
with	O
slightly	O
different	O
recording	O
systems	O
and	O
content	O
.	O

As	O
clear	O
from	O
the	O
preceding	O
discussion	O
and	O
illustrated	O
in	O
Table	O
2	O
,	O
in	O
addition	O
to	O
training	O
on	O
the	O
word	O
unit	O
,	O
the	O
YMC	O
E2E	B-TaskName
ASR	I-TaskName
system	I-TaskName
was	O
trained	O
on	O
six	O
additional	O
linguistic	O
and	O
informational	O
sub	O
-	O
word	O
units	O
.	O

ROVER	B-MethodName
was	O
then	O
used	O
to	O
produce	O
composite	O
systems	O
in	O
which	O
the	O
outputs	O
of	O
all	O
seven	O
systems	O
were	O
combined	O
in	O
three	O
distinct	O
manners	O
.	O

In	O
all	O
cases	O
,	O
ROVER	B-MethodName
combinations	O
improved	O
the	O
result	O
of	O
any	O
individual	O
system	O
,	O
including	O
the	O
averages	O
for	O
either	O
of	O
the	O
two	O
types	O
of	O
units	O
:	O
linguistic	O
and	O
informational	O
.	O

The	O
average	O
CER	B-MetricName
/	O
WER	B-MetricName
for	O
linguistic	O
units	O
(	O
rows	O
A	O
-	O
C	O
)	O
was	O
10.4/	B-MetricValue
19.5	B-MetricValue
(	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
,	O
13.6/23.3	B-MetricValue
(	O
FB	B-DatasetName
)	O
,	O
and	O
10.7/21.7	B-MetricValue
(	O
VN	B-DatasetName
)	O
.	O

The	O
corresponding	O
figures	O
for	O
the	O
BPE	O
units	O
(	O
rows	O
D	O
-	O
G	O
)	O
were	O
7.7/16.0	B-MetricValue
(	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
,	O
9.7/19.5	B-MetricValue
(	O
FB	B-DatasetName
)	O
,	O
and	O
6.8/16.8	B-MetricValue
(	O
VN	B-DatasetName
)	O
.	O

In	O
terms	O
of	O
percentage	O
differences	O
between	O
the	O
two	O
types	O
of	O
units	O
,	O
the	O
numbers	O
are	O
not	O
insignificant	O
.	O

In	O
regards	O
to	O
CER	B-MetricName
,	O
performance	O
improved	O
from	O
linguistic	O
to	O
informational	O
units	O
by	O
26.0	B-MetricValue
,	O
28.7	B-MetricValue
,	O
and	O
36.4	B-MetricValue
percent	O
across	O
the	O
Exp(Test	B-DatasetName
)	I-DatasetName
,	O
FB	B-DatasetName
,	O
and	O
VN	B-DatasetName
corpora	O
.	O

In	O
regards	O
to	O
WER	B-MetricName
,	O
performance	O
improved	O
by	O
17.9	B-MetricValue
,	O
16.3	B-MetricValue
,	O
and	O
22.6	B-MetricValue
percent	O
across	O
the	O
same	O
three	O
corpora	O
.	O

The	O
experiments	O
also	O
addressed	O
two	O
remaining	O
questions	O
:	O
(	O
1	O
)	O
does	O
unweighted	O
ROVER	B-MethodName
combination	O
improve	O
the	O
accuracy	O
of	O
ASR	B-TaskName
results	O
;	O
(	O
2	O
)	O
does	O
adding	O
linguistic	O
unit	O
performance	O
units	O
to	O
the	O
ROVER	B-MethodName
"	O
voting	O
pool	O
"	O
improve	O
results	O
over	O
a	O
combination	O
of	O
only	O
BPE	B-MethodName
units	I-MethodName
.	O

In	O
regards	O
to	O
the	O
first	O
question	O
:	O
ROVER	B-MethodName
always	O
improves	O
results	O
over	O
any	O
individual	O
system	O
(	O
compare	O
row	O
H	O
to	O
rows	O
A	O
,	O
B	O
,	O
and	O
C	O
,	O
and	O
row	O
I	O
to	O
rows	O
D	O
,	O
E	O
,	O
F	O
,	O
and	O
G	O
)	O
.	O

The	O
second	O
question	O
is	O
addressed	O
by	O
comparing	O
rows	O
I	O
(	O
ROVER	B-MethodName
applied	O
only	O
to	O
the	O
four	O
BPE	B-MethodName
results	O
)	O
to	O
J	O
(	O
adding	O
the	O
ASR	O
results	O
for	O
the	O
three	O
linguistic	O
units	O
into	O
the	O
combination	O
)	O
.	O

In	O
only	O
one	O
of	O
the	O
six	O
cases	O
(	O
CER	B-MetricName
of	O
Exp[test	B-DatasetName
]	I-DatasetName
)	O
does	O
including	O
word	B-MethodName
,	O
morpheme	B-MethodName
,	O
and	O
morae	O
lower	O
the	O
error	O
rate	O
from	O
the	O
results	O
of	O
a	O
simple	O
combination	O
of	O
the	O
four	O
BPE	B-MethodName
results	O
(	O
in	O
this	O
case	O
from	O
7.6	B-MetricValue
[	O
row	O
I	O
]	O
to	O
7.4	B-MetricValue
[	O
row	O
J	O
]	O
)	O
.	O

In	O
one	O
case	O
,	O
there	O
is	O
no	O
change	O
(	O
CER	B-MetricName
for	O
the	O
VN	B-DatasetName
corpus	O
)	O
and	O
in	O
four	O
cases	O
,	O
including	O
linguistic	O
units	O
slightly	O
worsens	O
the	O
score	O
from	O
the	O
combination	O
of	O
BPE	B-MethodName
units	I-MethodName
alone	O
(	O
row	O
I	O
with	O
bold	O
numbers	O
)	O
.	O

The	O
implication	O
of	O
the	O
preceding	O
is	O
that	O
ASR	O
using	O
linguistic	O
units	O
yields	O
significantly	O
lower	O
accuracy	O
than	O
ASR	B-TaskName
that	O
uses	O
informational	O
(	O
BPE	B-MethodName
)	O
units	O
.	O

Combining	O
the	O
former	O
with	O
the	O
latter	O
in	O
an	O
unweighted	O
ROVER	B-MethodName
system	O
in	O
most	O
cases	O
does	O
not	O
improve	O
results	O
.	O

The	O
project	O
here	O
discussed	O
suggests	O
a	O
path	O
to	O
creating	O
such	O
corpora	O
using	O
end	O
-	O
to	O
-	O
end	O
ASR	O
technology	O
to	O
build	O
up	O
the	O
resources	O
(	O
30	O
-	O
50	O
hours	O
)	O
necessary	O
to	O
train	O
an	O
ASR	B-TaskName
system	O
with	O
perhaps	O
a	O
6	B-MetricValue
-	I-MetricValue
10	I-MetricValue
percent	I-MetricValue
CER	B-MetricName
.	O

Once	O
this	O
threshold	O
is	O
reached	O
,	O
it	O
is	O
unlikely	O
that	O
further	O
improvement	O
will	O
significantly	O
reduce	O
the	O
human	O
effort	O
needed	O
to	O
check	O
the	O
ASR	B-TaskName
output	O
for	O
accuracy	O
.	O

Indeed	O
,	O
even	O
if	O
there	O
are	O
no	O
"	O
errors	O
"	O
in	O
the	O
ASR	B-TaskName
output	O
,	O
confirmation	O
of	O
this	O
through	O
careful	O
revision	O
of	O
the	O
recording	O
of	O
the	O
transcription	O
would	O
probably	O
still	O
take	O
3	O
-	O
4	O
hours	O
.	O

The	O
effort	O
reduction	O
of	O
75	O
percent	O
documented	O
here	O
for	O
YMC	B-DatasetName
is	O
,	O
therefore	O
,	O
approaching	O
what	O
may	O
be	O
considered	O
the	O
minimum	O
amount	O
of	O
time	O
to	O
proofread	O
transcription	O
of	O
natural	O
speech	O
in	O
an	O
endangered	O
language	O
.	O

In	O
a	O
relatively	O
isolating	O
language	O
such	O
as	O
YM	O
,	O
such	O
a	O
system	O
is	O
not	O
difficult	O
for	O
native	O
speakers	O
to	O
write	O
nor	O
for	O
ASR	B-TaskName
systems	O
to	O
learn	O
.	O

It	O
has	O
the	O
advantage	O
of	O
creating	O
a	O
workflow	O
in	O
which	O
parsed	O
text	O
is	O
the	O
direct	O
output	O
of	O
E2E	B-TaskName
ASR	I-TaskName
.	O

The	O
error	O
rate	O
evaluations	O
across	O
the	O
spectrum	O
of	O
corpora	O
and	O
CER	B-MetricName
/	O
WER	B-MetricName
also	O
demonstrate	O
the	O
advantage	O
of	O
using	O
subword	O
units	O
such	O
as	O
BPE	O
and	O
subsequent	O
processing	O
by	O
ROVER	B-MethodName
for	O
system	O
combination	O
(	O
see	O
above	O
and	O
Table	O
2	O
)	O
.	O

A	O
final	O
question	O
concerns	O
additional	O
steps	O
once	O
CER	B-MetricName
is	O
reduced	O
to	O
6	B-MetricValue
-	I-MetricValue
8	I-MetricValue
percent	I-MetricValue
,	O
and	O
additional	O
improvements	O
to	O
ASR	B-TaskName
would	O
not	O
significantly	O
affect	O
the	O
human	O
effort	O
needed	O
to	O
produce	O
a	O
high	O
-	O
quality	O
time	O
-	O
coded	O
transcription	O
and	O
segmentation	O
.	O

Four	O
topics	O
are	O
suggested	O
:	O
(	O
1	O
)	O
address	O
issues	O
of	O
noise	O
,	O
overlapping	O
speech	O
,	O
and	O
other	O
challenging	O
recording	O
situations	O
;	O
(	O
2	O
)	O
focus	O
on	O
transfer	O
learning	O
to	O
related	O
languages	O
;	O
(	O
3	O
)	O
explore	O
the	O
impact	O
of	O
"	O
colonialization	O
"	O
by	O
a	O
dominant	O
language	O
;	O
and	O
(	O
4	O
)	O
focus	O
additional	O
ASR	B-TaskName
-	O
supported	O
corpus	O
development	O
on	O
producing	O
material	O
for	O
documentation	O
of	O
endangered	O
cultural	O
knowledge	O
,	O
a	O
facet	O
of	O
documentation	O
that	O
is	O
often	O
absent	O
from	O
endangered	O
language	O
documentation	O
projects	O
.	O

A	O
Analysis	O
of	O
ASR	B-TaskName
errors	O
in	O
one	O
recording	O
from	O
the	O
FB	O
corpus	O
Unique	O
identifier	O
:	O
2017	O
-	O
12	O
-	O
01	O
-	O
b	O
Speakers	O
:	O
Constantino	O
Teodoro	O
Bautista	O
and	O
Esteban	O
Guadalupe	O
Sierra	O
Spanish	O
:	O
The	O
first	O
13	O
seconds	O
(	O
3	O
segments	O
)	O
of	O
the	O
recording	O
were	O
of	O
a	O
Spanish	O
speaker	O
describing	O
the	O
plant	O
being	O
collected	O
(	O
Passiflora	O
biflora	O
Lam	O
.	O
)	O

Note	O
:	O
A	O
total	O
16	O
out	O
of	O
33	O
segments	O
/	O
utterances	O
are	O
without	O
ASR	B-TaskName
error	O
.	O

Notes	O
:	O
ASR	B-TaskName
does	O
not	O
output	O
caps	O
or	O
punctuation	O
.	O

00:00:17.105	O
-	O
>	O
00:00:19.477	O
ASR	B-TaskName
ya	O
1	O
mi	O
4	O
i	O
4	O
tu	O
1	O
tu	O
'	O
4	O
un	O
4	O
ku	O
3	O
rra	O
42	O
Exp	O
Ya	O
1	O
mi	O
4	O
i	O
4	O
tu	O
1	O
tu	O
'	O
4	O
un	O
4	O
ku	O
3	O
rra	O
42	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.6	O
.	O

00:00:19.477	O
-	O
>	O
00:00:23.688	O
ASR	O
ta	O
1	O
mas	O
4	O
tru	O
2	O
tela	O
ya	O
1	O
i	O
3	O
chi	O
4	O
ya	O
3	O
tin	O
3	O
ye	O
'	O
1	O
4e	O
4	O
ku	O
3	O
rra	O
42	O
ndi	O
4	O
covalentín	O
yo	O
'	O
4	O
o	O
4	O
Exp	O
ta	O
1	O
mas	O
4	O
tru	O
2	O
Tele	O
ya	O
1	O
i	O
3	O
chi	O
4	O
ya	O
3	O
tin	O
3	O
ye	O
'	O
1	O
4e	O
4	O
ku	O
3	O
rra	O
42	O
Nicu	O
Valentín	O
yo	O
'	O
4	O
o	O
4	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
the	O
proper	O
name	O
,	O
Nicu	O
Valentín	O
(	O
short	O
for	O
Nicolás	O
Valentín	O
)	O
but	O
did	O
get	O
the	O
accent	O
on	O
Valentín	O
,	O
while	O
mistaking	O
the	O
first	O
name	O
Nicu	O
for	O
ndi	O
4	O
co[valentín	O
]	O
7	O
*	O
.	O

00:00:23.688	O
-	O
>	O
00:00:31.086	O
ASR	B-TaskName
ya	O
1	O
i	O
3	O
chi	O
4	O
kwa	O
'	O
1	O
an	O
(	O
1	O
)	O
=	O
e	O
4	O
tan	O
3	O
xa	O
1	O
a	O
(	O
1	O
)	O
=	O
e	O
4	O
ku	O
3	O
rra	O
42	O
chi	O
4	O
ñu	O
3	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
ya	O
1	O
kwa	O
'	O
1	O
an	O
1	O
ni	O
1	O
nu	O
3	O
yo	O
'	O
4	O
o	O
4	O
ju	O
13	O
ta	O
'	O
3	O
an	O
2	O
=	O
ndu	O
1	O
ya	O
1	O
ko	O
4	O
ndo	O
3	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
Exp	O
ya	O
1	O
i	O
3	O
chi	O
4	O
kwa	O
'	O
1	O
an	O
(	O
1	O
)	O
=	O
e	O
4	O
tan	O
3	O
xa	O
1	O
a	O
(	O
1	O
)	O
=	O
e	O
4	O
ku	O
3	O
rra	O
42	O
chi	O
4	O
ñu	O
3	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
ya	O
1	O
kwa	O
'	O
1	O
an	O
1	O
ni	O
1	O
nu	O
3	O
yo	O
'	O
4	O
o	O
4	O
ju	O
13	O
ta	O
'	O
3	O
an	O
2	O
=	O
ndu	O
1	O
ya	O
1	O
ko	O
4	O
ndo	O
3	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.8	O
*	O
.	O

00:00:31.086	O
-	O
>	O
00:00:37.318	O
ASR	B-TaskName
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
kwi	O
4	O
i	O
24	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
ya	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
i	O
4	O
yo	O
(	O
2	O
)	O
=	O
a	O
2	O
mi	O
4	O
i	O
4	O
bi	O
1	O
xin	O
3	O
tan	O
3	O
Exp	O
kwi	O
1	O
yo	O
'	O
1	O
o	O
4	O
ndi	O
3	O
ku	O
'	O
3	O
un	O
3	O
kwi	O
4	O
i	O
24	O
ka	O
4	O
chi	O
2	O
=	O
na	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
ya	O
1	O
yo	O
'	O
4	O
o	O
4	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
i	O
4	O
yo	O
(	O
2	O
)	O
=	O
a	O
2	O
mi	O
4	O
i	O
4	O
bi	O
1	O
xin	O
3	O
tan	O
3	O
Notes	O
:	O
The	O
ASR	B-TaskName
hypothesis	O
missed	O
the	O
inanimate	O
enclitic	O
after	O
the	O
verb	O
su	O
4	O
kun	O
1	O
and	O
as	O
a	O
result	O
failed	O
to	O
mark	O
the	O
elision	O
of	O
the	O
stem	O
-	O
final	O
low	O
tone	O
as	O
would	O
occur	O
before	O
a	O
following	O
low	O
-	O
tone	O
enclitic.9	O
.	O

00:00:37.318	O
-	O
>	O
00:00:42.959	O
ASR	O
yo	O
'	O
3	O
o	O
4	O
xi	O
13	O
i	O
2	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
yo	O
4	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
Exp	O
yo	O
'	O
3	O
o	O
4	O
xi	O
1	O
i	O
32	O
ba	O
42	O
ndi	O
4	O
ba	O
'	O
1	O
a	O
3	O
=	O
e	O
2	O
ku	O
3	O
-nu	O
'	O
3	O
ni	O
2	O
tu	O
3	O
tun	O
4	O
kwi	O
3	O
so	O
(	O
3	O
)	O
=	O
e	O
4	O
mi	O
4	O
i	O
4	O
ti	O
4	O
ba	O
42	O
ko	O
14	O
o	O
3	O
yo	O
'	O
3	O
o	O
4	O
kwa	O
'	O
1	O
an	O
1	O
ji	O
'	O
4	O
in	O
(	O
4	O
)	O
=	O
o	O
4	O
xa	O
14	O
ku	O
'	O
1	O
u	O
1	O
,	O
Notes	O
:	O
ASR	O
missed	O
the	O
word	O
ji	O
'	O
4	O
in	O
4	O
(	O
'	O
with	O
'	O
,	O
comitative	O
)	O
and	O
as	O
a	O
result	O
wrote	O
the	O
1plInclusive	O
as	O
an	O
independent	O
pronoun	O
and	O
not	O
an	O
enclitic.10	O
.	O

00:00:42.959	O
-	O
>	O
00:00:49.142	O
ASR	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
coco	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2Exp	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
2	O
tio	O
3	O
o	O
2	O
yu	O
3	O
ku	O
4	O
ya	O
1	O
ba	O
4	O
li	O
4	O
ko	O
4	O
ko	O
13	O
nu	O
14	O
u	O
3	O
ñu	O
'	O
3	O
u	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
i	O
3	O
ta	O
(	O
2	O
)	O
=	O
e	O
2	O
,	O
Notes	O
:	O
ASR	B-TaskName
suggested	O
Spanish	O
'	O
coco	O
'	O
coconut	O
for	O
Mixtec	O
ko	O
4	O
ko	O
13	O
(	O
'	O
to	O
be	O
abundant[plants	O
]	O
'	O
)	O
.	O

Note	O
that	O
'	O
coco	O
'	O
was	O
spelled	O
as	O
it	O
is	O
in	O
Spanish	O
and	O
no	O
tones	O
were	O
included	O
in	O
the	O
ASR	B-TaskName
output.11	O
.	O

00:00:49.142	O
-	O
>	O
00:00:53.458	O
ASR	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
43	O
Exp	O
la	O
3	O
tun	O
4	O
=	O
ni	O
42	O
ya	O
3	O
a	O
(	O
3	O
)	O
=	O
e	O
2	O
tan	O
3	O
ti	O
1	O
xin	O
3	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
nde	O
'	O
3	O
e	O
4	O
ba	O
42	O
tan	O
3	O
o	O
4	O
ra	O
2	O
xi	O
4	O
yo	O
13	O
ndu	O
1	O
u	O
4	O
=	O
a	O
2	O
ndi	O
4	O
ya	O
1	O
kwi	O
4	O
i	O
24	O
ba	O
42	O
,	O
Notes	O
:	O
ASR	B-TaskName
missed	O
tone	O
42	O
,	O
writing	O
43	O
instead	O
.	O

00:00:57.279	O
-	O
>	O
00:01:02.728	O
ASR	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
(	O
4	O
)	O
=	O
a	O
2	O
ni	O
1	O
-xa	O
'	O
3	O
nda	O
2	O
=	O
e	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
tun	O
4	O
si	O
13	O
su	O
2	O
kan	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
tan	O
3	O
ndi	O
4	O
Exp	O
Yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
ndi	O
4	O
tan	O
42	O
i	O
4	O
in	O
(	O
4	O
)	O
=	O
a	O
2	O
ni	O
1	O
-xa	O
'	O
3	O
nda	O
2	O
=	O
e	O
4	O
tan	O
42	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
tun	O
4	O
si	O
13	O
su	O
2	O
kan	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
(	O
1	O
)	O
=	O
a	O
1	O
tan	O
3	O
ndi	O
4	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis.14	O
.	O

00:01:02.728	O
-	O
>	O
00:01:06.296	O
ASR	O
su	O
14	O
u	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
1	O
ba	O
42	O
ndi	O
4	O
su	O
14	O
u	O
3	O
ki	O
3	O
ti	O
4	O
ja	O
4	O
xi	O
24	O
=	O
ri	O
4	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
mi	O
4	O
i	O
4	O
ba	O
(	O
3	O
)	O
=	O
e	O
3	O
Exp	O
su	O
14	O
u	O
3	O
ya	O
1	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
1	O
ba	O
42	O
tan	O
3	O
ni	O
4	O
su	O
14	O
u	O
3	O
ki	O
3	O
ti	O
4	O
ja	O
4	O
xi	O
24	O
=	O
ri	O
4	O
,	O
sa	O
3	O
kan	O
4	O
i	O
4	O
in	O
4	O
yu	O
1	O
ku	O
1	O
mi	O
4	O
i	O
4	O
ba	O
(	O
3	O
)	O
=	O
e	O
3	O
,	O
Notes	O
:	O
ASR	B-TaskName
mistakenly	O
proposed	O
ndi	O
4	O
for	O
tan	O
3	O
ni	O
4	O
.15	O
*	O
.	O

00:01:14.768	O
-	O
>	O
00:01:18.281	O
ASR	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
Exp	O
mi	O
4	O
i	O
4	O
ba	O
143	O
xa	O
'	O
4	O
nda	O
2	O
=	O
na	O
(	O
1	O
)	O
=	O
e	O
1	O
ndi	O
4	O
xa	O
'	O
4	O
nu	O
3	O
su	O
4	O
kun	O
(	O
1	O
)	O
=	O
a	O
1	O
,	O
Notes	O
:	O
No	O
errors	O
in	O
the	O
ASR	B-TaskName
hypothesis	O
.	O

The	O
authors	O
gratefully	O
acknowledge	O
the	O
following	O
support	O
for	O
documenting	O
and	O
studying	O
Yoloxóchitl	O
Mixtec	O
:	O
National	O
Science	O
Foundation	O
,	O
Documenting	O
Endangered	O
Languages	O
(	O
DEL	O
):	O
Awards	O
1761421	O
,	O
1500595	O
,	O
0966462	O
(	O
Amith	O
,	O
PI	O
on	O
all	O
three	O
;	O
the	O
second	O
was	O
a	O
collaborative	O
project	O
with	O
SRI	O
International	O
,	O
Award	O
1500738	O
,	O
Andreas	O
Kathol	O
,	O
PI	O
)	O
;	O
Endangered	O
Language	O
Documentation	O
Programme	O
:	O
Awards	O
MDP0201	O
,	O
PPG0048	O
(	O
Amith	O
,	O
PI	O
on	O
both	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
conduct	O
comprehensive	O
evaluation	O
and	O
analysis	O
with	O
respect	O
to	O
the	O
robustness	B-TaskName
of	I-TaskName
natural	I-TaskName
language	I-TaskName
understanding	I-TaskName
models	I-TaskName
,	O
and	O
introduce	O
three	O
important	O
aspects	O
related	O
to	O
language	O
understanding	O
in	O
realworld	O
dialog	O
systems	O
,	O
namely	O
,	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

We	O
propose	O
a	O
model	O
-	O
agnostic	O
toolkit	O
LAUG	B-MethodName
to	O
approximate	O
natural	O
language	O
perturbations	O
for	O
testing	O
the	O
robustness	B-TaskName
issues	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
.	O

Four	O
data	O
augmentation	O
approaches	O
covering	O
the	O
three	O
aspects	O
are	O
assembled	O
in	O
LAUG	B-MethodName
,	O
which	O
reveals	O
critical	O
robustness	O
issues	O
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

The	O
augmented	O
dataset	O
through	O
LAUG	B-MethodName
can	O
be	O
used	O
to	O
facilitate	O
future	O
research	O
on	O
the	O
robustness	B-TaskName
testing	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
.	O

2019;Shah	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
their	O
robustness	O
to	O
changes	O
in	O
the	O
input	O
distribution	O
is	O
still	O
one	O
of	O
the	O
biggest	O
challenges	O
in	O
practical	O
use	O
.	O

As	O
shown	O
in	O
Fig	O
.	O

1	O
,	O
user	O
expressions	O
can	O
be	O
of	O
high	O
lexical	O
and	O
syntactic	O
diversity	O
when	O
a	O
system	O
is	O
deployed	O
to	O
users	O
;	O
typed	O
texts	O
may	O
differ	O
significantly	O
from	O
those	O
recognized	O
from	O
voice	O
speech	O
;	O
interaction	O
environments	O
may	O
be	O
full	O
of	O
chaos	O
and	O
even	O
users	O
themselves	O
may	O
introduce	O
irrelevant	O
noises	O
such	O
that	O
the	O
system	O
can	O
hardly	O
get	O
clean	O
user	O
input	O
.	O

Although	O
many	O
studies	O
have	O
discussed	O
the	O
LU	B-TaskName
robustness	I-TaskName
(	O
Ray	O
et	O
al	O
.	O
,	O

2018;Zhu	O
et	O
al	O
.	O
,	O

In	O
order	O
to	O
study	O
the	O
real	O
-	O
world	O
robustness	O
issues	O
,	O
we	O
define	O
the	O
LU	B-TaskName
robustness	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
.	O

Therefore	O
,	O
we	O
propose	O
an	O
automatic	O
method	O
LAUG	B-MethodName
for	O
Language	B-MethodName
understanding	I-MethodName
AUGmentation	I-MethodName
in	O
this	O
paper	O
to	O
approximate	O
the	O
natural	O
perturbations	O
to	O
existing	O
data	O
.	O

LAUG	B-MethodName
is	O
a	O
black	O
-	O
box	O
testing	O
toolkit	O
on	O
LU	B-TaskName
robustness	I-TaskName
composed	O
of	O
four	O
data	O
augmentation	O
methods	O
,	O
including	O
word	B-TaskName
perturbation	I-TaskName
,	O
text	B-TaskName
paraphrasing	I-TaskName
,	O
speech	B-TaskName
recognition	I-TaskName
,	O
and	O
speech	B-TaskName
disfluency	I-TaskName
.	O

We	O
instantiate	O
LAUG	B-MethodName
on	O
two	O
dialog	O
corporaFrames	B-DatasetName
(	O
El	O
Asri	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MultiWOZ	B-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

Quality	O
evaluation	O
by	O
annotators	O
indicates	O
that	O
the	O
utterances	O
augmented	O
by	O
LAUG	B-MethodName
are	O
reasonable	O
and	O
appropriate	O
with	O
regards	O
to	O
each	O
augmentation	O
approach	O
's	O
target	O
.	O

Real	O
user	O
evaluation	O
further	O
verifies	O
that	O
LAUG	B-MethodName
well	O
reflects	O
real	O
-	O
world	O
robustness	O
issues	O
.	O

Our	O
contributions	O
can	O
be	O
summarized	O
as	O
follows	O
:	O
(	O
1	O
)	O
We	O
classify	O
the	O
LU	B-TaskName
robustness	I-TaskName
systematically	O
into	O
three	O
aspects	O
that	O
occur	O
in	O
real	O
-	O
world	O
dialog	O
,	O
including	O
linguistic	O
variety	O
,	O
speech	O
characteristics	O
and	O
noise	O
perturbation	O
;	O
(	O
2	O
)	O
We	O
propose	O
a	O
general	O
and	O
model	O
-	O
agnostic	O
toolkit	O
,	O
LAUG	B-MethodName
,	O
which	O
is	O
an	O
integration	O
of	O
four	O
data	O
augmentation	O
methods	O
on	O
LU	O
that	O
covers	O
the	O
three	O
aspects	O
.	O
(	O

3	O
)	O
We	O
conduct	O
an	O
in	O
-	O
depth	O
analysis	O
of	O
LU	B-TaskName
robustness	I-TaskName
on	O
two	O
dialog	O
corpora	O
with	O
a	O
variety	O
of	O
baselines	O
and	O
standardized	O
evaluation	O
measures	O
.	O
(	O

4	O
)	O
Quality	O
and	O
user	O
evaluation	O
results	O
demonstrate	O
that	O
the	O
augmented	O
data	O
are	O
representative	O
of	O
real	O
-	O
world	O
noisy	O
data	O
,	O
therefore	O
can	O
be	O
used	O
for	O
future	O
research	O
to	O
test	O
the	O
LU	B-TaskName
robustness	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
1	O
.	O

2020;He	O
et	O
al	O
.	O
,	O

2018).Noise	O
Perturbation	O
Most	O
dialog	O
systems	O
are	O
trained	O
only	O
on	O
noise	O
-	O
free	O
interactions	O
.	O

From	O
another	O
perspective	O
,	O
four	O
operations	O
from	O
EDA	O
perform	O
an	O
Invariance	O
test	O
,	O
while	O
slot	O
value	O
replacement	O
conducts	O
a	O
Directional	O
Expectation	O
test	O
according	O
to	O
CheckList	O
(	O
Ribeiro	O
et	O
al	O
.	O
,	O

2020).Text	O
Paraphrasing	O
The	O
target	O
of	O
text	O
paraphrasing	O
is	O
to	O
generate	O
a	O
new	O
utterance	O
x	O
=	O
x	O
while	O
maintaining	O
its	O
dialog	O
act	O
unchanged	O
,	O
i.e.	O
y	O
=	O
y.	O
We	O
applied	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
,	O
a	O
finetuned	O
language	O
model	O
conditioned	O
on	O
the	O
dialog	O
acts	O
,	O
to	O
paraphrase	O
the	O
sentences	O
as	O
data	O
augmentation	O
.	O

Then	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
is	O
finetuned	O
on	O
the	O
processed	O
data	O
so	O
that	O
it	O
can	O
be	O
aware	O
of	O
dialog	O
context	O
when	O
generating	O
paraphrases	O
.	O

As	O
a	O
result	O
,	O
we	O
find	O
that	O
the	O
average	O
token	O
length	O
of	O
generated	O
utterances	O
with	O
/	O
without	O
"	O
*	O
"	O
is	O
15.96/12.67	O
respectively	O
after	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
's	O
finetuning	O
on	O
MultiWOZ.It	B-DatasetName
should	O
be	O
noted	O
that	O
slot	O
values	O
of	O
an	O
utterance	O
can	O
be	O
paraphrased	O
by	O
models	O
,	O
resulting	O
in	O
a	O
different	O
semantic	O
meaning	O
y	O
.	O

To	O
prevent	O
generating	O
irrelevant	O
sentences	O
,	O
we	O
apply	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
in	O
paraphrases	O
with	O
original	O
slot	O
values	O
by	O
fuzzy	O
matching	O
3	O
,	O
and	O
replace	O
the	O
detected	O
values	O
in	O
bad	O
paraphrases	O
with	O
original	O
values	O
.	O

Speech	B-TaskName
Recognition	I-TaskName
We	O
simulate	O
the	O
speech	B-TaskName
recognition	I-TaskName
(	O
SR	B-TaskName
)	O
process	O
with	O
a	O
TTS	B-MethodName
-	I-MethodName
ASR	I-MethodName
pipeline	I-MethodName
(	O
Park	O
et	O
al	O
.	O
,	O

First	O
we	O
transfer	O
textual	O
user	O
utterance	O
x	O
to	O
its	O
audio	O
form	O
a	O
using	O
gTTS	B-MethodName
4	I-MethodName
(	O
Oord	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
a	O
Text	O
-	O
to	O
-	O
Speech	O
system	O
.	O

Then	O
audio	O
data	O
is	O
translated	O
back	O
into	O
text	O
x	O
by	O
DeepSpeech2	B-MethodName
(	O
Amodei	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
an	O
Automatic	B-MethodName
Speech	I-MethodName
Recognition	I-MethodName
(	I-MethodName
ASR	I-MethodName
)	I-MethodName
system	I-MethodName
.	O

We	O
directly	O
use	O
the	O
released	O
models	O
in	O
the	O
DeepSpeech2	B-MethodName
repository	O
5	O
with	O
the	O
original	O
configuration	O
,	O
where	O
the	O
speech	O
model	O
is	O
trained	O
on	O
Baidu	B-DatasetName
Internal	I-DatasetName
English	I-DatasetName
Dataset	O
,	O
and	O
the	O
language	O
model	O
is	O
trained	O
on	O
CommonCrawl	B-DatasetName
Data	I-DatasetName
.	O

ASR	B-TaskName
sometimes	O
wrongly	O
identifies	O
one	O
word	O
as	O
another	O
with	O
similar	O
pronunciation	O
.	O

Most	O
slot	O
values	O
could	O
be	O
relocated	O
by	O
our	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
rules	O
.	O

Repeats	O
I	O
,	O
I	O
want	O
to	O
go	O
to	O
,	O
go	O
to	O
Cambridge	O
.	O

We	O
present	O
some	O
examples	O
of	O
SD	B-TaskName
in	O
Table	O
5	O
.	O

In	O
order	O
to	O
approximate	O
the	O
real	O
distribution	O
of	O
disfluency	O
,	O
the	O
interruption	O
points	O
of	O
filled	O
pauses	O
and	O
repeats	O
are	O
predicted	O
by	O
a	O
Bi	B-MethodName
-	I-MethodName
LSTM+CRF	I-MethodName
model	O
(	O
Zayats	O
et	O
al	O
.	O
,	O

2016	O
)	O
trained	O
on	O
an	O
annotated	O
dataset	O
SwitchBoard	B-DatasetName
(	O
Godfrey	O
et	O
al	O
.	O
,	O

The	O
filler	O
words	O
,	O
restart	O
terms	O
,	O
and	O
edit	O
terms	O
and	O
their	O
occurrence	O
frequency	O
are	O
all	O
sampled	O
from	O
their	O
distribution	O
in	O
SwitchBoard	B-DatasetName
.	O

Therefore	O
,	O
SD	B-TaskName
augmentation	O
do	O
not	O
change	O
the	O
original	O
semantic	O
and	O
labels	O
of	O
the	O
utterance	O
,	O
i.e.	O
y	O
=	O
y.	O
In	O
our	O
experiments	O
we	O
adopt	O
Frames	B-DatasetName
6	O
(	O
El	O
Asri	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MultiWOZ	B-DatasetName
(	O
Budzianowski	O
et	O
al	O
.	O
,	O

In	O
particular	O
,	O
MultiWOZ	O
is	O
one	O
of	O
the	O
most	O
challenging	O
datasets	O
due	O
to	O
its	O
multi	O
-	O
domain	O
setting	O
and	O
complex	O
ontology	O
,	O
and	O
we	O
conduct	O
our	O
experiments	O
on	O
the	O
latest	O
annotation	O
-	O
enhanced	O
version	O
MultiWOZ	B-DatasetName
2.3	I-DatasetName
(	O
Han	O
et	O
al	O
.	O
,	O

Table	O
7	O
shows	O
the	O
change	O
rates	O
in	O
different	O
as-	O
7	O
See	O
appendix	O
for	O
the	O
hyperparameter	O
setting	O
of	O
LAUG.pects	B-MethodName
by	O
comparing	O
our	O
augmented	O
utterances	O
with	O
the	O
original	O
counterparts	O
.	O

For	O
instance	O
,	O
TP	B-TaskName
rewrites	O
the	O
text	O
without	O
changing	O
the	O
original	O
meaning	O
,	O
thus	O
lexical	O
and	O
syntactic	O
representations	O
dramatically	O
change	O
,	O
while	O
most	O
slot	O
values	O
remain	O
unchanged	O
.	O

To	O
ensure	O
the	O
quality	O
of	O
our	O
augmented	O
test	O
set	O
,	O
we	O
conduct	O
human	O
annotation	O
on	O
1,000	O
sampled	O
utterances	O
in	O
each	O
augmented	O
test	O
set	O
of	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
.	O

According	O
to	O
the	O
feature	O
of	O
each	O
augmentation	O
method	O
,	O
different	O
evaluation	O
protocols	O
are	O
used	O
.	O

For	O
TP	B-TaskName
and	O
SD	B-TaskName
,	O
annotators	O
check	O
whether	O
the	O
meaning	O
of	O
utterances	O
and	O
dialog	O
acts	O
are	O
unchanged	O
.	O

For	O
WP	B-TaskName
,	O
changing	O
slot	O
values	O
is	O
allowed	O
due	O
to	O
slot	O
value	O
replacement	O
,	O
but	O
the	O
slot	O
name	O
should	O
be	O
the	O
same	O
.	O

To	O
support	O
a	O
multi	O
-	O
intent	O
setting	O
in	O
classificationbased	O
models	O
,	O
we	O
decouple	O
the	O
LU	B-TaskName
process	O
as	O
follows	O
:	O
first	O
perform	O
domain	O
classification	O
and	O
intent	O
detection	O
,	O
then	O
concatenate	O
two	O
special	O
tokens	O
which	O
indicate	O
the	O
detected	O
domain	O
and	O
intent	O
(	O
e.g.[restaurant][inf	O
orm	O
]	O
)	O
at	O
the	O
beginning	O
of	O
the	O
input	O
sequence	O
,	O
and	O
last	O
encode	O
the	O
new	O
sequence	O
to	O
predict	O
slot	O
tags	O
.	O

We	O
conduct	O
robustness	O
testing	O
on	O
all	O
three	O
capacities	O
for	O
five	O
base	O
models	O
using	O
four	O
augmentation	O
methods	O
in	O
LAUG	B-MethodName
.	O

Overall	O
F1	B-MetricName
-	O
measure	O
performance	O
on	O
Frames	B-DatasetName
and	O
MultiWOZ	B-DatasetName
is	O
shown	O
in	O
Table	O
8	O
.	O

This	O
indicates	O
the	O
effectiveness	O
of	O
LAUG	B-MethodName
in	O
improving	O
the	O
model	O
's	O
robustness	O
.	O

ToD	B-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
the	O
state-	O
of	O
-	O
the	O
-	O
art	O
model	O
which	O
was	O
further	O
pre	O
-	O
trained	O
on	O
task	O
-	O
oriented	O
dialog	O
data	O
,	O
has	O
comparable	O
performance	O
with	O
BERT	B-MethodName
.	O

With	O
most	O
augmentation	O
methods	O
,	O
ToD	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
slightly	O
better	O
robustness	O
than	O
BERT.Since	B-MethodName
the	O
data	O
volume	O
of	O
Frames	B-DatasetName
is	O
far	O
less	O
than	O
that	O
of	O
MultiWOZ	B-DatasetName
,	O
the	O
performance	O
improvement	O
of	O
pre	O
-	O
trained	O
models	O
on	O
Frames	B-DatasetName
is	O
larger	O
than	O
that	O
on	O
MultiWOZ	B-DatasetName
.	O

test	O
set	O
more	O
remarkably	O
in	O
Frames	B-DatasetName
where	O
data	O
is	O
not	O
sufficient	O
.	O

The	O
dramatic	O
performance	O
drop	O
when	O
testing	O
on	O
SR	O
and	O
SD	O
data	O
indicates	O
that	O
robustness	O
for	O
speech	O
characteristics	O
may	O
be	O
the	O
most	O
challenging	O
issue	O
.	O

3	O
shows	O
how	O
the	O
performance	O
of	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
changes	O
on	O
MultiWOZ	B-DatasetName
when	O
the	O
ratio	O
of	O
augmented	O
training	O
data	O
to	O
the	O
original	O
data	O
varies	O
from	O
0.1	O
to	O
4.0	O
.	O

F1	B-MetricName
scores	O
on	O
augmented	O
test	O
sets	O
increase	O
when	O
there	O
are	O
more	O
augmented	O
data	O
for	O
training	O
.	O

The	O
performance	O
of	O
BERT	B-MethodName
on	O
augmented	O
test	O
sets	O
is	O
improved	O
when	O
augmentation	O
ratio	O
is	O
less	O
than	O
0.5	O
but	O
becomes	O
almost	O
unchanged	O
after	O
0.5	O
while	O
GPT-2	B-MethodName
keeps	O
increasing	O
stably	O
.	O

Between	O
augmentation	O
approaches	O
In	O
order	O
to	O
study	O
the	O
influence	O
of	O
each	O
augmentation	O
approach	O
in	O
LAUG	B-MethodName
,	O
we	O
test	O
the	O
performance	O
changes	O
when	O
one	O
augmentation	O
approach	O
is	O
removed	O
from	O
constructing	O
augmented	O
training	O
data	O
.	O

Results	O
on	O
Mul	B-DatasetName
-	I-DatasetName
tiWOZ	I-DatasetName
are	O
shown	O
in	O
Table	O
10	O
.	O

We	O
can	O
also	O
observe	O
an	O
increase	O
in	O
performance	O
when	O
it	O
is	O
removed	O
,	O
especially	O
for	O
MILU	B-MethodName
.	O

This	O
implies	O
a	O
lack	O
of	O
LU	B-TaskName
robustness	I-TaskName
in	O
detecting	O
unseen	O
entities	O
.	O

Table	O
11b	O
shows	O
the	O
results	O
of	O
ablation	O
study	O
on	O
SD	B-TaskName
.	O

We	O
collected	O
240	O
speech	O
utterances	O
from	O
real	O
humans	O
as	O
follows	O
:	O
First	O
,	O
we	O
sampled	O
120	O
combinations	O
of	O
DA	O
from	O
the	O
test	O
set	O
of	O
MultiWOZ	B-DatasetName
.	O

This	O
is	O
because	O
multiple	O
robustness	O
issues	O
may	O
be	O
included	O
in	O
one	O
real	O
case	O
,	O
while	O
each	O
augmentation	O
method	O
in	O
LAUG	B-MethodName
evaluates	O
them	O
separately	O
.	O

Despite	O
the	O
difference	O
,	O
model	O
performance	O
on	O
the	O
real	O
data	O
is	O
remarkably	O
improved	O
after	O
every	O
model	O
is	O
finetuned	O
on	O
the	O
augmented	O
data	O
,	O
verifying	O
that	O
LAUG	B-MethodName
effectively	O
enhances	O
the	O
model	O
's	O
real	O
-	O
world	O
robustness	O
.	O

Table	O
13	O
investigates	O
which	O
error	O
type	O
the	O
model	O
has	O
made	O
on	O
the	O
real	O
test	O
set	O
by	O
manually	O
checking	O
all	O
the	O
error	O
outputs	O
of	O
BERT	B-MethodName
Ori	O
.	O
"	O

It	O
can	O
be	O
observed	O
that	O
the	O
model	O
seriously	O
suffers	O
to	O
LU	B-TaskName
robustness	I-TaskName
(	O
over	O
70	O
%	O
)	O
,	O
and	O
that	O
almost	O
half	O
of	O
the	O
error	O
is	O
due	O
to	O
Language	O
Variety	O
.	O

This	O
shows	O
that	O
BERT	B-MethodName
Aug.	O
can	O
solve	O
these	O
two	O
kinds	O
of	O
problems	O
better	O
.	O

Robustness	B-TaskName
in	I-TaskName
LU	I-TaskName
has	O
always	O
been	O
a	O
challenge	O
in	O
task	O
-	O
oriented	O
dialog	O
.	O

This	O
paper	O
aims	O
to	O
provide	O
an	O
automatic	O
way	O
to	O
test	O
the	O
LU	B-TaskName
robustness	I-TaskName
in	I-TaskName
task	I-TaskName
-	I-TaskName
oriented	I-TaskName
dialog	I-TaskName
.	O

2016;Ebrahimi	O
et	O
al	O
.	O
,	O

While	O
data	O
augmentation	O
can	O
be	O
an	O
efficient	O
method	O
to	O
address	O
data	O
sparsity	O
,	O
it	O
can	O
improve	O
the	O
generalization	O
abilities	O
and	O
measure	O
the	O
model	B-TaskName
robustness	I-TaskName
as	O
well	O
(	O
Eshghi	O
et	O
al	O
.	O
,	O

2019;Iyyer	O
et	O
al	O
.	O
,	O

Word	O
-	O
level	O
operations	O
(	O
Kolomiyets	O
et	O
al	O
.	O
,	O

2019;Xu	O
and	O
Sarikaya	O
,	O
2014	O
)	O
worked	O
on	O
the	O
out	O
-	O
of	O
-	O
vocabulary	O
problem	O
when	O
facing	O
unseen	O
user	O
expression	O
.	O

Simulating	O
ASR	B-TaskName
errors	O
(	O
Schatzmann	O
et	O
al	O
.	O
,	O

As	O
most	O
work	O
tackles	O
LU	O
robustness	O
from	O
only	O
one	O
perspective	O
,	O
we	O
present	O
a	O
comprehensive	O
study	O
to	O
reveal	O
three	O
critical	O
issues	O
in	O
this	O
paper	O
,	O
and	O
shed	O
light	O
on	O
a	O
thorough	O
robustness	O
evaluation	O
of	O
LU	B-TaskName
in	I-TaskName
dialog	I-TaskName
systems	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
systematic	O
robustness	B-TaskName
evaluation	I-TaskName
of	I-TaskName
language	I-TaskName
understanding	I-TaskName
(	I-TaskName
LU	I-TaskName
)	I-TaskName
in	I-TaskName
taskoriented	I-TaskName
dialog	I-TaskName
from	O
three	O
aspects	O
:	O
language	O
variety	O
,	O
speech	O
characteristics	O
,	O
and	O
noise	O
perturbation	O
.	O

In	O
-	O
depth	O
experiments	O
and	O
analysis	O
are	O
conducted	O
on	O
MultiWOZ	B-DatasetName
and	O
Frames	B-DatasetName
,	O
with	O
both	O
classification	O
-	O
and	O
generation	O
-	O
based	O
LU	O
models	O
.	O

In	O
addition	O
to	O
the	O
four	O
approaches	O
in	O
LAUG	B-MethodName
,	O
more	O
methods	O
to	O
evaluate	O
LU	O
robustness	O
can	O
be	O
considered	O
in	O
the	O
future	O
.	O

As	O
for	O
hyperparameters	O
in	O
LAUG	B-MethodName
,	O
we	O
set	O
the	O
ratio	B-HyperparameterName
of	I-HyperparameterName
perturbation	I-HyperparameterName
number	I-HyperparameterName
to	I-HyperparameterName
text	I-HyperparameterName
length	I-HyperparameterName
α	B-HyperparameterName
=	O
n	O
/	O
l	O
=	O
0.1	B-HyperparameterValue
in	O
EDA	O
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
used	O
to	O
finetune	O
SC	B-MethodName
-	I-MethodName
GPT	I-MethodName
in	O
TP	O
is	O
1e-4	B-HyperparameterValue
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
training	I-HyperparameterName
epoch	I-HyperparameterName
is	O
5	B-HyperparameterValue
,	O
and	O
the	O
beam	B-HyperparameterName
size	I-HyperparameterName
during	O
inference	O
is	O
5	B-HyperparameterValue
.	O

In	O
SR	B-TaskName
,	O
the	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
the	O
language	O
model	O
in	O
DeepSpeech2	O
is	O
set	O
to	O
50	B-HyperparameterValue
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
Bi	B-MethodName
-	I-MethodName
LSTM+CRF	I-MethodName
in	O
SD	B-TaskName
is	O
1e-3	B-HyperparameterValue
.	O

The	O
threshold	B-HyperparameterName
of	I-HyperparameterName
fuzzy	I-HyperparameterName
matching	I-HyperparameterName
in	O
automatic	B-TaskName
value	I-TaskName
detection	I-TaskName
is	O
set	O
to	O
0.9	B-HyperparameterValue
in	O
TP	B-TaskName
and	O
0.7	B-HyperparameterValue
in	O
SR.For	B-TaskName
hyperparameters	O
of	O
base	O
models	O
.	O

The	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
1e-4	B-HyperparameterValue
for	O
BERT	B-MethodName
,	O
1e-5	B-HyperparameterValue
for	O
GPT2	B-MethodName
,	O
and	O
1e-3	B-HyperparameterName
for	O
MILU	B-MethodName
and	O
CopyNet	B-MethodName
.	O

The	O
beam	B-HyperparameterName
-	I-HyperparameterName
size	I-HyperparameterName
of	O
GPT2	B-MethodName
and	O
CopyNet	B-MethodName
is	O
5	B-HyperparameterValue
during	O
the	O
decoding	O
step	O
.	O

We	O
use	O
the	O
same	O
settings	O
of	O
DeepSpeech2	B-MethodName
in	O
SR	O
to	O
recognize	O
the	O
collected	O
audios	O
.	O

After	O
automatic	B-TaskName
span	I-TaskName
detection	I-TaskName
(	O
also	O
the	O
same	O
as	O
SR	O
's	O
)	O
are	O
applied	O
,	O
we	O
conduct	O
human	O
check	O
and	O
annotation	O
to	O
ensure	O
the	O
quality	O
of	O
labels	O
.	O

14	O
:	O
Robustness	O
on	O
different	O
schemes	O
on	O
Multi	B-DatasetName
-	I-DatasetName
WOZ	I-DatasetName
.	O

In	O
this	O
section	O
,	O
we	O
study	O
the	O
influence	O
of	O
training	O
/	O
prediction	O
schemes	O
on	O
LU	B-TaskName
robustness	I-TaskName
.	O

4.3	O
of	O
the	O
main	O
paper	O
,	O
the	O
process	O
of	O
classification	O
-	O
based	O
LU	O
models	O
is	O
decoupled	O
into	O
two	O
steps	O
to	O
handle	O
multiple	O
labels	O
:	O
one	O
for	O
domain	O
/	O
intent	O
classification	O
and	O
the	O
other	O
for	O
slot	O
tagging	O
.	O

The	O
classificationbased	O
models	O
can	O
predict	O
the	O
dialog	O
acts	O
within	O
a	O
single	O
step	O
in	O
this	O
way	O
.	O

Table	O
14	O
shows	O
that	O
MILU	B-MethodName
and	O
BERT	B-MethodName
gain	O
from	O
the	O
decoupled	O
scheme	O
on	O
the	O
original	O
test	O
set	O
.	O

This	O
indicates	O
that	O
the	O
decoupled	O
scheme	O
decreases	O
the	O
model	O
complexity	O
by	O
decomposing	O
the	O
output	O
space	O
.	O

MILU	B-MethodName
via	O
the	O
coupled	O
scheme	O
behaves	O
more	O
robustly	O
than	O
the	O
decoupled	O
counterpart	O
(	O
-2.61	B-MetricValue
vs.-7.05	B-MetricValue
)	O
,	O
while	O
BERT	B-MethodName
with	O
the	O
decoupled	O
scheme	O
outperforms	O
its	O
coupled	O
version	O
in	O
robustness	O
(	O
-6.45	B-MetricValue
vs.	O
-8.61	B-MetricValue
)	O
.	O

Meanwhile	O
,	O
BERT	B-MethodName
benefits	O
from	O
the	O
decoupled	O
scheme	O
and	O
still	O
achieves	O
86.95	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
but	O
BERT	B-MethodName
training	O
with	O
the	O
coupled	O
scheme	O
seems	O
more	O
susceptible	O
.	O

In	O
addition	O
,	O
both	O
MILU	B-MethodName
and	O
BERT	B-MethodName
recover	O
more	O
performance	O
by	O
the	O
proposed	O
decoupled	O
scheme	O
.	O

All	O
these	O
results	O
demonstrate	O
the	O
superiority	O
of	O
the	O
decoupled	O
scheme	O
in	O
classification	B-TaskName
-	I-TaskName
based	I-TaskName
LU	I-TaskName
models	O
.	O

In	O
Table	O
15	O
,	O
we	O
present	O
some	O
examples	O
of	O
augmented	O
utterances	O
in	O
MultiWOZ	B-DatasetName
.	O

In	O
terms	O
of	O
model	O
performance	O
,	O
MILU	B-MethodName
,	O
BERT	B-MethodName
and	O
GPT-2	B-MethodName
perform	O
well	O
on	O
WP	B-TaskName
and	O
TP	B-TaskName
in	O
the	O
example	O
while	O
Copy	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
misses	O
some	O
dialog	O
acts	O
.	O

For	O
the	O
SR	O
utterance	O
,	O
only	O
BERT	B-MethodName
obtains	O
all	O
the	O
correct	O
labels	O
.	O

MILU	B-MethodName
and	O
Copynet	O
both	O
fail	O
to	O
find	O
the	O
changed	O
value	O
spans	O
"	O
lester	O
"	O
and	O
"	O
thirteen	O
forty	O
five	O
"	O
.	O

Copynet	B-MethodName
's	O
copy	O
mechanism	O
is	O
fully	O
confused	O
by	O
recognition	O
error	O
and	O
even	O
predicts	O
discontinuous	O
slot	O
values	O
.	O

GPT-2	B-MethodName
successfully	O
finds	O
the	O
non	O
-	O
numerical	O
time	O
but	O
misses	O
"	O
leseter	O
"	O
.	O

Overall	O
,	O
in	O
this	O
example	O
,	O
BERT	B-MethodName
performs	O
quite	O
well	O
while	O
MILU	B-MethodName
and	O
Copy	B-MethodName
-	I-MethodName
Net	I-MethodName
expose	O
some	O
of	O
their	O
defects	O
in	O
robustness	O
.	O

Case-2	O
could	O
be	O
regarded	O
as	O
a	O
Speech	O
Characteristics	O
or	O
Noise	O
Perturbation	O
case	O
because	O
"	O
please	O
"	O
is	O
wrongly	O
recognized	O
as	O
"	O
police	O
"	O
by	O
ASR	B-TaskName
models	O
.	O

MILU	B-MethodName
and	O
BERT	B-MethodName
failed	O
in	O
most	O
of	O
these	O
cases	O
but	O
fixed	O
some	O
error	O
after	O
augmented	O
training	O
.	O

Extensive	O
experiments	O
show	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
model	O
obtained	O
by	O
our	O
method	O
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
on	O
various	O
downstream	O
tasks	O
.	O

For	O
example	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
achieves	O
78.8	B-MetricValue
on	O
the	O
GLUE	B-MetricName
testing	O
set	O
,	O
1.8	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
.	O

1	O
In	O
recent	O
years	O
,	O
pre	O
-	O
trained	O
language	O
models	O
,	O
such	O
as	O
the	O
representative	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
GPT-3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O

Except	O
BERT	B-MethodName
pre	O
-	O
trained	O
with	O
the	O
Masked	O
Language	O
Modeling	O
objective	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

We	O
first	O
consider	O
the	O
layer	O
types	O
.	O

2018Wu	O
et	O
al	O
.	O
,	O
,	O

It	O
has	O
been	O
shown	O
that	O
the	O
sandwich	O
order	O
can	O
bring	O
improvement	O
on	O
language	O
modeling	O
task	O
,	O
indicating	O
the	O
layer	O
order	O
contributes	O
to	O
model	O
performance	O
.	O

We	O
show	O
the	O
different	O
layer	O
variety	O
designs	O
of	O
existing	O
models	O
in	O
Figure	O
1(b	O
)	O
,	O
including	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019)/ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
Dynamic	B-MethodName
-	I-MethodName
Conv	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
Sandwich	B-MethodName
(	O
Press	O
et	O
al	O
.	O
,	O

It	O
can	O
be	O
seen	O
that	O
layer	O
variety	O
significantly	O
influences	O
model	O
performance	O
.	O

Pre	O
-	O
training	O
a	O
single	O
language	O
model	O
already	O
needs	O
to	O
consume	O
a	O
large	O
amount	O
of	O
computation	O
,	O
e.g.	O
,	O
2400	O
P100	O
GPU	O
days	O
for	O
pre	O
-	O
training	O
BERT	B-MethodName
.	O

After	O
obtaining	O
the	O
pre	O
-	O
trained	O
supernet	O
,	O
we	O
develop	O
an	O
evolutionary	O
algorithm	O
guided	O
by	O
MLM	O
evaluation	O
accuracy	B-MetricName
to	O
search	O
an	O
effective	O
architecture	O
with	O
specific	O
layer	O
variety	O
.	O

We	O
call	O
the	O
resulted	O
model	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
.	O

Extensive	O
experiments	O
show	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

Secondly	O
,	O
our	O
obtained	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
shows	O
superiority	O
over	O
BERT	B-MethodName
and	O
its	O
variants	O
.	O

For	O
example	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	O
small	O
achieves	O
79.8	O
on	O
GLUE	B-DatasetName
testing	O
set	O
,	O
1.8	O
higher	O
than	O
the	O
baseline	O
ELECTRAsmall	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

2017	O
and	O
Peters	O
et	O
al	O
.	O
(	O

2018b	O
)	O
propose	O
CoVe	B-MethodName
and	O
ELMo	B-MethodName
respectively	O
which	O
both	O
utilize	O
LSTM	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
to	O
generate	O
contextualized	O
word	O
representations	O
.	O

2018	O
)	O
introduce	O
GPT	B-MethodName
that	O
changes	O
the	O
backbone	O
to	O
transformers	O
where	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
are	O
arrayed	O
interleavedly	O
.	O

BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Besides	O
designing	O
pre	O
-	O
training	O
objectives	O
,	O
some	O
other	O
works	O
try	O
to	O
extend	O
BERT	B-MethodName
by	O
incorporating	O
knowledge	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020	O
)	O
or	O
with	O
multiple	O
languages	O
Conneau	O
and	O
Lample	O
,	O
2019;Chi	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

2019	O
)	O
.	O

To	O
solve	O
this	O
,	O
many	O
neural	O
architecture	O
search	O
algorithms	O
are	O
proposed	O
.	O

leverage	O
differentiable	O
neural	O
architecture	O
to	O
automatically	O
compress	O
BERT	B-MethodName
with	O
task	O
-	O
oriented	O
knowledge	O
distillation	O
for	O
specific	O
tasks	O
.	O

2020	O
)	O
utilize	O
architecture	O
search	O
to	O
improve	O
models	O
based	O
on	O
pre	O
-	O
trained	O
BERT	B-MethodName
for	O
the	O
relation	O
classification	O
task	O
.	O

Besides	O
,	O
Khetan	O
and	O
Karnin	O
(	O
2020	O
)	O
employ	O
pre	O
-	O
training	O
loss	O
to	O
help	O
prune	O
BERT	B-MethodName
,	O
but	O
their	O
method	O
can	O
not	O
find	O
new	O
architectures	O
.	O

The	O
layer	O
type	O
set	O
of	O
current	O
BERTlike	B-MethodName
models	O
consists	O
of	O
self	O
-	O
attention	O
for	O
information	O
communication	O
and	O
feed	O
-	O
forward	O
for	O
nonlinear	O
transformation	O
.	O

2019b	O
;	O
.	O

We	O
notice	O
that	O
convolution	O
(	O
LeCun	O
et	O
al	O
.	O
,	O

For	O
a	O
model	O
with	O
24	B-HyperparameterValue
layers	B-HyperparameterName
,	O
the	O
interleaved	O
order	O
can	O
be	O
expressed	O
by	O
the	O
following	O
list,[L	O
SA	O
1	O
,	O
L	O
FF	O
2	O
,	O
L	O
SA	O
3	O
,	O
L	O
FF	O
4	O
,	O
...	O
,	O
L	O
SA	O
23	O
,	O
L	O
FF	O
24	O
]	O
.	O
(	O

2020	O
)	O
can	O
be	O
expressed	O
as	O
}	O
The	O
accuracy	B-MetricName
is	O
used	O
to	O
guide	O
the	O
evolutionary	O
algorithm	O
for	O
generating	O
new	O
candidate	O
models.~After	O
T	O
iterations	O
,	O
the	O
candidate	O
with	O
best	O
pre	O
-	O
training	O
accuracy	B-MetricName
is	O
output	O
as	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
.	O

LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
can	O
be	O
scaled	O
up	O
to	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
medium	I-MethodName
/	I-MethodName
base	I-MethodName
with	O
larger	O
hidden	O
size	O
.	O

Beyond	O
the	O
above	O
manually	O
designed	O
orders	O
,	O
we	O
take	O
advantage	O
of	O
neural	O
architecture	O
search	O
to	O
identify	O
more	O
effective	O
layer	O
orders	O
for	O
pre	O
-	O
trained	O
models	O
.	O

The	O
order	O
to	O
be	O
discovered	O
can	O
be	O
expressed	O
as[L	O
1	O
,	O
L	O
2	O
,	O
...	O
,	O
L	O
i	O
,	O
...	O
,	O
L	O
N	O
]	O
,	O
(	O
4)where	O
L	B-HyperparameterName
i	O
∈	O
L	O
type	O
and	O
N	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
layers	I-HyperparameterName
.	O

Here	O
,	O
N	B-HyperparameterName
is	O
set	O
to	O
24	B-HyperparameterValue
,	O
following	O
common	O
practice	O
.	O

To	O
reduce	O
the	O
search	O
computations	O
,	O
recent	O
NAS	O
works	O
(	O
Pham	O
et	O
al	O
.	O
,	O

Inspired	O
by	O
this	O
strategy	O
,	O
we	O
construct	O
a	O
supernet	O
where	O
each	O
layer	O
contains	O
all	O
types	O
of	O
layers	O
,	O
i.e.	O
,	O
self	O
-	O
attention	O
,	O
feedforward	O
,	O
and	O
dynamic	O
convolution	O
.	O

2019	O
)	O
is	O
utilized	O
as	O
the	O
pre	O
-	O
training	O
objective	O
to	O
pretrain	O
the	O
supernet	O
since	O
MLM	O
accuracy	B-MetricName
can	O
reflect	O
the	O
model	O
performance	O
on	O
downstream	O
tasks	O
(	O
Lan	O
et	O
al	O
.	O
,	O

However	O
,	O
BooksCorpus	B-DatasetName
is	O
no	O
longer	O
publicly	O
available	O
.	O

To	O
ease	O
reproduction	O
,	O
we	O
train	O
models	O
on	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
that	O
is	O
open	O
-	O
sourced	O
and	O
of	O
similar	O
size	O
with	O
the	O
corpus	O
used	O
by	O
BERT	B-DatasetName
.	O

Fine	O
-	O
tuning	O
Datasets	O
To	O
compare	O
our	O
model	O
with	O
other	O
pre	O
-	O
trained	O
models	O
,	O
we	O
fine	O
-	O
tune	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
including	O
various	O
tasks	O
for	O
general	O
language	O
understanding	O
,	O
and	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2018	O
for	O
question	B-TaskName
answering	I-TaskName
.	O

2020	O
)	O
,	O
we	O
define	O
different	O
model	O
sizes	O
,	O
i.e.	O
,	O
"	O
small	O
"	O
,	O
"	O
medium	O
"	O
and	O
"	O
base	O
"	O
,	O
with	O
the	O
same	O
layer	O
number	O
of	O
24	B-HyperparameterValue
but	O
different	O
hidden	B-HyperparameterName
sizes	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
384	B-HyperparameterValue
,	O
and	O
768	B-HyperparameterValue
,	O
respectively	O
.	O

The	O
detailed	O
hyperparameters	O
are	O
shown	O
in	O
Appendix	O
.	O

Since	O
the	O
layer	O
number	O
of	O
models	O
in	O
medium	O
and	O
base	O
sizes	O
are	O
the	O
same	O
as	O
that	O
of	O
the	O
small	O
-	O
sized	O
one	O
,	O
the	O
obtained	O
architecture	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
can	O
be	O
easily	O
scaled	O
up	O
to	O
the	O
ones	O
of	O
medium	O
and	O
base	O
sizes	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
to	O
pre	O
-	O
train	O
the	O
supernet	O
with	O
MLM	O
loss	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-4	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
128	B-HyperparameterValue
,	O
max	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
128	B-HyperparameterValue
and	O
pre	B-HyperparameterName
-	I-HyperparameterName
training	I-HyperparameterName
step	I-HyperparameterName
number	I-HyperparameterName
of	O
2	B-HyperparameterValue
million	I-HyperparameterValue
.	O

Evaluation	O
Setup	O
To	O
compare	O
with	O
other	O
pretrained	O
models	O
,	O
we	O
pre	O
-	O
train	O
the	O
searched	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
architecture	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
from	O
scratch	O
on	O
the	O
Open	B-DatasetName
-	I-DatasetName
WebText	I-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
using	O
Re	O
-	O
placed	O
Token	O
Detection	O
(	O
Clark	O
et	O
al	O
.	O
,	O

We	O
fine	O
-	O
tune	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
GLUE	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2018	O
downstream	O
tasks	O
with	O
most	O
hyperparameters	O
the	O
same	O
as	O
those	O
of	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

For	O
GLUE	B-DatasetName
tasks	O
,	O
the	O
evaluation	O
metrics	O
are	O
Matthews	B-MetricName
correlation	I-MetricName
for	O
CoLA	B-DatasetName
,	O
Spearman	B-MetricName
correlation	I-MetricName
for	O
STS	B-DatasetName
,	O
and	O
accuracy	B-MetricName
for	O
other	O
tasks	O
,	O
which	O
are	O
averaged	O
to	O
get	O
GLUE	O
score	O
.	O

We	O
utilize	O
evaluation	O
metrics	O
of	O
Exact	B-MetricName
-	I-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F1	B-MetricName
for	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
.	O

Similar	O
to	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

See	O
Appendix	O
for	O
more	O
evaluation	O
details	O
.	O

Layer	O
Variety	O
Various	O
models	O
are	O
constructed	O
with	O
different	O
layer	O
variety	O
designs	O
,	O
and	O
their	O
results	O
on	O
GLUE	B-DatasetName
development	O
set	O
are	O
shown	O
in	O
Table	O
1	O
.	O

For	O
the	O
layer	O
types	O
,	O
if	O
only	O
two	O
layer	O
types	O
are	O
provided	O
,	O
selecting	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
yields	O
the	O
best	O
result	O
,	O
which	O
can	O
always	O
achieve	O
performance	O
higher	O
than	O
80	B-MetricValue
under	O
different	O
search	O
methods	O
.	O

With	O
only	O
dynamic	O
convolution	O
and	O
feedforward	O
,	O
the	O
performance	O
drops	O
dramatically	O
to	O
around	O
65	B-MetricValue
.	O

Surprisingly	O
,	O
without	O
feed	O
-	O
forward	O
,	O
the	O
layer	O
set	O
of	O
dynamic	O
convolution	O
and	O
self	O
-	O
attention	O
can	O
still	O
achieve	O
relatively	O
good	O
score	O
,	O
near	O
80	B-MetricValue
.	O

When	O
using	O
all	O
the	O
three	O
layer	O
types	O
,	O
we	O
can	O
obtain	O
the	O
best	O
81.8	B-MetricValue
score	O
,	O
1.4	B-MetricValue
higher	O
than	O
the	O
strong	O
baseline	O
ELECTRA	B-MethodName
(	O
80.4	B-MetricValue
)	O
and	O
0.6	B-MetricValue
higher	O
than	O
the	O
model	O
searched	O
with	O
only	O
self	O
-	O
attention	O
and	O
feedforward	O
(	O
81.2	B-MetricValue
)	O
.	O

For	O
example	O
,	O
with	O
the	O
same	O
layer	O
types	O
of	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
,	O
the	O
EA	O
searched	O
model	O
obtains	O
81.2	B-MetricValue
score	O
,	O
improving	O
BERT	B-MethodName
/	O
ELECTRA	B-MethodName
by	O
6.1/0.8	B-MetricValue
as	O
well	O
as	O
Sandwich	B-MethodName
by	O
2.6	B-MetricValue
.	O

Blue	O
and	O
yellow	O
dots	O
denote	O
the	O
accuracy	B-MetricName
of	O
top	O
10	O
candidates	O
for	O
each	O
method	O
respectively	O
,	O
while	O
the	O
plots	O
mean	O
their	O
averages	O
.	O

Randomly	O
searched	O
"	O
produces	O
candidate	O
models	O
at	O
random	O
for	O
estimation	O
while	O
"	O
EA	O
searched	O
"	O
generates	O
candidate	O
models	O
with	O
evolutionary	O
algorithm	O
guided	O
by	O
the	O
pre	O
-	O
training	O
MLM	O
accuracy	B-MetricName
.	O

With	O
the	O
same	O
layer	O
types	O
,	O
EA	O
searched	O
orders	O
are	O
generally	O
better	O
than	O
randomly	O
searched	O
ones	O
while	O
the	O
randomly	O
searched	O
ones	O
are	O
generally	O
better	O
than	O
random	O
ones	O
.	O

Figure	O
3	O
plots	O
the	O
pre	O
-	O
trianing	O
MLM	O
evaluation	O
accuracy	B-MetricName
over	O
search	O
iterations	O
with	O
both	O
random	O
and	O
evolutionary	O
search	O
methods	O
.	O

It	O
shows	O
that	O
the	O
accuracy	B-MetricName
of	O
evolutionary	O
search	O
is	O
obviously	O
higher	O
than	O
that	O
of	O
random	O
search	O
,	O
demonstrating	O
the	O
effectiveness	O
of	O
evolutionary	O
search	O
.	O

According	O
to	O
these	O
observation	O
,	O
for	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
,	O
if	O
we	O
replace	O
the	O
bottom	O
two	O
layers	O
with	O
dynamic	O
convolutions	O
or	O
the	O
top	O
layer	O
with	O
self	O
-	O
attention	O
,	O
the	O
performance	O
can	O
be	O
improved	O
by	O
0.3	B-MetricValue
or	O
0.5	B-MetricValue
respectively	O
on	O
GLUE	B-DatasetName
development	O
set	O
.	O

If	O
we	O
replace	O
the	O
bottom	O
8	B-HyperparameterValue
layers	B-HyperparameterName
with	O
manually	O
designed	O
'	O
ccsfccsf	O
'	O
(	O
'	O
c	O
'	O
,	O
's	O
'	O
and	O
'	O
f	O
'	O
denote	O
dynamic	O
convolution	O
,	O
self	O
-	O
attention	O
and	O
feed	O
-	O
forward	O
layers	O
,	O
respectively	O
)	O
and	O
replace	O
the	O
top	O
8	B-HyperparameterValue
layers	B-HyperparameterName
with	O
manually	O
designed	O
'	O
ssfsssfs	O
'	O
together	O
,	O
we	O
observe	O
0.7	B-MetricValue
performance	O
improvement	O
.	O

For	O
larger	O
model	O
size	O
"	O
medium	O
"	O
and	O
"	O
base	O
"	O
,	O
LV	B-MethodName
-	I-MethodName
BERTs	I-MethodName
still	O
outperform	O
other	O
baseline	O
models	O
,	O
demonstrating	O
the	O
good	O
generalization	O
in	O
terms	O
of	O
model	O
size	O
.	O

We	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pretrained	O
models	O
(	O
Radford	O
et	O
al	O
.	O
,	O

2020	O
;	O
on	O
GLUE	B-DatasetName
testing	O
set	O
and	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
to	O
show	O
its	O
advantages	O
.	O

2020	O
)	O
,	O
due	O
to	O
the	O
computation	O
resource	O
limit	O
,	O
we	O
only	O
pre	O
-	O
train	O
our	O
models	O
in	O
small	O
/	O
medium	O
/	O
base	O
sizes	O
for	O
1	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
OpenWebText	B-DatasetName
(	O
Gokaslan	O
and	O
Cohen	O
,	O
2019	O
)	O
.	O

However	O
,	O
note	O
that	O
these	O
methods	O
rely	O
on	O
a	O
pre	O
-	O
trained	O
large	O
teacher	O
network	O
and	O
thus	O
are	O
orthogonal	O
to	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
methods	O
.	O

Table	O
3	O
presents	O
the	O
performance	O
of	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
other	O
pre	O
-	O
trained	O
models	O
on	O
GLUE	B-DatasetName
testing	O
set	O
.	O

It	O
shows	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
other	O
pre	O
-	O
trained	O
models	O
with	O
similar	O
model	O
size	O
.	O

Remarkably	O
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
/	I-MethodName
base	I-MethodName
achieve	O
79.8/85.1	B-MetricValue
,	O
1.8/1.6	B-MetricValue
higher	O
than	O
strong	O
baselines	O
ELECTRAsmall	B-MethodName
/	I-MethodName
base	I-MethodName
.	O

Even	O
compared	O
with	O
knowledge	O
distillation	O
based	O
model	O
MobileBERT	B-MethodName
,	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
medium	I-MethodName
still	O
outperforms	O
it	O
by	O
0.3	B-MetricValue
.	O

Since	O
there	O
is	O
nearly	O
no	O
single	O
model	O
submission	O
on	O
SQuAD	B-DatasetName
leaderboard	O
2	O
,	O
we	O
only	O
compare	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
with	O
other	O
pre	O
-	O
trained	O
models	O
on	O
the	O
development	O
sets	O
.	O

We	O
find	O
that	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
outperforms	O
ELECTRA	B-MethodName
-	I-MethodName
small	I-MethodName
significantly	O
,	O
like	O
F1	B-MetricName
score	O
73.7	B-MetricName
versus	O
69.4	B-MetricName
on	O
SQuAD	B-DatasetName
2.0	I-DatasetName
.	O

However	O
,	O
when	O
we	O
generalize	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
to	O
base	O
size	O
,	O
the	O
gap	O
between	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
and	O
ELECTRA	B-MethodName
with	O
base	O
size	O
is	O
narrower	O
than	O
that	O
with	O
small	O
size	O
.	O

One	O
reason	O
may	O
be	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
is	O
searched	O
by	O
our	O
method	O
while	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
base	I-MethodName
is	O
only	O
generalized	O
from	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
-	I-MethodName
small	I-MethodName
with	O
larger	O
hidden	O
size	O
.	O

Experiment	O
results	O
show	O
our	O
obtained	O
model	O
LV	B-MethodName
-	I-MethodName
BERT	I-MethodName
outperforms	O
BERT	B-MethodName
and	O
its	O
variants	O
on	O
various	O
downstream	O
tasks.program	O
and	O
Google	O
Cloud	O
Research	O
Credits	O
Program	O
for	O
the	O
support	O
of	O
computational	O
resources	O
.	O

Notice	O
that	O
h	B-HyperparameterName
×	O
d	B-HyperparameterName
=	O
cwhere	O
h	B-HyperparameterName
is	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
heads	I-HyperparameterName
and	O
d	B-HyperparameterName
is	O
the	O
head	B-HyperparameterName
dimension	I-HyperparameterName
.	O

The	O
above	O
K	O
and	O
Q	O
are	O
used	O
to	O
compute	O
their	O
similarity	O
matrix	O
M	O
which	O
is	O
then	O
used	O
to	O
generate	O
new	O
value	O
V	O
:	O
M	O
=	O
Softmax(KQ	B-HyperparameterValue
/	O
√	O
d	O
)	O
V	O
=	O
Reshape(M	O
V	O
)	O
,	O
(	O
10)where	O
M	O
∈	O
R	O
h×s×s	O
and	O
V	O
∈	O
R	O
s×c	O
.	O

Finally	O
,	O
a	O
linear	O
transformation	O
is	O
used	O
to	O
exchange	O
information	O
between	O
different	O
heads	O
,	O
followed	O
by	O
shortcut	O
connection	O
and	O
layer	O
normalization	O
,	O
O	O
=	O
Norm(V	B-HyperparameterValue
W	O
O	O
+	O
b	O
O	O
+	O
I),(11)whereW	O
O	O
∈	O
R	O
c×c	O
and	O
b	O
O	O
∈	O
R	O
c	O
.Feed	O
-	O
Forward	O
The	O
feed	O
-	O
forward	O
layer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
includes	O
two	O
linear	O
transformations	O
with	O
a	O
non	O
-	O
linear	O
activation	O
,	O
followed	O
by	O
a	O
shortcut	O
connection	O
and	O
layer	O
normalization	O
,	O
N	O
=	O
GELU(IW	B-HyperparameterValue
1	O
+	O
b	O
1	O
)	O
O	O
=	O
Norm(N	O
W	O
2	O
+	O
b	O
2	O
+	O
I),(12)where	O
W	O
1	O
∈	O
R	O
c×rc	O
and	O
W	O
2	O
∈	O
R	O
rc×c	O
with	O
a	O
ratio	O
r.	O
GELU(•	B-HyperparameterValue
)	O
denotes	O
the	O
Gaussian	O
Error	O
Linear	O
Unit	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
.	O

2018	O
)	O
,	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
GLUE	B-DatasetName
)	O
benchmark	O
is	O
a	O
collection	O
of	O
nine	O
tasks	O
for	O
natural	O
language	O
understanding	O
,	O
where	O
testing	O
set	O
labels	O
are	O
hidden	O
and	O
predictions	O
need	O
to	O
be	O
submitted	O
to	O
the	O
evaluation	O
server	O
3	O
.	O

We	O
provide	O
details	O
about	O
the	O
GLUE	B-DatasetName
tasks	O
below	O
.	O

CoLA	B-DatasetName
The	I-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Linguistic	I-DatasetName
Acceptability	I-DatasetName
(	O
Warstadt	O
et	O
al	O
.	O
,	O

2019	O
)	O
is	O
a	O
binary	B-TaskName
single	I-TaskName
-	I-TaskName
sentence	I-TaskName
classification	I-TaskName
dataset	O
for	O
predicting	O
whether	O
an	O
sentence	O
is	O
grammatical	O
or	O
not	O
.	O

MRPC	O
The	O
Microsoft	B-DatasetName
Research	I-DatasetName
Paraphrase	I-DatasetName
Corpus	I-DatasetName
(	O
Dolan	O
and	O
Brockett	O
,	O
2005	O
)	O
is	O
a	O
dataset	O
for	O
the	O
task	O
to	O
predict	O
whether	O
two	O
sentences	O
are	O
semantically	O
equivalent	O
or	O
not	O
.	O

MNLI	B-DatasetName
The	I-DatasetName
Multi	I-DatasetName
-	I-DatasetName
Genre	I-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
Corpus	I-DatasetName
(	O
Williams	O
et	O
al	O
.	O
,	O

SST	B-DatasetName
The	O
Stanford	B-DatasetName
Sentiment	I-DatasetName
Treebank	I-DatasetName
(	O
Socher	O
et	O
al	O
.	O
,	O

2007	O
)	O
,	O
and	O
RTE5	B-DatasetName
(	O
Bentivogli	O
et	O
al	O
.	O
,	O

QNLI	B-DatasetName
Question	B-DatasetName
Natural	I-DatasetName
Language	I-DatasetName
Inference	I-DatasetName
is	O
a	O
dataset	O
converted	O
from	O
The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

QQP	B-DatasetName
The	O
Quora	B-DatasetName
Question	I-DatasetName
Pairs	I-DatasetName
dataset	O
(	O
Chen	O
et	O
al	O
.	O
,	O

STS	B-DatasetName
The	O
Semantic	B-DatasetName
Textual	I-DatasetName
Similarity	I-DatasetName
Benchmark	O
(	O
Cer	O
et	O
al	O
.	O
,	O

WNLI	B-DatasetName
Winograd	B-DatasetName
NLI	I-DatasetName
(	O
Levesque	O
et	O
al	O
.	O
,	O

The	O
Stanford	B-DatasetName
Question	I-DatasetName
Answering	I-DatasetName
Dataset	O
(	O
SQuAD	O
1.1	O
)	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

Besides	O
this	O
data	O
,	O
the	O
upgraded	O
version	O
SQuAD	B-DatasetName
2.0	I-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

For	O
supernet	O
,	O
We	O
pre	O
-	O
train	O
it	O
for	O
2	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
with	O
hyperparameters	O
listed	O
in	O
Table	O
5	O
,	O
using	O
Masked	O
Language	O
Modeling	O
(	O
MLM	O
)	O
pre	O
-	O
training	O
objective	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

This	O
objective	O
masks	O
15	O
%	O
input	O
tokens	O
that	O
require	O
the	O
model	O
to	O
predict	O
.	O

The	O
reason	O
to	O
use	O
this	O
objective	O
is	O
that	O
the	O
MLM	O
valida-4	O
https://gluebenchmark.com/faq	O
tion	O
accuracy	B-MetricName
can	O
reflect	O
the	O
performance	O
of	O
models	O
on	O
downstream	O
tasks	O
(	O
Lan	O
et	O
al	O
.	O
,	O

2020).For	O
pre	O
-	O
training	O
LV	B-MethodName
-	I-MethodName
BERTs	I-MethodName
and	O
other	O
compared	O
baselines	O
like	O
DynamicConv	B-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2018	O
)	O
and	O
Sandwich	B-MethodName
(	O
Press	O
et	O
al	O
.	O
,	O

We	O
pre	O
-	O
train	O
the	O
models	O
for	O
1	O
M	O
steps	O
,	O
mostly	O
using	O
the	O
same	O
hyperparameters	O
as	O
ELEC	B-MethodName
-	I-MethodName
TRA	I-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

For	O
downstream	O
task	O
SQuAD	B-DatasetName
1.1/2.0	I-DatasetName
that	O
needs	O
longer	O
input	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
,	O
we	O
pre	O
-	O
train	O
more	O
10	B-HyperparameterValue
%	I-HyperparameterValue
steps	B-HyperparameterName
with	O
the	O
sequence	B-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
to	O
learn	O
the	O
position	O
embedding	O
before	O
fine	O
-	O
tuning	O
.	O

For	O
fine	O
-	O
tuning	O
on	O
downstream	O
tasks	O
,	O
most	O
of	O
the	O
hyperparameters	O
are	O
the	O
same	O
as	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

See	O
Table	O
6	O
.	O

We	O
would	O
like	O
to	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
comments	O
and	O
suggestions	O
.	O

This	O
paper	O
presents	O
a	O
technical	O
report	O
of	O
our	O
submission	O
to	O
the	O
4th	O
task	O
of	O
SemEval-2021	O
,	O
titled	O
:	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
.	O

Thus	O
,	O
common	O
contextualized	O
language	O
models	O
like	O
BERT	B-MethodName
miss	O
fine	O
representation	O
and	O
performance	O
due	O
to	O
the	O
limited	O
capacity	O
of	O
the	O
input	O
tokens	O
.	O

To	O
tackle	O
this	O
problem	O
,	O
we	O
used	O
the	O
longformer	B-MethodName
model	O
to	O
better	O
process	O
the	O
sequences	O
.	O

Furthermore	O
,	O
we	O
utilized	O
the	O
method	O
proposed	O
in	O
the	O
longformer	B-MethodName
benchmark	O
on	O
wikihop	O
dataset	O
which	O
improved	O
the	O
accuracy	B-MetricName
on	O
our	O
task	O
data	O
from	O
(	O
23.01	B-MetricValue
%	I-MetricValue
and	O
22.95	B-MetricValue
%	I-MetricValue
)	O
achieved	O
by	O
the	O
baselines	O
for	O
subtask	O
1	O
and	O
2	O
,	O
respectively	O
,	O
to	O
(	O
70.30	B-MetricValue
%	I-MetricValue
and	O
64.38	B-MetricValue
%	I-MetricValue
)	O
.	O

Reading	B-TaskName
comprehension	I-TaskName
is	O
the	O
ability	O
to	O
understand	O
a	O
passage	O
either	O
by	O
human	O
or	O
machine	O
.	O

Generally	O
,	O
this	O
problem	O
can	O
contain	O
single	O
or	O
multiple	O
documents	O
as	O
context	O
(	O
containing	O
relevant	O
information	O
needed	O
to	O
understand	O
and	O
answer	O
the	O
question	O
)	O
,	O
a	O
question	O
(	O
a	O
sentence	O
with	O
at	O
least	O
one	O
asking	O
parameter	O
)	O
,	O
and	O
an	O
answer	O
(	O
which	O
is	O
the	O
parameter	O
value	O
of	O
the	O
question).In	O
the	O
Task	O
of	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
(	O
ReCAM	B-TaskName
)	O
,	O
we	O
have	O
one	O
passage	O
as	O
a	O
context	O
,	O
one	O
question	O
and	O
five	O
candidate	O
answers	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

For	O
each	O
instance	O
of	O
the	O
data	O
,	O
there	O
is	O
a	O
passage	O
,	O
a	O
question	O
with	O
a	O
missing	O
word	O
that	O
should	O
be	O
filled	O
based	O
on	O
the	O
passage	O
,	O
and	O
five	O
candidate	O
answers	O
to	O
the	O
question	O
.	O
...	O

In	O
this	O
weekend	O
's	O
upper	O
house	O
elections	O
...	O
Question	O
Abenomics	O
:	O
The	O
@Placeholder	O
and	O
the	O
risks	O
Answer	O
(	O
A	O
)	O
chances	O
(	O
B	O
)	O
prospective	O
(	O
C	O
)	O
security	O
(	O
D	O
)	O
objectives	O
(	O
E	O
)	O
threats	O
The	O
task	O
divides	O
into	O
two	O
subtasks	O
:	O
imperceptibility	O
and	O
non	O
-	O
specificity	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

Since	O
we	O
use	O
the	O
long	B-MethodName
document	I-MethodName
transformer	I-MethodName
model	O
(	O
Longformer	B-MethodName
(	O
Beltagy	O
et	O
al	O
.	O
,	O

We	O
have	O
evaluated	O
this	O
model	O
both	O
on	O
subtask-1	O
and	O
subtask-2	O
which	O
resulted	O
in	O
70	B-MetricValue
%	I-MetricValue
and	O
64	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
respectively	O
.	O

Therefore	O
,	O
we	O
have	O
about	O
40	B-MetricValue
%	I-MetricValue
improvement	O
compared	O
to	O
the	O
baseline	O
,	O
which	O
is	O
a	O
Gated	B-MethodName
Attention	I-MethodName
(	O
GA	B-MethodName
)	O
model	O
(	O
Zheng	O
et	O
al	O
.	O
,	O

2020):•	O
Language	O
representation	O
:	O
this	O
module	O
is	O
responsible	O
to	O
encode	O
the	O
inputs	O
.	O

While	O
great	O
progress	O
has	O
been	O
made	O
in	O
this	O
field	O
by	O
using	O
contextual	O
word	O
representation	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

Deep	O
contextualized	O
language	O
models	O
like	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Generally	O
,	O
this	O
can	O
be	O
an	O
arbitrary	O
length	O
based	O
on	O
the	O
dataset	O
.	O

We	O
used	O
the	O
Longformer	B-MethodName
model	O
introduced	O
in	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O

2020	O
)	O
as	O
the	O
pre	O
-	O
trained	O
contextual	O
embedding	O
model	O
in	O
our	O
method	O
.	O

The	O
size	O
of	O
this	O
sequence	O
is	O
B	O
so	O
B	O
=	O
L	O
+	O
A.After	O
feeding	O
the	O
input	O
b	O
to	O
the	O
Longformer	O
model	O
,	O
we	O
apply	O
a	O
global	O
attention	O
only	O
on	O
a	O
(	O
concatenated	O
question	O
and	O
answer	O
candidates	O
)	O
,	O
and	O
the	O
rest	O
is	O
the	O
context	O
.	O

As	O
the	O
longformer	O
model	O
utilizes	O
a	O
base	O
model	O
(	O
like	O
RoBERTa	B-MethodName
without	O
the	O
self	O
-	O
attention	O
layer	O
,	O
in	O
our	O
case	O
)	O
,	O
we	O
denote	O
this	O
as	O
basemodel	O
function	O
that	O
outputs	O
the	O
encoded	O
sequence	O
of	O
the	O
input	O
.	O

If	O
GAttn	B-MethodName
denotes	O
the	O
global	O
attention	O
function	O
,	O
we	O
have	O
:	O
d	O
i	O
=	O
basemodel(b	O
)	O
(	O
3	O
)	O
g	O
i	O
=	O
GAttn(d	O
i	O
)	O
.1(i	O
∈	O
A	O
)	O
(	O
4)where	O
d	O
i	O
is	O
the	O
raw	O
output	O
vector	O
for	O
each	O
input	O
token	O
.	O

The	O
concatenated	O
input	O
vector	O
will	O
be	O
encoded	O
using	O
the	O
base	O
model	O
(	O
like	O
RoBERTa	B-MethodName
without	O
the	O
self	O
-	O
attention	O
layer	O
,	O
in	O
our	O
case	O
)	O
.	O

The	O
logit	O
(	O
score	O
)	O
of	O
each	O
ent	O
token	O
will	O
be	O
calculated	O
using	O
a	O
linear	O
transformation	O
function	O
,	O
then	O
the	O
prediction	O
distribution	O
over	O
the	O
answer	O
candidates	O
(	O
ent	O
tokens	O
)	O
will	O
be	O
outputted	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
.	O

And	O
the	O
probability	O
distribution	O
over	O
the	O
candidates	O
will	O
be	O
calculated	O
using	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
on	O
the	O
logits	O
.	O

Although	O
we	O
only	O
participated	O
in	O
the	O
second	O
subtask	O
,	O
we	O
will	O
evaluate	O
our	O
model	O
on	O
both	O
subtasks	O
here	O
.	O

We	O
will	O
explain	O
our	O
configurations	O
for	O
utilizing	O
the	O
model	O
on	O
the	O
task	O
as	O
well	O
as	O
other	O
baselines	O
which	O
are	O
the	O
BERT	B-MethodName
-	O
base	O
as	O
an	O
alternative	O
model	O
and	O
the	O
Gate	B-MethodName
-	I-MethodName
Attention	I-MethodName
(	I-MethodName
GA	I-MethodName
)	I-MethodName
as	O
our	O
task	O
baseline	O
.	O

Finally	O
,	O
a	O
brief	O
discussion	O
will	O
be	O
done	O
based	O
on	O
the	O
results	O
.	O

Popular	O
metrics	O
to	O
evaluate	O
these	O
models	O
are	O
F1	B-MetricName
,	O
EM	B-MetricName
(	O
Exact	B-MetricName
Match	I-MetricName
or	O
accuracy	B-MetricName
)	O
,	O
and	O
MRR	B-MetricName
(	O
Mean	B-MetricName
Reciprocal	I-MetricName
Rank	I-MetricName
)	O
.	O

As	O
the	O
precision	B-MetricName
and	O
recall	B-MetricName
in	O
our	O
task	O
are	O
equal	O
,	O
so	O
F1	B-MetricName
=	O
Precision	B-MetricName
=	O
Recall	B-MetricName
.	O

Also	O
,	O
F1	B-MetricName
and	O
EM	B-MetricName
are	O
the	O
same	O
.	O

And	O
,	O
the	O
use	O
of	O
MRR	B-MetricName
is	O
optional	O
,	O
so	O
the	O
metrics	O
used	O
to	O
evaluate	O
the	O
result	O
are	O
the	O
accuracy	O
and	O
the	O
F1	B-MetricName
.	O

The	O
baseline	O
model	O
(	O
GA	B-MethodName
)	O
is	O
trained	O
for	O
30	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
each	O
epoch	O
containing	O
101	B-HyperparameterValue
mini	B-HyperparameterName
-	I-HyperparameterName
batches	I-HyperparameterName
.	O

The	O
train	B-HyperparameterName
batch	I-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
32	B-HyperparameterValue
.	O

Dropout	B-HyperparameterName
with	O
the	O
rate	O
of	O
0.5	B-HyperparameterValue
is	O
also	O
applied	O
to	O
the	O
hidden	O
states	O
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
0.001	B-HyperparameterValue
.	O

The	O
dimensionality	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
GloVe	I-HyperparameterName
embedding	I-HyperparameterName
is	O
300	B-HyperparameterValue
,	O
and	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
128	B-HyperparameterValue
.	O

In	O
fact	O
,	O
we	O
consider	O
the	O
output	O
vector	O
of	O
each	O
chunk	O
as	O
our	O
final	O
vector	O
to	O
be	O
linearly	O
transformed	O
into	O
single	O
logit	O
,	O
followed	O
by	O
a	O
softmax	B-HyperparameterValue
layer	I-HyperparameterValue
using	O
the	O
crossentropy	O
loss	O
.	O

Note	O
that	O
the	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
here	O
is	O
bounded	O
to	O
512	B-HyperparameterValue
tokens	I-HyperparameterValue
,	O
and	O
the	O
model	O
includes	O
the	O
n	O
2	O
attention	O
mechanism	O
.	O

The	O
model	O
was	O
initialized	O
using	O
the	O
Longformer	B-MethodName
-	O
base	O
pre	O
-	O
training	O
weights	O
,	O
then	O
finetuned	O
in	O
each	O
of	O
the	O
subtasks	O
.	O

Due	O
to	O
the	O
performance	O
issues	O
,	O
the	O
model	B-HyperparameterName
max	I-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
is	O
set	O
to	O
4096	B-HyperparameterValue
tokens	I-HyperparameterValue
which	O
are	O
sufficient	O
in	O
our	O
case	O
.	O

We	O
also	O
used	O
the	O
RoBERTa	B-MethodName
-	O
large	O
tokenizer	O
to	O
tokenize	O
the	O
input	O
sequence	O
as	O
the	O
Longformer	B-MethodName
model	O
has	O
been	O
trained	O
on	O
using	O
this	O
configuration	O
.	O

We	O
used	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
and	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-5	B-HyperparameterValue
using	O
the	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
beta2=0.98	B-HyperparameterName
.	O

And	O
a	O
weight	B-HyperparameterName
decay	I-HyperparameterName
of	O
0.01	B-HyperparameterValue
has	O
been	O
considered	O
to	O
regularize	O
the	O
model	O
and	O
avoid	O
overfitting	O
.	O

Our	O
proposed	O
model	O
is	O
trained	O
for	O
15	B-HyperparameterValue
epochs	B-HyperparameterName
for	O
each	O
task	O
.	O

We	O
have	O
achieved	O
an	O
accuracy	B-MetricName
of	O
70	B-MetricValue
%	I-MetricValue
on	O
the	O
validation	O
set	O
,	O
which	O
improves	O
our	O
baseline	O
by	O
about	O
40	B-MetricValue
percent	I-MetricValue
.	O

Subtask2	O
measures	O
the	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
level	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
in	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

It	O
in	O
-	O
cludes	O
3318	O
training	O
samples	O
,	O
851	O
validation	O
samples	O
,	O
and	O
2017	O
test	O
samples	O
.	O

The	O
best	O
accuracy	B-MetricName
on	O
the	O
validation	O
set	O
is	O
64	B-MetricValue
%	I-MetricValue
.	O

We	O
used	O
two	O
baselines	O
to	O
find	O
out	O
the	O
effect	O
of	O
using	O
a	O
pre	O
-	O
trained	O
model	O
rather	O
than	O
a	O
simple	O
RNN	B-MethodName
model	O
.	O

As	O
most	O
of	O
the	O
available	O
texts	O
for	O
training	O
consist	O
of	O
concrete	O
words	O
,	O
it	O
is	O
more	O
likely	O
to	O
leverage	O
the	O
language	O
understanding	O
to	O
less	O
abstract	O
words	O
to	O
achieve	O
a	O
better	O
result	O
.	O

Comparing	O
our	O
method	O
which	O
is	O
based	O
on	O
longformer	B-MethodName
model	O
to	O
usual	O
language	O
models	O
like	O
BERT	B-MethodName
indicates	O
a	O
new	O
insight	O
in	O
terms	O
of	O
passage	O
length	O
and	O
the	O
attention	O
mechanism	O
.	O

Popular	O
language	O
models	O
like	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
use	O
a	O
n	O
2	O
attention	O
which	O
requires	O
a	O
large	O
receptive	O
field	O
to	O
represent	O
long	O
passages	O
.	O

In	O
contrast	O
,	O
the	O
longformer	B-MethodName
global	O
attention	O
mechanism	O
relaxes	O
this	O
limitation	O
as	O
we	O
only	O
need	O
to	O
pay	O
attention	O
to	O
a	O
small	O
factor	O
of	O
context	O
and	O
more	O
focus	O
on	O
the	O
local	O
window	O
.	O

We	O
have	O
shown	O
how	O
different	O
approaches	O
can	O
be	O
leveraged	O
to	O
machine	B-TaskName
reading	I-TaskName
comprehension	I-TaskName
of	I-TaskName
abstract	I-TaskName
meaning	I-TaskName
.	O

We	O
reformulated	O
the	O
longformer	B-MethodName
model	O
to	O
learn	O
abstract	B-TaskName
meaning	I-TaskName
as	I-TaskName
a	I-TaskName
new	I-TaskName
level	I-TaskName
of	I-TaskName
semantic	I-TaskName
in	I-TaskName
machine	I-TaskName
reading	I-TaskName
comprehension	I-TaskName
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
our	O
contribution	O
in	O
SemEval-2021	B-DatasetName
Task	O
1	O
:	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
,	O
where	O
we	O
integrate	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
properties	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
as	O
features	O
within	O
a	O
Machine	O
Learning	O
(	O
ML	O
)	O
framework	O
for	O
predicting	O
lexical	O
complexity	O
.	O

In	O
particular	O
,	O
we	O
use	O
BERT	B-MethodName
contextualized	I-MethodName
word	I-MethodName
embeddings	I-MethodName
to	O
represent	O
the	O
semantic	O
meaning	O
of	O
the	O
target	O
word	O
and	O
its	O
context	O
.	O

At	O
the	O
beginning	O
,	O
most	O
of	O
these	O
methods	O
assumed	O
that	B-MetricName
lexical	I-MetricName
complexity	I-MetricName
is	O
binary	O
,	O
words	O
are	O
either	O
"	O
difficult	O
"	O
or	O
"	O
not	O
difficult	O
"	O
.	O

Thus	O
,	O
the	O
first	O
Complex	B-TaskName
Word	I-TaskName
Identification	I-TaskName
(	O
CWI	O
)	O
shared	O
task	O
referred	O
to	O
binary	O
identification	O
of	O
complex	O
words	O
(	O
Zampieri	O
et	O
al	O
.	O
,	O

Therefore	O
,	O
three	O
years	O
ago	O
,	O
the	O
CWI	B-TaskName
included	O
an	O
additional	O
probabilistic	O
classification	O
task	O
where	O
the	O
participants	O
were	O
asked	O
to	O
give	O
a	O
probability	O
of	O
the	O
given	O
target	O
word	O
in	O
particular	O
context	O
being	O
complex	O
(	O
Štajner	O
et	O
al	O
.	O
,	O

2018).Recently	O
,	O
CompLex	B-DatasetName
,	O
a	O
new	O
English	O
corpus	O
for	O
lexical	O
complexity	O
prediction	O
was	O
introduced	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

The	O
corpus	O
is	O
annotated	O
using	O
a	O
5	O
-	O
point	O
Likert	B-MetricName
scale	I-MetricName
(	O
1	O
-	O
5	O
)	O
(	O
corresponding	O
to	O
very	O
easy	O
,	O
easy	O
,	O
neutral	O
,	O
difficult	O
,	O
and	O
very	O
difficult	O
)	O
,	O
and	O
covers	O
3	O
genres	O
:	O
Bible	O
translation	O
,	O
European	O
Pariliament	O
proceedings	O
,	O
and	O
biomedical	O
articles	O
.	O

SemEval-2021	B-DatasetName
(	O
Task	O
1	O
)	O
shared	O
task	O
on	B-TaskName
Lexical	I-TaskName
Complexity	I-TaskName
Prediction	I-TaskName
(	I-TaskName
LCP	I-TaskName
)	O
(	O
Shardlow	O
et	O
al	O
.	O
,	O

Linguistics	O
features	O
,	O
such	O
as	O
Part	O
-	O
Of	O
-	O
Speech	O
(	O
POS	O
)	O
tag	O
,	O
dependency	O
parsing	O
relations	O
,	O
and	O
syllable	O
counts	O
,	O
as	O
well	O
as	O
statistical	O
features	O
,	O
such	O
as	O
word	O
length	O
and	O
word	O
frequency	O
,	O
have	O
been	O
widely	O
used	O
for	O
predicting	O
lexical	B-MetricName
complexity	I-MetricName
(	O
Mukherjee	O
et	O
al	O
.	O
,	O

Some	O
of	O
these	O
works	O
found	O
WordNet	B-MethodName
(	O
Miller	O
,	O
1998	O
)	O
as	O
a	O
valuable	O
source	O
of	O
lexical	O
features	O
.	O

The	O
main	O
extracted	O
feature	O
is	O
the	O
number	O
of	O
synsets	O
,	O
but	O
also	O
information	O
on	O
hypernyms	O
,	O
hyponyms	O
,	O
holonym	O
,	O
and	O
meronym	O
is	O
useful	O
(	O
Gooding	O
and	O
Kochmar	O
,	O
2018;Hartmann	O
and	O
Dos	O
Santos	O
,	O
2018;Wani	O
et	O
al	O
.	O
,	O

These	O
word	O
embeddings	O
were	O
generated	O
using	O
Word2Vec	B-MethodName
context	O
-	O
independent	O
models	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O

Word2Vec	B-MethodName
models	O
combine	O
different	O
senses	O
of	O
the	O
word	O
into	O
one	O
single	O
vector	O
.	O

However	O
,	O
recently	O
,	O
there	O
is	O
a	O
growing	O
interest	O
in	O
contextualized	O
word	O
representations	O
,	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

BERT	B-MethodName
model	O
generates	O
context	O
-	O
dependent	O
embeddings	O
that	O
allow	O
a	O
word	O
to	O
have	O
several	O
vector	O
representations	O
depending	O
on	O
the	O
context	O
in	O
which	O
it	O
is	O
used	O
.	O

In	O
contrast	O
to	O
previous	O
works	O
that	O
only	O
use	O
context	O
-	O
independent	O
embeddings	O
,	O
our	O
system	O
uses	O
the	O
BERT	B-MethodName
-	O
based	O
contextdependent	O
embeddings	O
.	O

We	O
adopt	O
a	O
supervised	O
Machine	O
Learning	O
(	O
ML	O
)	O
approach	O
for	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

Our	O
dataset	O
contains	O
three	O
corpora	O
:	O
Bible	B-DatasetName
,	I-DatasetName
Europarl	I-DatasetName
,	I-DatasetName
and	I-DatasetName
Biomedical	I-DatasetName
,	O
to	O
add	O
variation	O
.	O

The	O
POS	O
is	O
extracted	O
by	O
the	O
Spacy	B-MethodName
's	O
statistical	O
POS	O
tagger	O
1	O
.	O

Then	O
,	O
we	O
calculate	O
the	O
number	O
of	O
punctuation	O
marks	O
and	O
stopwords	O
in	O
the	O
sentence	O
(	O
two	O
features).Next	O
,	O
we	O
represent	O
syntactic	O
forms	O
by	O
POS	O
patterns	O
.	O

The	O
POS	O
pattern	O
refers	O
to	O
seven	O
words	O
,	O
the	O
target	O
word	O
and	O
three	O
words	O
before	O
and	O
after	O
it	O
.	O

We	O
also	O
measure	O
the	O
polysemy	O
degree	O
of	O
the	O
target	O
word	O
using	O
the	O
number	O
of	O
senses	O
in	O
WordNet	B-MethodName
.	O

Then	O
,	O
we	O
extract	O
the	O
target	O
word	O
frequency	O
using	O
Google	B-MethodName
N	I-MethodName
-	I-MethodName
gram	I-MethodName
4	I-MethodName
word	I-MethodName
frequencies	I-MethodName
.	O

We	O
use	O
the	O
BERT	B-MethodName
semantic	O
space	O
.	O

BERT	B-MethodName
is	O
a	O
bidirectional	B-MethodName
transformer	I-MethodName
pre	O
-	O
trained	O
on	O
a	O
large	O
corpus	O
containing	O
the	O
Toronto	B-DatasetName
Book	I-DatasetName
Corpus	I-DatasetName
and	O
Wikipedia	B-DatasetName
using	O
a	O
combination	O
of	O
masked	O
language	O
modeling	O
objective	O
and	O
next	O
sentence	O
prediction	O
.	O

BERT	B-MethodName
contextualizing	O
vectors	O
are	O
used	O
to	O
represent	O
the	O
semantic	O
meaning	O
of	O
the	O
sentence	O
by	O
averaging	O
the	O
BERT	B-MethodName
vectors	O
of	O
seven	O
words	O
,	O
the	O
target	O
word	O
and	O
three	O
words	O
before	O
and	O
after	O
it	O
.	O

Thus	O
,	O
our	O
semantic	O
representation	O
add	O
768	O
features	O
(	O
the	O
size	O
of	O
BERT	O
output	O
layer).To	O
extract	O
additional	O
features	O
,	O
we	O
use	O
two	O
machine	O
learning	O
algorithm	O
:	O
K	B-MethodName
-	I-MethodName
Means	I-MethodName
and	I-MethodName
k	I-MethodName
-	I-MethodName
Nearest	I-MethodName
Neighbors	I-MethodName
(	I-MethodName
KNN	I-MethodName
)	I-MethodName
algorithm	I-MethodName
.	O

K	B-MethodName
-	I-MethodName
Means	I-MethodName
is	O
an	O
unsupervised	O
learning	O
algorithm	O
used	O
for	O
clustering	O
.	O

We	O
encode	O
the	O
K	B-MethodName
-	I-MethodName
Mean	I-MethodName
results	O
by	O
four	O
binary	O
features	O
,	O
a	O
feature	O
per	O
cluster	O
(	O
k=4	B-HyperparameterName
)	O
.	O

The	O
results	O
of	O
the	O
KNN	B-MethodName
algorithm	O
are	O
encoded	O
similarly	O
.	O

KNN	B-MethodName
classifies	O
an	O
unseen	O
sentence	O
using	O
it	O
k	B-MethodName
nearest	I-MethodName
neighbors	I-MethodName
voting	O
.	O

We	O
use	O
four	O
complexity	B-HyperparameterName
classes	O
:	O
0	B-HyperparameterValue
-	I-HyperparameterValue
0.25	I-HyperparameterValue
,	I-HyperparameterValue
0.26	I-HyperparameterValue
-	I-HyperparameterValue
0.5	I-HyperparameterValue
,	I-HyperparameterValue
0.51	I-HyperparameterValue
-	I-HyperparameterValue
0.75	I-HyperparameterValue
,	I-HyperparameterValue
0.76	I-HyperparameterValue
-	I-HyperparameterValue
1	I-HyperparameterValue
.	O

First	O
,	O
we	O
discharged	O
features	O
that	O
decrease	O
the	O
system	O
performance	O
on	O
the	O
training	O
set	O
,	O
namely	O
,	O
the	O
POS	B-MethodName
pattern	I-MethodName
features	O
,	O
the	O
WordNet	B-MethodName
features	O
,	O
and	O
the	O
K	B-MethodName
-	I-MethodName
Means	I-MethodName
and	I-MethodName
KNN	I-MethodName
features	O
.	O

These	O
features	O
were	O
selected	O
using	O
the	O
Linear	B-MethodName
Regression	I-MethodName
algorithm	O
,	O
which	O
was	O
also	O
selected	O
as	O
a	O
baseline	O
algorithm	O
by	O
the	O
task	O
organizers	O
.	O

To	O
further	O
improve	O
the	O
performance	O
of	O
our	O
systems	O
,	O
we	O
used	O
additional	O
ML	O
algorithms	O
,	O
such	O
as	O
SVM	B-MethodName
and	O
XGBoost	B-MethodName
(	O
see	O
more	O
details	O
in	O
Section	O
3.3).Next	O
,	O
since	O
correlated	O
features	O
do	O
not	O
carry	O
unique	O
information	O
and	O
may	O
interfere	O
the	O
learning	O
,	O
we	O
tried	O
to	O
discharge	O
highly	O
correlated	O
features	O
.	O

First	O
,	O
we	O
define	O
an	O
initial	O
correlation	O
threshold	O
(	O
0.9	O
)	O
.	O

Next	O
,	O
if	O
we	O
still	O
have	O
more	O
features	O
than	O
desired	O
,	O
we	O
will	O
lower	O
the	O
correlation	B-HyperparameterName
threshold	I-HyperparameterName
(	O
by	O
10	B-HyperparameterValue
%	I-HyperparameterValue
)	O
and	O
repeat	O
the	O
process	O
.	O

This	O
approach	O
improved	O
the	O
performance	O
of	O
the	O
SVM	B-MethodName
and	O
Linear	B-MethodName
Regression	I-MethodName
models	O
(	O
selecting	O
97	O
features	O
)	O
,	O
but	O
did	O
not	O
increase	O
the	O
performance	O
of	O
the	O
XGBOOST	B-MethodName
method	O
.	O

We	O
note	O
that	O
we	O
also	O
tried	O
to	O
filter	O
out	O
feature	O
using	O
the	O
principal	B-MethodName
component	I-MethodName
analysis	I-MethodName
(	I-MethodName
PCA	I-MethodName
)	I-MethodName
feature	O
selection	O
method	O
(	O
Song	O
et	O
al	O
.	O
,	O

2010	O
)	O
.	O

PCA	B-MethodName
aims	O
to	O
pick	O
a	O
subset	O
of	O
features	O
that	O
retains	O
as	O
much	O
information	O
present	O
in	O
the	O
full	O
data	O
as	O
possible	O
.	O

PCA	B-MethodName
was	O
performed	O
both	O
on	O
the	O
full	O
feature	O
list	O
and	O
on	O
specific	O
features	O
,	O
such	O
as	O
BERT	B-MethodName
features	O
,	O
but	O
it	O
was	O
not	O
successful	O
.	O

We	O
resulted	O
with	O
the	O
following	O
list	O
of	O
101	O
features:•	O
Biomedical	O
corpus	O
indicator	O
•	O
94	O
features	O
from	O
BERT	B-MethodName
vector	O
It	O
is	O
interesting	O
to	O
note	O
that	O
even	O
though	O
,	O
there	O
are	O
12	O
POS	O
tags	O
,	O
only	O
2	O
are	O
informative	O
for	O
the	O
complexity	O
prediction	O
task	O
.	O

Out	O
of	O
the	O
BERT	B-MethodName
768	O
features	O
,	O
only	O
94	O
remained	O
(	O
12.2	O
%	O
of	O
the	O
vector).The	O
BERT	B-MethodName
representation	O
of	O
the	O
sentence	O
is	O
generated	O
by	O
pre	O
-	O
trained	O
language	O
representation	O
model	O
.	O

Since	O
one	O
of	O
our	O
corpora	O
is	O
from	O
the	O
Biomedical	O
domain	O
,	O
we	O
examined	O
the	O
system	O
performance	O
using	O
the	O
domain	O
specific	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O

Figure	O
1	O
shows	O
a	O
comparison	O
between	O
the	O
error	O
rate	O
of	O
our	O
system	O
using	O
the	O
classic	O
BERT	B-MethodName
and	O
BioBERT	B-MethodName
(	O
BERT	B-MethodName
on	O
the	O
left	O
and	O
BioBERT	B-MethodName
on	O
the	O
right	O
)	O
.	O

Columns	O
from	O
left	O
to	O
right	O
:	B-DatasetName
Bible	I-DatasetName
,	I-DatasetName
Biomedical	I-DatasetName
,	I-DatasetName
and	I-DatasetName
Europarl	I-DatasetName
.	O

Surprisingly	O
,	O
the	O
error	O
rate	O
of	O
the	O
BioBERT	B-MethodName
on	O
the	O
Biomedical	O
domain	O
is	O
higher	O
than	O
that	O
of	O
the	O
classic	O
BERT	B-MethodName
.	O

We	O
combined	O
the	O
features	O
in	O
a	O
supervised	O
classification	O
framework	O
using	O
five	O
ML	O
methods	O
:	O
Linear	B-MethodName
Regression	I-MethodName
,	I-MethodName
Supported	I-MethodName
Vector	I-MethodName
Machine	I-MethodName
(	I-MethodName
SVM	I-MethodName
)	I-MethodName
,	I-MethodName
XGBoost	I-MethodName
(	I-MethodName
XGB	I-MethodName
)	I-MethodName
,	I-MethodName
KNN	I-MethodName
,	I-MethodName
and	I-MethodName
Stacking	I-MethodName
(	I-MethodName
Stack	I-MethodName
)	I-MethodName
.	O

We	O
ran	O
these	O
ML	O
methods	O
by	O
the	O
scikit	B-MethodName
-	I-MethodName
learn	I-MethodName
open	O
-	O
source	O
machine	O
-	O
learning	O
package	O
in	O
python	O
5	O
(	O
Pedregosa	O
et	O
al	O
.	O
,	O

The	O
MAE	B-MethodName
is	O
omitted	O
from	O
the	O
table	O
because	O
it	O
is	O
similar	O
for	O
all	O
the	O
ML	O
algorithms	O
(	O
0.01	O
)	O
.	O

In	O
Figure	O
2	O
,	O
we	O
present	O
the	O
classification	O
confusion	O
matrix	O
of	O
the	O
XGBoost	B-MethodName
algorithm	O
.	O

Most	O
of	O
the	O
classification	B-MetricName
errors	I-MetricName
(	O
18.54	B-MetricValue
%	I-MetricValue
)	O
were	O
due	O
to	O
incorrect	O
classification	O
of	O
very	O
easy	O
words	O
as	O
easy	O
.	O

There	O
were	O
also	O
errors	O
in	O
the	O
opposite	O
direction	O
(	O
4.36	B-MetricValue
%	I-MetricValue
)	O
.	O

Most	O
of	O
the	O
rest	O
of	O
the	O
classifications	O
were	O
between	O
neutral	O
and	O
easy	O
in	O
both	O
directions	O
(	O
7.42	B-MetricValue
%	I-MetricValue
+	I-MetricValue
6.43	I-MetricValue
%	I-MetricValue
=	I-MetricValue
13.85	I-MetricValue
%	I-MetricValue
)	O
.	O

We	O
have	O
implemented	O
a	O
system	O
that	O
incorporates	O
linguistic	O
,	O
statistical	O
,	O
and	O
semantic	O
features	O
to	O
predict	O
lexical	O
complexity	O
of	O
target	O
word	O
in	O
context	O
.	O

BERT	B-MethodName
semantic	O
space	O
was	O
used	O
to	O
represent	O
the	O
word	O
and	O
its	O
context	O
.	O

Even	O
though	O
our	O
system	O
was	O
not	O
highly	O
ranked	O
,	O
we	O
believe	O
that	O
some	O
of	O
the	O
presented	O
ideas	O
can	O
be	O
useful	O
for	O
future	O
research	O
on	O
lexical	B-TaskName
complexity	I-TaskName
prediction	I-TaskName
.	O

In	O
particular	O
,	O
we	O
think	O
that	O
BERT	B-MethodName
is	O
a	O
powerful	O
model	O
that	O
should	O
be	O
explored	O
.	O

Perhaps	O
,	O
fine	O
-	O
tuning	O
BERT	B-MethodName
for	O
the	O
complexity	O
prediction	O
task	O
would	O
increase	O
the	O
system	O
performance	O
.	O

We	O
investigate	O
the	O
less	O
-	O
explored	O
task	O
of	O
generating	B-TaskName
open	I-TaskName
-	I-TaskName
ended	I-TaskName
questions	I-TaskName
that	I-TaskName
are	I-TaskName
typically	I-TaskName
answered	I-TaskName
by	I-TaskName
multiple	I-TaskName
sentences	I-TaskName
.	O

We	O
then	O
propose	O
a	O
novel	O
question	B-TaskName
type	I-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
,	O
augmented	O
by	O
a	O
semantic	B-TaskName
graph	I-TaskName
representation	I-TaskName
,	O
to	O
jointly	O
predict	O
question	O
focuses	O
and	O
produce	O
the	O
question	O
.	O

Human	O
judges	O
also	O
rate	O
our	O
model	O
outputs	O
highly	O
in	O
answerability	B-MetricName
,	O
coverage	B-MetricName
of	I-MetricName
scope	I-MetricName
,	O
and	O
overall	B-MetricName
quality	I-MetricName
.	O

Question	B-TaskName
-	I-TaskName
asking	I-TaskName
has	O
long	O
served	O
as	O
an	O
effective	O
instrument	O
for	O
knowledge	B-TaskName
learning	I-TaskName
(	O
Andre	O
,	O
1979;Tobin	O
,	O
1990	O
)	O
and	O
assessing	O
learning	O
progress	O
(	O
Holme	O
,	O
2003;Downing	O
and	O
Yudkowsky	O
,	O
2009;Livingston	O
,	O
2009	O
)	O
.	O

2019	O
)	O
,	O
this	O
work	O
is	O
interested	O
in	O
generating	B-TaskName
open	I-TaskName
-	I-TaskName
ended	I-TaskName
questions	I-TaskName
that	I-TaskName
require	I-TaskName
deep	I-TaskName
comprehension	I-TaskName
and	I-TaskName
long	I-TaskName
-	I-TaskName
form	I-TaskName
answers	I-TaskName
(	O
Labutov	O
et	O
al	O
.	O
,	O

2015	O
)	O
.	O

It	O
also	O
provides	O
a	O
little	O
bit	O
of	O
respect	O
on	O
the	O
street	O
...	O
Figure	O
1	O
:	O
Open	O
-	O
ended	O
questions	O
generated	O
by	O
different	O
models	O
after	O
reading	O
the	O
same	O
input	O
:	O
(	O
1	O
)	O
BART	B-MethodName
decoded	O
with	O
nucleus	O
sampling	O
,	O
(	O
2	O
)	O
BART	B-MethodName
that	O
considers	O
different	O
question	O
words	O
,	O
and	O
(	O
3	O
)	O
our	O
type	B-MethodName
-	I-MethodName
aware	I-MethodName
generator	I-MethodName
TPLGEN	I-MethodName
,	O
that	O
predicts	O
focuses	O
and	O
operates	O
with	O
generated	O
templates	O
(	O
to	O
the	O
left	O
of	O
the	O
arrows	O
)	O
.	O

Questions	O
generated	O
by	O
our	O
model	O
have	O
diverse	O
TYPEs	O
.	O

2019	O
)	O
and	O
building	O
open	B-MethodName
-	I-MethodName
domain	I-MethodName
dialogue	I-MethodName
systems	I-MethodName
(	O
Shum	O
et	O
al	O
.	O
,	O

We	O
first	O
introduce	O
a	O
new	O
question	O
type	O
ontology	O
,	O
drawn	O
upon	O
researches	O
in	O
cognitive	O
science	O
and	O
psychology	O
(	O
Graesser	O
et	O
al	O
.	O
,	O

1992	O
)	O
,	O
to	O
capture	O
deeper	O
levels	O
of	O
cognition	O
,	O
such	O
as	O
causal	B-TaskName
reasoning	I-TaskName
and	I-TaskName
judgments	I-TaskName
.	O

2018	O
)	O
)	O
,	O
our	O
framework	O
is	O
built	O
on	O
large	O
pre	O
-	O
trained	B-MethodName
BART	I-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

It	O
is	O
further	O
augmented	O
by	O
a	O
semantic	B-MethodName
graph	I-MethodName
that	O
leverages	O
both	O
semantic	O
roles	O
and	O
dependency	O
relations	O
,	O
facilitating	O
long	O
text	O
comprehension	O
to	O
pinpoint	O
salient	O
concepts	O
.	O

Moreover	O
,	O
to	O
achieve	O
the	O
goal	O
of	O
producing	O
various	O
types	O
of	O
questions	O
from	O
the	O
same	O
input	O
,	O
we	O
investigate	O
two	O
model	O
variants	O
that	O
use	O
templates	O
to	O
improve	O
controllability	O
and	O
generation	O
diversity	O
:	O
one	O
using	O
pre	O
-	O
identified	O
exemplars	O
,	O
the	O
other	O
employing	O
generated	O
templates	O
to	O
guide	O
question	B-TaskName
writing	I-TaskName
,	O
with	O
sample	O
outputs	O
displayed	O
in	O
Figure	O
1.For	O
experiments	O
,	O
we	O
collect	O
two	O
new	O
large	O
-	O
scale	O
datasets	O
consisting	O
of	O
open	O
-	O
ended	O
questions	O
with	O
answers	O
from	O
(	O
1	O
)	O
Yahoo	B-DatasetName
Answers	I-DatasetName
2	I-DatasetName
L6	I-DatasetName
dataset	O
and	O
(	O
2	O
)	O
popular	O
question	O
-	O
asking	O
communities	O
on	O
Reddit	B-DatasetName
3	I-DatasetName
,	O
consisting	O
of	O
291	O
K	O
and	O
720	O
K	O
question	O
-	O
answer	O
pairs	O
,	O
respectively	O
.	O

Compared	O
to	O
existing	O
popular	O
QA	O
datasets	O
,	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
and	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
.	O
,	O

Automatic	O
metrics	O
show	O
that	O
our	O
type	B-MethodName
-	I-MethodName
aware	I-MethodName
question	I-MethodName
generation	I-MethodName
model	O
outperforms	O
competitive	O
comparisons	O
,	O
highlighting	O
the	O
effectiveness	O
of	O
semantic	B-MethodName
graph	I-MethodName
-	I-MethodName
augmented	I-MethodName
representation	I-MethodName
and	I-MethodName
joint	I-MethodName
modeling	I-MethodName
of	O
focus	O
prediction	O
and	O
question	O
generation	O
.	O

Adding	O
templates	B-MethodName
further	O
promotes	O
question	O
diversity	O
,	O
as	O
evaluated	O
by	O
both	O
automatic	O
evaluation	O
and	O
human	O
assessment	O
.	O

Question	B-TaskName
generation	I-TaskName
has	O
long	O
been	O
studied	O
to	O
reduce	O
human	O
efforts	O
in	O
constructing	O
questions	O
for	O
knowledge	O
learning	O
evaluation	O
(	O
Mitkov	O
and	O
Ha	O
,	O
2003;Brown	O
et	O
al	O
.	O
,	O

Early	O
work	O
relies	O
on	O
syntactic	B-MethodName
transformation	I-MethodName
to	O
convert	O
declarative	O
sentences	O
to	O
questions	O
(	O
Heilman	O
and	O
Smith	O
,	O
2010;Chali	O
and	O
Hasan	O
,	O
2015	O
)	O
.	O

2018;Zhou	O
et	O
al	O
.	O
,	O

Attempts	O
are	O
also	O
made	O
toward	O
creating	O
complex	O
questions	O
that	O
require	O
multi	B-TaskName
-	I-TaskName
hop	I-TaskName
reasoning	I-TaskName
over	O
the	O
given	O
text	O
,	O
and	O
graph	O
-	O
based	O
representations	O
have	O
been	O
an	O
enabling	O
tool	O
to	O
facilitate	O
the	O
access	O
to	O
both	O
entities	O
and	O
relations	O
(	O
Pan	O
et	O
al	O
.	O
,	O

While	O
our	O
model	O
also	O
enhances	O
the	O
input	O
with	O
a	O
semantic	B-MethodName
graph	I-MethodName
,	O
it	O
boasts	O
a	O
richer	O
representation	O
by	O
including	O
both	O
dependency	O
and	O
semantic	O
relations	O
,	O
with	O
predicted	O
question	O
focuses	O
highlighted	O
via	O
extra	B-MethodName
node	I-MethodName
embeddings	I-MethodName
.	O

Given	O
the	O
data	O
-	O
driven	O
nature	O
of	O
question	O
generation	O
and	O
answering	O
tasks	O
,	O
recent	O
studies	O
take	O
advantage	O
of	O
the	O
availability	O
of	O
large	O
-	O
scale	O
QA	O
datasets	O
,	O
such	O
as	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
MS	B-DatasetName
MARCO	I-DatasetName
(	O
Bajaj	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
HotpotQA	B-DatasetName
(	O
Yang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
DROP	B-DatasetName
(	O
Dua	O
et	O
al	O
.	O
,	O

A	O
dataset	O
closer	O
to	O
ours	O
is	O
ELI5	B-DatasetName
,	O
which	O
also	O
obtains	O
open	O
-	O
ended	O
questionanswer	O
pairs	O
from	O
Reddit	B-DatasetName
,	O
while	O
one	O
of	O
our	O
datasets	O
includes	O
more	O
Reddit	O
communities	O
and	O
thus	O
covers	O
a	O
wider	O
range	O
of	O
topics	O
.	O

Generating	O
diverse	O
questions	O
is	O
much	O
less	O
studied	O
,	O
with	O
existing	O
approaches	O
mainly	O
focusing	O
on	B-MethodName
entity	I-MethodName
replacement	I-MethodName
(	O
Cho	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
sampling	B-MethodName
decoding	I-MethodName
(	O
Sultan	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
post	B-MethodName
-	I-MethodName
filtering	I-MethodName
.	O

DISJUNCTIVE	O
the	O
true	O
one	O
given	O
multiple	O
events	O
or	O
concepts	O
,	O
where	O
comparison	O
among	O
options	O
is	O
not	O
needed	O
.	O

EXTENT	O
the	O
extent	O
or	O
quantity	O
of	O
an	O
event	O
or	O
a	O
concept	O
.	O

CONSEQUENCE	O
the	O
consequences	O
or	O
results	O
of	O
an	O
event	O
.	O

A	O
second	O
dataset	O
with	O
question	O
-	O
answer	O
pairs	O
is	O
collected	O
from	O
the	O
Yahoo	B-DatasetName
Answers	I-DatasetName
L6	I-DatasetName
corpus	O
4	O
,	O
which	O
covers	O
a	O
broader	O
range	O
of	O
topics	O
than	O
the	O
Reddit	B-DatasetName
data	O
.	O

To	O
ensure	O
both	O
questions	O
and	O
answers	O
are	O
well	O
-	O
formed	O
,	O
human	O
inspection	O
is	O
conducted	O
in	O
multiple	O
iterations	O
to	O
design	O
rules	O
to	O
filter	O
out	O
improper	O
samples	O
.	O

Each	O
dataset	O
is	O
then	O
divided	O
into	O
train	O
,	O
validation	O
and	O
test	O
sets	O
with	O
a	O
90%/5%/5	B-HyperparameterValue
%	I-HyperparameterValue
split	O
.	O

The	O
annotation	O
guideline	O
and	O
examples	O
for	O
each	O
question	O
type	O
are	O
shown	O
in	O
Table	O
12	O
in	O
Appendix	O
A.Training	O
Question	O
Type	O
Classifiers	O
.	O

Both	O
classifiers	O
are	O
based	O
on	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

γ	O
q	O
achieves	O
a	O
macro	B-MetricName
F1	I-MetricName
score	O
of	O
0.80	B-MetricValue
on	O
a	O
reserved	O
test	O
set	O
,	O
with	O
data	O
splits	O
detailed	O
in	O
Appendix	O
B.	O
To	O
train	O
γ	O
a	O
,	O
in	O
addition	O
to	O
the	O
annotated	O
questions	O
,	O
we	O
run	O
γ	O
q	O
on	O
unlabeled	O
questions	O
in	O
Reddit	O
and	O
Yahoo	O
and	O
include	O
samples	O
whose	O
type	O
prediction	B-MetricName
confidence	I-MetricName
score	I-MetricName
is	O
above	O
0.9	B-MetricValue
.	O

γ	O
a	O
obtains	O
macro	B-MetricName
F1	I-MetricName
scores	O
of	O
0.48	B-MetricValue
and	O
0.46	B-MetricValue
on	O
the	O
same	O
reserved	O
test	O
set	O
over	O
all	O
types	O
after	O
training	O
on	O
Yahoo	O
and	O
Reddit	O
,	O
respectively	O
.	O

Yahoo	B-DatasetName
dataset	O
is	O
more	O
balanced	O
,	O
with	O
PROCEDURAL	O
questions	O
being	O
the	O
most	O
frequent	O
type	O
(	O
19.9	O
%	O
of	O
all	O
samples	O
)	O
.	O

Distri-	O
butions	O
of	O
question	O
types	O
for	O
the	O
two	O
datasets	O
are	O
listed	O
in	O
Table	O
8	O
in	O
Appendix	O
B.4	O
Type	B-TaskName
-	I-TaskName
aware	I-TaskName
Open	I-TaskName
-	I-TaskName
ended	I-TaskName
Question	I-TaskName
GenerationIn	I-TaskName
this	O
section	O
,	O
we	O
present	O
our	O
type	B-TaskName
-	I-TaskName
aware	I-TaskName
question	I-TaskName
generation	I-TaskName
framework	O
.	O

Our	O
generator	O
is	O
built	O
on	O
top	O
of	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

Question	O
focuses	O
are	O
first	O
detected	O
based	O
on	O
the	O
semantic	B-MethodName
graph	I-MethodName
,	O
which	O
then	O
guide	O
question	O
generation	O
via	O
cross	O
-	O
attentions	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

Although	O
the	O
joint	O
modeling	O
of	O
focus	B-TaskName
prediction	I-TaskName
and	I-TaskName
question	I-TaskName
generation	I-TaskName
has	O
been	O
studied	O
before	O
,	O
our	O
design	O
differs	O
by	O
using	O
shared	O
representations	O
consisting	O
of	O
the	O
input	O
text	O
and	O
semantic	O
graph	O
,	O
and	O
the	O
prediction	O
of	O
focuses	O
are	O
included	O
through	O
gating	O
mechanisms	O
,	O
whereas	O
previous	O
work	O
,	O
e.g.	O
Pan	O
et	O
al	O
.	O
(	O

Below	O
,	O
we	O
first	O
describe	O
constructing	O
the	O
semantic	B-MethodName
graph	I-MethodName
-	I-MethodName
augmented	I-MethodName
encoder	I-MethodName
,	O
followed	O
by	O
the	O
joint	O
modeling	O
of	O
two	O
tasks	O
.	O

Improving	O
Long	O
Text	O
Comprehension	O
with	O
Semantic	B-MethodName
Graph	I-MethodName
.	O

To	O
construct	O
the	O
semantic	B-MethodName
graph	I-MethodName
,	O
for	O
each	O
sentence	O
,	O
we	O
start	O
with	O
obtaining	O
its	O
dependency	O
tree	O
using	O
Stanford	B-MethodName
CoreNLP	I-MethodName
(	O
Manning	O
et	O
al	O
.	O
,	O

2010	O
)	O
,	O
we	O
extract	O
semantic	O
roles	O
and	O
their	O
relations	O
with	B-MethodName
AllenNLP	I-MethodName
(	O
Shi	O
and	O
Lin	O
,	O
2019	O
)	O
.	O

To	O
merge	O
the	O
two	O
sources	O
of	O
information	O
,	O
we	O
add	O
an	O
edge	O
in	O
the	O
dependency	O
tree	O
to	O
connect	O
the	O
head	O
word	O
of	O
the	O
predicate	O
and	O
the	O
head	O
word	O
of	O
each	O
semantic	O
role	O
.	O

Joint	B-MethodName
Modeling	I-MethodName
with	O
Cross	O
-	O
attentions	O
.	O

Given	O
a	O
predicted	O
question	O
type	O
t	O
and	O
a	O
multi	O
-	O
sentence	O
textx	O
=	O
{	O
x	O
1	O
,	O
•	O
•	O
•	O
,	O
x	O
n	O
}	O
,	O
the	B-MethodName
BART	I-MethodName
encoder	O
builds	O
the	O
contextual	O
representation	O
H	O
=	O
{	O
h	O
0	O
,	O
h	O
1	O
,	O
•	O
•	O
•	O
,	O
h	O
n	O
}	O
at	O
the	O
last	O
layer	O
,	O
where	O
h	O
0	O
is	O
for	O
t.	O
To	O
encode	O
the	O
semantic	O
graph	O
,	O
we	O
initialize	O
the	O
node	O
representation	O
for	O
node	O
v	O
i	O
by	O
taking	O
the	O
average	O
contextual	O
representations	O
of	O
its	O
tokens	O
and	O
appending	O
four	O
bits	O
encoding	O
the	O
number	O
of	O
nodes	O
(	O
capped	O
at	O
10	O
)	O
that	O
are	O
merged	O
into	O
v	O
i	O
,	O
to	O
add	O
frequency	O
information	O
.	O

We	O
then	O
apply	O
graph	B-MethodName
attention	I-MethodName
networks	I-MethodName
(	O
GATs	O
)	O
(	O
Veličković	O
et	O
al	O
.	O
,	O

2018	O
)	O
of	O
L	O
layers	O
to	O
update	O
the	O
representations	O
as	O
follows	O
:	O
v	O
(	O
l	O
)	O
i	O
=	O
j∈Ni	O
a	O
i	O
,	O
j	O
W	O
(	O
l	O
)	O
v	O
(	O
l−1	O
)	O
j	O
(	O
1)where	O
W	O
(	O
l	O
)	O
is	O
a	O
learnable	O
parameter	O
for	O
the	O
l	O
-	O
th	O
layer	O
,	O
and	O
N	O
i	O
denotes	O
the	O
neighbors	O
of	O
v	O
i	O
.	O

The	O
attention	O
score	O
a	O
i	O
,	O
j	O
is	O
calculated	O
as	O
in	O
GATs	B-MethodName
.	O

To	O
predict	O
focuses	O
,	O
the	O
final	O
node	O
representation	O
v	O
(	O
L	O
)	O
i	O
is	O
fed	O
into	O
the	O
following	O
feedforward	B-MethodName
network	I-MethodName
,	O
yielding	O
the	O
probability	O
of	O
v	O
i	O
being	O
a	O
focus	O
as	O
:p	O
f	O
ocus	O
(	O
v	O
i	O
=	O
1	O
)	O
=	O
σ(W	O
1	O
tanh(W	O
2	O
v	O
(	O
L	O
)	O
i	O
)	O
)	O
(	O
2)where	O
W	O
1	O
and	O
W	O
2	O
are	O
learnable	O
parameters	O
.	O

To	O
generate	O
the	O
question	O
,	O
we	O
use	O
the	O
gating	O
mechanism	O
to	O
inform	O
the	O
focus	O
prediction	O
results	O
,	O
where	O
new	O
node	O
representations	O
after	O
being	O
weighted	O
by	O
the	O
focus	O
probability	O
are	O
:	O
v	O
(	O
L	O
)	O
i	O
=	O
g	O
i	O
v	O
(	O
L	O
)	O
i	O
g	O
i	O
=	O
p	O
f	O
ocus	O
(	O
v	O
i	O
=	O
1)(3)Our	O
model	O
benefits	O
from	O
both	O
large	O
pre	O
-	O
training	O
and	O
hybrid	O
semantic	O
graphs	O
by	O
adding	O
a	O
separate	O
cross	O
attention	O
for	O
node	O
presentations	O
in	O
each	O
BART	B-MethodName
decoder	O
layer	O
.	O

We	O
then	O
design	O
separate	O
cross	O
attentions	O
to	O
attend	O
(	O
1	O
)	O
the	O
output	O
of	O
the	O
BART	B-MethodName
encoder	O
,	O
yielding	O
z	O
e	O
,	O
and	O
(	O
2	O
)	O
the	O
node	O
representations	O
V	O
(	O
L	O
)	O
,	O
producing	O
z	O
v	O
,	O
which	O
are	O
formulated	O
as	O
:	O
z	O
e	O
=	O
LN(z	O
s	O
+	O
Attn(z	O
s	O
,	O
H	O
)	O
)	O
(	O
4)z	O
v	O
=	O
LN(z	O
e	O
+	O
Attn(z	O
e	O
,	O
V	O
(	O
L	O
)	O
)	O
)	O
(	O
5	O
)	O
z	O
=	O
LN(z	O
v	O
+	O
FFN(z	O
v	O
)	O
)	O
(	O
6)where	O
z	O
s	O
denotes	O
the	O
output	O
of	O
self	O
attentions	O
for	O
the	O
current	O
layer	O
,	O
and	O
z	O
is	O
the	O
output	O
for	O
the	O
layer	O
.	O

We	O
thus	O
propose	O
to	O
leverage	O
question	O
templates	O
to	O
gain	O
stronger	O
controllability	O
.	O

Below	O
we	O
first	O
present	O
how	O
to	O
automatically	O
extract	O
templates	O
from	O
the	O
training	O
set	O
,	O
and	O
then	O
introduce	O
two	O
model	O
variants	O
that	O
are	O
built	O
on	O
the	O
JOINTGEN	B-MethodName
framework	O
:	O
EXPLGEN	B-MethodName
uses	O
exemplar	O
templates	O
to	O
guide	O
the	O
model	O
to	O
generate	O
questions	O
of	O
selected	O
types	O
,	O
and	O
TPLGEN	B-MethodName
adds	O
an	O
extra	O
step	O
to	O
first	O
generate	O
type	O
-	O
specific	O
templates	O
.	O

We	O
further	O
consider	O
topically	O
related	O
words	O
in	O
the	O
questions	O
,	O
by	O
calculating	O
word	O
-	O
level	O
semantic	O
similarities	O
based	O
on	O
Numberbatch	B-MethodName
word	I-MethodName
embeddings	I-MethodName
(	O
Speer	O
et	O
al	O
.	O
,	O

For	O
instance	O
,	O
a	O
question	O
"	O
What	O
are	O
the	O
differences	O
between	O
global	O
warming	O
and	O
climate	O
change	O
?	O
"	O
becomes	O
"	O
What	O
are	O
the	O
differences	O
between	O
[	O
NP	O
]	O
and	O
[	O
NP]?"Exemplars	B-MethodName
for	I-MethodName
Guidance	I-MethodName
(	I-MethodName
EXPLGEN	I-MethodName
)	I-MethodName
.	O

They	O
are	O
listed	O
in	O
Table	O
10	O
in	O
Appendix	O
D.During	O
training	O
,	O
we	O
choose	O
the	O
exemplar	O
that	O
has	O
the	O
lowest	O
edit	O
distance	O
with	O
the	O
question	O
,	O
which	O
is	O
also	O
used	O
for	O
training	O
an	O
exemplar	O
selector	O
based	O
on	O
RoBERTa	B-MethodName
.	O

During	O
testing	O
,	O
the	O
exemplar	O
with	O
the	O
highest	O
selector	O
score	O
is	O
used	O
.	O

Specifically	O
,	O
we	O
reuse	O
EXPLGEN	B-MethodName
to	O
learn	O
to	O
generate	O
a	O
target	O
template	O
,	O
as	O
derived	O
from	O
the	O
template	O
extraction	O
procedure	O
.	O

During	O
question	O
realization	O
,	O
TPLGEN	B-MethodName
uses	O
a	O
BART	B-MethodName
-	I-MethodName
based	I-MethodName
generator	I-MethodName
that	O
takes	O
as	O
input	O
the	O
question	O
type	O
,	O
the	O
input	O
text	O
,	O
the	O
generated	O
template	O
,	O
and	O
the	O
words	O
that	O
are	O
predicted	O
as	O
focuses	O
.	O

We	O
use	O
separate	O
cross	O
attentions	O
to	O
attend	O
the	O
representations	O
of	O
the	O
focused	O
words	O
,	O
similar	O
to	O
how	O
node	O
representations	O
are	O
attended	O
in	O
JOINTGEN.We	B-MethodName
recognize	O
that	O
having	O
separate	O
stages	O
of	O
exemplar	O
selection	O
and	O
template	O
generation	O
introduces	O
extra	O
model	O
training	O
cost	O
and	O
potential	O
errors	O
in	O
the	O
pipeline	O
.	O

Comparisons	O
and	O
Metrics	O
.	O

We	O
compare	O
with	O
DEEPQG	B-MethodName
(	O
Pan	O
et	O
al	O
.	O
,	O

We	O
also	O
compare	O
with	O
BART	O
models	O
that	O
are	O
finetuned	O
on	O
the	O
same	O
datasets	O
as	O
in	O
our	O
models	O
,	O
by	O
using	O
inputs	O
of	O
(	O
1	O
)	O
the	O
answer	O
(	B-MethodName
BART	I-MethodName
)	O
,	O
(	O
2	O
)	O
the	O
answer	O
and	O
a	O
predicted	O
question	O
word	O
(	O
BART+QWORD	B-MethodName
)	O
,	O
and	O
(	O
3	O
)	O
the	O
answer	O
and	O
a	O
predicted	O
question	O
type	O
(	O
BART+QTYPE	B-MethodName
)	O
.	O

For	O
BART+QWORD	B-MethodName
,	O
the	O
question	O
word	O
is	O
predicted	O
by	O
a	O
RoBERTa	B-MethodName
classifier	O
that	O
considers	O
the	O
answer	O
and	O
is	O
trained	O
on	O
our	O
training	O
sets	O
.	O

For	O
both	O
our	O
models	O
and	O
BART+QTYPE	B-MethodName
,	O
the	O
most	O
confident	O
type	O
predicted	O
by	O
the	O
classifier	O
γ	O
a	O
(	O
described	O
in	O
§	O
3.2	O
)	O
,	O
which	O
reads	O
in	O
the	O
answer	O
,	O
is	O
used	O
as	O
input	O
.	O

To	O
test	O
the	O
efficacy	O
of	O
semantic	B-MethodName
graphs	I-MethodName
,	O
we	O
further	O
compare	O
with	O
a	O
variant	O
of	O
JOINTGEN	B-MethodName
that	O
only	O
uses	O
the	O
flat	O
Transformer	B-MethodName
for	O
focus	O
prediction	O
and	O
question	O
generation	O
,	O
denoted	O
as	O
JOINTGEN	B-MethodName
w/o	O
graph	O
.	O

We	O
evaluate	O
the	O
generated	O
questions	O
with	O
BLEU	B-MetricName
(	O
Papineni	O
et	O
al	O
.	O
,	O

2002	O
)	O
,	O
METEOR	B-MetricName
(	O
Lavie	O
and	O
Agarwal	O
,	O
2007	O
)	O
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
(	O
Lin	O
,	O
2004	O
having	O
structured	O
representation	O
is	O
useful	O
for	O
focus	O
detection	O
and	O
the	O
final	O
question	O
generation	O
task	O
.	O

We	O
also	O
observe	O
a	O
huge	O
performance	O
gap	O
between	O
DEEPQG	B-MethodName
and	O
systems	O
based	O
on	O
BART	B-MethodName
,	O
signifying	O
the	O
importance	O
of	O
leveraging	O
pre	O
-	O
trained	O
models	O
for	O
open	O
-	O
ended	O
question	O
generation	O
.	O

Meanwhile	O
,	O
adding	O
question	O
types	O
helps	O
BART	B-MethodName
generate	O
more	O
relevant	O
questions	O
than	O
using	O
question	O
words	O
,	O
indicating	O
the	O
value	O
of	O
our	O
new	O
question	O
type	O
ontology	O
.	O

Notably	O
,	O
our	O
template	O
-	O
based	O
generators	O
,	O
EX	B-MethodName
-	I-MethodName
PLGEN	I-MethodName
and	O
TPLGEN	B-MethodName
,	O
which	O
are	O
trained	O
to	O
comply	O
with	O
the	O
given	O
templates	O
,	O
still	O
produce	O
comparable	O
scores	O
.	O

For	B-MethodName
BART	I-MethodName
,	O
we	O
use	B-HyperparameterName
nucleus	I-HyperparameterName
sampling	I-HyperparameterName
(	O
Holtzman	O
et	O
al	O
.	O
,	O

2020	O
)	O
with	B-HyperparameterName
k	I-HyperparameterName
=	O
10	B-HyperparameterValue
and	O
p	B-HyperparameterName
=	O
0.7	B-HyperparameterValue
to	O
sample	O
diverse	O
questions	O
.	O

To	O
evaluate	O
,	O
we	O
first	O
calculate	O
the	O
question	B-MetricName
type	I-MetricName
accuracy	I-MetricName
by	O
comparing	O
whether	O
the	O
types	O
of	O
the	O
generated	O
questions	O
match	O
the	O
specified	O
ones	O
,	O
with	O
types	O
labeled	O
by	O
our	O
classifier	O
γ	O
q	O
(	O
§	O
3.2	O
)	O
.	O

Finally	O
,	O
we	O
consider	O
pairwise	B-MetricName
BLEU-4	I-MetricName
(	O
Cho	O
et	O
al	O
.	O
,	O

2019	O
)	O
by	O
computing	O
the	O
BLEU-4	B-MetricName
between	O
pairwise	O
generated	O
questions	O
per	O
sample	O
,	O
where	O
lower	O
values	O
suggest	O
higher	O
content	O
diversity	O
.	O

First	O
,	O
our	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
can	O
generate	O
questions	O
with	O
diverse	O
types	O
and	O
content	O
,	O
as	O
shown	O
by	O
the	O
significantly	O
higher	O
numbers	O
of	O
unique	O
types	O
than	O
all	O
comparisons	O
and	O
lower	O
pairwise	O
BLEU	B-MetricName
scores	O
than	O
comparisons	O
except	O
for	O
BART	B-MethodName
with	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
in	O
Table	O
3	O
.	O

This	O
implies	O
stronger	O
type	O
control	O
by	O
template	O
-	O
based	O
generators	O
,	O
compared	O
to	O
BART+QTYPE	B-MethodName
and	O
JOINTGEN	B-MethodName
which	O
only	O
use	O
the	O
question	O
type	O
token	O
as	O
input	O
.	O

Results	O
on	O
numbers	O
of	O
unique	O
types	O
by	O
varying	O
numbers	O
of	O
question	O
types	O
specified	O
in	O
the	O
input	O
are	O
displayed	O
in	O
Figure	O
3	O
,	O
where	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
maintain	O
steady	O
controllability	O
.	O

Among	O
the	O
comparisons	O
,	O
although	O
BART	B-MethodName
with	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
and	O
BART+QWORD	B-MethodName
both	O
have	O
low	O
pairwise	B-MetricName
BLEU	I-MetricName
,	O
the	O
types	O
of	O
questions	O
they	O
can	O
generate	O
are	O
limited	O
.	O

We	O
hire	O
three	O
annotators	O
who	O
have	O
participated	O
in	O
our	O
question	O
type	O
annotation	O
study	O
to	O
evaluate	O
80	O
groups	O
of	O
questions	O
generated	O
by	O
four	O
selected	O
models	O
on	O
each	O
dataset	O
.	O

We	O
find	O
that	O
human	O
judges	O
rate	O
questions	O
generated	O
by	O
our	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
as	O
having	O
greater	O
diversities	O
over	O
all	O
aspects	O
,	O
except	O
for	O
syntax	O
diversity	O
on	O
Reddit	O
,	O
as	O
shown	O
in	O
Table	O
4	O
.	O

Among	O
the	O
two	O
model	O
variants	O
,	O
questions	O
by	O
TPLGEN	B-MethodName
yield	O
more	O
diverse	O
answers	O
.	O

Based	O
on	O
our	O
observation	O
,	B-MethodName
TPLGEN	I-MethodName
uses	O
automatically	O
generated	O
templates	O
to	O
produce	O
more	O
focused	O
questions	O
with	O
different	O
answers	O
,	O
compared	O
to	O
EXPLGEN	B-MethodName
which	O
employs	O
exemplars	O
.	O

Besides	O
Figure	O
1	O
,	O
we	O
show	O
more	O
sample	O
outputs	O
in	O
Figure	O
4	O
,	O
where	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
exhibit	O
stronger	O
controllability	O
than	O
JOINTGEN	B-MethodName
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
our	O
JOINTGEN	B-MethodName
model	O
produces	O
questions	O
with	O
better	O
answerability	O
and	O
that	O
cover	O
broader	O
content	O
in	O
the	O
answers	O
.	O

Between	O
BART+QWORD	B-MethodName
and	O
BART+QTYPE	B-MethodName
,	O
human	O
judges	O
rate	O
the	O
system	O
outputs	O
that	O
conditioned	O
on	O
our	O
question	O
types	O
to	O
have	O
better	O
overall	O
quality	O
.	O

Does	O
focus	O
prediction	O
correlate	O
with	O
question	O
quality	O
?	O
We	O
first	O
investigate	O
the	O
relationship	O
between	O
focus	O
prediction	O
and	O
question	O
generation	O
by	O
using	O
our	O
joint	O
model	O
JOINTGEN	B-MethodName
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
5	O
,	O
there	O
is	O
a	O
strong	O
correlation	O
between	O
F1	B-MetricName
scores	O
of	O
focus	O
prediction	O
and	O
BLEU-4	B-MetricName
as	O
well	O
We	O
also	O
show	O
the	O
F1	B-MetricName
scores	O
and	O
BLEU-4	B-MetricName
for	O
selected	O
question	O
types	O
on	O
the	O
right	O
of	O
Figure	O
5	O
,	O
again	O
demonstrating	O
the	O
effect	O
of	O
focus	O
detection	O
on	O
question	O
quality	O
.	O

When	O
do	O
our	O
models	O
fail	O
to	O
respect	O
the	O
given	O
types	O
?	O
Next	O
,	O
we	O
provide	O
insights	O
into	O
which	O
types	O
of	O
questions	O
are	O
challenging	O
to	O
generate	O
by	O
using	O
our	O
template	O
-	O
based	O
models	O
EXPLGEN	B-MethodName
and	O
TPLGEN	B-MethodName
.	O

We	O
describe	O
a	O
joint	O
question	O
focus	O
detection	O
and	O
question	O
generation	O
framework	O
with	O
a	O
novel	O
semantic	B-TaskName
graphaugmented	I-TaskName
representation	I-TaskName
,	O
which	O
is	O
directly	O
built	O
on	O
large	O
pre	O
-	O
trained	O
models	O
.	O

We	O
discard	O
secondary	O
dependency	O
relations	O
for	O
graph	O
construction	O
,	O
including	O
case	O
,	O
mark	O
,	O
cc	O
,	O
cc	O
:	O
preconj	O
,	O
aux	O
,	O
aux	O
:	O
pass	O
,	O
cop	O
,	O
det	O
,	O
discourse	O
,	O
expl	O
,	O
det	O
:	O
predet	O
,	O
punct	O
,	O
ref	O
.	O

The	O
definition	O
for	O
each	O
dependency	O
can	O
be	O
found	O
in	O
Universal	B-MethodName
Dependency	I-MethodName
.	O

For	O
the	O
Graph	B-MethodName
Attention	I-MethodName
Networks	I-MethodName
(	I-MethodName
GATs	I-MethodName
)	O
in	O
our	O
focus	O
predictor	O
,	O
we	O
adopt	O
the	O
implementation	O
by	O
PyTorch	O
Geometric	O
(	O
Fey	O
and	O
Lenssen	O
,	O
2019	O
)	O
.	O

We	O
use	O
Adam	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
for	O
the	O
training	O
of	O
all	O
our	O
models	O
.	O

Our	O
question	O
type	O
classifiers	O
and	O
template	O
exemplar	O
classifiers	O
are	O
trained	O
with	O
a	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
.	O

For	O
training	O
generation	O
models	O
,	O
the	O
maximum	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
3	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
and	O
each	O
batch	B-HyperparameterName
contains	O
at	O
most	O
32,768	B-HyperparameterValue
models	O
except	O
for	O
models	O
with	O
GATs	B-MethodName
.	O

We	O
use	B-HyperparameterName
beam	I-HyperparameterName
search	I-HyperparameterName
for	O
decoding	O
.	O

A	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
5	B-HyperparameterValue
and	O
a	O
length	B-HyperparameterName
penalty	I-HyperparameterName
of	O
1.5	B-HyperparameterValue
are	O
used	O
for	O
all	O
models	O
.	O

Repeated	B-HyperparameterName
trigram	I-HyperparameterName
blocking	I-HyperparameterName
is	O
applied	O
to	O
question	O
generation	O
.	O

The	O
minimum	B-HyperparameterName
and	I-HyperparameterName
maximum	I-HyperparameterName
lengths	I-HyperparameterName
for	O
generation	O
are	O
set	O
to	O
1	B-HyperparameterValue
and	O
100	B-HyperparameterValue
,	O
respectively	O
.	O

It	O
is	O
not	O
determined	O
by	O
the	O
interrogative	O
word	O
of	O
the	O
question	O
.	O

We	O
build	O
models	O
with	O
different	O
textual	O
representations	O
,	O
and	O
show	O
that	O
the	O
identified	O
features	O
are	O
highly	O
predictive	O
of	O
engagement	O
.	O

Our	O
metric	O
of	O
engagement	O
is	O
stream	B-DatasetName
rate	I-DatasetName
,	O
which	O
we	O
define	O
as	O
the	O
proportion	O
of	O
first	O
-	O
time	O
listeners	O
-of	O
those	O
who	O
have	O
begun	O
streaming	O
the	O
episode	O
-who	O
listen	O
for	O
at	O
least	O
five	O
minutes	O
.	O

Notably	O
,	O
stream	O
rate	O
is	O
different	O
from	O
the	O
metric	O
of	O
popularity	B-MetricName
as	O
given	O
by	O
the	O
raw	O
number	O
of	O
streams	O
;	O
the	O
latter	O
is	O
inevitably	O
influenced	O
by	O
factors	O
unrelated	O
to	O
the	O
content	O
,	O
such	O
as	O
the	O
host	O
or	O
publisher	O
reputation	O
,	O
publicity	O
,	O
expo	O
-	O
sure	O
in	O
recommendations	O
and	O
search	O
engines	O
,	O
and	O
time	O
of	O
publication	O
,	O
whereas	O
a	O
listener	O
's	O
decision	O
to	O
continue	O
listening	O
for	O
as	O
long	O
as	O
five	O
minutes	O
is	O
likely	O
to	O
be	O
influenced	O
by	O
the	O
content	O
.	O

Our	O
predictive	O
models	O
prove	O
that	O
stylistic	O
factors	O
alone	O
play	O
a	O
significant	O
role	O
in	O
determining	O
if	O
a	O
podcast	O
has	O
high	O
or	O
low	O
engagement	O
,	O
achieving	O
an	O
accuracy	B-MetricName
of	O
72	B-MetricValue
%	I-MetricValue
in	O
distinguishing	O
between	O
very	O
high	O
engagement	O
(	O
top	O
25	B-MetricValue
%	I-MetricValue
of	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
in	O
the	O
corpus	O
)	O
and	O
very	O
low	O
engagement	O
(	O
bottom	O
25	B-MetricValue
%	I-MetricValue
)	O
examples	O
.	O

We	O
also	O
show	O
that	O
the	O
overall	O
textual	O
information	O
in	O
podcasts	O
is	O
highly	O
predictive	O
of	O
engagement	O
in	O
this	O
experiment	O
,	O
with	O
an	O
accuracy	B-MetricName
as	O
high	O
as	O
81	B-MetricValue
%	I-MetricValue
.	O

To	O
understand	O
how	O
style	O
in	O
podcasts	O
compares	O
to	O
other	O
spoken	O
media	O
,	O
we	O
apply	O
our	O
analysis	O
to	O
a	O
corpus	B-TaskName
of	I-TaskName
TED	I-TaskName
talks	I-TaskName
.	O

Finally	O
,	O
we	O
manually	O
examine	O
the	O
highest	O
engagement	O
podcasts	O
in	O
our	O
dataset	O
to	O
characterize	O
their	O
content	O
.	B-MethodName

Content	I-MethodName
-	I-MethodName
Based	I-MethodName
Podcast	I-MethodName
Recommendations	I-MethodName
Yang	O
et	O
al	O
.	O
(	O

2019	O
)	O
model	O
transcripts	O
with	O
a	O
topic	B-MethodName
model	I-MethodName
,	O
and	O
the	O
audio	O
with	O
a	O
representation	O
they	O
trained	O
to	O
predict	O
the	O
non	O
-	O
textual	O
attributes	O
of	O
seriousness	O
and	O
energy	O
.	O

They	O
find	O
that	O
combining	O
these	O
representations	O
improves	O
over	O
the	O
purely	O
topic	O
based	O
model	O
on	O
popularity	B-TaskName
prediction	I-TaskName
.	O

Predicting	B-TaskName
Performance	I-TaskName
from	I-TaskName
Language	I-TaskName
Previous	O
research	O
in	O
natural	O
language	O
processing	O
has	O
explored	O
the	O
connections	O
between	O
textual	O
features	O
and	O
audience	O
engagement	O
in	O
books	O
(	O
Ganjigunte	O
Ashok	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
YouTube	O
(	O
Kleinberg	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
in	O
addition	O
to	O
the	O
entire	O
field	O
of	O
sentiment	B-TaskName
and	I-TaskName
opinion	I-TaskName
mining	I-TaskName
of	O
data	O
such	O
as	O
user	O
reviews	O
(	O
Pang	O
et	O
al	O
.	O
,	O

The	O
Spotify	B-DatasetName
Podcast	I-DatasetName
Dataset	O
is	O
a	O
recently	O
released	O
corpus	O
of	O
over	O
100	O
,	O
000	O
podcast	O
episodes	O
,	O
mostly	O
in	O
English	O
,	O
that	O
are	O
transcribed	O
with	O
Google	B-MethodName
's	I-MethodName
Speech	I-MethodName
to	I-MethodName
Text	I-MethodName
commercial	I-MethodName
speech	I-MethodName
recognition	I-MethodName
,	O
reported	O
in	O
the	O
paper	O
to	O
have	O
an	O
18	B-MetricValue
%	I-MetricValue
word	B-MetricName
error	I-MetricName
on	O
podcasts	O
.	O

A	O
podcast	O
,	O
also	O
known	O
as	O
a	O
'	O
show	O
'	O
in	O
the	O
dataset	O
,	O
is	O
a	O
collection	O
of	O
episodes	O
.	O

All	O
textual	O
data	O
was	O
normalized	O
and	O
part	B-TaskName
-	I-TaskName
of	I-TaskName
-	I-TaskName
speech	I-TaskName
tagged	I-TaskName
with	O
spacy	O
.	O

2	O
Promotional	O
and	O
extraneous	O
material	O
was	O
detected	O
by	O
the	O
classifier	O
described	O
by	O
,	O
a	O
model	O
using	O
BERT	B-MethodName
with	O
a	O
classification	O
head	O
,	O
trained	O
on	O
a	O
manually	O
annotated	O
set	O
of	O
episode	O
descriptions	O
.	O

This	O
classifier	O
is	O
reported	O
to	O
have	O
a	O
sentence	B-MetricName
classification	I-MetricName
accuracy	I-MetricName
of	O
95	B-MetricValue
%	I-MetricValue
on	O
episode	O
descriptions	O
.	O

As	O
described	O
in	O
the	O
introduction	O
,	O
we	O
use	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
engagement	O
metric	O
,	O
defined	O
as	O
the	O
proportion	O
of	O
the	O
show	O
's	O
first	O
-	O
time	O
listeners	O
who	O
stream	O
at	O
least	O
five	O
minutes	O
of	O
the	O
episode	O
.	O

Stream	B-MetricName
rate	I-MetricName
in	O
the	O
dataset	O
shows	O
a	O
weak	O
but	O
statistically	O
significant	O
inverse	O
rank	O
correlation	O
with	O
popularity	B-MetricName
(	O
Spearman	B-HyperparameterName
's	I-HyperparameterName
ρ	I-HyperparameterName
=	O
−0.12	B-HyperparameterValue
,	O
p	B-HyperparameterName
<	O
0.001	B-HyperparameterValue
)	O
.	O

70	B-MetricValue
%	I-MetricValue
stream	B-MetricName
rate	I-MetricName
in	O
a	O
well	O
-	O
known	O
podcast	O
which	O
A	O
weekly	O
podcast	O
covering	O
all	O
things	O
witchcraft	O
in	O
the	O
modern	O
world	O
.	O

Join	O
us	O
,	O
two	O
best	O
friends	O
and	O
Midwestern	O
witches	O
(	O
one	O
Wiccan	O
,	O
one	O
not	O
)	O
,	O
as	O
we	O
dive	O
into	O
all	O
things	O
witchy	O
.	O

We	O
're	O
starting	O
at	O
the	O
beginning	O
,	O
making	O
this	O
podcast	O
a	O
great	O
resource	O
for	O
newbies	O
...	O
would	O
have	O
attracted	O
a	O
broad	O
array	O
of	O
listeners	O
is	O
not	O
comparable	O
to	O
70	B-MetricValue
%	I-MetricValue
stream	B-MetricName
rate	I-MetricName
in	O
a	O
relatively	O
unknown	O
podcast	O
.	O

Therefore	O
,	O
we	O
bin	O
the	O
dataset	O
into	O
popularity	O
quartiles	O
for	O
analysis	O
on	O
stream	B-MetricName
rate	I-MetricName
,	O
which	O
is	O
found	O
to	O
be	O
uncorrelated	O
with	O
popularity	O
within	O
each	O
quartile	O
.	O

Stream	B-MetricName
rate	I-MetricName
is	O
uncorrelated	O
with	O
the	O
time	O
of	O
publication	O
.	O

To	O
control	O
for	O
duration	O
effects	O
in	O
the	O
analysis	O
of	O
transcripts	O
,	O
we	O
truncate	O
transcripts	O
at	O
ten	B-HyperparameterValue
minutes	I-HyperparameterValue
.	O

We	O
select	O
the	O
moststreamed	O
episode	O
from	O
each	O
show	O
as	O
its	O
representative	O
,	O
thereby	O
ensuring	O
that	O
every	O
show	O
is	O
represented	O
by	O
a	O
single	O
episode	O
in	O
the	O
data	O
.	O

Since	O
the	O
original	O
corpus	O
is	O
an	O
English	O
-	O
language	O
collection	O
,	O
all	O
of	O
our	O
analysis	O
is	O
constrained	O
to	O
English	O
,	O
and	O
we	O
filter	O
out	O
any	O
stray	O
examples	O
in	O
the	O
corpus	O
that	O
are	O
detected	O
as	O
non	O
-	O
English	O
after	O
running	B-TaskName
language	I-TaskName
identification	I-TaskName
(	O
Lui	O
and	O
Baldwin	O
,	O
2011	O
)	O
on	O
the	O
descriptions	O
.	O

The	O
resulting	O
dataset	O
has	O
5371	B-HyperparameterValue
episodes	O
.	O

For	O
example	O
,	O
technical	O
podcasts	O
are	O
expected	O
to	O
contain	O
more	O
complex	O
language	O
compared	O
to	O
chit	O
-	O
chat	O
,	O
crime	O
podcasts	O
to	O
contain	O
words	O
with	O
negative	O
sentiments	O
as	O
opposed	O
to	O
motivational	O
podcasts	O
,	O
and	O
so	O
on	O
.	O

Instead	O
,	O
we	O
fit	O
an	O
LDA	B-MethodName
topic	I-MethodName
model	O
(	O
Blei	O
et	O
al	O
.	O
,	O

2003	O
)	O
with	O
100	B-HyperparameterValue
topics	B-HyperparameterName
3	O
to	O
transcripts	O
of	O
the	O
entire	O
100k	B-HyperparameterValue
podcast	O
corpus	O
as	O
in	O
previous	O
works	O
Yang	O
et	O
al	O
.	O
,	O

Table	O
2	O
shows	O
a	O
sample	O
of	O
the	O
inferred	O
topics	O
.	O

Length	B-HyperparameterName
Descriptions	I-HyperparameterName
are	O
known	O
to	O
be	O
important	O
for	O
listeners	O
on	O
their	O
first	O
encounter	O
with	O
the	O
pod	O
-	O
cast	O
.	O

We	O
also	O
measure	O
audio	B-HyperparameterName
duration	I-HyperparameterName
,	O
since	O
surveys	O
show	O
it	O
is	O
a	O
consideration	O
(	O
McLean	O
,	O
2020).Proportion	O
of	O
ads	O
and	O
show	O
notes	O
Descriptions	O
of	O
well	O
-	O
known	O
podcasts	O
tend	O
to	O
contain	O
advertisements	O
of	O
other	O
podcasts	O
made	O
by	O
the	O
same	O
network	O
,	O
links	O
to	O
the	O
hosts	O
'	O
or	O
guests	O
'	O
social	O
media	O
presence	O
and	O
websites	O
,	O
or	O
show	O
notes	O
and	O
transcripts	O
,	O
and	O
podcast	O
creators	O
are	O
often	O
advised	O
to	O
include	O
such	O
information	O
(	O
Dennis	O
,	O
2020	O
)	O
,	O
and	O
surveys	O
have	O
shown	O
that	O
the	O
majority	O
of	O
podcast	O
listeners	O
do	O
not	O
mind	O
sponsor	O
ads	O
in	O
the	O
content	O
(	O
McLean	O
,	O
2020	O
)	O
.	O

The	O
proportion	O
of	O
ads	O
in	O
transcripts	O
is	O
given	O
by	O
a	O
manually	O
identified	B-MethodName
LDA	I-MethodName
topic	O
that	O
corresponds	O
to	O
words	O
indicative	O
of	O
ads	O
.	O

Do	O
listeners	O
seem	O
to	O
prefer	O
descriptions	O
that	O
accurately	O
convey	O
the	O
topics	O
and	O
synopsis	O
of	O
the	O
episode	O
?	O
We	O
measure	O
faithfulness	O
of	O
the	O
episode	O
description	O
to	O
the	O
first	O
ten	O
minutes	O
of	O
the	O
transcript	O
as	O
the	O
cosine	B-MethodName
similarity	I-MethodName
between	O
the	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
bag	I-MethodName
of	I-MethodName
words	I-MethodName
representation	O
of	O
both	O
texts	O
.	O

4	O
Distinctiveness	O
Podcast	O
creators	O
are	O
often	O
encouraged	O
to	O
develop	O
a	O
distinctive	O
style	O
(	O
Gray	O
,	O
2021a	O
)	O
.	O

We	O
define	O
distinctiveness	O
as	O
the	O
perplexity	B-MetricName
of	O
the	O
given	O
text	O
under	O
a	O
unigram	B-MethodName
language	I-MethodName
model	I-MethodName
trained	O
over	O
all	O
the	O
episodes	O
in	O
the	O
dataset	O
.	O

2019	O
)	O
of	O
randomly	O
sampling	O
a	O
constant	O
number	O
of	O
words	O
from	O
each	O
text	O
and	O
taking	O
the	O
mean	B-MetricName
cross	I-MetricName
entropy	I-MetricName
over	O
a	O
few	O
samples	O
.	O

2020	O
)	O
,	O
we	O
make	O
two	O
measurements	O
:	O
the	O
Flesch	B-MetricName
-	I-MetricName
Kincaid	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Flesch	O
,	O
1948	O
)	O
that	O
measures	O
the	O
number	O
of	O
syllables	O
per	O
word	O
and	O
the	O
number	O
of	O
words	O
per	O
sentence	O
,	O
and	O
the	O
Dale	B-MetricName
-	I-MetricName
Chall	I-MetricName
grade	I-MetricName
level	I-MetricName
(	O
Chall	O
and	O
Dale	O
,	O
1948	O
)	O
which	O
measures	O
word	O
'	O
difficulty	O
'	O
using	O
a	O
lookup	O
table	O
.	O

While	O
caution	O
must	O
be	O
taken	O
on	O
interpreting	O
reading	O
grade	B-MetricName
level	I-MetricName
for	O
transcribed	O
speech	O
,	O
these	O
measures	O
have	O
been	O
explored	O
for	O
speech	O
in	O
prior	O
work	O
(	O
Schumacher	O
and	O
Eskenazi	O
,	O
2016).Vocabulary	O
Diversity	O
We	O
examine	O
whether	O
podcast	O
creators	O
of	O
high	O
engagement	O
podcasts	O
use	O
more	O
diverse	O
vocabularies	O
,	O
quantified	O
by	O
the	O
entropy	B-MetricName
of	O
the	O
unigram	O
words	O
in	O
the	O
text	O
,	O
motivated	O
by	O
advice	O
to	O
avoid	O
word	O
repetition	O
(	O
Bellis	O
,	O
2017	O
)	O
.	O

Popular	O
advice	O
often	O
encourages	O
podcast	O
creators	O
to	O
be	O
upbeat	O
and	O
positive	O
(	O
Briggman	O
,	O
2020	O
)	O
.	O

The	O
NRC	B-DatasetName
Emotion	I-DatasetName
Lexicon	I-DatasetName
(	O
Mohammad	O
and	O
Turney	O
,	O
2013	O
)	O
contains	O
positive	O
and	O
negative	O
sentiment	O
assignments	O
,	O
as	O
well	O
as	O
emotions	O
such	O
as	O
anger	O
,	O
trust	O
,	O
and	O
fear	O
,	O
for	O
14182	O
words	O
.	O

Since	O
a	O
lexicon	O
lookup	O
for	O
sentiment	O
is	O
naturally	O
limited	O
in	O
that	O
it	O
does	O
not	O
account	O
for	O
compositionality	O
and	O
can	O
not	O
model	O
words	O
and	O
variants	O
that	O
are	O
missing	O
in	O
the	O
lexicon	O
,	O
we	O
also	O
apply	O
a	O
fullsentence	O
classifier	O
,	O
the	O
sentiment	O
model	O
from	O
the	O
Google	B-MethodName
Natural	I-MethodName
Language	I-MethodName
API	I-MethodName
7	I-MethodName
.	O

Swearing	O
and	O
fillers	O
We	O
conjecture	O
that	O
podcasts	O
with	O
swearing	O
and	O
adult	O
language	O
may	O
not	O
have	O
broad	O
appeal	O
.	O

Public	O
speaking	O
recommendations	O
in	O
podcasting	O
guides	O
(	O
Coips	O
and	O
Kramer	O
,	O
2020	O
)	O
emphasize	O
the	O
reduction	O
of	O
filler	O
words	O
like	O
'	O
yeah	O
'	O
or	O
'	O
okay	O
'	O
,	O
and	O
the	O
use	O
of	O
professional	O
speech	O
.	O

Instead	O
,	O
we	O
take	O
advantage	O
of	O
the	O
observation	O
that	O
some	O
of	O
the	O
topics	O
inferred	O
by	O
the	B-MethodName
LDA	I-MethodName
model	O
correspond	O
to	O
swear	O
words	O
and	O
filler	O
terms	O
,	O
and	O
measure	O
the	O
proportions	O
of	O
these	O
topics	O
.	O

In	O
this	O
section	O
,	O
we	O
analyze	O
the	O
different	O
linguistic	O
features	O
by	O
comparing	O
group	O
means	O
between	O
the	O
top	O
and	O
bottom	O
25	O
%	O
of	O
podcasts	O
by	O
engagement	O
within	O
each	O
popularity	O
quartile	O
(	O
approximately	O
335	O
podcasts	O
per	O
group	O
)	O
with	O
bootstrapped	O
Welch	B-MethodName
's	I-MethodName
ttests	I-MethodName
.	O

We	O
report	O
the	O
group	O
mean	O
differences	O
of	B-MethodName
LDA	I-MethodName
topic	O
proportions	O
in	O
order	O
to	O
contextualize	O
results	O
on	O
the	O
other	O
features	O
.	O

For	O
LDA	B-MethodName
features	O
,	O
we	O
note	O
significance	O
after	O
a	O
Bonferroni	B-MetricName
correction	I-MetricName
of	I-MetricName
α	I-MetricName
=	O
0.05/100	B-MetricValue
,	O
and	O
for	O
the	O
other	O
linguistic	O
features	O
,	O
a	O
Bonferroni	B-HyperparameterName
correction	I-HyperparameterName
of	I-HyperparameterName
α	I-HyperparameterName
=	O
0.05/30.In	B-HyperparameterValue
the	O
results	O
,	O
'	O
description	O
'	O
refers	O
to	O
the	O
concatenation	O
of	O
the	O
show	O
description	O
and	O
the	O
representative	O
episode	O
's	O
description	O
.	O

Similarly	O
,	O
swearing	O
is	O
associated	O
with	O
low	O
engagement	O
.	O

Filler	O
words	O
are	O
only	O
negatively	O
associated	O
with	O
engagement	O
in	O
the	O
lowest	O
popularity	O
quartile	O
,	O
though	O
the	O
lack	O
of	O
correlation	O
in	O
other	O
quartiles	O
could	O
be	O
because	O
the	B-MethodName
LDA	I-MethodName
topics	O
representing	O
fillers	O
do	O
n't	O
model	O
context	O
,	O
and	O
therefore	O
do	O
not	O
capture	O
their	O
discourse	O
function	O
in	O
the	O
way	O
the	O
tagger	O
does	O
for	O
interjections	O
.	O

Next	O
,	O
we	O
build	O
classifiers	O
to	O
automatically	O
distinguish	O
high	O
and	O
low	O
engagement	O
podcasts	O
.	O

We	O
make	O
a	O
single	O
dataset	O
for	O
podcasts	O
across	O
all	O
quartiles	O
by	O
aggregating	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
by	O
stream	B-MetricName
rate	I-MetricName
within	O
each	O
quartile	O
.	O

This	O
aggregation	O
is	O
to	O
ensure	O
fair	O
comparisons	O
of	O
podcasts	O
in	O
different	O
quartiles	O
,	O
since	O
a	O
stream	B-MetricName
rate	I-MetricName
value	O
that	O
is	O
considered	O
high	O
for	O
a	O
popular	O
podcast	O
,	O
for	O
example	O
,	O
may	O
not	O
be	O
so	O
in	O
the	O
low	O
quartiles	O
.	O

We	O
train	O
logistic	B-MethodName
regression	I-MethodName
classifiers	O
using	O
different	O
representations	O
of	O
the	O
content	O
:	O
the	O
linguistic	O
features	O
listed	O
previously	O
,	O
the	O
non	O
-	O
stylistic	O
LDA	B-MethodName
topic	O
proportions	O
,	O
and	O
bag	B-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
ngrams	I-MethodName
(	O
unigram	O
and	O
bigram	O
words	O
)	O
with	O
TF	B-MethodName
-	I-MethodName
IDF	I-MethodName
scoring	I-MethodName
.	O

In	O
addition	O
,	O
we	O
train	O
two	O
neural	O
classifiers	O
-a	O
feedforward	B-MethodName
neural	I-MethodName
network	I-MethodName
with	I-MethodName
a	I-MethodName
single	I-MethodName
hidden	I-MethodName
layer	I-MethodName
,	O
using	O
a	O
paragraph	O
vector	O
representation	O
(	O
Le	O
and	O
Mikolov	O
,	O
2014	O
)	O
of	O
the	O
document	O
as	O
input	O
8	O
,	O
and	O
the	O
pre	O
-	O
trained	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Prediction	O
accuracies	B-MetricName
(	O
Table	O
4	O
)	O
are	O
over	O
70	B-MetricValue
%	I-MetricValue
with	O
linguistic	O
features	O
only	O
,	O
indicating	O
that	O
the	O
features	O
that	O
we	O
have	O
identified	O
are	O
relatively	O
strong	O
predictors	O
of	O
engagement	O
.	O

Analysis	O
of	O
the	O
weights	O
of	O
the	O
bag	B-MethodName
of	I-MethodName
n	I-MethodName
-	I-MethodName
grams	I-MethodName
models	O
surface	O
patterns	O
in	O
language	O
usage	O
that	O
corroborate	O
our	O
analysis	O
on	O
linguistic	O
features	O
-swearing	O
and	O
negative	O
sentiment	O
is	O
predictive	O
of	O
low	O
engagement	O
,	O
for	O
example	O
.	O

The	O
BERT	B-MethodName
classifiers	O
achieve	O
nearly	O
81	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
,	O
indicating	O
that	O
podcast	O
content	O
is	O
highly	O
predictive	O
of	O
engagement	O
.	O

Table	O
6	O
shows	O
how	O
classification	O
accuracies	O
change	O
when	O
the	O
task	O
is	O
to	O
distinguish	O
the	O
top	O
and	O
bottom	O
K%	O
podcasts	O
,	O
with	O
K	B-HyperparameterName
ranging	O
from	O
10	B-HyperparameterValue
to	I-HyperparameterValue
50	I-HyperparameterValue
(	O
all	O
reports	O
thus	O
far	O
have	O
been	O
with	O
K	B-HyperparameterName
=	O
25	B-HyperparameterValue
)	O
.	O

Performance	O
drops	O
as	O
K	B-HyperparameterName
increases	O
(	O
and	O
the	O
gap	O
between	O
the	O
two	O
sets	O
thereby	O
decreases	O
)	O
although	O
the	O
amount	O
of	O
training	O
data	O
goes	O
up	O
,	O
showing	O
that	O
the	O
differences	O
in	O
language	O
usage	O
are	O
more	O
predictable	O
at	O
the	O
extremes	O
of	O
engagement	O
.	O

2018;Acharyya	O
et	O
al	O
.	O
,	O

While	O
we	O
do	O
n't	O
have	O
access	O
to	O
the	O
stream	B-MetricName
rate	I-MetricName
of	O
the	O
lectures	O
,	O
the	O
data	O
includes	O
the	O
total	O
view	O
count	O
and	O
ratings	O
.	O

We	O
test	O
the	O
same	O
features	O
that	O
we	O
formulated	O
for	O
podcasts	O
,	O
except	O
for	B-MethodName
LDA	I-MethodName
topic	O
distributions	O
(	O
due	O
to	O
the	O
small	O
size	O
of	O
the	O
TED	B-DatasetName
corpus	I-DatasetName
relative	O
to	O
the	O
full	O
100k+	O
podcast	O
data	O
)	O
,	O
and	O
ads	O
and	O
swear	O
words	O
since	O
these	O
occur	O
rarely	O
if	O
at	O
all	O
in	O
TED	O
talks	O
.	O

On	O
the	O
prediction	O
task	O
,	O
we	O
achieve	O
up	O
to	O
71.15	B-MetricValue
%	I-MetricValue
(	O
Table	O
8)	O
accuracy	B-MetricName
using	O
only	O
linguistic	O
features	O
,	O
similar	O
to	O
the	O
performance	O
on	O
podcasts	O
.	O

However	O
,	O
the	O
bag	B-MethodName
-	I-MethodName
of	I-MethodName
-	I-MethodName
ngrams	I-MethodName
features	O
are	O
less	O
predictive	O
than	O
linguistic	O
features	O
,	O
and	O
the	O
BERT	B-MethodName
model	O
only	O
matches	O
the	O
classifier	O
with	O
linguistic	O
features	O
rather	O
than	O
exceeding	O
it	O
.	O

Our	O
paper	O
centers	O
five	O
minute	O
stream	B-MetricName
rate	I-MetricName
as	O
the	O
target	O
metric	O
for	O
analysis	O
and	O
prediction	O
.	O

2018).Aggregate	O
stream	B-MetricName
rate	I-MetricName
in	O
podcasts	O
is	O
a	O
specific	O
engagement	O
metric	O
distinct	O
from	O
metrics	O
and	O
media	O
in	O
previous	O
studies	O
.	O

2020	O
)	O
find	O
that	O
algorithms	O
driven	O
by	O
engagement	O
lead	O
to	O
less	O
diverse	O
recommendations	O
;	O
however	O
,	O
that	O
work	O
does	O
not	O
study	O
the	O
relationship	O
between	O
the	O
type	O
of	O
content	O
that	O
is	O
favored	O
by	O
the	O
engagement	O
metric	O
.	O

While	O
a	O
comprehensive	O
analysis	O
of	O
podcast	O
engagement	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
work	O
,	O
we	O
manually	O
examine	O
the	O
top	O
10	O
%	O
of	O
podcast	O
episodes	O
by	O
engagement	O
in	O
our	O
collection	O
,	O
a	O
total	O
of	O
537	O
episodes	O
.	O

As	O
we	O
noted	O
in	O
§	O
5.1.1	O
,	O
the	O
LDA	B-MethodName
topics	O
associated	O
with	O
high	O
engagement	O
are	O
broad	O
:	O
lifestyle	O
,	O
mental	O
health	O
,	O
spirituality	O
,	O
crime	O
,	O
investing	O
,	O
working	O
out	O
,	O
careers	O
,	O
business	O
,	O
parenting	O
,	O
health	O
,	O
art	O
,	O
and	O
relationships	O
.	O

Our	O
predictive	O
models	O
perform	O
well	O
at	O
distinguishing	O
high	O
and	O
low	O
engagement	O
podcasts	O
using	O
only	O
textual	O
information	O
.	O

Our	O
comparison	O
with	O
a	O
similar	O
task	O
on	O
TED	B-DatasetName
data	O
shows	O
similarities	O
and	O
differences	O
between	O
podcasts	O
and	O
public	O
lectures	O
vis	O
a	O
vis	O
engagement	O
.	O

As	O
with	O
all	O
user	O
data	O
,	O
the	O
engagement	O
metric	O
is	O
influenced	O
by	O
the	O
interface	O
and	O
recommendations	O
of	O
the	O
streaming	O
platform	O
from	O
which	O
the	O
data	O
was	O
collected	O
,	O
and	O
may	O
not	O
translate	O
to	O
other	O
platforms	O
,	O
nor	O
reflect	O
an	O
objective	O
notion	O
of	O
listener	O
engagement	O
.	O

Automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
is	O
interesting	O
,	O
owing	O
to	O
the	O
various	O
linguistic	O
peculiarities	O
present	O
in	O
the	O
language	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
the	O
first	O
large	O
scale	O
study	O
of	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
(	O
ASR	B-TaskName
)	O
in	O
Sanskrit	O
,	O
with	O
an	O
emphasis	O
on	O
the	O
impact	O
of	O
unit	O
selection	O
in	O
Sanskrit	O
ASR	B-TaskName
.	O

In	O
this	O
work	O
,	O
we	O
release	O
a	O
78	O
hour	O
ASR	B-TaskName
dataset	O
for	O
Sanskrit	O
,	O
which	O
faithfully	O
captures	O
several	O
of	O
the	O
linguistic	O
characteristics	O
expressed	O
by	O
the	O
language	O
.	O

We	O
investigate	O
the	O
role	O
of	O
different	O
acoustic	O
model	O
and	O
language	O
model	O
units	O
in	O
ASR	B-TaskName
systems	O
for	O
Sanskrit	O
.	O

We	O
also	O
propose	O
a	O
new	O
modelling	O
unit	O
,	O
inspired	O
by	O
the	O
syllable	O
level	O
unit	O
selection	O
,	O
that	O
captures	O
character	O
sequences	O
from	O
one	O
vowel	O
in	O
the	O
word	O
to	O
the	O
next	O
vowel	O
.	O

We	O
also	O
highlight	O
the	O
importance	O
of	O
choosing	O
graphemic	O
representations	O
for	O
Sanskrit	O
and	O
show	O
the	O
impact	O
of	O
this	O
choice	O
on	O
word	B-MetricName
error	I-MetricName
rates	I-MetricName
(	O
WER	B-MetricName
)	O
.	O

Finally	O
,	O
we	O
extend	O
these	O
insights	O
from	O
Sanskrit	O
ASR	B-TaskName
for	O
building	O
ASR	B-TaskName
systems	O
in	O
two	O
other	O
Indic	O
languages	O
,	O
Gujarati	O
and	O
Telugu	O
.	O

For	O
both	O
these	O
languages	O
,	O
our	O
experimental	O
results	O
show	O
that	O
the	O
use	O
of	O
phonetic	O
based	O
graphemic	O
representations	O
in	O
ASR	B-TaskName
results	O
in	O
performance	O
improvements	O
as	O
compared	O
to	O
ASR	B-TaskName
systems	O
that	O
use	O
native	O
scripts	O
.	O

Phonemic	O
orthography	O
is	O
beneficial	O
for	O
a	O
language	O
,	O
when	O
it	O
comes	O
to	O
designing	O
automatic	B-TaskName
speech	I-TaskName
recognition	I-TaskName
Systems	O
(	O
ASR	B-TaskName
)	O
,	O
specifically	O
for	O
unit	O
selection	O
at	O
both	O
the	O
Acoustic	O
Model	O
(	O
AM	O
)	O
and	O
Language	O
Model	O
(	O
LM	O
)	O
levels	O
.	O

Regardless	O
of	O
the	O
aforementioned	O
commonalities	O
preserved	O
in	O
both	O
the	O
speech	O
and	O
text	O
in	O
Sanskrit	O
,	O
designing	O
a	O
large	O
scale	O
ASR	B-TaskName
system	O
raises	O
several	O
challenges	O
.	O

The	O
language	O
is	O
lexically	O
productive	O
,	O
which	O
results	O
in	O
long	O
compound	O
words	O
with	O
multiple	O
components	O
in	O
usage	O
.	O

This	O
makes	O
the	O
ASR	B-TaskName
task	O
further	O
challenging	O
,	O
as	O
the	O
speakers	O
are	O
prone	O
to	O
carry	O
their	O
influence	O
from	O
their	O
corresponding	O
mother	O
tongues	O
into	O
the	O
Sanskrit	O
utterances	O
as	O
well	O
.	O

2010;Kulkarni	O
et	O
al	O
.	O
,	O

2012;Kulkarni	O
et	O
al	O
.	O
,	O

2021	O
)	O
,	O
large	O
scale	O
systems	O
for	O
processing	O
of	O
speech	O
in	O
Sanskrit	O
,	O
are	O
almost	O
non	O
-	O
existent	O
.	O

First	O
,	O
we	O
present	O
a	O
new	O
dataset	O
,	O
with	O
78	O
hours	O
of	O
speech	O
covering	O
about	O
46,000	O
sentences	O
,	O
for	O
ASR	B-TaskName
in	O
Sanskrit	O
.	O

Using	O
this	O
dataset	O
,	O
we	O
propose	O
a	O
new	O
,	O
largevocabulary	O
Sanskrit	O
ASR	B-TaskName
system	O
,	O
which	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
is	O
the	O
first	O
such	O
system	O
for	O
Sanskrit	O
.	O

native	O
script	O
(	O
Devanagari	O
)	O
and	O
SLP1.Finally	O
,	O
we	O
extend	O
our	O
insights	O
to	O
model	O
ASR	B-TaskName
systems	O
for	O
two	O
more	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

We	O
report	O
the	O
performance	O
of	O
these	O
ASR	B-TaskName
systems	O
on	O
two	O
publicly	O
available	O
ASR	B-TaskName
datasets	O
.	O

Our	O
main	O
contributions	O
in	O
this	O
work	O
are	O
:	O
1	O
)	O
We	O
present	O
(	O
in	O
Section	O
2	O
)	O
a	O
new	O
,	O
large	O
vocabulary	O
Sanskrit	O
ASR	B-TaskName
system	O
and	O
the	O
first	O
ever	O
ASRbased	B-TaskName
study	O
for	O
Sanskrit	O
using	O
a	O
new	O
,	O
large	O
and	O
diverse	O
,	O
labeled	O
speech	O
corpus	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañ	O
cayah	O
̣/	O
)	O
.	O

2	O
)	O
We	O
investigate	O
(	O
in	O
Sections	O
3	O
and	O
4	O
)	O
different	O
modeling	O
choices	O
for	O
both	O
acoustic	O
models	O
and	O
language	O
models	O
in	O
Sanskrit	O
ASR	B-TaskName
systems	O
,	O
along	O
with	O
different	O
graphemic	O
representations	O
.	O

3	O
)	O
We	O
also	O
contextualize	O
our	O
findings	O
for	O
Sanskrit	O
by	O
providing	O
comparisons	O
on	O
ASR	B-TaskName
systems	O
built	O
for	O
two	O
other	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

The	O
recordings	O
were	O
primarily	O
collected	O
with	O
the	O
help	O
of	O
volunteers	O
,	O
recording	O
their	O
speech	O
by	O
using	O
the	O
Recorder	O
app	O
on	O
Android	O
phones	O
and	O
the	O
Audacity	O
platform	O
,	O
and	O
from	O
various	O
sources	O
available	O
online	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
various	O
linguistic	O
phenomena	O
that	O
are	O
important	O
to	O
consider	O
when	O
preparing	O
datasets	O
and	O
building	O
ASR	B-TaskName
systems	O
for	O
Sanskrit	O
with	O
the	O
help	O
of	O
illustrative	O
examples	O
.	O

However	O
,	O
this	O
makes	O
it	O
challenging	O
for	O
an	O
ASR	B-TaskName
system	O
.	O

The	O
Unicode	O
encoding	O
for	O
the	O
native	O
scripts	O
in	O
Sanskrit	O
,	O
similar	O
to	O
several	O
indian	O
languages	O
,	O
does	O
not	O
preserve	O
the	O
correspondence	O
with	O
the	O
phonemic	O
encoding	O
.	O

However	O
,	O
Sandhi	O
splitting	O
can	O
change	O
some	O
phonemes	O
corresponding	O
to	O
the	O
words	O
in	O
almost	O
all	O
cases	O
.	O

This	O
leads	O
to	O
a	O
mismatch	O
between	O
the	O
speech	O
transcript	O
and	O
the	O
speech	O
audio	O
,	O
potentially	O
creating	O
further	O
complications	O
for	O
ASR	B-TaskName
.	O

We	O
consider	O
the	O
benefits	O
of	O
using	O
BPE	O
as	O
a	O
subword	O
unit	O
for	O
Sanskrit	O
ASR.While	B-TaskName
BPE	O
is	O
a	O
purely	O
data	O
-	O
driven	O
segmentation	O
strategy	O
,	O
we	O
next	O
present	O
a	O
linguistically	O
motivated	O
segmentation	O
approach	O
that	O
might	O
be	O
aligned	O
with	O
finding	O
syllable	O
units	O
for	O
ASR	B-TaskName
that	O
are	O
more	O
phonetically	O
compliant	O
.	O

We	O
propose	O
segmenting	O
words	O
at	O
vowel	O
boundaries	O
to	O
extract	O
the	O
units	O
for	O
which	O
alignment	O
with	O
speech	O
is	O
learnt	O
within	O
the	O
ASR	B-TaskName
system	O
.	O

For	O
acoustic	O
models	O
,	O
an	O
effective	O
unit	O
of	O
a	O
word	O
for	O
ASR	B-TaskName
would	O
arguably	O
be	O
the	O
syllable	O
(	O
Lee	O
et	O
al	O
.	O
,	O

To	O
create	O
syllable	O
units	O
,	O
phonemes	O
are	O
then	O
combined	O
together	O
based	O
on	O
the	O
sonority	O
sequencing	O
principle	O
(	O
Clements	O
,	O
1990	O
)	O
.	O

native	O
script	O
and	O
SLP1	O
)	O
described	O
in	O
Section	O
3.1	O
,	O
we	O
study	O
three	O
different	O
units	O
for	O
the	O
acoustic	O
modeling	O
(	O
AM	O
)	O
in	O
ASR	B-TaskName
,	O
viz	O
.	O
,	O

Whereas	O
,	O
for	O
language	O
modeling	O
(	O
LM	O
)	O
,	O
we	O
study	O
word	O
,	O
BPE	O
and	O
VS	O
based	O
units	O
.	O

In	O
Figure	O
3	O
,	O
we	O
report	O
the	O
vocabulary	O
size	O
based	O
on	O
each	O
of	O
these	O
three	O
different	O
unit	O
selections	O
and	O
contrast	O
the	O
sizes	O
with	O
that	O
of	O
two	O
extreme	O
hypothetical	O
systems	O
-one	O
that	O
considers	O
the	O
entire	O
word	O
as	O
a	O
single	O
unit	O
for	O
AM	O
and	O
the	O
other	O
that	O
treats	O
the	O
phoneme	O
as	O
a	O
single	O
unit	O
for	O
AM	O
.	O

Description	O
of	O
Datasets	O
:	O
In	O
addition	O
to	O
reporting	O
ASR	B-TaskName
results	O
on	O
the	O
carefully	O
created	O
वाक्	O
सञ्चयः	O
(	O
/Vāksañcayah	O
̣/	O
)	O
dataset	O
(	O
described	O
in	O
Section	O
2	O
)	O
,	O
we	O
also	O
contrast	O
through	O
experimental	O
analysis	O
on	O
two	O
other	O
Indian	O
languages	O
,	O
viz	O
.	O
,	O

Corpora	O
in	O
these	O
two	O
languages	O
were	O
accompanied	O
by	O
pronunciation	O
lexicons	O
,	O
which	O
we	O
used	O
to	O
build	O
phoneme	O
-	O
based	O
ASR	B-TaskName
systems	O
to	O
compare	O
against	O
our	O
grapheme	O
-	O
based	O
systems	O
.	O

2011	O
)	O
for	O
all	O
our	O
ASR	B-TaskName
experiments	O
.	O

Our	O
acoustic	O
model	O
is	O
implemented	O
using	O
Time	B-MethodName
Delay	I-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
TDNNs	B-MethodName
)	O
(	O
Peddinti	O
et	O
al	O
.	O
,	O

2015	O
)	O
containing	O
14	B-HyperparameterValue
layers	B-HyperparameterName
.	O

We	O
use	O
40dimensional	O
MFCCs	O
as	O
our	O
input	O
features	O
along	O
with	O
100	B-HyperparameterValue
-	I-HyperparameterValue
dimensional	I-HyperparameterValue
i	B-HyperparameterName
-	I-HyperparameterName
vector	I-HyperparameterName
based	I-HyperparameterName
speaker	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Saon	O
et	O
al	O
.	O
,	O

The	O
language	O
models	O
were	O
trained	O
using	O
both	O
training	O
transcripts	O
from	O
the	O
speech	O
data	O
,	O
as	O
well	O
as	O
additional	O
textual	O
data	O
derived	O
from	O
the	O
Leipzig	B-DatasetName
Corpora	I-DatasetName
Collection	I-DatasetName
for	O
Gujarati	O
and	O
Telugu	O
(	O
Goldhahn	O
et	O
al	O
.	O
,	O

2012	O
)	O
and	O
the	O
Digital	B-DatasetName
Corpus	I-DatasetName
of	I-DatasetName
Sanskrit	I-DatasetName
(	O
Hellwig	O
,	O
2010	O
)	O
for	O
Sanskrit	O
.	O

The	O
word	O
vocabulary	O
sizes	O
in	O
the	O
lexicons	O
for	O
Sanskrit	O
,	O
Telugu	O
and	O
Gujarati	O
are	O
76	O
K	O
,	O
43	O
K	O
and	O
48	O
K	O
,	O
respectively	O
.	O

Results	O
:	O
Tables	O
3	O
,	O
4	O
and	O
5	O
,	O
present	O
the	O
WERs	B-MetricName
from	O
ASR	B-TaskName
systems	O
built	O
using	O
different	O
choices	O
of	O
AM	O
and	O
LM	O
units	O
using	O
both	O
the	O
graphemic	O
representations	O
(	O
Native	O
and	O
SLP1	O
)	O
for	O
Sanskrit	O
,	O
Gujarati	O
and	O
Telugu	O
,	O
respectively	O
.	O

Gujarati	O
and	O
Telugu	O
have	O
lower	O
OOV	O
rates	O
of	O
18.63	O
%	O
and	O
15.26	O
%	O
.Table	O
6	O
shows	O
the	O
distribution	O
of	O
words	O
with	O
1	O
-	O
4	O
continuous	O
consonants	O
in	O
all	O
three	O
languages	O
.	O

In	O
the	O
training	O
dataset	O
used	O
in	O
the	O
Sanskrit	O
ASR	B-TaskName
experiments	O
with	O
the	O
vocab	O
size	O
of	O
70.5	O
K	O
,	O
more	O
than	O
87.25	O
%	O
words	O
have	O
a	O
frequency	O
less	O
than	O
3	O
,	O
where	O
as	O
in	O
Telugu	O
and	O
Gujarati	O
training	O
dataset	O
,	O
this	O
is	O
76.76	O
%	O
and	O
77.26	O
%	O
,	O
respectively	O
.	O

We	O
also	O
find	O
that	O
ASR	B-TaskName
performance	O
using	O
phonemes	O
is	O
comparable	O
to	O
graphemes	O
for	O
Gujarati	O
and	O
Telugu	O
.	O

With	O
the	O
consistent	O
mapping	O
between	O
graphemes	O
and	O
phonemes	O
and	O
the	O
absence	O
of	O
schwa	O
deletion	O
,	O
it	O
is	O
intuitive	O
that	O
grapheme	O
-	O
based	O
models	O
would	O
be	O
most	O
appropriate	O
for	O
Sanskrit	O
.	O

In	O
Sanskrit	O
the	O
pause	O
given	O
between	O
the	O
subwords	O
of	O
a	O
compound	O
word	O
and	O
in	O
between	O
two	O
words	O
varies	O
depending	O
on	O
the	O
fluency	O
of	O
the	O
speaker	O
and	O
the	O
complexity	O
of	O
the	O
text	O
,	O
which	O
can	O
deteriorate	O
the	O
WER	B-MetricName
.	O

After	O
negating	O
these	O
two	O
particular	O
errors	O
,	O
we	O
will	O
get	O
17.79	B-MetricValue
%	I-MetricValue
as	O
the	O
modulo	B-MetricName
substitution	I-MetricName
deletion	I-MetricName
WER	I-MetricName
for	O
our	O
best	O
model	O
of	O
Sanskrit	O
(	O
Sr	O
.	O

The	O
character	B-MetricName
error	I-MetricName
rate	I-MetricName
3.10	B-MetricValue
%	I-MetricValue
for	O
the	O
best	O
model	O
in	O
Sanskrit	O
also	O
ensures	O
the	O
performance	O
of	O
the	O
model	O
and	O
the	O
quality	O
of	O
the	O
dataset	O
,	O
where	O
as	O
the	O
CER	B-MetricName
for	O
the	O
best	O
model	O
of	O
Gujarati	O
and	O
Telugu	O
are	O
5.49	B-MetricValue
%	I-MetricValue
and	O
5.60	B-MetricValue
%	I-MetricValue
respectively	O
,	O
much	O
higher	O
than	O
Sanskrit	O
.	O

It	O
shows	O
the	O
WERs	B-MetricName
we	O
can	O
expect	O
from	O
our	O
models	O
when	O
the	O
speakers	O
and	O
content	O
largely	O
vary	O
in	O
domain	O
from	O
our	O
dataset	O
.	O

These	O
test	O
utterances	O
were	O
evaluated	O
using	O
our	O
best	O
performing	O
Sanskrit	O
ASR	B-TaskName
models	O
.	O

We	O
would	O
like	O
to	O
thank	O
Prof.	O
K.	O
Ramasubramanian	O
,	O
IIT	O
Bombay	O
,	O
for	O
supporting	O
the	O
creation	O
of	O
Sanskrit	O
speech	O
corpus	O
.	O

A	O
Differences	O
between	O
Sanskrit	O
and	O
other	O
Indic	O
languages	O
for	O
ASR	B-TaskName
Many	O
Indian	O
languages	O
are	O
known	O
to	O
be	O
derived	O
from	O
Sanskrit	O
(	O
Kulkarni	O
et	O
al	O
.	O
,	O

This	O
phenomenon	O
is	O
not	O
observed	O
in	O
the	O
South	O
Indian	O
languages	O
.	O

For	O
example	O
,	O
the	O
word	O
'	O
गलती	O
'	O
(	O
/galtī/	O
meaning	O
mistake	O
)	O
in	O
Hindi	O
observes	O
implicit	O
schwa	O
deletion	O
after	O
the	O
consonant	O
'	O
ल'(/la/	O
)	O
.	O

ASR	B-TaskName
becomes	O
challenging	O
because	O
of	O
this	O
phenomenon	O
since	O
the	O
occurrence	O
of	O
schwa	O
deletion	O
is	O
not	O
always	O
explicitly	O
specified	O
in	O
the	O
orthography	O
.	O

While	O
constructing	O
phonetic	O
representations	O
for	O
ASR	B-TaskName
,	O
such	O
deletions	O
introduce	O
ambiguities	O
in	O
pronunciation	O
which	O
could	O
be	O
alleviated	O
by	O
enforcing	O
more	O
consistency	O
between	O
graphemes	O
and	O
phonemes	O
.	O

In	O
contrast	O
,	O
in	O
the	O
case	O
of	O
Sanskrit	O
,	O
since	O
pronunciation	O
is	O
strictly	O
governed	O
by	O
the	O
शक्षा(/śiks	O
̣ā/	O
)	O
(	O
Manomohan	O
and	O
Pān	O
̣ini	O
,	O
1938	O
)	O
,	O
a	O
treatise	O
on	O
phonetics	O
,	O
schwa	O
deletion	O
is	O
not	O
observed	O
.	O

Therefore	O
we	O
experimented	O
varying	O
number	O
of	O
subword	O
unit	O
with	O
vocabulary	O
sizes	O
of	O
2	O
K	O
,	O
4	O
K	O
,	O
8	O
K	O
,	O
16	O
K	O
,	O
32	O
K	O
and	O
64	O
K	O
(	O
K=1000	O
)	O
.	O

Even	O
in	O
this	O
configuration	O
,	O
BPE	O
outperforms	O
VS	O
,	O
as	O
BPE	O
reports	O
a	O
WER	B-MetricName
of	O
21.94	B-MetricValue
as	O

To	O
address	O
the	O
gaps	O
at	O
both	O
ends	O
of	O
the	O
spectrum	O
,	O
we	O
propose	O
MERGEDISTILL	B-MethodName
,	O
a	O
framework	O
to	O
merge	O
pre	O
-	O
trained	O
LMs	O
in	O
a	O
way	O
that	O
can	O
best	O
leverage	O
their	O
assets	O
with	O
minimal	O
dependencies	O
,	O
using	O
task	O
-	O
agnostic	O
knowledge	O
distillation	O
.	O

2020;Artetxe	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
pre	O
-	O
training	O
data	O
volume	O
(	O
Liu	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
an	O
ability	O
to	O
handle	O
codemixed	O
text	O
(	O
Pires	O
et	O
al	O
.	O
,	O

While	O
with	O
a	O
BERT	B-MethodName
-	O
compatible	O
tokenization	O
tokens	O
will	O
appear	O
twice	O
,	O
once	O
with	O
"	O
Al-	O
"	O
and	O
once	O
without	O
it	O
,	O
AraBERT	B-MethodName
first	O
segments	O
the	O
words	O
using	O
Farasa	O
(	O
Abdelali	O
et	O
al	O
.	O
,	O

Figure	O
2	O
:	O
Overview	O
of	O
MERGEDISTILL	B-MethodName
:	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pretraining	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
effort	O
of	O
its	O
kind	O
,	O
and	O
makes	O
the	O
following	O
contributions:•	O
We	O
propose	O
MERGEDISTILL	B-MethodName
,	O
a	O
task	O
-	O
agnostic	O
distillation	O
approach	O
to	O
merge	O
multiple	O
teacher	O
LMs	O
at	O
the	O
pre	O
-	O
training	O
stage	O
,	O
to	O
train	O
a	O
strong	O
multilingual	O
student	O
LM	O
that	O
can	O
then	O
be	O
finetuned	O
for	O
any	O
task	O
on	O
all	O
languages	O
in	O
the	O
student	O
LM	O
.	O

Our	O
approach	O
is	O
more	O
maintainable	O
(	O
fewer	O
models	O
)	O
,	O
compute	O
efficient	O
and	O
teacherarchitecture	O
agnostic	O
(	O
since	O
we	O
obtain	O
offline	O
predictions).•	O
We	O
use	O
MERGEDISTILL	B-MethodName
to	O
i	O
)	O
combine	O
monolingual	O
teacher	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
that	O
is	O
competitive	O
with	O
or	O
outperforms	O
individual	O
teachers	O
,	O
ii	O
)	O
combine	O
multilingual	O
teacher	O
LMs	O
,	O
such	O
that	O
the	O
overlapping	O
languages	O
can	O
learn	O
from	O
multiple	O
teachers.•	O
Through	O
extensive	O
experiments	O
and	O
analysis	O
,	O
we	O
study	O
the	O
importance	O
of	O
typological	O
similarity	O
in	O
building	O
multilingual	O
models	O
,	O
and	O
the	O
impact	O
of	O
strong	O
teacher	O
LM	O
vocabularies	O
and	O
predictions	O
in	O
our	O
framework	O
.	O

2021	O
)	O
,	O
trained	O
on	O
massive	O
amounts	O
of	O
multilingual	O
data	O
,	O
have	O
surpassed	O
cross	O
-	O
lingual	O
word	O
embedding	O
spaces	O
(	O
Glavaš	O
et	O
al	O
.	O
,	O

2019	O
)	O
;	O
Wu	O
and	O
Dredze	O
(	O
2019	O
)	O
highlight	O
their	O
cross	O
-	O
lingual	O
ability	O
,	O
several	O
limitations	O
have	O
been	O
studied	O
.	O

2020	O
)	O
highlight	O
that	O
even	O
the	O
best	O
multilingual	O
models	O
do	O
not	O
yield	O
satisfactory	O
transfer	O
performance	O
on	O
the	O
XTREME	B-DatasetName
bechmark	O
covering	O
9	O
tasks	O
and	O
40	O
languages	O
.	O

A	O
few	O
examples	O
of	O
these	O
are	O
AraBERT	B-MethodName
(	O
Antoun	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
CamemBERT	B-MethodName
(	O
Martin	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
and	O
FinBERT	B-MethodName
(	O
Virtanen	O
et	O
al	O
.	O
,	O

2020	O
)	O
maintain	O
an	O
ever	O
-	O
growing	O
list	O
of	O
BERT	B-MethodName
models	O
here	O
most	O
commonly	O
been	O
used	O
for	O
task	O
-	O
specific	O
model	O
compression	O
of	O
a	O
teacher	O
into	O
a	O
single	O
-	O
task	O
student	O
(	O
Tang	O
et	O
al	O
.	O
,	O

In	O
the	O
context	O
of	O
neural	O
machine	O
translation	O
,	O
Tan	O
et	O
al	O
.	O
(	O

Notations	O
:	O
Let	O
K	O
denote	O
the	O
set	O
of	O
languages	O
we	O
train	O
our	O
student	O
LM	O
on	O
and	O
T	O
denote	O
the	O
set	O
of	O
teacher	O
LMs	O
input	O
to	O
MERGEDISTILL	B-MethodName
3	O
.	O

Consequently	O
,	O
T	O
k	O
denotes	O
the	O
set	O
of	O
teacher	O
LMs	O
trained	O
on	O
language	O
k	O
,	O
where|T	O
k	O
|	O
≥	O
1	O
∀	O
k	O
∈	O
K.	O
An	O
overview	O
of	O
MERGEDISTILL	B-MethodName
is	O
presented	O
in	O
Figure	O
2	O
.	O

Step	O
1	O
:	O
Input	O
The	O
input	O
to	O
MERGEDISTILL	B-MethodName
is	O
a	O
set	O
of	O
pre	O
-	O
trained	O
teacher	O
LMs	O
and	O
pre	O
-	O
training	O
transfer	O
corpora	O
for	O
all	O
the	O
languages	O
we	O
wish	O
to	O
train	O
our	O
student	O
LM	O
on	O
.	O

Hence	O
,	O
we	O
first	O
store	O
the	O
top	O
-	O
k	B-HyperparameterName
logits	O
for	O
each	O
masked	O
word	O
offline	O
,	O
loading	O
and	O
normalizing	O
them	O
during	O
student	O
LM	O
training	O
,	O
similar	O
to	O
(	O
Tan	O
et	O
al	O
.	O
,	O

This	O
converts	O
each	O
teacher	O
token	O
index	O
to	O
its	O
corresponding	O
student	O
token	O
index	O
,	O
ready	O
for	O
consumption	O
by	O
the	O
student	O
model	O
.	O

Step	O
L	O
MLM	O
(	O
x	O
m	O
|x	O
−m	O
)	O
=	O
−	O
1	O
n	O
n	O
i=1	O
|v|	O
j=1	O
P(x	O
m	O
i	O
,	O
v	O
j	O
)	O
In	O
addition	O
to	O
learning	O
from	O
gold	O
labels	O
,	O
we	O
use	O
teacher	O
predictions	O
as	O
soft	O
labels	O
and	O
minimize	O
the	O
cross	O
entropy	O
between	O
student	O
and	O
teacher	O
distributions	O
.	O

In	O
this	O
section	O
,	O
we	O
aim	O
to	O
answer	O
the	O
following	O
questions	O
:	O
Distillation	O
Parameters	O
:	O
We	O
have	O
two	O
hyperparameter	O
choices	O
here	O
:	O
1	O
)	O
k	B-HyperparameterName
in	O
top	O
-	O
k	B-HyperparameterName
logits	O
-as	O
it	O
increases	O
,	O
we	O
observe	O
that	O
while	O
performances	O
remain	O
similar	O
,	O
storing	O
k>8	B-HyperparameterName
number	O
of	O
predictions	O
for	O
each	O
masked	O
word	O
offline	O
significantly	O
increases	O
resource	O
requirements	O
4	O
.	O

Hence	O
,	O
we	O
set	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

2	O
)	O
the	O
value	O
of	O
λ	B-HyperparameterName
in	O
the	O
loss	O
function	O
,	O
which	O
decides	O
the	O
proportion	O
of	O
teacher	O
loss	O
,	O
is	O
annealed	O
through	O
training	O
similar	O
to	O
Clark	O
et	O
al	O
.	O
(	O

2019).Evaluation	O
Metrics	O
:	O
We	O
report	O
F1	B-MetricName
scores	O
for	O
structured	O
prediction	O
tasks	O
(	O
NER	O
,	O
POS	O
)	O
,	O
accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O

scores	O
for	O
sentence	O
classification	O
tasks	O
(	O
XNLI	O
,	O
PAWS	O
-	O
X	O
)	O
,	O
and	O
F1	B-MetricName
/	O
Exact	B-MetricName
Match	I-MetricName
(	O
F1	B-MetricName
/	O
EM	B-MetricName
)	O
scores	O
for	O
question	O
answering	O
tasks	O
(	O
XQuAD	O
,	O
MLQA	O
,	O
TyDiQA	O
)	O
.	O

We	O
also	O
report	O
a	O
task	O
-	O
specific	O
relative	B-MetricName
deviation	I-MetricName
from	O
teachers	O
(	O
RDT	B-MetricName
)	O
(	O
in	O
%	O
)	O
averaged	O
across	O
all	O
languages	O
(	O
n	O
)	O
.	O

For	O
each	O
task	O
,	O
RDT	B-MetricName
is	O
calculated	O
as	O
:	O
RDT(S	B-MetricName
,	O
{	O
T	O
1	O
,	O
...	O
,	O
T	O
n	O
}	O
)	O
=	O
100	O
n	O
n	O
i=1	O
(	O
P	O
T	O
i	O
−	O
P	O
S	O
)	O
P	O
T	O
i(3)where	O
P	O
T	O
i	O
and	O
P	O
S	O
are	O
performances	O
of	O
the	O
i	O
th	O
teacher	O
and	O
student	O
LMs	O
,	O
respectively	O
.	O

In	O
this	O
setup,|T	O
k	O
|	O
=	O
1	O
∀	O
k	O
∈	O
K	O
,	O
i.e.	O
,	O
each	O
language	O
can	O
learn	O
from	O
its	O
respective	O
monolingual	O
teacher	O
LM	O
only	O
.	O

Our	O
teacher	O
selection	O
and	O
setup	O
follows	O
a	O
two	O
-	O
step	O
process	O
.	O

This	O
makes	O
us	O
choose	O
teacher	O
LMs	O
for	O
:	O
Arabic	O
(	O
ar	O
)	O
,	O
Chinese	O
(	O
zh	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Finnish	O
(	O
fi	O
)	O
,	O
Table	O
4	O
:	O
Results	O
for	O
multilingual	O
teacher	O
and	O
student	O
LMs	O
on	O
the	O
XTREME	B-MetricName
benchmark	O
.	O

We	O
compare	O
performances	O
of	O
three	O
student	O
LM	O
variants	O
as	O
described	O
in	O
Section	O
4.3	O
to	O
the	O
two	O
teachers	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

Relative	B-MetricName
deviations	I-MetricName
of	O
5	B-MetricValue
%	I-MetricValue
or	O
less	O
from	O
teacher	O
(	O
i.e.	O
,	O
RDT	B-MetricName
≥	O
−5	B-MetricValue
%	I-MetricValue
)	O
are	O
marked	O
in	O
bold	O
.	O

Overall	O
,	O
we	O
find	O
that	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
among	O
all	O
student	O
variants	O
and	O
report	O
its	O
RDT	B-MetricName
(	O
in	O
%	O
)	O
(	O
Equation	O
3	O
)	O
from	O
the	O
two	O
teachers	O
.	O

This	O
encouraging	O
result	O
proves	O
that	O
even	O
with	O
very	O
limited	O
data	O
,	O
MERGEDISTILL	B-MethodName
enables	O
one	O
to	O
combine	O
strong	O
monolingual	O
teacher	O
LMs	O
to	O
train	O
competitive	O
student	O
LMs	O
that	O
can	O
leverage	O
the	O
benefits	O
of	O
multilinguality	O
.	O

Pre	O
-	O
training	O
:	O
In	O
this	O
experiment	O
,	O
we	O
make	O
use	O
of	O
pre	O
-	O
existing	O
multilingual	O
models	O
:	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
.	O

mBERT	B-MethodName
is	O
trained	O
on	O
104	O
languages	O
and	O
MuRIL	B-MethodName
covers	O
12	O
of	O
these	O
(	O
11	O
Indian	O
languages	O
+	O
English	O
):	O
Bengali	O
(	O
bn	O
)	O
,	O
English	O
(	O
en	O
)	O
,	O
Gujarati	O
(	O
gu	O
)	O
,	O
Hindi	O
(	O
hi	O
)	O
,	O
Kannada	O
(	O
kn	O
)	O
,	O
Malayalam	O
(	O
ml	O
)	O
,	O
Marathi	O
(	O
mr	O
)	O
,	O
Nepali	O
(	O
ne	O
)	O
,	O
Punjabi	O
(	O
pa	O
)	O
,	O
Tamil	O
(	O
ta	O
)	O
,	O
Telugu	O
(	O
te	O
)	O
,	O
and	O
Urdu	O
(	O
ur	O
)	O
,	O
with	O
higher	O
performance	O
for	O
these	O
languages	O
on	O
the	O
XTREME	B-DatasetName
benchmark	O
.	O

In	O
this	O
case	O
,	O
the	O
MuRIL	B-MethodName
Languages	O
(	O
MuL	O
)	O
have	O
two	O
as	O
shown	O
in	O
Table	O
3	O
teachers	O
(	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
)	O
and	O
the	O
Non	O
-	O
MuRIL	B-MethodName
Languages	O
(	O
Non	O
-	O
MuL	O
)	O
can	O
learn	O
from	O
mBERT	B-MethodName
only	O
.	O

Therefore	O
,	O
while	O
we	O
only	O
use	O
mBERT	B-MethodName
as	O
the	O
teacher	O
LM	O
for	O
Non	O
-	O
MuL	O
across	O
all	O
experiments	O
,	O
we	O
consider	O
three	O
possibilities	O
for	O
MuL	O
:	O
•	O
Student	O
Both	O
all	O
:	O
Tokenize	O
each	O
input	O
example	O
using	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
separately	O
and	O
include	O
both	O
copies	O
in	O
training.•	O
Student	O
Both	O
best	O
:	O
Tokenize	O
each	O
input	O
example	O
using	O
mBERT	B-MethodName
and	O
MuRIL	B-MethodName
separately	O
and	O
include	O
only	O
the	O
best	O
copy	O
in	O
training	O
.	O

All	O
the	O
student	O
LMs	O
use	O
a	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
architecture	O
and	O
have	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
288,973	B-HyperparameterValue
.	O

We	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512.Finetuning	B-HyperparameterValue
:	O
We	O
report	O
zero	O
-	O
shot	O
performance	O
for	O
all	O
languages	O
in	O
the	O
XTREME	B-DatasetName
(	O
Hu	O
et	O
al	O
.	O
,	O

Overall	O
,	O
we	O
find	O
that	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
among	O
all	O
student	O
variants	O
.	O

For	O
Non	O
-	O
MuL	O
,	O
Student	O
MuRIL	B-MethodName
beats	O
the	O
teacher	O
(	O
mBERT	B-MethodName
)	O
by	O
an	O
average	B-MetricName
relative	I-MetricName
score	I-MetricName
of	O
3.8	B-MetricValue
%	I-MetricValue
.	O

For	O
MuL	O
,	O
Student	O
MuRIL	B-MethodName
beats	O
one	O
teacher	O
(	O
mBERT	B-MethodName
)	O
by	O
8.8	B-MetricValue
%	I-MetricValue
,	O
but	O
underperforms	O
the	O
other	O
teacher	O
(	O
MuRIL	B-MethodName
)	O
by	O
3.8	B-MetricValue
%	I-MetricValue
.	O

MuRIL	B-MethodName
is	O
trained	O
on	O
monolingual	O
and	O
parallel	O
data	O
9	O
while	O
the	O
student	O
LMs	O
only	O
see	O
∼22	O
%	O
of	O
unique	O
tokens	O
in	O
comparison	O
.	O

MuRIL	B-MethodName
also	O
has	O
different	O
language	O
sampling	O
strategies	O
(	O
α	B-HyperparameterName
=	O
0.3	B-HyperparameterValue
as	O
opposed	O
to	O
0.7	B-HyperparameterValue
in	O
our	O
setting	O
,	O
where	O
a	O
lower	O
α	B-HyperparameterName
value	O
upsamples	O
more	O
rigorously	O
from	O
the	O
tail	O
languages	O
)	O
,	O
which	O
have	O
a	O
significant	O
role	O
to	O
play	O
in	O
multilingual	O
model	O
performances	O
(	O
Conneau	O
et	O
al	O
.	O
,	O

We	O
also	O
observe	O
a	O
significant	O
drop	O
in	O
Student	O
mBERT	B-MethodName
's	O
performance	O
for	O
MuL	O
when	O
compared	O
to	O
the	O
other	O
student	O
LM	O
variants	O
.	O

This	O
might	O
be	O
because	O
the	O
input	O
is	O
tokenized	O
using	O
the	O
mBERT	B-MethodName
tokenizer	O
which	O
prevents	O
learning	O
from	O
MuRIL	B-MethodName
tokens	O
in	O
the	O
student	O
vocabulary	O
.	O

In	O
our	O
case	O
,	O
we	O
do	O
n't	O
observe	O
much	O
of	O
a	O
difference	O
in	O
incorporating	O
mBERT	B-MethodName
predictions	O
for	O
MuL.8	O
More	O
details	O
in	O
Appendix	O
A.3	O
9	O
More	O
details	O
in	O
Appendix	O
A.2	O
The	O
importance	O
of	O
vocabulary	O
and	O
teacher	O
LM	O
preditions	O
:	O
In	O
Furthermore	O
,	O
we	O
also	O
observe	O
that	O
SM2	B-MethodName
and	O
SM3	B-MethodName
achieve	O
competitive	O
performances	O
despite	O
SM3	B-MethodName
being	O
additionally	O
trained	O
on	O
teacher	O
LM	O
labels	O
.	O

In	O
our	O
case	O
,	O
we	O
hypothesize	O
that	O
training	O
on	O
500,000	B-HyperparameterValue
steps	B-HyperparameterName
exposes	O
the	O
model	O
to	O
sufficient	O
data	O
for	O
it	O
to	O
generalize	O
well	O
enough	O
and	O
mask	O
the	O
benefits	O
of	O
teacher	O
LM	O
predictions	O
.	O

To	O
validate	O
this	O
,	O
we	O
evaluate	O
the	O
performances	O
of	O
SM2	B-MethodName
and	O
SM3	B-MethodName
,	O
20	O
%	O
into	O
training	O
(	O
i.e.	O
100,000	O
steps	O
/	O
500,000	O
total	O
steps	O
)	O
as	O
shown	O
in	O
Table	O
5	O
.	O

We	O
observe	O
a	O
∼2.9	B-MetricValue
%	I-MetricValue
gain	B-MetricName
in	O
average	O
performance	O
for	O
SM3	B-MethodName
over	O
SM2	B-MethodName
,	O
clearly	O
highlighting	O
the	O
importance	O
of	O
teacher	O
LM	O
predictions	O
in	O
a	O
limited	O
data	O
scenario	O
.	O

Pre	O
-	O
trained	O
zero	O
-	O
shot	O
transfer	O
:	O
Interestingly	O
,	O
Student	O
MuRIL	B-MethodName
performs	O
the	O
best	O
on	O
almost	O
all	O
tasks	O
for	O
Non	O
-	O
MuL.	O
This	O
hints	O
at	O
positive	O
transfer	O
from	O
strong	O
teachers	O
to	O
languages	O
that	O
the	O
teacher	O
does	O
not	O
cover	O
at	O
all	O
,	O
due	O
to	O
the	O
shared	O
multilingual	O
representations	O
.	O

This	O
would	O
make	O
MERGEDISTILL	B-MethodName
highly	O
beneficial	O
for	O
low	O
-	O
resource	O
languages	O
that	O
do	O
not	O
have	O
a	O
strong	O
teacher	O
or	O
limited	O
gold	O
data	O
.	O

In	O
this	O
paper	O
we	O
address	O
the	O
problem	O
of	O
merging	O
multiple	O
pre	O
-	O
trained	O
teacher	O
LMs	O
into	O
a	O
single	O
multilingual	O
student	O
LM	O
by	O
proposing	O
MERGEDIS	B-MethodName
-	I-MethodName
TILL	I-MethodName
,	O
a	O
task	O
-	O
agnostic	O
distillation	O
method	O
.	O

The	O
student	O
LM	O
learned	O
by	O
MERGEDISTILL	B-MethodName
may	O
be	O
further	O
fine	O
-	O
tuned	O
for	O
any	O
task	O
across	O
all	O
of	O
the	O
languages	O
covered	O
by	O
the	O
teacher	O
LMs	O
.	O

We	O
use	O
MERGEDISTILL	B-MethodName
to	O
i	O
)	O
combine	O
monolingual	O
teacher	O
LMs	O
into	O
one	O
student	O
multilingual	O
LM	O
which	O
is	O
competitive	O
with	O
the	O
teachers	O
,	O
thereby	O
demonstrating	O
positive	O
crosslingual	O
transfer	O
,	O
and	O
ii	O
)	O
combine	O
multilingual	O
LMs	O
to	O
train	O
student	O
LMs	O
that	O
learn	O
from	O
multiple	O
teachers	O
.	O

Through	O
experiments	O
on	O
multiple	O
benchmark	O
datasets	O
,	O
we	O
show	O
that	O
student	O
LMs	O
learned	O
by	O
MERGEDISTILL	B-MethodName
perform	O
competitively	O
or	O
even	O
outperform	O
teacher	O
LMs	O
trained	O
on	O
orders	O
of	O
magnitude	O
more	O
data	O
.	O

We	O
also	O
find	O
that	O
MERGEDIS	B-MethodName
-	I-MethodName
TILL	I-MethodName
enables	O
positive	O
transfer	O
from	O
strong	O
teachers	O
to	O
languages	O
not	O
covered	O
by	O
them	O
(	O
i.e.	O
zero	O
-	O
shot	O
transfer	O
)	O
.	O

We	O
pre	O
-	O
train	O
our	O
student	O
models	O
using	O
the	O
BERT	B-MethodName
base	I-MethodName
architecture	O
.	O

Student	O
similar	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
99112	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
162	O
M	O
parameters	O
.	O

Student	O
different	O
has	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
180996	B-HyperparameterValue
and	O
a	O
model	O
size	O
of	O
225	O
M	O
parameters	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
250k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

We	O
pre	O
-	O
train	O
our	O
student	O
models	O
using	O
the	O
BERT	B-MethodName
base	I-MethodName
architecture	O
.	O

All	O
student	O
LMs	O
have	O
a	O
vocabulary	B-HyperparameterName
size	I-HyperparameterName
of	O
288973	B-HyperparameterName
.	O

Hence	O
,	O
we	O
reduce	O
our	O
embedding	B-HyperparameterName
dimension	I-HyperparameterName
to	O
256	B-HyperparameterValue
as	O
opposed	O
to	O
768	B-HyperparameterValue
to	O
bring	O
down	O
the	O
model	O
size	O
to	O
be	O
around	O
160	O
M	O
,	O
comparable	O
to	O
mBERT	B-MethodName
(	O
178	O
M	O
)	O
.	O

We	O
keep	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
4096	B-HyperparameterValue
and	O
train	O
for	O
500k	B-HyperparameterValue
steps	B-HyperparameterName
with	O
a	O
maximum	B-HyperparameterName
sequence	I-HyperparameterName
length	I-HyperparameterName
of	O
512	B-HyperparameterValue
.	O

We	O
use	O
TPUs	O
,	O
and	O
it	O
takes	O
around	O
3	O
days	O
to	O
pre	O
-	O
train	O
each	O
student	O
LM.We	O
present	O
pre	O
-	O
training	O
data	O
statistics	O
for	O
MuRIL	B-MethodName
and	O
the	O
student	O
LMs	O
in	O
Table	O
6	O
.	O

Here	O
we	O
only	O
include	O
the	O
monolingual	O
data	O
statistics	O
,	O
but	O
MuRIL	B-MethodName
is	O
additionally	O
trained	O
on	O
parallel	O
translated	O
and	O
transliterated	O
data	O
.	O

We	O
report	O
results	O
on	O
the	O
best	O
-	O
performing	O
checkpoint	O
for	O
the	O
We	O
present	O
results	O
for	O
Student	O
MuRIL	B-MethodName
trained	O
with	O
different	O
top	O
-	O
k	B-HyperparameterName
values	O
from	O
teacher	O
predictions	O
in	O
Table	O
8	O
.	O

Hence	O
,	O
we	O
stick	O
to	O
a	O
value	O
of	O
k=8	B-HyperparameterName
in	O
all	O
our	O
experiments	O
.	O

The	O
dataset	O
was	O
annotated	O
using	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
a	O
form	O
of	O
comparative	O
annotation	O
that	O
has	O
been	O
shown	O
to	O
alleviate	O
known	O
biases	O
of	O
using	O
rating	O
scales	O
.	O

Automated	B-TaskName
offensive	I-TaskName
language	I-TaskName
detection	I-TaskName
has	O
thus	O
been	O
gaining	O
interest	O
in	O
the	O
NLP	O
community	O
,	O
as	O
a	O
promising	O
direction	O
to	O
better	O
understand	O
the	O
nature	O
and	O
spread	O
of	O
such	O
content	O
.	O

There	O
are	O
several	O
challenges	O
in	O
the	O
automatic	B-TaskName
detection	I-TaskName
of	I-TaskName
offensive	I-TaskName
language	I-TaskName
(	O
Wiedemann	O
et	O
al	O
.	O
,	O

Hovy	O
(	O
2016	O
)	O
classified	O
comments	O
as	O
racist	O
,	O
sexist	O
,	O
neither	O
;	O
as	O
hate	O
-	O
speech	O
,	O
offensive	O
but	O
not	O
hate	O
-	O
speech	O
,	O
neither	O
offensive	O
nor	O
hate	O
-	O
speech	O
and	O
Founta	O
et	O
al	O
.	O
(	O

However	O
,	O
these	O
categories	O
have	O
significant	O
overlaps	O
with	O
each	O
other	O
,	O
creating	O
ill	O
-	O
defined	O
boundaries	O
,	O
thus	O
introducing	O
ambiguity	O
and	O
annotation	O
inconsistency	O
(	O
Founta	O
et	O
al	O
.	O
,	O

2020;Soral	O
et	O
al	O
.	O
,	O

The	O
most	O
common	O
strategy	O
used	O
is	O
key	B-MethodName
-	I-MethodName
word	I-MethodName
based	I-MethodName
sampling	I-MethodName
.	O

This	O
results	O
in	O
datasets	O
that	O
are	O
rich	O
in	O
explicit	O
offensive	O
language	O
(	O
language	O
that	O
is	O
unambiguous	O
in	O
its	O
potential	O
to	O
be	O
offensive	O
,	O
such	O
as	O
those	O
using	O
slurs	O
or	O
swear	O
words	O
(	O
Waseem	O
et	O
al	O
.	O
,	O

2017;Wiegand	O
et	O
al	O
.	O
,	O

key	B-MethodName
-	I-MethodName
word	I-MethodName
based	I-MethodName
sampling	I-MethodName
often	O
results	O
in	O
spurious	O
correlations	O
(	O
e.g.	O
,	O
sports	O
-	O
related	O
expressions	O
such	O
as	O
announcer	O
and	O
sport	O
occur	O
very	O
frequently	O
in	O
offensive	O
tweets	O
)	O
.	O

Thus	O
,	O
we	O
annotate	O
our	O
dataset	O
using	O
an	O
efficient	O
form	O
of	O
comparative	O
annotation	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
(	I-MethodName
BWS	I-MethodName
)	I-MethodName
(	O
Louviere	O
,	O
1991;Louviere	O
et	O
al	O
.	O
,	O

By	O
obtaining	O
real	O
-	O
valued	O
offensiveness	O
scores	O
,	O
different	O
thresholds	O
can	O
be	O
used	O
in	O
downstream	O
applications	O
to	O
handle	O
varying	O
degrees	O
of	O
offensiveness	O
appropriately	O
.	O

We	O
also	O
greatly	O
mitigate	O
issues	O
of	O
annotator	O
de	O
-	O
sensitization	O
as	O
one	O
will	O
still	O
be	O
able	O
to	O
recognize	O
if	O
one	O
comment	O
is	O
more	O
offensive	O
than	O
another	O
,	O
even	O
if	O
they	O
think	O
both	O
comments	O
are	O
not	O
that	O
offensive	O
.	O

Finally	O
,	O
we	O
benchmark	O
several	O
widely	O
-	O
used	O
neural	O
models	O
in	O
their	O
ability	O
to	O
predict	O
offensiveness	O
scores	O
on	O
this	O
new	O
dataset	O
.	O

Waseem	O
and	O
Hovy	O
(	O
2016	O
)	O
used	O
terms	O
frequently	O
occurring	O
in	O
offensive	O
tweets	O
,	O
while	O
used	O
a	O
list	O
of	O
hate	O
-	O
related	O
terms	O
to	O
extract	O
offensive	O
tweets	O
from	O
the	O
Twitter	O
search	O
API	O
.	O

2018	O
)	O
,	O
Wiegand	O
et	O
al	O
.	O
(	O

2021	O
)	O
.	O

The	O
comments	O
were	O
sampled	O
at	O
random	O
from	O
a	O
large	O
dump	O
of	O
English	O
Wikipedia	O
,	O
and	O
boosted	O
by	O
including	O
comments	O
from	O
blocked	O
users	O
.	O

While	O
these	O
labels	O
were	O
introduced	O
to	O
create	O
a	O
separation	O
between	O
the	O
nature	O
of	O
comments	O
with	O
a	O
score	O
of	O
1.0	O
and	O
those	O
with	O
a	O
score	O
of	O
0.6	O
(	O
which	O
would	O
otherwise	O
be	O
classified	O
as	O
attacks	O
)	O
,	O
they	O
are	O
discrete	O
.	O

In	O
our	O
work	O
,	O
using	O
the	O
BWS	B-MethodName
comparative	O
annotation	O
setup	O
,	O
we	O
assign	O
fine	O
-	O
grained	O
continuous	O
scores	O
to	O
comments	O
to	O
denote	O
their	O
degree	O
of	O
offensiveness	O
.	O

BWS	B-MethodName
was	O
proposed	O
by	O
Louviere	O
(	O
1991	O
)	O
.	O

Kiritchenko	O
and	O
Mohammad	O
(	O
2017	O
)	O
have	O
experimentally	O
shown	O
that	O
BWS	B-MethodName
produces	O
more	O
reliable	O
finegrained	O
scores	O
than	O
the	O
scores	O
acquired	O
utilizing	O
rating	O
scales	O
.	O

In	O
the	O
BWS	B-MethodName
annotation	O
setup	O
,	O
the	O
annotators	O
are	O
given	O
an	O
n	O
-	O
tuple	O
(	O
where	O
n	O
>	O
1	O
,	O
and	O
commonly	O
n	O
=	O
4	O
)	O
,	O
and	O
asked	O
which	O
item	O
is	O
the	O
best	O
and	O
which	O
is	O
the	O
worst	O
(	O
best	O
and	O
worst	O
correspond	O
to	O
the	O
highest	O
and	O
the	O
lowest	O
with	O
respect	O
to	O
a	O
property	O
of	O
interest	O
)	O
.	O

Within	O
the	O
NLP	O
community	O
,	O
BWS	B-MethodName
has	O
thus	O
far	O
been	O
used	O
only	O
for	O
creating	O
datasets	O
for	O
relational	O
similarity	O
(	O
Jurgens	O
et	O
al	O
.	O
,	O

2012	O
)	O
,	O
word	O
-	O
sense	O
disambiguation	O
(	O
Jurgens	O
,	O
2013	O
)	O
,	O
word	O
-	O
sentiment	O
intensity	O
(	O
Kiritchenko	O
et	O
al	O
.	O
,	O

Using	O
BWS	B-MethodName
,	O
we	O
create	O
the	O
first	O
dataset	O
with	O
degree	O
of	O
offensiveness	O
scores	O
for	O
social	O
media	O
comments	O
.	O

As	O
users	O
can	O
also	O
reply	O
to	O
a	O
comment	O
,	O
the	O
entire	O
discussion	O
has	O
a	O
hierarchical	O
structure	O
called	O
the	O
comment	O
thread	O
.	O

These	O
subreddits	O
were	O
chosen	O
to	O
cover	O
a	O
diverse	O
range	O
of	O
topics	O
.	O

The	O
CMV	O
subreddit	O
(	O
with	O
over	O
a	O
million	O
users	O
)	O
has	O
posts	O
and	O
comments	O
on	O
controversial	O
topics.3	O
.	O

For	O
example	O
,	O
Jay	O
and	O
Janschewitz	O
(	O
2008	O
)	O
show	O
that	O
people	O
tend	O
to	O
swear	O
when	O
they	O
are	O
angry	O
,	O
frustrated	O
or	O
anxious	O
.	O

We	O
followed	O
the	O
procedure	O
described	O
in	O
Kiritchenko	O
and	O
Mohammad	O
(	O
2016	O
)	O
to	O
obtain	O
BWS	B-MethodName
annotations	O
.	O

The	O
BWS	B-MethodName
responses	O
were	O
converted	O
to	O
scores	O
using	O
a	O
simple	O
counting	O
procedure	O
(	O
Orme	O
,	O
2009;Flynn	O
and	O
Marley	O
,	O
2014	O
)	O
.	O

We	O
release	O
the	O
aggregated	O
annotations	O
as	O
well	O
as	O
the	O
individual	O
annotations	O
of	O
Ruddit	B-DatasetName
,	O
to	O
allow	O
further	O
work	O
on	O
examining	O
and	O
understanding	O
the	O
variability	O
.	O

5	O
We	O
can	O
not	O
use	O
standard	O
inter	O
-	O
annotator	O
agreement	O
measures	O
to	O
ascertain	O
the	O
quality	O
of	O
comparative	O
annotations	O
.	O

The	O
disagreement	O
that	O
arises	O
in	O
tuples	O
having	O
two	O
items	O
that	O
are	O
close	O
together	O
in	O
their	O
degree	O
of	O
offensiveness	O
is	O
a	O
useful	O
signal	O
for	O
BWS	B-MethodName
(	O
helping	O
it	O
give	O
similar	O
scores	O
to	O
the	O
two	O
items	O
)	O
.	O

To	O
assess	O
this	O
reproducibility	O
,	O
we	O
computed	O
average	O
split	B-MetricName
-	I-MetricName
half	I-MetricName
reliability	I-MetricName
(	I-MetricName
SHR	I-MetricName
)	I-MetricName
values	O
over	O
100	O
trials	O
.	O

SHR	B-MetricName
is	O
a	O
commonly	O
used	O
approach	O
to	O
determine	O
consistency	O
in	O
psychological	O
studies	O
.	O

For	O
computing	O
SHR	B-MetricName
values	O
,	O
the	O
annotations	O
for	O
each	O
4	O
-	O
tuple	O
were	O
randomly	O
split	O
in	O
two	O
halves	O
.	O

Table	O
1	O
shows	O
the	O
SHR	B-MetricName
for	O
our	O
annotations	O
.	O

SHR	B-MetricName
scores	O
of	O
over	O
0.8	B-MetricValue
indicate	O
substantial	O
reliability	O
.	O

As	O
mentioned	O
earlier	O
,	O
in	O
Ruddit	B-DatasetName
,	O
certain	O
words	O
such	O
as	O
gay	O
,	O
trans	O
,	O
male	O
,	O
female	O
,	O
black	O
,	O
white	O
were	O
found	O
to	O
exhibit	O
a	O
relatively	O
higher	O
association	O
with	O
the	O
offensive	O
bins	O
than	O
with	O
the	O
supportive	O
bins	O
.	O

In	O
order	O
to	O
probe	O
the	O
effect	O
of	O
this	O
on	O
the	O
computational	O
models	O
,	O
we	O
created	O
a	O
variant	O
of	O
Ruddit	B-DatasetName
by	O
replacing	O
all	O
the	O
identity	O
terms	O
(	O
from	O
the	O
list	O
given	O
in	O
Appendix	O
A.4	O
)	O
in	O
the	O
comments	O
with	O
the	O
[	O
group	O
]	O
token	O
and	O
observed	O
the	O
effect	O
on	O
the	O
models	O
'	O
per	O
-	O
formance	O
.	O

We	O
refer	O
to	O
this	O
variant	O
of	O
the	O
dataset	O
as	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

We	O
examined	O
this	O
relationship	O
quantitatively	O
using	O
Ruddit	B-DatasetName
and	O
the	O
NRC	B-DatasetName
VAD	I-DatasetName
Lexicon	I-DatasetName
(	O
which	O
has	O
intensity	O
scores	O
along	O
the	O
valence	O
,	O
arousal	O
,	O
and	O
dominance	O
dimensions	O
)	O
.	O

For	O
each	O
comment	O
in	O
Ruddit	B-DatasetName
,	O
we	O
calculated	O
three	O
scores	O
that	O
captured	O
the	O
intensities	O
of	O
the	O
V	O
,	O
A	O
,	O
D	O
words	O
(	O
the	O
averages	O
of	O
the	O
intensities	O
of	O
the	O
V	O
/	O
A	O
/	O
D	O
words	O
in	O
the	O
comment	O
)	O
,	O
using	O
the	O
entire	O
lexicon	O
.	O

Only	O
comments	O
containing	O
at	O
least	O
4	O
words	O
from	O
the	O
VAD	O
lexicon	O
were	O
considered	O
for	O
the	O
score	O
and	O
correlation	O
calculation	O
.	O

two	O
sources	O
,	O
comments	O
are	O
more	O
prevalent	O
in	O
the	O
supportive	O
bins	O
.	O

The	O
higher	O
representation	O
of	O
comments	O
from	O
Topics	O
than	O
the	O
other	O
two	O
sources	O
in	O
the	O
offensive	O
bins	O
,	O
is	O
likely	O
due	O
to	O
the	O
fact	O
that	O
the	O
Topics	O
category	O
includes	O
subreddits	O
such	O
as	O
worldnews	O
and	O
worldpolitics	O
.	O

To	O
study	O
the	O
impact	O
of	O
comments	O
containing	O
swear	O
words	O
on	O
computational	O
models	O
,	O
we	O
created	O
another	O
variant	O
of	O
Ruddit	B-DatasetName
in	O
which	O
we	O
removed	O
all	O
the	O
comments	O
containing	O
at	O
least	O
one	O
swear	O
word	O
.	O

We	O
refer	O
to	O
this	O
variant	O
as	O
the	O
no	O
-	O
swearing	O
dataset	O
.	O

Thus	O
,	O
we	O
created	O
a	O
subset	O
of	O
Ruddit	B-DatasetName
containing	O
comments	O
with	O
scores	O
from	O
−0.5	O
to	O
0.5	O
.	O

In	O
this	O
section	O
,	O
we	O
present	O
benchmark	O
experiments	O
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
by	O
implementing	O
some	O
commonly	O
used	O
model	O
architectures	O
.	O

We	O
performed	O
5	B-HyperparameterValue
-	O
fold	O
crossvalidation	B-MetricName
for	O
each	O
of	O
the	O
models	O
.	O

6	O
Bidirectional	O
LSTM	O
We	O
fed	O
pre	O
-	O
trained	O
300	B-HyperparameterValue
dimensional	O
GloVe	B-HyperparameterName
word	I-HyperparameterName
embeddings	I-HyperparameterName
(	O
Pennington	O
et	O
al	O
.	O
,	O

2014	O
)	O
to	O
a	O
2	B-HyperparameterValue
-	O
layered	O
BiLSTM	B-MethodName
to	O
obtain	O
a	O
sentence	O
representation	O
(	O
using	O
a	O
concatenation	O
of	O
the	O
last	O
hidden	O
state	O
from	O
the	O
forward	O
and	O
backward	O
direction	O
)	O
.	O

We	O
used	O
Mean	O
Squared	O
Error	O
(	O
MSE	O
)	O
loss	O
as	O
the	O
objective	O
function	O
,	O
Adam	B-HyperparameterValue
with	O
0.001	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
as	O
the	O
optimizer	B-HyperparameterName
,	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
of	O
256	B-HyperparameterValue
,	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
,	O
and	O
a	O
dropout	B-HyperparameterName
of	O
0.5	B-HyperparameterValue
.	O

The	O
model	O
was	O
trained	O
for	O
7	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

Dataset	O
HateBERT	B-MethodName
BERT	B-MethodName
BiLSTM	B-MethodName
r	B-MetricName
MSE	B-MetricName
r	B-MetricName
MSE	B-MetricName
r	B-MetricName
MSEa	B-MetricName
.	O

Ruddit	B-DatasetName
0.886	O
±	O
0.003	O
0.025	O
±	O
0.001	O
0.873	O
±	O
0.005	O
0.027	O
±	O
0.001	O
0.831	O
±	O
0.005	O
0.035	O
±	O
0.001	O
b.	O
Identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
0.883	O
±	O
0.006	O
0.025	O
±	O
0.001	O
0.869	O
±	O
0.007	O
0.027	O
±	O
0.001	O
0.824	O
±	O
0.007	O
0.036	O
±	O
0.001	O
c.	O
No	O
-	O
swearing	O
0.808	O
±	O
0.013	O
0.023	O
±	O
0.001	O
0.783	O
±	O
0.012	O
0.027	O
±	O
0.001	O
0.704	O
±	O
0.014	O
0.036	O
±	O
0.002	O
d.	O
Reduced	O
-	O
range	O
0.781	O
±	O
0.014	O
0.022	O
±	O
0.001	O
0.757	O
±	O
0.011	O
0.025	O
±	O
0.001	O
0.659	O
±	O
0.008	O
0.033	O
±	O
0.001	O
BERT	B-MethodName
We	O
fine	O
-	O
tuned	O
BERT	B-MethodName
base	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

More	O
details	O
in	O
Appendix	O
A.5.)HateBERT	B-MethodName
HateBERT	B-MethodName
(	O
Caselli	O
et	O
al	O
.	O
,	O

2020b	O
)	O
is	O
a	O
version	O
of	O
BERT	B-MethodName
pretrained	O
for	O
abusive	O
language	O
detection	O
in	O
English	O
.	O

HateBERT	B-MethodName
was	O
trained	O
on	O
RAL	O
-	O
E	O
,	O
a	O
large	O
dataset	O
of	O
English	O
language	O
Reddit	O
comments	O
from	O
communities	O
banned	O
for	O
being	O
offensive	O
or	O
hateful	O
.	O

2019).We	O
fine	O
-	O
tuned	O
HateBERT	B-MethodName
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
.	O

The	O
experimental	O
setup	O
for	O
this	O
model	O
is	O
the	O
same	O
as	O
that	O
described	O
for	O
the	O
BERT	B-MethodName
model	O
.	O

We	O
report	O
Pearson	B-MetricName
correlation	I-MetricName
(	I-MetricName
r	I-MetricName
)	I-MetricName
and	O
MSE	B-MetricName
,	O
averaged	O
over	O
all	O
folds	O
.	O

The	O
performance	O
of	O
the	O
models	O
on	O
Ruddit	B-DatasetName
and	O
its	O
variants	O
is	O
shown	O
in	O
the	O
Table	O
5	O
.	O

Note	O
that	O
the	O
performance	O
values	O
on	O
the	O
noswearing	O
and	O
the	O
reduced	O
-	O
range	O
datasets	O
are	O
not	O
directly	O
comparable	O
to	O
the	O
performance	O
values	O
on	O
the	O
full	O
Ruddit	B-DatasetName
as	O
their	O
score	O
range	O
is	O
different	O
.	O

We	O
can	O
see	O
that	O
on	O
all	O
the	O
datasets	O
,	O
the	O
HateBERT	B-MethodName
model	O
performs	O
the	O
best	O
,	O
followed	O
by	O
the	O
BERT	B-MethodName
model	O
.	O

Interestingly	O
,	O
the	O
model	O
performance	O
(	O
for	O
all	O
models	O
)	O
does	O
not	O
change	O
substantially	O
when	O
trained	O
on	O
Ruddit	O
or	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

This	O
indicates	O
that	O
the	O
computational	O
models	O
are	O
not	O
learning	O
to	O
benefit	O
from	O
the	O
association	O
of	O
certain	O
identity	O
terms	O
with	O
a	O
specific	O
range	O
of	O
scores	O
on	O
the	O
offensiveness	O
scale	O
.	O

Yet	O
,	O
the	O
fact	O
that	O
the	O
models	O
still	O
obtain	O
performance	O
of	O
up	O
to	O
0.8	B-MetricValue
(	O
r	B-MetricName
)	O
demonstrates	O
that	O
they	O
necessitate	O
and	O
are	O
able	O
to	O
learn	O
other	O
types	O
of	O
offensiveness	O
features	O
.	O

It	O
is	O
also	O
worth	O
mentioning	O
that	O
even	O
if	O
they	O
encounter	O
swear	O
words	O
in	O
a	O
comment	O
,	O
the	O
task	O
is	O
not	O
simply	O
to	O
label	O
the	O
comment	O
as	O
offensive	O
but	O
to	O
provide	O
a	O
suitable	O
score	O
.	O

Finally	O
,	O
the	O
models	O
obtained	O
the	O
performance	O
of	O
up	O
to	O
0.78	B-MetricValue
(	O
r	B-MetricName
)	O
on	O
the	O
reduced	O
-	O
range	O
dataset	O
,	O
which	O
shows	O
that	O
even	O
if	O
the	O
comments	O
from	O
the	O
extreme	O
ends	O
of	O
the	O
offensiveness	O
scale	O
are	O
removed	O
,	O
Ruddit	O
still	O
presents	O
an	O
interesting	O
and	O
feasible	O
offensiveness	O
scoring	O
task	O
.	O

Error	O
Analysis	O
Figure	O
4	O
shows	O
the	O
squared	O
error	O
values	O
of	O
the	O
3	O
models	O
over	O
the	O
offensiveness	O
score	O
range	O
in	O
Ruddit	B-DatasetName
.	O

It	O
is	O
interesting	O
to	O
observe	O
that	O
HateBERT	B-MethodName
,	O
unlike	O
the	O
other	O
two	O
models	O
,	O
does	O
not	O
have	O
high	O
error	O
values	O
for	O
samples	O
within	O
the	O
score	O
range	O
0.25	O
-	O
0.75	O
.	O

This	O
indicates	O
that	O
HateBERT	B-MethodName
is	O
efficient	O
in	O
dealing	O
with	O
offensive	O
language	O
that	O
does	O
not	O
lie	O
in	O
the	O
extreme	O
offensive	O
end	O
.	O

BiLSTM	B-MethodName
seems	O
relatively	O
less	O
accurate	O
for	O
samples	O
in	O
the	O
supportive	O
range	O
(	O
−0.75	O
to	O
−0.25	O
)	O
.	O

We	O
used	O
a	O
comparative	O
annotation	O
technique	O
called	O
Best	B-MethodName
-	I-MethodName
Worst	I-MethodName
Scaling	I-MethodName
,	O
which	O
addresses	O
the	O
limitations	O
of	O
traditional	O
rating	O
scales	O
.	O

We	O
found	O
that	O
computational	O
models	O
are	O
not	O
benefiting	O
from	O
the	O
association	O
of	O
identity	O
terms	O
with	O
specific	O
range	O
of	O
scores	O
on	O
the	O
offensiveness	O
scale	O
.	O

We	O
create	O
Ruddit	B-DatasetName
to	O
study	O
,	O
understand	O
and	O
explore	O
the	O
nature	O
of	O
offensive	O
language	O
.	O

This	O
includes	O
several	O
aspects	O
such	O
as	O
mitigating	O
harm	O
to	O
people	O
involved	O
,	O
improving	O
data	O
privacy	O
,	O
and	O
informed	O
consent	O
.	O

We	O
release	O
data	O
in	O
a	O
manner	O
that	O
is	O
GDPR	O
compliant	O
.	O

8	O
The	O
researchers	O
using	O
the	O
dataset	O
need	O
to	O
retrieve	O
the	O
data	O
using	O
the	O
Reddit	O
API.Speaker	O
and	O
Annotator	O
Demographic	O
:	O
No	O
specific	O
speaker	O
demographic	O
information	O
is	O
available	O
for	O
the	O
comments	O
included	O
in	O
Ruddit	B-DatasetName
.	O

In	O
this	O
survey	O
,	O
3370	O
workers	O
participated	O
.	O

Finally	O
,	O
Ruddit	B-DatasetName
was	O
created	O
with	O
the	O
intention	O
to	O
look	O
at	O
wide	O
ranging	O
offensive	O
language	O
of	O
various	O
degrees	O
as	O
opposed	O
to	O
detecting	O
offensive	O
language	O
towards	O
specific	O
target	O
groups	O
.	O

Annotation	O
Guidelines	O
:	O
We	O
created	O
our	O
annotation	O
guidelines	O
drawing	O
inspiration	O
from	O
the	O
community	O
standards	O
set	O
for	O
offensive	O
language	O
on	O
several	O
social	O
media	O
platforms	O
.	O

AMT	O
provides	O
a	O
checkbox	O
where	O
requesters	O
can	O
indicate	O
that	O
some	O
content	O
in	O
the	O
task	O
may	O
be	O
offensive	O
.	O

These	O
tasks	O
are	O
not	O
shown	O
to	O
annotators	O
who	O
have	O
specified	O
so	O
in	O
their	O
profile	O
.	O

We	O
call	O
this	O
variant	O
the	O
identity	B-DatasetName
-	I-DatasetName
agnostic	I-DatasetName
dataset	O
.	O

2018	O
)	O
with	O
a	O
few	O
of	O
our	O
own	O
additions	O
.	O

Hyperparameter	O
Tuning	O
We	O
tuned	O
hyperparameters	O
for	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
.	O

All	O
experiments	O
were	O
performed	O
on	O
a	O
fixed	O
seed	O
value	O
of	O
12.For	O
the	O
BiLSTM	B-MethodName
model	O
,	O
the	O
batch	O
size	O
was	O
fixed	O
at	O
32	O
and	O
the	O
number	O
of	O
epochs	O
was	O
set	O
to	O
7	O
.	O

The	O
hyperparameter	O
search	O
space	O
is	O
as	O
follows	O
:	O
For	O
the	O
BERT	B-MethodName
model	O
,	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
was	O
fixed	O
at	O
16	B-HyperparameterValue
and	O
BERT	B-MethodName
tokenizer	B-HyperparameterName
's	I-HyperparameterName
maximum	I-HyperparameterName
length	I-HyperparameterName
was	O
set	O
to	O
200	B-HyperparameterValue
.	O

2019	O
)	O
found	O
to	O
work	O
best	O
on	O
all	O
tasks	O
.	O

The	O
search	O
space	O
is	O
as	O
follows:2714•	O
Learning	B-HyperparameterName
rate	I-HyperparameterName
:	O
2e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
,	O
3e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
,	O
5e	B-HyperparameterValue
−	I-HyperparameterValue
5	I-HyperparameterValue
We	O
reported	O
the	O
best	O
setting	O
for	O
the	O
models	O
in	O
section	O
6.1	O
.	O

The	O
average	B-MetricName
r	I-MetricName
of	O
the	O
BERT	B-MethodName
and	O
the	O
BiLSTM	B-MethodName
models	O
across	O
all	O
hyperparameter	O
search	O
This	O
research	O
was	O
funded	O
by	O
the	O
Facebook	O
Online	O
Safety	O
Benchmark	O
Research	O
award	O
for	O
the	O
project	O
"	O
A	O
Benchmark	O
and	O
Evaluation	O
Framework	O
for	O
Abusive	O
Language	O
Detection	O
.	O
"	O

With	O
the	O
increasing	O
interest	O
in	O
low	O
-	O
resource	O
languages	O
,	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
has	O
become	O
an	O
active	O
area	O
of	O
research	O
,	O
where	O
approaches	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O

We	O
demonstrate	O
the	O
power	O
of	O
harnessing	O
linguistic	O
knowledge	O
as	O
priors	O
within	O
Adaptor	B-MethodName
Grammars	I-MethodName
in	O
a	O
minimally	O
-	O
supervised	O
learning	O
fashion	O
.	O

We	O
introduce	O
two	O
types	O
of	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
language	O
-	O
specific	O
grammars	O
;	O
and	O
2	O
)	O
linguistprovided	B-MethodName
affixes	I-MethodName
,	O
collected	O
by	O
an	O
expert	O
in	O
the	O
language	O
and	O
seeded	O
into	O
the	O
grammars	O
.	O

We	O
use	O
Japanese	O
and	O
Georgian	O
as	O
respective	O
case	O
studies	O
for	O
the	O
two	O
types	O
of	O
priors	O
and	O
introduce	O
new	O
datasets	O
for	O
these	O
languages	O
,	O
with	O
gold	O
morphological	B-TaskName
segmentation	I-TaskName
for	O
evaluation	O
.	O

We	O
show	O
that	O
the	O
use	O
of	O
priors	O
results	O
in	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
and	O
34.2	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
over	O
the	O
equivalent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
system	O
.	O

Morphological	B-TaskName
segmentation	I-TaskName
is	O
an	O
essential	O
subtask	O
in	O
many	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
applications	O
,	O
especially	O
in	O
the	O
case	O
of	O
morphologically	O
complex	O
languages	O
.	O

With	O
the	O
need	O
to	O
develop	O
NLP	O
tools	O
for	O
low	O
-	O
resource	O
languages	O
,	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
has	O
been	O
receiving	O
increasing	O
interest	O
over	O
the	O
last	O
two	O
decades	O
(	O
Goldsmith	O
,	O
2001;Creutz	O
and	O
Lagus	O
,	O
2007a;Poon	O
et	O
al	O
.	O
,	O

2019.In	O
this	O
work	O
,	O
we	O
show	O
how	O
linguistic	O
priors	O
effectively	O
boost	O
morphological	O
-	O
segmentation	O
performance	O
in	O
a	O
minimally	O
-	O
supervised	O
manner	O
that	O
does	O
not	O
require	O
segmented	O
words	O
for	O
training	O
.	O

We	O
integrate	O
our	O
priors	O
within	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
Johnson	O
et	O
al	O
.	O
,	O

2007	O
)	O
,	O
a	O
type	O
of	O
nonparametric	O
Bayesian	O
models	O
that	O
generalize	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammars	I-MethodName
(	O
PCFGs	O
)	O
.	O

Adaptor	B-MethodName
Grammars	I-MethodName
have	O
proved	O
successful	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
,	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
across	O
a	O
variety	O
of	O
typologically	O
diverse	O
languages	O
(	O
Eskander	O
et	O
al	O
.	O
,	O

2020).We	O
introduce	O
two	O
types	O
of	O
linguistic	O
priors	O
:	O
1	O
)	O
grammar	B-MethodName
definition	I-MethodName
,	O
where	O
we	O
design	O
a	O
languagespecific	O
grammar	O
that	O
is	O
tailored	O
for	O
the	O
language	O
of	O
interest	O
by	O
modeling	O
specific	O
morphological	O
phenomena	O
,	O
and	O
2	O
)	O
linguist	B-MethodName
-	I-MethodName
provided	I-MethodName
affixes	I-MethodName
,	O
where	O
an	O
expert	O
in	O
the	O
underlying	O
language	O
compiles	O
a	O
list	O
of	O
carefully	O
selected	O
affixes	O
and	O
seeds	O
it	O
into	O
the	O
grammars	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
.	O

1	O
We	O
utilize	O
MorphAGram	B-MethodName
(	O
Eskander	O
et	O
al	O
.	O
,	O

2020	O
)	O
2	O
,	O
an	O
open	O
-	O
source	O
morphologicalsegmentation	B-TaskName
framework	O
that	O
is	O
based	O
on	O
Adaptor	B-MethodName
Grammars	I-MethodName
(	O
AGs	O
)	O
(	O
Johnson	O
et	O
al	O
.	O
,	O

2020.Adaptor	O
Grammars	O
are	O
non	O
-	O
parametric	O
Bayesian	O
models	O
that	O
are	O
composed	O
of	O
two	O
main	O
components	O
:	O
1	O
)	O
a	B-MethodName
Probabilistic	I-MethodName
Context	I-MethodName
-	I-MethodName
Free	I-MethodName
Grammar	I-MethodName
(	I-MethodName
PCFG	I-MethodName
)	I-MethodName
whose	O
definition	O
relies	O
on	O
the	O
underlying	O
task	O
(	O
in	O
the	O
case	O
of	O
morphological	O
segmentation	O
,	O
a	O
PCFG	O
models	O
word	O
structure	O
)	O
;	O
and	O
2	O
)	O
an	O
adaptor	B-MethodName
that	O
is	O
based	O
on	O
the	O
Pitman	B-MethodName
-	I-MethodName
Yor	I-MethodName
process	I-MethodName
(	O
Pitman	O
,	O
1995	O
)	O
.	O

The	O
adaptor	B-MethodName
keeps	O
the	O
posterior	O
probability	O
of	O
a	O
subtree	O
proportional	O
to	O
the	O
number	O
of	O
times	O
that	O
subtree	O
is	O
utilized	O
to	O
parse	O
the	O
input	O
data	O
and	O
manages	O
the	O
caching	O
of	O
the	O
subtrees	O
.	O

The	O
learning	O
process	O
is	B-MethodName
Markov	I-MethodName
Chain	I-MethodName
Monte	I-MethodName
Carlo	I-MethodName
sampling	I-MethodName
(	I-MethodName
MCMC	I-MethodName
)	I-MethodName
(	O
Andrieu	O
et	O
al	O
.	O
,	O

2016	O
)	O
define	O
a	O
set	O
of	O
languageindependent	O
grammars	O
and	O
three	O
learning	O
settings	O
for	B-MethodName
Adaptor	I-MethodName
Grammars	I-MethodName
:	O
1	O
)	O
Standard	O
,	O
fully	O
unsupervised	O
;	O
2	O
)	O
Scholar	O
-	O
Seeded	O
,	O
minimally	O
-	O
supervised	O
by	O
manually	O
seeding	O
affixes	O
into	O
the	O
grammar	O
prior	O
to	O
training	O
the	O
segmentation	O
model	O
,	O
and	O
3	O
)	O
Cascaded	O
,	O
fully	O
unsupervised	O
by	O
approximating	O
the	O
Scholar	O
-	O
Seeded	O
setting	O
using	O
automatically	O
generated	O
af	O
-	O
fixes	O
from	O
an	O
initial	O
round	O
of	O
learning	O
.	O

We	O
consider	O
their	O
PrStSu+SM	B-MethodName
grammar	I-MethodName
in	O
the	O
current	O
study	O
as	O
it	O
is	O
the	O
grammar	O
that	O
performed	O
best	O
on	O
average	O
across	O
different	O
languages	O
.	O

Similar	O
to	O
the	O
Scholar	O
-	O
Seeded	O
setting	O
,	O
we	O
compile	O
a	O
list	O
of	O
affixes	O
and	O
seed	O
it	O
into	O
the	O
grammar	O
trees	O
before	O
learning	O
the	O
segmentation	O
model	O
.	O

We	O
annotate	O
two	O
datasets	O
with	O
morphological	B-TaskName
segmentation	I-TaskName
that	O
we	O
use	O
as	O
the	O
gold	O
standard	O
to	O
evaluate	O
our	O
segmentation	B-TaskName
models	O
for	O
Japanese	O
and	O
Georgian	O
.	O

For	O
Georgian	O
,	O
which	O
has	O
highly	O
complex	O
morphology	O
,	O
we	O
started	O
with	O
the	O
gold	O
-	O
standard	O
dataset	O
of	O
1000	O
words	O
introduced	O
by	O
Eskander	O
We	O
evaluate	O
our	O
morphological	O
-	O
segmentation	O
models	O
for	O
Japanese	O
in	O
the	O
Standard	O
(	O
STD	O
)	O
and	O
Cascaded	O
(	O
CAS	O
)	O
5	O
settings	O
,	O
both	O
with	O
generic	O
and	O
language	O
-	O
specific	O
(	O
LS	O
)	O
grammar	O
definitions	O
.	O

For	O
Georgian	O
,	O
we	O
evaluate	O
our	O
morphologicalsegmentation	O
models	O
in	O
the	O
Standard	O
(	O
STD	O
)	O
,	O
Cascaded	O
(	O
CAS	O
)	O
and	O
Scholar	O
-	O
Seeded	O
(	O
SS	O
)	O
settings	O
,	O
in	O
addition	O
to	O
the	O
proposed	O
Scholar	O
-	O
Seeded	O
setting	O
with	O
linguist	O
-	O
provided	O
affixes	O
(	O
SS	O
-	O
Ling).We	O
perform	O
the	O
evaluation	O
in	O
a	O
transductive	O
manner	O
,	O
where	O
the	O
unsegmented	O
words	O
in	O
the	O
gold	O
standard	O
are	O
part	O
of	O
the	O
training	O
sets	O
;	O
this	O
is	O
common	O
in	O
evaluating	O
unsupervised	O
and	O
minimally	B-TaskName
-	I-TaskName
supervised	I-TaskName
morphological	I-TaskName
segmentation	I-TaskName
(	O
Poon	O
et	O
al	O
.	O
,	O

BPR	B-MetricName
is	O
the	O
classical	O
metric	O
for	O
evaluating	O
morphological	B-TaskName
segmentation	I-TaskName
;	O
it	O
compares	O
the	O
boundaries	O
in	O
the	O
proposed	O
segmentation	O
to	O
those	O
in	O
the	O
reference	O
.	O

We	O
evaluate	O
our	O
system	O
versus	O
two	O
state	O
-	O
of	O
-	O
theart	O
unsupervised	O
baselines	O
:	O
MorphAGram	B-MethodName
without	O
the	O
use	O
of	O
linguistic	O
priors	O
and	O
Morfessor	B-MethodName
(	O
Virpioja	O
et	O
al	O
.	O
,	O

Morfessor	B-MethodName
is	O
a	O
commonly	O
-	O
used	O
framework	O
for	O
unsupervised	B-TaskName
morphological	I-TaskName
segmentation	I-TaskName
.	O

It	O
is	O
based	O
on	O
an	O
HMM	B-MethodName
model	O
that	O
relies	O
on	O
the	O
Minimum	B-MethodName
Description	I-MethodName
Length	I-MethodName
(	O
MDL	O
)	O
concept	O
for	O
deriving	O
the	O
optimal	O
segmentation	O
(	O
Creutz	O
and	O
Lagus	O
,	O
2007b	O
)	O
.	O

Finally	O
,	O
we	O
report	O
all	O
the	O
Adaptor	B-MethodName
-	I-MethodName
Grammar	I-MethodName
results	O
as	O
the	O
average	O
over	O
three	O
runs	O
of	O
different	O
randomization	O
parameters	O
.	O

Table	O
2	O
reports	O
the	O
overall	O
performance	O
of	O
our	O
models	O
for	O
both	O
Japanese	O
and	O
Georgian	O
,	O
while	O
Table	O
3	O
shows	O
the	O
results	O
per	O
part	O
-	O
of	O
-	O
speech	O
category	O
for	O
Georgian	O
.	O

For	O
Japanese	O
,	O
the	O
use	O
of	O
a	O
language	O
-	O
specific	O
grammar	O
definition	O
improves	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
resulting	O
in	O
BPR	B-MetricName
F1	I-MetricName
-	O
score	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
and	O
7.1	B-MetricValue
%	I-MetricValue
over	O
the	O
generic	O
Standard	O
and	O
Cascaded	O
settings	O
,	O
respectively	O
,	O
and	O
a	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reduction	O
of	B-MetricValue
9.8	I-MetricValue
%	I-MetricValue
over	O
Morfessor	B-MethodName
.	O

For	O
Georgian	O
,	O
the	O
use	O
of	O
linguist	O
-	O
provided	O
seeded	O
affixes	O
improves	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
where	O
the	O
recall	B-MetricName
significantly	O
increases	O
by	O
absolute	O
13.3	B-MetricValue
%	I-MetricValue
over	O
using	O
an	O
affix	O
list	O
of	O
lower	O
quality	O
.	O

In	O
addition	O
,	O
the	O
proposed	O
linguistic	O
priors	O
result	O
in	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reductions	O
of	O
34.2	B-MetricValue
%	I-MetricValue
,	O
30.0	B-MetricValue
%	I-MetricValue
and	O
31.1	B-MetricValue
%	I-MetricValue
over	O
the	O
Standard	O
,	O
Cascaded	O
and	O
regular	O
Scholar	O
-	O
Seeded	O
settings	O
,	O
respectively	O
,	O
and	O
a	O
BPR	B-MetricName
F1	I-MetricName
-	I-MetricName
score	I-MetricName
error	I-MetricName
reduction	O
of	O
53.3	B-MetricValue
%	I-MetricValue
over	O
Morfessor	B-MethodName
.	O

Analysing	O
results	O
per	O
category	O
,	O
verbs	O
and	O
nouns	O
receive	O
the	O
biggest	O
F1	B-MetricName
-	O
score	O
improvements	O
of	O
absolute	O
14.3	B-MetricValue
%	I-MetricValue
and	O
4.9	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
with	O
the	O
use	O
of	O
linguist	O
-	O
provided	O
affixes	O
.	O

A	O
similar	O
pattern	O
of	O
results	O
is	O
found	O
with	O
EMMA-2	B-MethodName
.	O

Finally	O
,	O
all	O
the	O
improvements	O
due	O
to	O
the	O
use	O
of	O
linguistic	O
priors	O
are	O
statistically	O
significant	O
(	O
P	B-HyperparameterName
<	O
0.01	B-HyperparameterValue
)	O
on	O
both	O
metrics	O
.	O

Georgian	B-MethodName
segmentation	I-MethodName
models	I-MethodName
.	O

We	O
discuss	O
the	O
most	O
prominent	O
observations	O
below	O
.	O

Japanese	O
:	O
Both	O
the	O
STD	B-MethodName
and	B-MethodName
STD	I-MethodName
-	I-MethodName
LS	I-MethodName
models	O
perform	O
well	O
on	B-MethodName
prefix	I-MethodName
segmentation	I-MethodName
,	O
achieving	B-MetricName
F1	I-MetricName
-	O
scores	O
of	O
more	O
than	O
90	B-MetricValue
%	I-MetricValue
in	O
the	O
detection	O
of	O
several	O
one	O
-	O
character	O
prefixes	O
,	O
such	O
as	O
お	O
and	O
ご	O
.	O

However	O
,	O
STD	B-MethodName
-	I-MethodName
LS	I-MethodName
outperforms	O
its	O
languageindependent	O
counterpart	O
in	O
the	O
detection	O
of	O
stems	O
,	O
where	O
compounding	O
is	O
explicitly	O
modeled	O
.	O

For	O
instance	O
,	O
STD	B-MethodName
and	O
STD	B-MethodName
-	I-MethodName
LS	I-MethodName
achieve	O
F1	B-MetricName
-	O
scores	O
of	O
15.8	B-MetricValue
%	I-MetricValue
and	O
98.6	B-MetricValue
%	I-MetricValue
,	O
respectively	O
,	O
in	O
the	O
detection	O
of	O
the	O
common	O
stem	O
られ	O
(	O
be	O
)	O
.	O

Georgian	O
:	O
SS	O
-	O
Ling	O
outperforms	O
both	O
STD	B-MethodName
and	O
SS	B-MethodName
at	O
discovering	O
the	O
top	O
most	O
frequent	O
one	O
-	O
letter	O
morphemes	O
,	O
such	O
as	O
i	O
,	O
a	O
,	O
s	O
,	O
e	O
,	O
m	O
,	O
o	O
and	O
v	O
,	O
achieving	O
an	O
average	B-MetricName
F1	I-MetricName
-	O
score	O
of	O
76.0	B-MetricValue
%	I-MetricValue
,	O
compared	O
to	O
57.7	B-MetricValue
%	I-MetricValue
and	O
57.3	B-MetricValue
%	I-MetricValue
by	O
STD	B-MethodName
and	O
SS	B-MethodName
,	O
respectively	O
.	O

In	O
addition	O
,	O
SS	B-MethodName
and	O
STD	B-MethodName
suffer	O
lower	O
precision	B-MetricName
as	O
they	O
tend	O
to	O
oversegment	O
the	O
morphemes	O
represented	O
by	O
a	O
single	O
letter	O
.	O

Similarly	O
,	B-MethodName
SS	I-MethodName
-	I-MethodName
Ling	I-MethodName
can	O
recognize	O
the	O
most	O
frequent	O
two	O
-	O
letter	O
morphemes	O
,	O
namely	O
eb	O
and	O
da	O
,	O
with	O
absolute	O
increases	O
in	O
precision	B-MetricName
of	B-MetricValue
59.0	I-MetricValue
%	I-MetricValue
and	O
62.0	B-MetricValue
%	I-MetricValue
over	O
STD	B-MethodName
and	O
SS	B-MethodName
,	O
respectively	O
;	O
both	O
morphemes	O
are	O
explicitly	O
seeded	O
into	O
the	O
SS	O
-	O
Ling	O
grammar	O
prior	O
to	O
training	O
the	O
model	O
.	O

We	O
proposed	O
two	O
types	O
of	O
linguistic	O
priors	O
for	O
minimally	B-TaskName
-	I-TaskName
supervised	I-TaskName
morphological	I-TaskName
segmentation	I-TaskName
using	O
Adaptor	B-MethodName
Grammars	I-MethodName
.	O

Our	O
approaches	O
result	O
in	O
error	B-MetricName
reductions	O
of	O
8.9	B-MetricValue
%	I-MetricValue
,	O
for	O
Japanese	O
,	O
and	O
34.2	B-MetricValue
%	I-MetricValue
,	O
for	O
Georgian	O
,	O
as	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
system	O
.	O

In	O
this	O
article	O
,	O
we	O
present	O
our	O
methodologies	O
for	O
SemEval-2021	B-DatasetName
Task-4	I-DatasetName
:	O
Reading	B-TaskName
Comprehension	I-TaskName
of	I-TaskName
Abstract	I-TaskName
Meaning	I-TaskName
.	O

There	O
are	O
three	O
sub	O
-	O
tasks	O
within	O
this	O
task	O
:	O
Imperceptibility	B-TaskName
(	O
subtask	O
-	O
I	O
)	O
,	O
Non	B-TaskName
-	I-TaskName
Specificity	I-TaskName
(	O
subtask	O
-	O
II	O
)	O
,	O
and	O
Intersection	B-TaskName
(	O
subtask	O
-	O
III	O
)	O
.	O

We	O
use	O
encoders	B-MethodName
of	I-MethodName
transformers	I-MethodName
-	I-MethodName
based	I-MethodName
models	I-MethodName
pre	O
-	O
trained	O
on	O
the	O
masked	O
language	O
modelling	O
(	O
MLM	O
)	O
task	O
to	O
build	O
our	O
Fill	B-MethodName
-	I-MethodName
in	I-MethodName
-	I-MethodName
the	I-MethodName
-	I-MethodName
blank	I-MethodName
(	I-MethodName
FitB	I-MethodName
)	I-MethodName
models	O
.	O

Moreover	O
,	O
to	O
model	O
imperceptibility	B-MetricValue
,	O
we	O
define	O
certain	O
linguistic	O
features	O
,	O
and	O
to	O
model	O
non	B-MetricValue
-	I-MetricValue
specificity	I-MetricValue
,	O
we	O
leverage	O
information	O
from	O
hypernyms	O
and	O
hyponyms	O
provided	O
by	O
a	O
lexical	O
database	O
.	O

Specifically	O
,	O
for	B-MetricValue
non	I-MetricValue
-	I-MetricValue
specificity	I-MetricValue
,	O
we	O
try	O
out	O
augmentation	O
techniques	O
,	O
and	O
other	O
statistical	O
techniques	O
.	O

We	O
also	O
propose	O
variants	O
,	O
namely	O
Chunk	B-MethodName
Voting	I-MethodName
and	O
Max	B-MethodName
Context	I-MethodName
,	O
to	O
take	O
care	O
of	O
input	O
length	O
restrictions	O
for	O
BERT	B-MethodName
,	O
etc	O
.	O

Additionally	O
,	O
we	O
perform	O
a	O
thorough	O
ablation	O
study	O
,	O
and	O
use	O
Integrated	B-MethodName
Gradients	I-MethodName
to	O
explain	O
our	O
predictions	O
on	O
a	O
few	O
samples	O
.	O

Our	O
best	O
submissions	O
achieve	O
accuracies	B-MetricName
of	O
75.31	B-MetricValue
%	I-MetricValue
and	O
77.84	B-MetricValue
%	I-MetricValue
,	O
on	O
the	O
test	O
sets	O
for	O
subtask	O
-	O
I	O
and	O
subtask	O
-	O
II	O
,	O
respectively	O
.	O

For	O
subtask	O
-	O
III	O
,	O
we	O
achieve	O
accuracies	B-MetricName
of	O
65.64	B-MetricValue
%	I-MetricValue
and	O
62.27	B-MetricValue
%	I-MetricValue
.	O

One	O
of	O
the	O
earlier	O
pretraining	O
tasks	O
was	O
"	O
Masked	O
Language	O
Modelling	O
(	O
MLM	O
)	O
"	O
,	O
one	O
of	O
the	O
two	O
pretraining	O
tasks	O
of	O
the	O
breakthrough	O
model	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

SemEval-2021	B-DatasetName
Task-4	I-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

The	O
novelty	O
in	O
the	O
task	O
lies	O
in	O
its	O
3	O
subtasks	O
:	O
Imperceptibility	B-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
I	I-TaskName
)	I-TaskName
,	I-TaskName
Non	I-TaskName
-	I-TaskName
Specificity	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
II	I-TaskName
)	I-TaskName
,	I-TaskName
and	I-TaskName
Intersection	I-TaskName
(	I-TaskName
subtask	I-TaskName
-	I-TaskName
III	I-TaskName
)	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
using	O
BERT	B-MethodName
and	O
its	O
derivative	O
models	O
such	O
as	O
DistilBERT	B-MethodName
,	O
ALBERT	B-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

Further	O
,	O
we	O
propose	O
2	O
BERT	B-MethodName
variants	O
:	O
(	O
1	O
)	O
BERT	B-MethodName
Voting	I-MethodName
;	O
(	O
2	O
)	O
BERT	B-MethodName
Max	I-MethodName
.	O

Most	O
importantly	O
,	O
we	O
also	O
model	O
the	O
concepts	O
of	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

For	O
imperceptibility	B-TaskName
,	O
we	O
create	O
statistical	O
embeddings	O
using	O
features	O
that	O
have	O
a	O
high	O
correlation	O
with	O
concreteness	O
.	O

For	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
,	O
we	O
propose	O
two	O
approaches	O
:	O
(	O
1	O
)	O
we	O
augment	O
the	O
dataset	O
by	O
replacing	O
some	O
nouns	O
in	O
the	O
article	O
by	O
their	O
hypernyms	O
;	O
and	O
(	O
2	O
)	O
we	O
use	O
the	O
options	O
'	O
hyponyms	O
to	O
decide	O
the	O
most	O
appropriate	O
option	O
.	O

2019	O
)	O
by	O
trying	O
out	O
their	O
various	O
combinations	O
with	O
BERT.In	B-MethodName
Section	O
2	O
,	O
we	O
perform	O
a	O
succinct	O
literature	O
survey	O
.	O

Section	O
3	O
elucidates	O
our	O
approach	O
,	O
including	O
the	O
modelling	O
aspect	O
,	O
the	O
various	O
variants	O
of	O
the	O
base	O
model	O
,	O
and	O
the	O
different	O
ways	O
we	O
model	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

In	O
Section	O
4	O
,	O
we	O
perform	O
an	O
extensive	O
ablation	O
and	O
comparative	O
study	O
.	O

The	O
advent	O
of	O
large	O
-	O
scale	O
question	O
answering	O
systems	O
began	O
with	O
straightforward	O
tasks	O
,	O
like	O
the	O
one	O
introduced	O
by	O
the	O
SimpleQuestions	B-DatasetName
Dataset	O
(	O
Bordes	O
et	O
al	O
.	O
,	O

The	O
purpose	O
of	O
NLP	O
research	O
is	O
to	O
be	O
able	O
to	O
create	O
a	O
generalised	O
model	O
that	O
may	O
answer	O
questions	O
based	O
on	O
any	O
context	O
,	O
thus	O
datasets	O
like	O
the	O
CNN	B-DatasetName
Daily	I-DatasetName
Mail	I-DatasetName
(	O
Hermann	O
et	O
al	O
.	O
,	O

2015	O
)	O
and	O
SQuAD	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

Before	O
transformers	O
,	O
methods	O
consisting	O
of	O
LSTM	B-MethodName
/	I-MethodName
GRUs	I-MethodName
were	O
used	O
to	O
achieve	O
good	O
results	O
on	O
the	O
aforementioned	O
tasks	O
.	O

The	O
CLOTH	B-DatasetName
(	O
Xie	O
et	O
al	O
.	O
,	O

The	O
ReCAM	B-DatasetName
(	O
Zheng	O
et	O
al	O
.	O
,	O

2021	O
)	O
dataset	O
puts	O
a	O
twist	O
to	O
archetypal	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
datasets	O
by	O
providing	O
answer	O
choices	O
that	O
are	O
abstract	O
in	O
some	O
form	O
and	O
which	O
are	O
not	O
available	O
in	O
the	O
passage	O
itself	O
.	O

The	O
models	O
created	O
for	O
the	O
QA	O
task	O
have	O
to	O
take	O
into	O
account	O
semantic	O
relations	O
between	O
the	O
options	O
and	O
the	O
context	O
.	B-MethodName

GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

Cloze	O
-	O
Style	O
QAThe	O
first	O
model	O
we	O
employ	O
follows	O
a	O
cloze	B-MethodName
-	I-MethodName
style	I-MethodName
question	I-MethodName
answering	I-MethodName
approach	O
,	O
in	O
which	O
we	O
use	O
various	O
pretrained	O
transformer	O
models	O
as	O
encoders	O
,	O
followed	O
by	O
a	O
decoder	O
layer	O
,	O
which	O
helps	O
us	O
to	O
select	O
the	O
correct	O
answer	O
.	O

Specifically	O
,	O
we	O
leverage	O
BERT	B-MethodName
along	O
with	O
some	O
of	O
its	O
popular	O
and	O
successful	O
variants	O
such	O
as	O
:	B-MethodName
Dis	I-MethodName
-	I-MethodName
tilBERT	I-MethodName
,	I-MethodName
ALBERT	I-MethodName
,	I-MethodName
and	I-MethodName
RoBERTa	I-MethodName
.	O

Each	O
candidate	O
option	O
is	O
first	O
tokenised	O
using	O
WordPiece	B-MethodName
tokeniser	I-MethodName
(	O
Wu	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
and	O
mapped	O
to	O
the	O
vector	O
in	O
the	O
output	O
vocabulary	O
space	O
.	O

For	O
example	O
,	O
in	O
Word	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
(	O
Fellbaum	O
,	O
1998	O
)	O
,	O
the	O
word	O
"	O
dog	O
"	O
has	O
8	O
senses	O
,	O
while	O
the	O
word	O
"	O
love	O
"	O
has	O
10	O
senses	O
.	O

SentiWordNet	B-MethodName
(	O
Baccianella	O
et	O
al	O
.	O
,	O

2010	O
)	O
,	O
another	O
lexical	O
database	O
like	O
Word	B-DatasetName
-	I-DatasetName
Net	I-DatasetName
,	O
gives	O
scores	O
based	O
on	O
the	O
how	O
positive	O
,	O
negative	O
or	O
objective	O
they	O
are	O
.	O

The	O
large	O
value	O
chosen	O
was	O
the	O
same	O
for	O
all	O
features	O
which	O
are	O
indirectly	O
proportional	O
to	O
concreteness	O
.	O

Towards	O
improving	O
the	O
trained	O
model	O
,	O
we	O
use	O
a	O
method	O
which	O
we	O
term	O
as	O
the	O
Difference	B-MethodName
Method	I-MethodName
.	O

Furthermore	O
,	O
we	O
use	O
a	O
Threshold	B-MethodName
Method	I-MethodName
towards	O
improving	O
the	O
model	O
performance	O
.	O

For	O
each	O
noun	O
,	O
we	O
use	O
the	O
Lesk	B-MethodName
algorithm	I-MethodName
(	O
Lesk	O
,	O
1986	O
)	O
to	O
find	O
the	O
most	O
appropriate	O
sense	O
of	O
the	O
word	O
based	O
on	O
its	O
context	O
.	O

Furthermore	O
,	O
we	O
randomly	O
mask	O
tokens	O
in	O
this	O
dataset	O
and	O
train	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
on	O
this	O
dataset	O
.	O

Firstly	O
,	O
it	O
serves	O
as	O
a	O
sort	O
of	O
domain	B-MethodName
adaptation	I-MethodName
,	O
and	O
secondly	O
,	O
it	O
infuses	O
a	O
sense	O
of	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
in	O
the	O
model	O
.	O

While	O
finetuning	O
BERT	B-MethodName
MLM	O
on	O
the	O
augmented	O
dataset	O
,	O
we	O
freeze	O
two	O
layers	O
,	O
due	O
to	O
time	O
and	O
computational	O
constraints	O
.	O

We	O
replace	O
the	O
normal	O
BERT	B-MethodName
Encoder	O
in	O
our	O
BERT	B-MethodName
FitB	I-MethodName
model	O
with	O
the	O
BERT	B-MethodName
Encoder	I-MethodName
fine	O
-	O
tuned	O
on	O
the	O
augmented	O
dataset	O
.	O

Hyponyms	B-MethodName
Options	I-MethodName
Method	I-MethodName
Here	O
,	O
we	O
use	O
the	O
Difference	B-MethodName
Method	I-MethodName
/	I-MethodName
Threshold	I-MethodName
Method	I-MethodName
.	O

However	O
,	O
"	O
drink	O
"	O
is	O
more	O
non	O
-	O
specific	O
than	O
"	O
beer	O
"	O
.	O

To	O
address	O
the	O
limitations	O
of	O
the	O
vanilla	O
transformer	O
-	O
based	O
models	O
,	O
we	O
attempt	O
multiple	O
modifications	O
to	O
the	O
proposed	O
baseline	O
transformer	O
models	O
,	O
specifically	O
for	O
BERT	B-MethodName
.	O

The	O
major	O
limitation	O
of	O
the	O
pretrained	O
BERT	B-MethodName
model	O
that	O
we	O
'	O
ve	O
used	O
,	O
is	O
the	O
restriction	O
on	O
the	O
length	O
of	O
the	O
tokenised	O
inputs	O
.	O

Only	O
512	B-HyperparameterValue
tokens	B-HyperparameterName
from	O
a	O
sample	O
can	O
be	O
processed	O
by	O
BERT	B-MethodName
in	O
one	O
parse	O
and	O
hence	O
,	O
some	O
articles	O
end	O
up	O
getting	O
truncated	O
and	O
context	O
is	O
lost	O
.	O

We	O
split	O
the	O
article	O
into	O
chunks	O
and	O
pair	O
each	O
chunk	O
with	O
the	O
question	O
such	O
that	O
the	O
length	O
of	O
the	O
tokenised	O
(	O
chunk	O
,	O
question	O
)	O
pair	O
is	O
512	B-HyperparameterValue
.	O

While	O
splitting	O
the	O
article	O
into	O
chunks	O
,	O
we	O
keep	O
a	O
maxoverlap	B-HyperparameterName
stride	I-HyperparameterName
of	O
128	B-HyperparameterValue
so	O
that	O
the	O
context	O
of	O
the	O
previous	O
chunk	O
is	O
not	O
lost	O
.	O

For	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Similarity	I-MethodName
)	I-MethodName
,	O
the	O
weights	O
are	O
calculated	O
as	O
:	O
weight	O
ij	O
=	O
u	O
i	O
.v	O
j	O
||u	O
i	O
||||v	O
j	O
||	O
(	O
1)where	O
u	O
i	O
is	O
the	O
embedding	O
of	O
the	O
question	O
in	O
the	O
i	O
th	O
sample	O
,	O
and	O
v	O
j	O
is	O
the	O
embedding	O
of	O
the	O
j	O
th	O
chunk	O
of	O
the	O
sample	O
's	O
article	O
.	O

To	O
find	O
the	O
embeddings	O
,	O
we	O
extract	O
the	O
[	O
CLS	O
]	O
embedding	O
from	O
a	O
pretrained	O
BERT	B-MethodName
encoder	O
.	O

We	O
call	O
the	O
method	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching).We	I-MethodName
normalise	O
the	O
computed	O
weights	O
:	O
norm	O
weight	O
ij	O
=	O
weight	O
ij	O
n	O
i	O
j=1	O
weight	O
ij	O
(	O
3)where	O
n	O
i	O
is	O
the	O
number	O
of	O
chunks	O
in	O
the	O
i	O
th	O
sample	O
.	O

Max	B-MethodName
Context	I-MethodName
This	O
method	O
is	O
a	O
slight	O
modification	O
of	O
the	O
Voting	B-MethodName
Method	I-MethodName
.	O

We	O
propose	O
a	O
few	O
modifications	O
to	O
the	O
baseline	O
,	O
namely	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
(	O
Dhingra	O
et	O
al	O
.	O
,	O

2017a	O
)	O
provided	O
by	O
the	O
organisers	O
.	O

GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
BERT	I-MethodName
We	O
use	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
on	O
top	O
of	O
BERT	B-MethodName
embeddings	O
.	O

This	O
could	O
lead	O
to	O
potential	O
improvement	O
in	O
performance	O
for	O
subtask	O
-	O
I	O
as	O
BERT	B-MethodName
embeddings	O
are	O
more	O
feature	O
-	O
rich	O
than	O
GloVe	B-MethodName
embeddings	O
.	O

Reader	O
,	O
we	O
came	O
up	O
with	O
an	O
approach	O
that	O
uses	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
across	O
two	O
-	O
BERT	B-MethodName
streams	O
.	O

The	O
first	O
stream	O
takes	O
in	O
the	O
question	O
input	O
,	O
and	O
works	O
like	O
the	O
regular	O
BERT	O
model	O
.	O

Then	O
,	O
to	O
the	O
layer	O
L	O
+	O
1	O
for	O
question	O
stream	O
,	O
Q	O
L	O
is	O
passed	O
as	O
input	O
,	O
while	O
to	O
layer	O
L	O
+	O
1	O
for	O
article	O
stream	O
,	O
GA(Q	O
L	O
,	O
A	O
L	O
)	O
is	O
passed	O
,	O
where	O
GA	O
is	O
the	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
function	O
.	O

This	O
is	O
done	O
for	O
all	O
12	O
layers	O
of	O
BERT	B-MethodName
-	I-MethodName
BASE	I-MethodName
.	O

Finally	O
,	O
on	O
this	O
model	O
,	O
two	O
types	O
of	O
heads	O
are	O
attached	O
-Selection	B-MethodName
and	I-MethodName
Pooling	I-MethodName
(	I-MethodName
similar	I-MethodName
to	I-MethodName
BERT	I-MethodName
FitB	I-MethodName
)	I-MethodName
,	O
and	O
Attention	B-MethodName
Classification	I-MethodName
(	I-MethodName
similar	I-MethodName
to	I-MethodName
GA	I-MethodName
-	I-MethodName
Reader	I-MethodName
)	I-MethodName
.	O

Since	O
this	O
is	O
a	O
major	O
change	O
in	O
the	O
architecture	O
of	O
BERT	B-MethodName
,	O
this	O
model	O
needs	O
a	O
significant	O
amount	O
of	O
pretraining	O
.	O

Answer	O
-	O
Attention	O
Since	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
also	O
attends	O
to	O
the	O
candidate	O
answer	O
embeddings	O
,	O
we	O
also	O
attempt	O
an	O
approach	O
where	O
we	O
pass	O
the	O
options	O
to	O
the	O
BERT	B-MethodName
model	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
Lai	O
et	O
al	O
.	O
(	O

2019	O
)	O
propose	O
a	O
combination	O
of	O
Gated	B-MethodName
-	I-MethodName
Attention	I-MethodName
and	I-MethodName
Self	I-MethodName
-	I-MethodName
Attention	I-MethodName
-Gated	I-MethodName
Self	I-MethodName
-	I-MethodName
Attention	I-MethodName
(	I-MethodName
GSA	I-MethodName
)	I-MethodName
.	O

They	O
show	O
improvements	O
on	O
smaller	O
datasets	O
compared	O
to	O
Compare	B-MethodName
-	I-MethodName
Aggregate	I-MethodName
Approaches	O
.	O

We	O
use	O
two	O
GSA	B-MethodName
layers	O
on	O
top	O
of	O
BERT	B-MethodName
Embeddings	O
,	O
and	O
use	O
the	O
same	O
decoder	O
and	O
selection	O
method	O
as	O
BERT	B-MethodName
FitB.	I-MethodName
In	O
all	O
our	O
experiments	O
,	O
we	O
use	O
the	O
PyTorch	O
implementations	O
of	O
the	O
transformers	O
-	O
based	O
models	O
provided	O
by	O
the	O
HuggingFace	O
.The	O
metric	O
for	O
all	O
the	O
3	O
subtasks	O
is	O
accuracy	B-MetricName
.	O

For	O
subtask	O
-	O
I	O
,	O
to	O
obtain	O
the	O
linguistic	O
features	O
mentioned	O
in	O
3.2	O
,	O
and	O
to	O
obtain	O
the	O
hypernyms	O
and	O
hyponyms	O
for	O
subtask	O
-	O
II	O
,	O
we	O
use	O
the	O
lexical	O
database	O
,	O
WordNet	B-DatasetName
provided	O
by	O
NLTK	O
(	O
Bird	O
and	O
Loper	O
,	O
2004	O
)	O
,	O
a	O
library	O
in	O
Python	O
.	O

The	O
training	O
and	O
the	O
evaluation	O
of	O
systems	O
was	O
on	O
Google	O
Colaboratory	O
's	O
free	O
GPU	O
(	O
NVIDIA	O
K80	O
/	O
P100	O
)	O
.	O

DistilBERT	B-MethodName
took	O
about	O
half	O
an	O
hour	O
for	O
training	O
.	O

For	O
finetuning	O
the	O
BERT	B-MethodName
FitB	I-MethodName
Hypr	I-MethodName
Aug	I-MethodName
Model	O
on	O
the	O
augmented	O
dataset	O
on	O
the	O
MLM	O
task	O
,	O
we	O
use	O
Nvidia	O
-	O
DGX	O
Station	O
with	O
the	O
following	O
specifications	O
:	O
four	O
32	O
GB	O
Tesla	O
V100	O
GPUs	O
,	O
256	O
GB	O
RAM	O
and	O
forty	O
Intel	O
Xeon	O
2.20GHz	O
processors	O
since	O
it	O
is	O
a	O
computationally	O
intensive	O
task	O
.	O

For	O
all	O
our	O
experiments	O
,	O
we	O
use	O
Adam	B-HyperparameterName
Optimiser	I-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2017	O
)	O
and	O
Cross	B-HyperparameterName
Entropy	I-HyperparameterName
Loss	I-HyperparameterName
.	O

For	O
choosing	O
the	O
optimal	O
set	O
of	O
hyperparameters	O
,	O
we	O
run	O
a	O
Grid	B-MethodName
Search	I-MethodName
on	O
our	O
models	O
.	O

We	O
zero	O
in	O
on	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
1e-5	B-MetricValue
.	O

Schedulers	O
such	O
as	O
Linear	B-MetricName
Scheduler	I-MetricName
,	O
Cosine	B-MetricName
Annealing	I-MetricName
Scheduler	I-MetricName
,	O
etc	O
.	O

For	O
the	O
FitB	B-HyperparameterName
models	O
,	O
we	O
keep	O
all	O
the	O
layers	O
unfrozen	O
.	O

Additionally	O
,	O
the	O
maximum	B-HyperparameterName
input	I-HyperparameterName
length	I-HyperparameterName
is	O
kept	O
as	O
512	B-HyperparameterValue
.	O

We	O
train	O
our	O
models	O
for	O
4	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
keeping	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
2	B-HyperparameterValue
.	O

Among	O
the	O
vanilla	O
models	O
,	O
BERT	B-MethodName
FitB	I-MethodName
Large	I-MethodName
performs	O
the	O
best	O
.	O

This	O
is	O
understandable	O
when	O
it	O
comes	O
to	O
DistilBERT	B-MethodName
and	O
ALBERT	B-MethodName
,	O
since	O
these	O
models	O
are	O
pruned	O
and	O
distilled	O
for	O
faster	O
computation	O
.	O

Notably	O
,	O
DistilBERT	B-MethodName
gives	O
comparable	O
performance	O
to	O
BERT	B-MethodName
FitB	I-MethodName
Base	I-MethodName
.	O

A	O
slightly	O
surprising	O
observation	O
was	O
that	O
there	O
is	O
a	O
degradation	O
in	O
accuracy	O
on	O
using	O
RoBERTa	B-MethodName
.	O

This	O
could	O
be	O
because	O
even	O
though	O
it	O
was	O
pretrained	O
more	O
robustly	O
than	O
BERT	B-MethodName
on	O
the	O
MLM	O
task	O
,	O
it	O
was	O
not	O
pretrained	O
on	O
the	O
Next	B-TaskName
Sentence	I-TaskName
Prediction	I-TaskName
Task	I-TaskName
,	O
and	O
hence	O
,	O
might	O
perform	O
worse	O
on	O
Textual	B-TaskName
Entailment	I-TaskName
tasks	I-TaskName
.	O

A	O
peculiar	O
observation	O
is	O
that	O
the	O
large	O
variants	O
of	O
ALBERT	B-MethodName
FitB	I-MethodName
and	O
RoBERTa	B-MethodName
FitB	I-MethodName
models	O
perform	O
worse	O
than	O
their	O
base	O
variants	O
.	O

For	O
subtask	O
-	O
I	O
,	O
in	O
table	O
2	O
,	O
we	O
also	O
demonstrate	O
the	O
results	O
of	O
BERT	B-MethodName
Ensemble	I-MethodName
,	O
in	O
which	O
we	O
ensemble	O
(	O
i.e.	O
,	O
averaging	O
over	O
the	O
predictions	O
)	O
two	O
checkpoints	O
saved	O
during	O
the	O
training	O
process	O
.	O

When	O
it	O
comes	O
to	O
the	O
Difference	B-MethodName
Method	I-MethodName
using	I-MethodName
Linguistic	I-MethodName
Features	I-MethodName
for	O
imperceptibility	B-TaskName
,	O
we	O
observe	O
an	O
improvement	O
on	O
the	O
dev	O
set	O
,	O
but	O
a	O
slight	O
fall	O
is	O
observed	O
while	O
evaluating	O
it	O
on	O
the	O
test	O
set	O
.	O

No	O
new	O
date	O
has	O
been	O
set	O
,	O
but	O
the	O
statement	O
said	O
that	O
"	O
President	O
Michel	O
Martelly	O
,	O
in	O
his	O
constant	O
concern	O
to	O
guarantee	O
political	O
stability	O
,	O
promises	O
to	O
pursue	O
consultations	O
with	O
the	O
different	O
sectors	O
of	O
national	O
life	O
in	O
order	O
to	O
hold	O
the	O
elections	O
as	O
soon	O
as	O
possible	O
"	O
.	O

2013).For	O
non	O
-	O
specificity	O
,	O
with	O
the	O
hypernym	B-MethodName
augmentation	I-MethodName
method	I-MethodName
,	O
BERT	B-MethodName
FitB	I-MethodName
achieves	O
lower	O
accuracy	O
.	O

A	O
possible	O
reason	O
for	O
this	O
could	O
be	O
that	O
replacing	O
the	O
nouns	O
with	O
their	O
hypernyms	O
in	O
some	O
contexts	O
changes	O
the	O
meaning	O
of	O
the	O
sentence	O
(	O
even	O
though	O
we	O
use	B-MethodName
Lesk	I-MethodName
Algorithm	I-MethodName
for	O
WSD	O
,	O
not	O
all	O
hypernyms	O
make	O
sense	O
)	O
.	O

For	O
the	O
hyponyms	B-MethodName
method	I-MethodName
,	O
we	O
can	O
improve	O
our	O
results	O
by	O
recursively	O
generating	O
hyponyms	O
for	O
a	O
particular	O
option	O
,	O
instead	O
of	O
taking	O
the	O
immediate	O
hyponyms	O
.	O

In	O
Table	O
3	O
,	O
a	O
positive	O
sign	O
for	O
the	O
Difference	B-MethodName
Method	I-MethodName
or	O
the	O
Threshold	B-MethodName
Method	I-MethodName
is	O
the	O
improvement	O
in	O
the	O
results	O
of	O
BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
(	I-MethodName
Exact	I-MethodName
Matching	I-MethodName
)	I-MethodName
when	O
we	O
consider	O
the	O
hyponyms	O
.	O

The	O
accuracy	B-MetricName
jumps	O
from	O
72.86	B-MetricValue
%	I-MetricValue
to	I-MetricValue
75.79	I-MetricValue
%	I-MetricValue
on	O
the	O
dev	O
set	O
and	O
from	O
77.83	B-MetricValue
%	I-MetricValue
to	I-MetricValue
78.98	I-MetricValue
%	I-MetricValue
on	O
the	O
test	O
set	O
.	O

BERT	B-MethodName
FitB	I-MethodName
Voting	I-MethodName
performs	O
better	O
than	O
vanilla	B-MethodName
BERT	I-MethodName
FitB	I-MethodName
on	O
both	O
subtasks	O
.	O

This	O
is	O
intuitive	O
since	O
in	O
the	O
latter	O
,	O
we	O
truncate	O
the	O
article	O
to	O
512	B-HyperparameterValue
tokens	I-HyperparameterValue
without	O
any	O
consideration	O
of	O
how	O
much	O
context	O
is	O
lost	O
.	O

For	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
when	O
compared	O
with	O
the	O
GA	B-MethodName
-	I-MethodName
Reader	I-MethodName
baseline	O
,	O
the	B-MetricName
accuracy	I-MetricName
improves	O
from	O
21	B-MetricValue
%	I-MetricValue
to	O
39	B-MetricValue
%	I-MetricValue
on	O
subtask	O
-	O
I	O
dev	O
set	O
.	O

Due	O
to	O
computational	O
restrictions	O
,	O
we	O
could	O
n't	O
pretrain	O
GA	O
-	O
BERT	O
,	O
and	O
only	O
fine	O
-	O
tuned	O
it	O
for	O
subtask	O
-	O
I	O
to	O
get	O
an	O
idea	O
about	O
its	O
performance	O
,	O
which	O
was	O
sub	O
-	O
optimal	O
(	B-MetricValue
19	I-MetricValue
%	I-MetricValue
)	O
.	O

The	O
Answer	B-MethodName
-	I-MethodName
Attention	I-MethodName
system	I-MethodName
gave	O
us	O
a	O
dev	O
score	O
of	O
≈61	B-MetricName
%	I-MetricName
on	O
subtask	O
-	O
I	O
,	O
which	O
is	O
much	O
higher	O
than	O
the	O
baseline	O
.	O

BERT	B-MethodName
-	I-MethodName
GSAMN	I-MethodName
-	I-MethodName
Cloze	I-MethodName
achieves	O
≈31	B-MetricValue
%	I-MetricValue
accuracy	B-MetricName
on	O
subtask	O
-	O
I	O
dev	O
set	O
.	O

We	O
see	O
improvement	O
as	O
we	O
reduced	O
number	O
of	O
layers	O
to	O
1(≈38	B-MetricValue
%	I-MetricValue
)	O
and	O
to	O
0(≈73	B-MetricValue
%	I-MetricValue
)	O
.	O

It	O
cited	O
research	O
which	O
suggested	O
a	O
20	O
%	O
tax	O
would	O
save	O
just	O
four	O
calories	O
per	O
day	O
.	O

Professor	O
Capewell	O
will	O
cite	O
Mexico	O
as	O
one	O
example	O
where	O
a	O
10	O
%	O
sugary	O
drinks	O
tax	O
is	O
believed	O
to	O
have	O
contributed	O
to	O
a	O
10	O
%	O
reduction	O
in	O
the	O
consumption	O
of	O
such	O
beverages	O
while	O
Finland	O
,	O
France	O
,	O
Hungary	O
,	O
Latvia	O
and	O
the	O
USA	O
have	O
also	O
introduced	O
sugar	O
taxes	O
.	O

2017	O
)	O
.	O

2020	O
)	O
to	O
compute	O
the	O
word	O
-	O
wise	O
attribution	O
scores	O
for	O
BERT	B-MethodName
FitB	I-MethodName
for	O
both	O
subtasks	O
.	O

We	O
compute	O
the	O
Integrated	B-MethodName
Gradients	I-MethodName
of	O
the	O
target	O
with	O
respect	O
to	O
the	O
embedding	O
outputs	O
.	O

The	O
Riemann	B-MethodName
Right	I-MethodName
Approximation	I-MethodName
Method	I-MethodName
with	O
n	B-HyperparameterName
steps	I-HyperparameterName
=	O
25	B-HyperparameterValue
is	O
used	O
.	O

After	O
obtaining	O
the	O
token	O
-	O
wise	O
attribution	O
scores	O
,	O
we	O
obtain	O
the	O
word	O
-	O
wise	O
attribution	O
scores	O
by	O
using	O
token	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
word	I-MethodName
offset	I-MethodName
mapping	I-MethodName
.	O

The	O
word	O
"	O
legislative	O
"	O
is	O
,	O
in	O
a	O
sense	O
,	O
more	O
imperceptible	O
than	O
any	O
of	O
the	O
words	O
mentioned	O
above	O
.	O

These	O
words	O
are	O
related	O
to	O
"	O
legislative	O
"	O
which	O
exhibits	O
the	O
fact	O
that	O
BERT	B-MethodName
FitB	I-MethodName
is	O
not	O
only	O
able	O
to	O
learn	O
the	O
concept	O
of	O
imperceptibility	B-TaskName
,	O
but	O
is	O
also	O
able	O
to	O
predict	O
a	O
suitable	O
word	O
.	O

Note	O
that	O
"	O
snacks	O
"	O
is	O
also	O
an	O
option	O
;	O
however	O
,	O
food	O
is	O
more	O
non	B-TaskName
-	I-TaskName
specific	I-TaskName
than	O
"	O
snacks	O
"	O
and	O
hence	O
,	O
food	O
is	O
the	O
correct	O
option	O
.	O

We	O
reckon	O
that	O
with	O
more	O
careful	O
tuning	O
of	O
parameters	O
such	O
as	O
the	O
threshold	O
in	O
the	O
Difference	B-MethodName
Method	I-MethodName
,	O
we	O
will	O
be	O
able	O
to	O
achieve	O
these	O
gains	O
on	O
the	O
test	O
set	O
.	O

We	O
further	O
interpreted	O
the	O
outputs	O
of	O
transformers	O
-	O
based	O
models	O
using	O
Integrated	B-MethodName
Gradients	I-MethodName
,	O
and	O
demonstrated	O
that	O
transformer	O
models	O
are	O
able	O
to	O
learn	O
the	O
concepts	O
of	O
imperceptibility	B-TaskName
and	O
non	B-TaskName
-	I-TaskName
specificity	I-TaskName
.	O

This	O
motivates	O
the	O
study	O
of	O
claim	B-TaskName
provenance	I-TaskName
,	O
which	O
seeks	O
to	O
trace	O
and	O
explain	O
the	O
origins	O
of	O
claims	O
.	O

This	O
establishes	O
relevant	O
search	O
queries	O
,	O
and	O
it	O
allows	O
us	O
to	O
obtain	O
source	O
article	O
candidates	O
for	O
each	O
identified	O
sentence	O
and	O
propose	O
an	O
ILP	B-MethodName
based	I-MethodName
algorithm	I-MethodName
to	O
infer	O
the	O
best	O
sources	O
.	O

We	O
experiment	O
with	O
a	O
newly	O
created	O
evaluation	O
dataset	O
1	O
,	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
,	O
based	O
on	O
fact	O
-	O
checking	O
articles	O
from	O
www.politifa	O
ct.com	O
;	O
our	O
experimental	O
results	O
show	O
that	O
our	O
solution	O
leads	O
to	O
a	O
significant	O
improvement	O
over	O
baselines	O
.	O

Misinformation	O
is	O
on	O
the	O
rise	O
,	O
and	O
people	O
are	O
fighting	O
it	O
with	O
fact	O
checking	O
.	O

2020	O
)	O
focuses	O
on	O
automating	O
factchecking	O
for	O
a	O
single	O
claim	O
.	O

In	O
reality	O
,	O
a	O
claim	O
can	O
be	O
complex	O
,	O
and	O
proposed	O
as	O
a	O
conclusion	O
of	O
an	O
article	O
.	O

This	O
motivates	O
the	O
study	O
of	O
provenance	B-TaskName
for	I-TaskName
natural	I-TaskName
language	I-TaskName
claims	I-TaskName
,	O
which	O
describes	O
where	O
a	O
specific	O
claim	O
may	O
have	O
come	O
from	O
and	O
how	O
it	O
has	O
spread	O
.	O

Early	O
work	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

However	O
,	O
that	O
model	O
is	O
insufficient	O
to	O
capture	O
the	O
provenance	O
of	O
an	O
article	O
,	O
because	O
(	O
1	O
)	O
an	O
article	O
consists	O
of	O
multiple	O
claims	O
,	O
and	O
it	O
leverages	O
information	O
from	O
other	O
sources	O
,	O
therefore	O
the	O
provenance	O
of	O
all	O
claims	O
should	O
be	O
included	O
in	O
the	O
article	O
's	O
provenance	O
;	O
(	O
2	O
)	O
the	O
inference	O
solution	O
they	O
proposed	O
can	O
only	O
extract	O
domain	O
-	O
level	O
provenance	O
information	O
,	O
e.g.	O
,	O
cbsnews.com	O
,	O
while	O
it	O
can	O
not	O
directly	O
link	O
the	O
claim	O
to	O
its	O
source	O
article	O
,	O
e.g.	O
,	O
https://www.cbsnews.com/news/preventingcoronavirus-facemask-60-minutes-2020-03-08/.	O
Such	O
fine	O
-	O
grained	O
provenance	O
information	O
is	O
important	O
because	O
it	O
can	O
help	O
people	O
understand	O
the	O
original	O
context	O
that	O
influenced	O
the	O
information	O
they	O
read	O
.	O

Technically	O
,	O
capturing	O
fine	O
-	O
grained	O
provenance	O
for	O
an	O
article	O
is	O
challenging	O
because	O
(	O
1	O
)	O
there	O
may	O
be	O
large	O
numbers	O
of	O
sentences	O
in	O
an	O
article	O
,	O
and	O
not	O
all	O
are	O
from	O
external	O
sources	O
nor	O
important	O
(	O
thus	O
,	O
their	O
provenance	O
may	O
not	O
be	O
worth	O
considering);(2	O
)	O
a	O
sentence	O
in	O
an	O
article	O
is	O
usually	O
just	O
a	O
textual	O
fragment	O
of	O
its	O
source	O
article	O
,	O
and	O
simply	O
looking	O
for	O
other	O
articles	O
with	O
related	O
content	O
may	O
result	O
in	O
low	O
precision	B-MetricName
with	O
regards	O
to	O
finding	O
the	O
correct	O
original	O
article	O
.	O

The	O
key	O
contributions	O
of	O
this	O
paper	O
are	O
(	O
1	O
)	O
we	O
introduce	O
and	O
formalize	O
the	O
problem	O
of	O
inferring	B-TaskName
finegrained	I-TaskName
provenance	I-TaskName
for	O
an	O
article	O
;	O
(	O
2	O
)	O
we	O
propose	O
a	O
general	O
framework	O
to	O
infer	O
the	O
source	O
articles	O
that	O
have	O
provided	O
important	O
information	O
for	O
the	O
given	O
article	O
,	O
including	O
(	O
a	O
)	O
a	O
ranking	O
module	O
that	O
can	O
identify	O
sentences	O
that	O
contain	O
important	O
external	O
information	O
based	O
on	O
the	O
main	O
topic	O
and	O
the	O
main	O
entities	O
in	O
the	O
article	O
;	O
(	O
b	O
)	O
a	O
query	O
generator	O
that	O
can	O
generate	O
possible	O
metadata	O
for	O
the	O
source	O
article	O
,	O
e.g.	O
,	O
the	O
title	O
,	O
the	O
published	O
date	O
,	O
the	O
source	O
website	O
,	O
based	O
on	O
the	O
context	O
of	O
the	O
selected	O
sentences	O
;	O
(	O
c	O
)	O
an	O
integer	O
linear	O
program	O
(	O
ILP	O
)	O
based	O
algorithm	O
to	O
jointly	O
identify	O
the	O
source	O
articles	O
from	O
all	O
of	O
the	O
candidates	O
.	O
(	O

3	O
)	O
to	O
evaluate	O
our	O
solutions	O
,	O
we	O
collect	O
a	O
new	O
dataset	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
from	O
politifact.com	O
,	O
and	O
our	O
experimental	O
results	O
show	O
that	O
the	O
solution	O
we	O
proposed	O
can	O
lead	O
to	O
a	O
significant	O
improvement	O
compared	O
with	O
baselines	O
.	O

Given	O
an	O
article	O
d	O
,	O
we	O
are	O
to	O
capture	O
its	O
finegrained	B-TaskName
provenance	I-TaskName
,	O
by	O
inferring	O
k	O
source	O
articles	O
SA	O
k	O
(	O
d	O
)	O
that	O
provide	O
the	O
most	O
important	O
information	O
for	O
d.	O
We	O
adopt	O
the	O
notion	O
of	O
provenance	O
from	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Then	O
we	O
will	O
choose	O
top	O
-	O
k	O
sentences	O
based	O
on	O
their	O
score	O
,	O
and	O
try	O
to	O
find	O
source	O
articles	O
for	O
those	O
sentences	O
.	O

As	O
we	O
have	O
discussed	O
in	O
Section	O
1	O
,	O
directly	O
searching	O
the	O
identified	O
sentence	O
on	O
a	O
search	O
engine	O
may	O
result	O
in	O
a	O
low	O
precision	B-MetricName
of	O
finding	O
the	O
correct	O
source	O
article	O
.	O

Figure	O
2	O
depicts	O
the	O
three	O
steps	O
we	O
need	O
to	O
conduct	O
to	O
infer	O
the	O
fine	B-TaskName
-	I-TaskName
grained	I-TaskName
provenance	I-TaskName
,	O
which	O
correspond	O
to	O
the	O
three	O
subproblems	O
listed	O
above	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
is	O
no	O
existing	O
dataset	O
that	O
can	O
support	O
inferring	O
fine	O
-	O
grained	O
provenance	O
for	O
an	O
article	O
,	O
therefore	O
we	O
create	O
a	O
new	O
dataset	O
based	O
on	O
the	O
fact	O
-	O
checks	O
from	O
politifact	O
.com	O
to	O
support	O
the	O
training	O
and	O
the	O
evaluation	O
of	O
this	O
problem	O
.	O

We	O
want	O
to	O
note	O
it	O
is	O
possible	O
that	O
there	O
may	O
be	O
some	O
sources	O
missing	O
in	O
the	O
ground	O
truth	O
we	O
can	O
obtain	O
,	O
therefore	O
,	O
we	O
focus	O
more	O
on	O
the	O
recall	O
in	O
the	O
evaluation	O
.	O

In	O
this	O
section	O
,	O
we	O
will	O
elaborate	O
how	O
we	O
solve	O
the	O
problems	O
proposed	O
in	O
Section	O
2	O
.	O

We	O
build	O
our	O
model	O
by	O
leveraging	O
Roberta	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

Using	O
the	O
same	O
notation	O
in	O
the	O
paper	O
,	O
we	O
concatenate	O
t	O
d	O
and	O
each	O
e	O
∈	O
E	O
d	O
,	O
feeding	O
it	O
to	O
the	O
model	O
as	O
sentence	O
A	O
,	O
and	O
s	O
∈	O
P	O
(	O
d	O
)	O
or	O
N	O
(	O
d	O
)	O
as	O
sentence	O
B	O
,	O
as	O
the	O
input	O
of	O
Roberta	B-MethodName
.	O

We	O
then	O
use	O
Roberta	B-MethodName
as	O
a	O
binary	O
classification	O
model	O
,	O
that	O
is	O
,	O
we	O
use	O
its	O
[	O
CLS	O
]	O
vector	O
as	O
input	O
to	O
a	O
two	B-HyperparameterValue
layer	B-HyperparameterName
neural	O
network	O
to	O
obtain	O
the	O
probability	O
of	O
s	O
referring	O
to	O
important	O
external	O
information	O
.	O

We	O
start	O
training	O
from	O
a	O
pre	O
-	O
trained	O
Roberta	B-MethodName
model	O
and	O
fine	O
-	O
tune	O
it	O
to	O
our	O
ranking	O
task	O
using	O
the	O
following	O
loss	O
,	O
given	O
s	O
i	O
∈	O
P	O
(	O
d	O
)	O
and	O
s	O
j	O
∈	O
N	O
(	O
d):Li	O
,	O
j	O
=	O
−	O
log	O
σi	O
−	O
log	O
(	O
1	O
−	O
σj	O
)	O
+	O
max	O
0	O
,	O
τ	O
(	O
sj	O
)	O
−	O
τ	O
(	O
si	O
)	O
+	O
(	O
1)where	O
τ	O
(	O
s	O
i	O
)	O
and	O
τ	O
(	O
s	O
j	O
)	O
are	O
the	O
representations	O
,	O
obtained	O
by	O
the	O
output	O
of	O
a	O
single	B-HyperparameterValue
layer	B-HyperparameterName
neural	O
network	O
τ	O
on	O
top	O
of	O
the	O
[	O
CLS	O
]	O
vector	O
of	O
Roberta	B-MethodName
.	O

The	O
next	O
step	O
is	O
to	O
find	O
candidate	O
articles	O
that	O
can	O
be	O
the	O
source	O
articles	O
based	O
on	O
the	O
identified	O
sentences	O
.	O

To	O
generate	O
a	O
query	O
that	O
can	O
improve	O
the	O
recall	B-MetricName
,	O
the	O
question	O
here	O
is	O
what	O
search	O
keywords	O
are	O
good	O
for	O
finding	O
the	O
source	O
articles	O
besides	O
the	O
identified	O
sentences	O
themselves	O
?	O
In	O
this	O
work	O
,	O
we	O
argue	O
that	O
the	O
metadata	O
of	O
the	O
target	O
article	O
,	O
including	O
its	O
source	O
domain	O
,	O
title	O
and	O
published	O
date	O
is	O
a	O
good	O
choice	O
.	O

As	O
a	O
baseline	O
,	O
we	O
train	O
this	O
model	O
via	O
fine	O
-	O
tuning	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
a	O
pretrained	O
text	O
generation	O
model	O
.	O

To	O
solve	O
this	O
problem	O
,	O
we	O
extend	O
the	O
BART	B-MethodName
baseline	O
to	O
incorporate	O
two	O
sources	O
of	O
inputs	O
,	O
by	O
first	O
feeding	O
the	O
text	O
inputs	O
independently	O
to	O
the	O
BART	B-MethodName
's	O
encoders	O
,	O
then	O
concatenating	O
the	O
outputs	O
of	O
the	O
encoders	O
together	O
,	O
and	O
finally	O
feeding	O
the	O
unified	O
representations	O
to	O
the	O
BART	B-MethodName
's	O
decoder	O
.	O

Therefore	O
,	O
we	O
propose	O
a	O
rank	O
-	O
aware	O
multi	O
-	O
head	O
cross	O
-	O
attention	O
to	O
relieve	O
this	O
problem	O
.	O

The	O
basic	O
idea	O
is	O
when	O
BART	B-MethodName
's	O
decoders	O
are	O
performing	O
cross	O
-	O
attention	O
over	O
the	O
text	O
input	O
of	O
the	O
sentences	O
and	O
the	O
possible	O
metadata	O
,	O
we	O
require	O
that	O
each	O
set	O
of	O
attention	O
heads	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

Note	O
that	O
the	O
candidate	O
metadata	O
from	O
the	O
urls	O
ranked	O
higher	O
will	O
always	O
receive	O
more	O
attention	O
than	O
the	O
others	O
in	O
this	O
case	O
.	O

The	O
model	O
extends	O
(	O
1	O
)	O
BART	B-MethodName
's	O
encoders	O
to	O
incorporate	O
two	O
types	O
of	O
input	O
,	O
one	O
is	O
the	O
context	O
of	O
the	O
selected	O
sentence	O
,	O
and	O
the	O
other	O
one	O
is	O
possible	O
metadata	O
collected	O
from	O
a	O
search	O
engine	O
,	O
(	O
2	O
)	O
BART	B-MethodName
's	O
decoders	O
with	O
a	O
rankaware	B-HyperparameterValue
multi	B-HyperparameterName
-	I-HyperparameterName
head	I-HyperparameterName
cross	I-HyperparameterName
attention	I-HyperparameterName
to	O
generate	O
the	O
gold	O
metadata	O
.	O

Based	O
on	O
our	O
observations	O
,	O
the	O
author	O
is	O
very	O
likely	O
to	O
leverage	O
the	O
external	O
information	O
coming	O
from	O
the	O
same	O
source	O
websites	O
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
the	O
last	O
hidden	O
layer	O
of	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Setup	O
We	O
use	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
dataset	O
introduced	O
in	O
Section	O
3	O
.	O

To	O
compare	O
the	O
performance	O
,	O
we	O
implement	O
our	O
solution	O
(	O
SR	B-MethodName
-	I-MethodName
TE	I-MethodName
)	O
as	O
described	O
in	O
Section	O
4.1	O
,	O
and	O
compare	O
it	O
with	O
(	O
1	O
)	O
a	O
retrieval	O
baseline	O
that	O
simply	O
computes	O
the	O
cosine	O
similarity	O
between	O
the	O
embedding	O
vectors	O
(	O
using	O
Roberta	B-MethodName
)	O
of	O
the	O
title	O
and	O
the	O
sentence	O
in	O
the	O
article	O
(	O
SR	B-MethodName
)	O
.	O

This	O
retrieval	O
baseline	O
only	O
captures	O
the	O
relatedness	O
between	O
the	O
sentence	O
and	O
the	O
main	O
topic	O
of	O
the	O
article;(2	O
)	O
a	O
retrieval	O
baseline	O
similar	O
to	O
SR	B-MethodName
,	O
but	O
computing	O
the	O
cosine	O
similarity	O
between	O
the	O
embedding	O
vectors	O
of	O
the	O
concatenation	O
of	O
the	O
title	O
and	O
the	O
most	O
important	O
entities	O
(	O
top-50	O
)	O
and	O
the	O
sentence	O
in	O
the	O
article	O
(	O
SR	B-MethodName
-	I-MethodName
E	I-MethodName
)	O
,	O
where	O
we	O
want	O
to	O
show	O
the	O
effect	O
of	O
considering	O
important	O
entities	O
;	O
(	O
3	O
)	O
our	O
learning	O
solution	O
without	O
considering	O
entities	O
(	O
SR	B-MethodName
-	I-MethodName
T	I-MethodName
)	O
.	O

We	O
report	O
the	O
mean	B-MetricName
precision	I-MetricName
and	O
recall	B-MetricName
of	I-MetricName
the	I-MetricName
top	I-MetricName
-	I-MetricName
k	I-MetricName
results	I-MetricName
respectively	O
.	O

The	O
gaps	O
between	O
SR	B-MethodName
,	O
SR	B-MethodName
-	I-MethodName
E	I-MethodName
,	O
and	O
SR	B-MethodName
-	I-MethodName
T	I-MethodName
,	O
SR	B-MethodName
-	I-MethodName
TE	I-MethodName
show	O
that	O
considering	O
important	O
entities	O
always	O
results	O
in	O
an	O
improvement	O
on	O
both	O
precision	B-MetricName
and	O
recall	B-MetricName
,	O
which	O
reveals	O
that	O
the	O
sentences	O
can	O
not	O
be	O
identified	O
based	O
on	O
their	O
relatedness	O
to	O
the	O
title	O
(	O
the	O
main	O
topic	O
)	O
only	O
,	O
but	O
also	O
requires	O
other	O
important	O
information	O
in	O
the	O
article	O
.	O

Setup	O
We	O
collect	O
all	O
of	O
the	O
sentences	O
that	O
correspond	O
to	O
the	O
source	O
articles	O
in	O
training	O
,	O
validation	O
and	O
test	O
set	O
of	O
Politi	B-DatasetName
-	I-DatasetName
Prov	I-DatasetName
serving	O
as	O
training	O
,	O
validation	O
and	O
testing	O
respectively	O
.	O

To	O
evaluate	O
the	O
performance	O
,	O
we	O
report	O
Rouge	B-MetricName
1	I-MetricName
,	O
Rouge	B-MetricName
2	I-MetricName
and	O
Rouge	B-MetricName
L	I-MetricName
score	O
of	O
the	O
text	O
generated	O
,	O
and	O
compare	O
with	O

In	O
this	O
work	O
,	O
we	O
introduce	O
BanglaBERT	B-MethodName
,	O
a	O
BERT	B-MethodName
-	O
based	O
Natural	B-TaskName
Language	I-TaskName
Understanding	I-TaskName
(	O
NLU	B-TaskName
)	O
model	O
pretrained	O
in	O
Bangla	O
,	O
a	O
widely	O
spoken	O
yet	O
low	O
-	O
resource	O
language	O
in	O
the	O
NLP	O
literature	O
.	O

To	O
pretrain	O
BanglaBERT	B-MethodName
,	O
we	O
collect	O
27.5	O
GB	O
of	O
Bangla	O
pretraining	O
data	O
(	O
dubbed	O
'	O
Bangla2B+	O
'	O
)	O
by	O
crawling	O
110	O
popular	O
Bangla	O
sites	O
.	O

We	O
introduce	O
two	O
downstream	O
task	O
datasets	O
on	O
natural	O
language	O
inference	O
and	O
question	O
answering	O
and	O
benchmark	O
on	O
four	O
diverse	O
NLU	B-TaskName
tasks	O
covering	O
text	O
classification	O
,	O
sequence	O
labeling	O
,	O
and	O
span	O
prediction	O
.	O

In	O
the	O
process	O
,	O
we	O
bring	O
them	O
under	O
the	O
first	O
-	O
ever	O
Bangla	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Benchmark	I-DatasetName
(	O
BLUB	B-DatasetName
)	O
.	O

BanglaBERT	B-MethodName
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
outperforming	O
multilingual	O
and	O
monolingual	O
models	O
.	O

com	O
/	O
csebuetnlp	O
/	O
banglabert	B-MethodName
to	O
advance	O
Bangla	O
NLP	O
.	O

2019	O
)	O
to	O
boost	O
the	O
natural	B-TaskName
language	I-TaskName
understanding	I-TaskName
(	O
NLU	B-TaskName
)	O
task	O
performances	O
.	O

They	O
also	O
tend	O
to	O
show	O
degraded	O
performance	O
for	O
low	O
-	O
resource	O
languages	O
(	O
Wu	O
and	O
Dredze	O
,	O
2020	O
)	O
on	O
downstream	O
NLU	B-TaskName
tasks	O
.	O

2020	O
)	O
;	O
Antoun	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
inter	O
alia	O
)	O
over	O
multilingual	O
models	O
in	O
many	O
other	O
languages	O
,	O
in	O
this	O
work	O
,	O
we	O
present	O
BanglaBERT	B-MethodName
-a	O
BERT	B-MethodName
-	O
based	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
Bangla	O
NLU	B-TaskName
model	O
pretrained	O
on	O
27.5	O
GB	O
data	O
(	O
which	O
we	O
name	O
'	O
Bangla2B+	O
'	O
)	O
we	O
meticulously	O
crawled	O
110	O
popular	O
Bangla	O
websites	O
to	O
facilitate	O
NLU	B-TaskName
applications	O
in	O
Bangla	O
.	O

Since	O
most	O
of	O
the	O
downstream	O
task	O
datasets	O
for	O
NLP	O
applications	O
are	O
in	O
the	O
English	O
language	O
,	O
to	O
facilitate	O
zero	O
-	O
shot	O
transfer	O
learning	O
between	O
English	O
and	O
Bangla	O
,	O
we	O
additionally	O
pretrain	O
a	O
model	O
in	O
both	O
languages	O
;	O
we	O
name	O
the	O
model	O
BanglishBERT.We	B-MethodName
also	O
introduce	O
two	O
datasets	O
on	O
Bangla	O
Natural	O
Language	O
Inference	O
(	O
NLI	O
)	O
and	O
Question	O
Answering	O
(	O
QA	O
)	O
,	O
tasks	O
previously	O
unexplored	O
in	O
Bangla	O
,	O
and	O
evaluate	O
both	O
pretrained	O
models	O
on	O
four	O
diverse	O
downstream	O
tasks	O
on	O
sentiment	O
classification	O
,	O
NLI	O
,	O
named	O
entity	O
recognition	O
,	O
and	O
QA	O
.	O

We	O
bring	O
these	O
tasks	O
together	O
to	O
establish	O
the	O
firstever	O
Bangla	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Benchmark	I-DatasetName
(	O
BLUB	B-DatasetName
)	O
.	O

We	O
compare	O
widely	O
used	O
multilingual	O
models	O
to	O
BanglaBERT	B-MethodName
using	O
BLUB	B-DatasetName
and	O
find	O
that	O
both	O
models	O
excel	O
on	O
all	O
the	O
tasks	O
.	O

We	O
summarize	O
our	O
contributions	O
as	O
follows:1	O
.	O

We	O
present	O
two	O
new	O
pretrained	O
models	O
:	O
3	O
.	O

We	O
provide	O
the	O
code	O
,	O
models	O
,	O
and	O
a	O
leaderboard	O
to	O
spur	O
future	O
research	O
on	O
Bangla	B-TaskName
NLU	I-TaskName
.	O

For	O
instance	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Subsequent	O
works	O
like	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
and	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
.	O
,	O

The	O
contents	O
included	O
encyclopedias	O
,	O
news	O
,	O
blogs	O
,	O
e	O
-	O
books	O
,	O
stories	O
,	O
social	O
media	O
/	O
forums	O
,	O
etc	O
.	O

2019	O
)	O
and	O
CCNet	O
.	O

2016	O
)	O
vocabulary	O
of	O
32k	O
subword	O
tokens	O
on	O
the	O
resulting	O
corpus	O
with	O
a	O
400	O
character	O
alphabet	O
,	O
kept	O
larger	O
than	O
the	O
native	O
Bangla	O
alphabet	O
to	O
capture	O
codeswitching	O
(	O
Poplack	O
,	O
1980	O
)	O
and	O
allow	O
romanized	O
Bangla	O
contents	O
for	O
better	O
generalization	O
.	O

For	O
example	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Several	O
works	O
built	O
on	O
top	O
of	O
this	O
,	O
e.g.	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
removed	O
NSP	O
and	O
pretrained	O
with	O
longer	O
sequences	O
,	O
SpanBERT	B-MethodName
(	O
Joshi	O
et	O
al	O
.	O
,	O

2020a	O
)	O
masked	O
contiguous	O
spans	O
of	O
tokens	O
,	O
while	O
works	O
like	O
XLNet	B-MethodName
(	O
Yang	O
et	O
al	O
.	O
,	O

We	O
pretrained	O
BanglaBERT	B-MethodName
using	O
ELECTRA	B-MethodName
(	O
Clark	O
et	O
al	O
.	O
,	O

The	O
masked	O
tokens	O
are	O
then	O
replaced	O
by	O
tokens	O
sampled	O
from	O
the	O
generator	O
's	O
output	O
distribution	O
for	O
the	O
corresponding	O
masks	O
,	O
and	O
the	O
discriminator	O
then	O
has	O
to	O
predict	O
whether	O
each	O
token	O
is	O
from	O
the	O
original	O
sequence	O
or	O
not	O
.	O

Evidently	O
,	O
ELECTRA	B-MethodName
achieves	O
comparable	O
downstream	O
performance	O
to	O
RoBERTa	B-MethodName
or	O
XLNet	B-MethodName
with	O
only	O
a	O
quarter	O
of	O
their	O
training	O
time	O
.	O

This	O
computational	O
efficiency	O
motivated	O
us	O
to	O
use	O
ELECTRA	B-MethodName
for	O
our	O
implementation	O
of	O
BanglaBERT	B-MethodName
.	O

We	O
pretrained	O
the	O
base	O
ELECTRA	B-MethodName
model	O
(	O
a	O
12layer	O
Transformer	O
encoder	O
with	O
768	B-HyperparameterValue
embedding	B-HyperparameterName
size	I-HyperparameterName
,	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
size	I-HyperparameterName
,	O
12	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
,	O
3072	B-HyperparameterValue
feed	B-HyperparameterName
-	I-HyperparameterName
forward	I-HyperparameterName
size	I-HyperparameterName
,	O
generator	B-HyperparameterName
-	I-HyperparameterName
to	I-HyperparameterName
-	I-HyperparameterName
discriminator	I-HyperparameterName
ratio	I-HyperparameterName
1	B-HyperparameterValue
3	I-HyperparameterValue
,	O
110	O
M	O
parameters	O
)	O
with	O
256	B-HyperparameterValue
batch	B-HyperparameterName
size	I-HyperparameterName
for	O
2.5	B-HyperparameterValue
M	I-HyperparameterValue
steps	B-HyperparameterName
on	O
a	O
v3	O
-	O
8	O
TPU	O
instance	O
on	O
GCP	O
.	O

We	O
used	O
the	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
optimizer	B-HyperparameterName
with	O
a	O
2e-4	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
and	O
linear	B-HyperparameterName
warmup	I-HyperparameterName
of	O
10k	B-HyperparameterValue
steps	I-HyperparameterValue
.	O

Often	O
labeled	O
data	O
in	O
a	O
low	O
-	O
resource	O
language	O
for	O
a	O
task	O
may	O
not	O
be	O
available	O
but	O
be	O
abundant	O
in	O
highresource	O
languages	O
like	O
English	O
.	O

To	O
this	O
end	O
,	O
we	O
pretrained	O
a	O
bilingual	O
model	O
,	O
named	O
BanglishBERT	B-MethodName
,	O
on	O
Bangla	O
and	O
English	O
together	O
using	O
the	O
same	O
set	O
of	O
hyperparameters	O
mentioned	O
earlier	O
.	O

We	O
upsampled	O
the	O
Bangla	O
data	O
during	O
training	O
to	O
equal	O
the	O
participation	O
of	O
both	O
languages.3	O
The	O
Bangla	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Benchmark	I-DatasetName
(	O
BLUB)Many	B-DatasetName
works	O
have	O
studied	O
different	O
Bangla	B-TaskName
NLU	I-TaskName
tasks	O
in	O
isolation	O
,	O
e.g.	O
,	O
sentiment	O
classification	O
(	O
Das	O
and	O
Bandyopadhyay	O
,	O
2010;Sharfuddin	O
et	O
al	O
.	O
,	O

However	O
,	O
Bangla	B-TaskName
NLU	I-TaskName
has	O
not	O
yet	O
had	O
a	O
comprehensive	O
,	O
unified	O
study	O
.	O

Motivated	O
by	O
the	O
surge	O
of	O
NLU	B-TaskName
research	O
brought	O
about	O
by	O
benchmarks	O
in	O
other	O
languages	O
,	O
e.g.	O
,	O
English	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
French	O
,	O
Korean	O
(	O
Park	O
et	O
al	O
.	O
,	O

2021	O
)	O
,	O
we	O
establish	O
the	O
first	O
-	O
ever	O
Bangla	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Benchmark	I-DatasetName
(	O
BLUB	B-DatasetName
)	O
.	O

NLU	B-TaskName
generally	O
comprises	O
three	O
types	O
of	O
tasks	O
:	O
text	O
classification	O
,	O
sequence	O
labeling	O
,	O
and	O
text	O
span	O
prediction	O
.	O

Therefore	O
,	O
we	O
consider	O
a	O
total	O
of	O
four	O
tasks	O
for	O
BLUB	B-DatasetName
.	O

For	O
each	O
task	O
type	O
,	O
we	O
carefully	O
select	O
one	O
downstream	O
task	O
dataset	O
.	O

Single	O
-	O
Sequence	O
Classification	O
Sentiment	O
classification	O
is	O
perhaps	O
the	O
most	O
-	O
studied	O
Bangla	B-TaskName
NLU	I-TaskName
task	O
,	O
with	O
some	O
of	O
the	O
earlier	O
works	O
dating	O
back	O
over	O
a	O
decade	O
(	O
Das	O
and	O
Bandyopadhyay	O
,	O
2010	O
)	O
.	O

We	O
could	O
only	O
find	O
two	O
public	O
datasets	O
:	O
BYSA	O
(	O
Tripto	O
and	O
Ali	O
,	O
2018	O
)	O
and	O
SentNoB	O
(	O
Islam	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

Even	O
worse	O
,	O
many	O
duplicates	O
had	O
different	O
labels	O
.	O

SentNoB	O
had	O
better	O
quality	O
and	O
covered	O
a	O
broader	O
set	O
of	O
domains	O
,	O
making	O
the	O
classification	O
task	O
more	O
challenging	O
.	O

We	O
found	O
work	O
on	O
semantic	O
textual	O
similarity	O
(	O
Shajalal	O
and	O
Aono	O
,	O
2018	O
)	O
,	O
but	O
the	O
dataset	O
is	O
not	O
publicly	O
available	O
.	O

We	O
chose	O
NLI	O
as	O
the	O
representative	O
task	O
due	O
to	O
its	O
fundamental	O
importance	O
in	O
NLU	B-TaskName
.	O

Given	O
two	O
sentences	O
,	O
a	O
premise	O
and	O
a	O
hypothesis	O
as	O
input	O
,	O
a	O
model	O
is	O
tasked	O
to	O
predict	O
whether	O
or	O
not	O
the	O
hypothesis	O
is	O
entailment	O
,	O
contradiction	O
,	O
or	O
neutral	O
to	O
the	O
premise	O
.	O

4	O
Due	O
to	O
the	O
possibility	O
of	O
the	O
incursion	O
of	O
errors	O
during	O
automatic	O
translation	O
,	O
we	O
used	O
the	O
Language	O
-	O
Agnostic	O
BERT	B-MethodName
Sentence	O
Embeddings	O
(	O
LaBSE	O
)	O
(	O
Feng	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

In	O
this	O
task	O
,	O
all	O
words	O
of	O
a	O
text	O
sequence	O
have	O
to	O
be	O
classified	O
.	O

We	O
chose	O
the	O
Bangla	O
portion	O
of	O
Se	O
-	O
mEval	O
2022	O
MultiCoNER	O
(	O
Malmasi	O
et	O
al	O
.	O
,	O

2022	O
)	O
dataset	O
for	O
BLUB	B-DatasetName
.	O

2018	O
)	O
dataset	O
and	O
used	O
it	O
as	O
the	O
training	O
set	O
(	O
BQA	O
)	O
.	O

We	O
present	O
detailed	O
statistics	O
of	O
the	O
BLUB	B-DatasetName
benchmark	O
in	O
Table	O
1	O
.	O

Setup	O
We	O
fine	O
-	O
tuned	O
BanglaBERT	B-MethodName
and	O
Banglish	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
the	O
four	O
downstream	O
tasks	O
and	O
compared	O
them	O
with	O
several	O
multilingual	O
models	O
:	O
mBERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
base	I-MethodName
and	O
large	O
,	O
and	O
IndicBERT	B-MethodName
(	O
Kakwani	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
a	O
multilingual	O
model	O
for	O
Indian	O
languages	O
;	O
and	O
sahajBERT	B-MethodName
(	O
Diskin	O
et	O
al	O
.	O
,	O

2021	O
)	O
,	O
an	O
ALBERTbased	B-MethodName
(	O
Lan	O
et	O
al	O
.	O
,	O

2020	O
)	O
PLM	O
for	O
Bangla	O
.	O

All	O
pre	O
-	O
trained	O
models	O
were	O
fine	O
-	O
tuned	O
for	O
3	B-HyperparameterValue
-	I-HyperparameterValue
20	I-HyperparameterValue
epochs	B-HyperparameterName
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
32	B-HyperparameterValue
,	O
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
was	O
tuned	O
from	O
{	O
2e-5	B-HyperparameterValue
,	O
3e-5	B-HyperparameterValue
,	O
4e-5	B-HyperparameterValue
,	O
5e-5	B-HyperparameterValue
}	O
.	O

We	O
reported	O
the	O
average	O
performance	O
of	O
all	O
tasks	O
as	O
the	O
BLUB	B-MetricName
score	I-MetricName
.	O

In	O
zero	O
-	O
shot	O
transfer	O
setting	O
,	O
BanglishBERT	B-MethodName
achieves	O
strong	O
cross	O
-	O
lingual	O
performance	O
over	O
similar	O
-	O
sized	O
models	O
and	O
falls	O
marginally	O
short	O
of	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	I-MethodName
large	I-MethodName
)	I-MethodName
.	O

This	O
is	O
an	O
expected	O
outcome	O
since	O
cross	O
-	O
lingual	O
effectiveness	O
depends	O
explicitly	O
on	O
model	O
size	O
(	O
K	O
et	O
al	O
.	O
,	O

2020).Supervised	O
Fine	O
-	O
tuning	O
In	O
the	O
supervised	O
finetuning	O
setup	O
,	O
BanglaBERT	B-MethodName
outperformed	O
multilingual	O
models	O
and	O
monolingual	O
sahajBERT	B-MethodName
on	O
all	O
the	O
tasks	O
,	O
achieving	O
a	O
BLUB	B-MetricName
score	I-MetricName
of	O
77.09	B-MetricValue
,	O
even	O
coming	O
head	O
-	O
to	O
-	O
head	O
with	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	I-MethodName
large	I-MethodName
)	I-MethodName
.	O

On	O
the	O
other	O
hand	O
,	O
BanglishBERT	B-MethodName
marginally	O
lags	O
behind	O
BanglaBERT	B-MethodName
and	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	I-MethodName
large	I-MethodName
)	I-MethodName
.	O

BanglaBERT	B-MethodName
is	O
not	O
only	O
superior	O
in	O
performance	O
but	O
also	O
substantially	O
compute	O
-	O
and	O
memory	O
-	O
efficient	O
.	O

For	O
instance	O
,	O
it	O
may	O
seem	O
that	O
sahajBERT	B-MethodName
is	O
more	O
efficient	O
than	O
BanglaBERT	B-MethodName
due	O
to	O
its	O
smaller	O
size	O
,	O
but	O
it	O
takes	O
2	O
-	O
3.5x	O
time	O
and	O
2.4	O
-	O
3.33x	O
memory	O
as	O
BanglaBERT	B-MethodName
to	O
fine	O
-	O
tune	O
.	O

To	O
assess	O
the	O
sample	O
efficiency	O
of	O
BanglaBERT	B-MethodName
,	O
we	O
limit	O
the	O
number	O
of	O
training	O
samples	O
and	O
see	O
how	O
it	O
fares	O
against	O
other	O
models	O
.	O

We	O
compare	O
it	O
with	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	I-MethodName
large	I-MethodName
)	I-MethodName
and	O
plot	O
their	O
performances	O
on	O
the	O
SC	O
and	O
NLI	O
tasks	O
7	O
for	O
different	O
sample	O
size	O
in	O
Figure	O
1.Results	O
show	O
that	O
when	O
we	O
have	O
fewer	O
number	O
of	O
samples	O
(	O
≤	O
1k	O
)	O
,	O
BanglaBERT	B-MethodName
has	O
substantially	O
better	O
performance	O
(	O
2	O
-	O
9	O
%	O
on	O
SC	O
and	O
6	O
-	O
10	O
%	O
on	O
NLI	O
with	O
p	O
<	O
0.05	O
)	O
over	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	I-MethodName
large	I-MethodName
)	I-MethodName
,	O
making	O
it	O
more	O
practically	O
applicable	O
for	O
resource	O
-	O
scarce	O
downstream	O
tasks	O
.	O

Creating	O
language	O
-	O
specific	O
models	O
is	O
often	O
infeasible	O
for	O
low	O
-	O
resource	O
languages	O
lacking	O
ample	O
data	O
.	O

7	O
Results	O
for	O
the	O
other	O
tasks	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O

To	O
this	O
end	O
,	O
we	O
introduced	O
BanglaBERT	B-MethodName
and	O
BanglishBERT	B-MethodName
,	O
two	O
NLU	B-TaskName
models	O
in	O
Bangla	O
,	O
a	O
widely	O
spoken	O
yet	O
lowresource	O
language	O
.	O

We	O
presented	O
new	O
downstream	O
datasets	O
on	O
NLI	O
and	O
QA	O
,	O
and	O
established	O
the	O
BLUB	B-DatasetName
benchmark	O
,	O
setting	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
with	O
BanglaBERT	B-MethodName
.	O

In	O
future	O
,	O
we	O
will	O
include	O
other	O
Bangla	B-TaskName
NLU	I-TaskName
benchmarks	O
(	O
e.g.	O
,	O
dependency	O
parsing	O
(	O
de	O
Marneffe	O
et	O
al	O
.	O
,	O

2021	O
)	O
)	O
in	O
BLUB	B-DatasetName
and	O
investigate	O
the	O
benefits	O
of	O
initializing	O
Bangla	O
NLG	O
models	O
from	O
BanglaBERT	B-MethodName
.	O

Dataset	O
and	O
Model	O
Release	O
The	O
Copy	O
Right	O
Act	O
,	O
2000	O
8	O
of	O
Bangladesh	O
allows	O
reproduction	O
and	O
public	O
release	O
of	O
copy	O
-	O
right	O
materials	O
for	O
noncommercial	O
research	O
purposes	O
.	O

As	O
a	O
transformative	O
research	O
work	O
,	O
we	O
will	O
release	O
BanglaBERT	B-MethodName
under	O
a	O
non	O
-	O
commercial	O
license	O
.	O

Similar	O
results	O
are	O
also	O
observed	O
here	O
for	O
the	O
NER	O
task	O
,	O
where	O
BanglaBERT	B-MethodName
is	O
more	O
sampleefficient	O
when	O
we	O
have	O
≤	O
1k	O
training	O
samples	O
.	O

To	O
validate	O
that	O
BanglaBERT	B-MethodName
is	O
more	O
efficient	O
in	O
terms	O
of	O
memory	O
and	O
compute	O
,	O
we	O
measured	O
each	O
model	O
's	O
training	O
time	O
and	O
memory	O
usage	O
during	O
the	O
fine	O
-	O
tuning	O
of	O
each	O
task	O
.	O

We	O
use	O
relative	O
time	O
and	O
memory	O
(	O
GPU	O
VRAM	O
)	O
usage	O
considering	O
those	O
of	O
BanglaBERT	B-MethodName
as	O
units	O
.	O

The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
for	O
structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
casts	O
the	O
task	O
as	O
a	O
dependency	O
parsing	O
problem	O
,	O
which	O
has	O
some	O
limitations	O
:	O
(	O
1	O
)	O
The	O
label	O
proportions	O
for	O
span	O
prediction	O
and	O
span	O
relation	O
prediction	O
are	O
imbalanced	O
.	O
(	O

3	O
)	O
Two	O
nodes	O
in	O
a	O
dependency	B-MethodName
graph	I-MethodName
can	O
not	O
have	O
multiple	O
arcs	O
,	O
therefore	O
some	O
overlapped	O
sentiment	O
tuples	O
can	O
not	O
be	O
recognized	O
.	O

1	O
Structured	B-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
(	O
SSA	B-TaskName
)	O
,	O
which	O
aims	O
to	O
predict	O
a	O
structured	O
sentiment	O
graph	O
as	O
shown	O
in	O
Figure	O
1(a	O
)	O
,	O
can	O
be	O
formulated	O
into	O
the	O
problem	O
of	O
tuple	O
extraction	O
,	O
where	O
a	O
tuple	O
(	O
h	O
,	O
e	O
,	O
t	O
,	O
p	O
)	O
denotes	O
a	O
holder	O
h	O
who	O
expressed	O
an	O
expression	O
e	O
towards	O
a	O
target	O
t	O
with	O
a	O
polarity	O
p.	O
SSA	B-TaskName
is	O
a	O
more	O
challenging	O
task	O
,	O
because	O
other	O
related	O
tasks	O
only	O
focus	O
†	O
Corresponding	O
author	O
1	O
Our	O
code	O
is	O
available	O
at	O
https://github.com/	O
Xgswlg	O
/	O
TGLS	O
Moscow	O
Government	O
has	O
expressed	O
the	O
wish	O
to	O
import	O
the	O
Mongolian	O
meat	O
The	O
head	O
-	O
first	O
parsing	O
graph	O
proposed	O
by	O
Barnes	O
et	O
al	O
.	O
(	O

c	O
)	O
Our	O
proposed	O
essential	O
label	O
set	O
,	O
which	O
has	O
more	O
balanced	O
label	O
distribution	O
for	O
holder	O
,	O
target	O
or	O
expression	O
span	O
prediction	O
and	O
their	O
linking	O
relation	O
prediction.on	O
extracting	O
part	O
of	O
tuple	O
components	O
or	O
the	O
text	O
spans	O
of	O
the	O
components	O
are	O
short	O
.	O

The	O
stateof	O
-	O
the	O
-	O
art	O
SSA	B-TaskName
model	O
is	O
proposed	O
by	O
Barnes	O
et	O
al	O
.	O
(	O

2021	O
)	O
,	O
which	O
casts	O
the	O
SSA	B-TaskName
task	O
as	O
the	O
dependency	O
parsing	O
problem	O
and	O
predicts	O
all	O
tuple	O
components	O
as	O
a	O
dependency	B-MethodName
graph	I-MethodName
(	O
Figure	O
1(b)).However	O
,	O
their	O
method	O
exists	O
some	O
shortages	O
.	O

Such	O
imbalanced	O
labeling	O
strat	O
-	O
Moscow	O
Government	O
has	O
expressed	O
the	O
wish	O
to	O
import	O
the	O
Mongolian	O
meat	O
egy	O
will	O
make	O
the	O
model	O
pay	O
more	O
attention	O
on	O
span	O
prediction	O
but	O
less	O
on	O
span	O
relation	O
prediction	O
.	O

Furthermore	O
,	O
since	O
the	O
span	O
lengths	O
of	O
sentiment	O
tuple	O
components	O
may	O
be	O
very	O
large	O
in	O
the	O
SSA	B-TaskName
task	O
,	O
the	O
label	O
imbalanced	O
problem	O
will	O
become	O
more	O
severe	O
.	O

2021	O
)	O
,	O
we	O
propose	O
a	O
novel	O
labeling	O
strategy	O
that	O
consists	O
of	O
two	O
parts	O
:	O
First	O
,	O
we	O
design	O
a	O
set	O
of	O
labels	O
called	O
essential	O
label	O
set	O
(	O
Figure	O
1(c	O
)	O
)	O
,	O
which	O
can	O
be	O
considered	O
as	O
the	O
basic	O
label	O
set	O
for	O
decoding	O
SSA	B-TaskName
tuples	O
,	O
since	O
it	O
only	O
includes	O
the	O
labels	O
to	O
tag	O
the	O
boundary	O
tokens	O
of	O
spans	O
.	O

However	O
,	O
the	O
labels	O
related	O
to	O
recognize	O
nonboundary	O
tokens	O
of	O
SSA	B-TaskName
components	O
are	O
also	O
important	O
.	O

To	O
well	O
collaborate	O
with	O
our	O
labeling	O
strategy	O
,	O
we	O
also	O
propose	O
an	O
effective	O
token	O
graph	O
model	O
,	O
namely	O
TGLS	B-MethodName
(	O
Token	B-MethodName
Graph	I-MethodName
with	I-MethodName
a	I-MethodName
novel	I-MethodName
Labeling	I-MethodName
Strategy	I-MethodName
)	O
,	O
which	O
uses	O
rich	O
features	O
such	O
as	O
word	O
,	O
part	O
-	O
of	O
-	O
speech	O
tags	O
and	O
characters	O
as	O
inputs	O
and	O
yields	O
contextualized	O
word	O
representations	O
by	O
BiLSTM	O
and	O
multilingual	O
BERT	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

In	O
the	O
prediction	O
layer	O
,	O
we	O
introduce	O
a	O
novel	O
adaptive	O
multi	O
-	O
label	O
classifier	O
to	O
extract	O
all	O
the	O
sentiment	O
tuples	O
no	O
matter	O
that	O
they	O
are	O
overlapped	O
or	O
not	O
.	O

We	O
conduct	O
extensive	O
experiments	O
on	O
five	O
benchmarks	O
,	O
including	O
NoReC	B-DatasetName
Fine	I-DatasetName
(	O
Øvrelid	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
MultiB	B-DatasetName
EU	I-DatasetName
,	O
MultiB	B-DatasetName
CA	I-DatasetName
(	O
Barnes	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
MPQA	B-DatasetName
(	O
Wiebe	O
et	O
al	O
.	O
,	O

2005	O
)	O
and	O
DS	B-DatasetName
Unis	I-DatasetName
(	O
Toprak	O
et	O
al	O
.	O
,	O

The	O
resluts	O
show	O
that	O
our	O
TGLS	B-MethodName
model	O
outperforms	O
the	O
SOTA	O
model	O
by	O
a	O
large	O
margin	O
.	O

We	O
propose	O
an	O
effective	O
token	O
graph	O
model	O
to	O
well	O
collaborate	O
with	O
our	O
labeling	O
strategy	O
,	O
which	O
learns	O
the	O
token	O
-	O
token	O
relations	O
via	O
multi	O
-	O
view	O
token	O
graph	O
networks	O
and	O
reasons	O
the	O
labels	O
between	O
each	O
pair	O
of	O
words	O
using	O
the	O
adaptive	O
multi	O
-	O
label	O
classifier	O
for	O
both	O
overlapped	O
and	O
non	O
-	O
overlapped	O
tuple	O
extraction	O
.	O
•	O

The	O
experimental	O
results	O
show	O
that	O
our	O
model	O
has	O
achieved	O
the	O
SOTA	O
performance	O
in	O
5	O
datasets	O
for	O
structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
,	O
especially	O
in	O
terms	O
of	O
the	O
end	O
-	O
to	O
-	O
end	O
sentiment	O
tuple	O
extraction	O
.	O

The	O
task	O
of	O
the	O
Structured	B-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
(	O
SSA	B-TaskName
)	O
can	O
be	O
divided	O
into	O
sub	O
-	O
tasks	O
such	O
as	O
span	O
extraction	O
of	O
the	O
holder	O
,	O
target	O
and	O
expression	O
,	O
relation	O
prediction	O
between	O
these	O
elements	O
and	O
assigning	O
polarity	O
.	O

Some	O
existing	O
works	O
in	O
Opinion	O
Mining	O
used	O
pipeline	O
methods	O
to	O
first	O
extract	O
spans	O
and	O
then	O
the	O
relations	O
mostly	O
on	O
the	O
MPQA	B-DatasetName
dataset	O
(	O
Wiebe	O
et	O
al	O
.	O
,	O

For	O
example	O
,	O
Katiyar	O
and	O
Cardie	O
(	O
2016	O
)	O
propose	O
a	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
which	O
is	O
the	O
first	O
such	O
attempt	O
using	O
a	O
deep	O
learning	O
approach	O
,	O
Zhang	O
et	O
al	O
.	O
(	O

2019	O
)	O
propose	O
a	O
transitionbased	O
model	O
which	O
identifies	O
opinion	O
elements	O
by	O
the	O
human	O
-	O
designed	O
transition	O
actions	O
,	O
and	O
Xia	O
et	O
al	O
.	O
(	O

However	O
,	O
all	O
of	O
these	O
works	O
ignore	O
the	O
polarity	O
classification	O
sub	O
-	O
task	O
.	O

However	O
,	O
Wang	O
et	O
al	O
.	O
(	O

2016	O
)	O
only	O
annotate	O
sentiment	O
-	O
bearing	O
words	O
not	O
phrases	O
and	O
do	O
not	O
specify	O
the	O
relationship	O
between	O
target	O
and	O
expression	O
,	O
it	O
therefore	O
may	O
not	O
be	O
adequate	O
for	O
full	O
structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
.	O

2021	O
)	O
propose	O
a	O
unified	O
approach	O
in	O
which	O
they	O
formulate	O
the	O
structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
task	O
into	O
a	O
dependency	B-MethodName
graph	I-MethodName
parsing	O
task	O
and	O
jointly	O
predicts	O
all	O
components	O
of	O
a	O
sentiment	O
graph	O
.	O

In	O
our	O
work	O
,	O
we	O
modify	O
the	O
handshaking	O
tagging	O
scheme	O
to	O
use	O
it	O
for	O
SSA	B-TaskName
.	O

Furthermore	O
,	O
since	O
the	O
component	O
span	O
of	O
this	O
task	O
is	O
relatively	O
long	O
,	O
only	O
utilizing	O
the	O
boundary	O
tokens	O
can	O
not	O
make	O
full	O
use	O
of	O
the	O
annotation	O
information	O
,	O
so	O
we	O
propose	O
a	O
new	O
label	O
set	O
called	O
whole	O
label	O
set	O
,	O
which	O
together	O
with	O
essential	O
label	O
set	O
constitutes	O
our	O
labeling	O
strategy	O
.	O

the	O
essential	O
label	O
set	O
consists	O
of	O
the	O
following	O
labels:•	O
Holder	O
:	O
Moscow	O
→	O
government	O
•	O
Exp	O
:	O
Neutral	O
:	O
expressed	O
→	O
Moscow	O
•	O
Target	O
:	O
import	O
→	O
meat	O
•	O
Exp	O
Head	O
to	O
Holder	O
Head	O
:	O
expressed	O
→	O
Moscow	O
•	O
Exp	O
Tail	O
to	O
Holder	O
Tail	O
:	O
wish	O
→	O
government	O
•	O
Exp	O
Head	O
to	O
Target	O
Head	O
:	O
expressed	O
→	O
import	O
•	O
Exp	O
Tail	O
to	O
Target	O
Tail	O
:	O
wish	O
→	O
meat	O
where	O
the	O
Holder	O
,	O
Exp	O
.	O

Our	O
whole	O
label	O
set	O
involves	O
both	O
the	O
labels	O
related	O
to	O
boundary	O
and	O
non	O
-	O
boundary	O
tokens	O
,	O
as	O
well	O
as	O
From	O
left	O
to	O
right	O
,	O
the	O
first	O
is	O
an	O
encoder	O
to	O
yield	O
contextualized	O
word	O
representations	O
from	O
input	O
sentences	O
,	O
and	O
the	O
next	O
is	O
a	O
graph	O
layer	O
where	O
we	O
produce	O
attention	O
scoring	O
matrices	O
by	O
whole	O
label	O
prediction	O
.	O

Then	O
we	O
build	O
a	O
multi	O
-	O
hop	O
reasoning	O
layer	O
and	O
refine	O
token	O
representations	O
.	O

Finally	O
,	O
a	O
prediction	O
layer	O
is	O
leveraged	O
for	O
reasoning	O
the	O
relations	O
in	O
essential	O
labels	O
and	O
based	O
on	O
which	O
we	O
decode	O
all	O
components	O
of	O
an	O
opinion	O
tuple.the	O
labels	O
related	O
to	O
[	O
CLS	O
]	O
and	O
all	O
tokens	O
in	O
the	O
sentiment	O
tuples	O
.	O

After	O
decoding	O
all	O
the	O
component	O
pairs	O
,	O
we	O
enumerate	O
all	O
possible	O
triples	O
from	O
pairs	O
with	O
the	O
same	O
expression	O
,	O
thus	O
finally	O
decode	O
all	O
the	O
sentiment	O
tuples	O
.	O

In	O
this	O
section	O
,	O
We	O
formally	O
present	O
our	O
proposed	O
TGLS	B-MethodName
model	O
in	O
detail	O
(	O
Figure	O
3	O
)	O
,	O
which	O
mainly	O
consists	O
of	O
four	O
parts	O
,	O
the	O
encoder	O
layer	O
,	O
the	O
multiview	O
token	O
graph	O
as	O
the	O
hidden	O
layer	O
,	O
the	O
adaptive	O
multi	O
-	O
label	O
classifier	O
as	O
the	O
prediction	O
layer	O
and	O
the	O
hierarchical	O
learning	O
strategy	O
to	O
train	O
our	O
model	O
.	O

Consider	O
the	O
i	O
th	O
token	O
in	O
a	O
sentence	O
with	O
n	O
tokens	O
,	O
we	O
represent	O
it	O
by	O
concatenating	O
its	O
token	O
embedding	O
e	O
word	O
i	O
,	O
part	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
embedding	O
e	O
pos	O
i	O
,	O
lemma	O
embedding	O
e	O
lemma	O
i	O
,	O
and	O
character	O
-	O
level	O
embedding	O
e	O
char	O
i	O
together	O
:	O
w	O
i	O
=	O
e	O
word	O
i	O
⊕	O
e	O
pos	O
i	O
⊕	O
e	O
lemma	O
i	O
⊕	O
e	O
char	O
i	O
(	O
1)where	O
⊕	O
denotes	O
the	O
concatenation	O
operation	O
.	O

Without	O
loss	O
of	O
generality	O
,	O
we	O
employS	O
G	O
=	O
{	O
S	O
G	O
o	O
,	O
S	O
G	O
s	O
,	O
S	O
G	O
r	O
,	O
S	O
G	O
c	O
}	O
unifiedly	O
to	O
represent	O
the	O
four	O
matrices	O
.	O

The	O
process	O
that	O
the	O
whole	O
label	O
set	O
learnt	O
by	O
attention	O
scoring	O
matrices	O
S	O
G	O
s	O
,	O
S	O
G	O
r	O
and	O
S	O
G	O
c	O
through	O
a	O
multilabel	O
adaptive	O
-	O
threshold	O
loss	O
will	O
be	O
introduced	O
in	O
Section	O
4.4	O
.	O

Firstly	O
,	O
we	O
take	O
a	O
shortcut	O
connection	O
between	O
the	O
outputs	O
of	O
the	O
encoder	O
layer	O
and	O
graph	O
layer	O
to	O
get	O
the	O
final	O
representation	O
c	O
i	O
=	O
h	O
i	O
⊕	O
u	O
i	O
for	O
each	O
token	O
.	O

And	O
by	O
taking	O
c	O
i	O
as	O
the	O
input	O
,	O
we	O
calculate	O
the	O
attention	O
scoring	O
matrices	O
S	O
P	O
based	O
on	O
the	O
mechanism	O
of	O
attention	O
scoring	O
(	O
cf	O
.	O

Eq.(4	O
)	O
,	O
Eq.(5	O
)	O
and	O
Eq.(6)):S	O
P	O
=	O
{	O
S	O
P	O
r	O
|r	O
∈	O
R	O
e	O
}	O
(	O
9)where	O
superscript	O
P	O
denotes	O
the	O
prediciton	O
layer	O
,	O
R	O
e	O
denotes	O
the	O
essential	O
label	O
set	O
.	O

Eq.(10	O
)	O
)	O
in	O
the	O
hidden	O
layer	O
,	O
where	O
we	O
compute	O
all	O
the	O
token	O
pair	O
dependent	O
threshold	O
T	O
H	O
G	O
=	O
T	O
H	O
G	O
ij	O
|1	O
≤	O
i	O
,	O
j	O
≤	O
n	O
by	O
taking	O
the	O
token	O
representation	O
h	O
i	O
and	O
h	O
j	O
as	O
the	O
input	O
.	O

2021	O
)	O
,	O
we	O
perform	O
experiments	O
on	O
five	O
structured	O
sentiment	O
datasets	O
in	O
four	O
languages	O
,	O
including	O
multi	O
-	O
domain	O
professional	O
reviews	O
NoReC	B-DatasetName
Fine	I-DatasetName
(	O
Øvrelid	O
et	O
al	O
.	O
,	O

2020	O
)	O
in	O
Norwegian	O
,	O
hotel	O
reviews	O
MultiB	B-DatasetName
EU	I-DatasetName
and	O
MultiB	B-DatasetName
CA	I-DatasetName
(	O
Barnes	O
et	O
al	O
.	O
,	O

2018	O
)	O
in	O
Basque	O
and	O
Catalan	O
respectively	O
,	O
news	O
MPQA	B-DatasetName
(	O
Wiebe	O
et	O
al	O
.	O
,	O

2005	O
)	O
in	O
English	O
and	O
reviews	O
of	O
online	O
universities	O
and	O
e	O
-	O
commerce	O
DS	B-DatasetName
Unis	I-DatasetName
(	O
Toprak	O
et	O
al	O
.	O
,	O

2018	O
)	O
Table	O
2	O
:	O
Main	O
experimental	O
results	O
of	O
our	O
TGLS	B-MethodName
model	O
and	O
comparison	O
with	O
previous	O
works	O
.	O

The	O
baseline	O
results	O
with	O
"	O
*	O
"	O
are	O
from	O
our	O
reimplementation	O
,	O
the	O
others	O
are	O
from	O
(	O
Barnes	O
et	O
al	O
.	O
,	O

The	O
char	B-HyperparameterName
embedding	I-HyperparameterName
size	I-HyperparameterName
is	O
set	O
to	O
100	B-HyperparameterValue
.	O

The	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
embeddings	O
and	O
other	O
network	O
components	O
are	O
set	O
to	O
0.4	B-HyperparameterValue
and	O
0.3	B-HyperparameterValue
respectively	O
.	O

We	O
employ	O
4layer	O
BiLSTMs	O
with	O
the	O
output	B-HyperparameterName
size	I-HyperparameterName
set	O
to	O
400	B-HyperparameterValue
and	O
2	O
-	O
layer	O
for	O
multi	O
-	O
hop	O
reasoning	O
with	O
output	B-HyperparameterName
size	I-HyperparameterName
set	O
to	O
768	B-HyperparameterValue
.	O

The	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
is	O
3e-5	B-HyperparameterValue
and	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
8	B-HyperparameterValue
.	O

The	O
hyperparameter	O
α	B-HyperparameterName
in	O
Eq.13	O
is	O
set	O
to	O
0.25	B-HyperparameterValue
(	O
cf	O
.	O

We	O
use	O
GeForce	O
RTX	O
3090	O
to	O
train	O
our	O
model	O
for	O
at	O
most	O
100	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
choose	O
the	O
model	O
with	O
the	O
highest	O
SF1	O
score	O
on	O
the	O
validation	O
set	O
to	O
output	O
results	O
on	O
the	O
test	O
set	O
.	O

We	O
compare	O
our	O
proposed	O
model	O
with	O
three	O
stateof	O
-	O
the	O
-	O
art	O
baselines	O
which	O
outperform	O
other	O
models	O
in	O
all	O
datasets	O
:	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
Chen	O
and	O
Qian	O
(	O
2020	O
)	O
propose	O
a	O
relation	O
-	O
aware	O
collaborative	O
learning	O
framework	O
for	O
end2end	O
sentiment	O
analysis	O
which	O
models	O
the	O
interactive	O
relations	O
between	O
each	O
pair	O
of	O
sub	O
-	O
tasks	O
(	O
target	O
extraction	O
,	O
expression	O
extraction	O
,	O
sentiment	O
classification	O
)	O
.	O

2021	O
)	O
reimplement	O
the	O
RACL	B-MethodName
as	O
a	O
baseline	O
for	O
SSA	B-TaskName
task	O
in	O
their	O
work	O
.	O

Head	B-MethodName
-	I-MethodName
first	I-MethodName
and	O
Head	B-MethodName
-	I-MethodName
final	I-MethodName
5	O
Barnes	O
et	O
al	O
.	O
(	O

2021	O
)	O
cast	O
the	O
structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
as	O
a	O
dependency	O
parsing	O
task	O
and	O
apply	O
a	O
reimplementation	O
of	O
the	O
neural	O
parser	O
by	O
Dozat	O
and	O
Manning	O
(	O
2018	O
)	O
,	O
where	O
the	O
main	O
architecture	O
of	O
the	O
model	O
is	O
based	O
on	O
a	O
biaffine	O
classifier	O
.	O

The	O
Head	B-MethodName
-	I-MethodName
first	I-MethodName
and	O
Head	B-MethodName
final	I-MethodName
are	O
two	O
models	O
with	O
different	O
setups	O
in	O
the	O
parsing	O
graph	O
.	O

2021	O
)	O
,	O
we	O
use	O
the	O
Span	B-MetricName
F1	I-MetricName
,	O
Targeted	B-MetricName
F1	I-MetricName
and	O
two	O
Sentiment	O
Graph	O
Metrics	O
to	O
measure	O
the	O
experimental	O
results	O
.	O

In	O
detail	O
,	O
Span	B-MetricName
F1	I-MetricName
evaluates	O
how	O
well	O
these	O
models	O
are	O
able	O
to	O
identify	O
the	O
holders	O
,	O
targets	O
,	O
and	O
expressions	O
.	O

Targeted	B-MetricName
F1	I-MetricName
requires	O
the	O
exact	O
extraction	O
of	O
the	O
correct	O
target	O
,	O
and	O
the	O
corresponding	O
polarity	O
.	O

Sentiment	O
Graph	O
Metrics	O
include	O
two	O
F1	O
score	O
,	O
Non	B-MetricName
-	I-MetricName
polar	I-MetricName
Sentiment	I-MetricName
Graph	I-MetricName
F1	I-MetricName
(	O
NSF1	B-MetricName
)	O
and	O
Sentiment	B-MetricName
Graph	I-MetricName
F1	I-MetricName
(	O
SF1	B-MetricName
)	O
,	O
which	O
aims	O
to	O
measure	O
the	O
overall	O
performance	O
of	O
a	O
model	O
to	O
capture	O
the	O
full	O
sentiment	O
graph	O
(	O
Figure	O
1(a	O
)	O
)	O
.	O

For	O
NSF1	B-MetricName
,	O
each	O
sentiment	O
graph	O
is	O
a	O
tuple	O
of	O
(	O
holder	O
,	O
target	O
,	O
expres-	O
sion	O
)	O
,	O
while	O
SF1	B-MetricName
adds	O
the	O
polarity	O
(	O
holder	O
,	O
target	O
,	O
expression	O
,	O
polarity	O
)	O
.	O

Moreover	O
,	O
for	O
ease	O
of	O
analysis	O
,	O
we	O
add	O
an	O
Overall	B-MetricName
Span	I-MetricName
F1	I-MetricName
score	O
which	O
evaluates	O
how	O
well	O
these	O
models	O
are	O
able	O
to	O
identify	O
all	O
three	O
elements	O
of	O
a	O
sentiment	O
graph	O
with	O
token	O
-	O
level	O
F1	O
score	O
.	O

In	O
this	O
section	O
,	O
we	O
introduce	O
the	O
main	O
experimental	O
results	O
compared	O
with	O
three	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Chen	O
and	O
Qian	O
,	O
2020	O
)	O
,	O
Head	B-MethodName
-	I-MethodName
first	I-MethodName
and	O
Head	B-MethodName
-	I-MethodName
final	I-MethodName
models	O
(	O
Barnes	O
et	O
al	O
.	O
,	O

2021).Table	O
2	O
shows	O
that	O
in	O
most	O
cases	O
our	O
model	O
performs	O
better	O
than	O
other	O
baselines	O
in	O
terms	O
of	O
the	O
Span	B-MetricName
F1	I-MetricName
metrics	O
across	O
all	O
datasets	O
.	O

The	O
average	O
improvement	O
(	O
↑	O
1.4	B-MetricValue
)	O
in	O
Overall	B-MetricName
Span	I-MetricName
F1	I-MetricName
score	O
proves	O
the	O
effectiveness	O
of	O
our	O
model	O
in	O
span	O
extraction	O
.	O

Besides	O
,	O
there	O
exists	O
some	O
significant	O
improvements	O
such	O
as	O
extracting	O
holder	O
on	O
DS	B-DatasetName
Unis	I-DatasetName
(	O
↑6.3	B-MetricValue
)	O
and	O
extracting	O
expression	O
on	O
NoReC	B-DatasetName
Fine	I-DatasetName
(	O
↑4.7	B-MetricValue
)	O
,	O
but	O
the	O
extracting	O
expression	O
on	O
DS	B-DatasetName
Unis	I-DatasetName
(	O
↓2.9	B-MetricValue
)	O
are	O
poor	O
.	O

As	O
for	O
the	O
metric	O
of	O
Targeted	B-MetricName
F1	I-MetricName
,	O
although	O
the	O
Head	O
-	O
first	O
model	O
performs	O
well	O
on	O
MPQA	B-DatasetName
,	O
our	O
TGLS	B-MethodName
model	O
is	O
obviously	O
more	O
robust	O
as	O
we	O
achieves	O
superior	O
performance	O
on	O
other	O
4	O
datasets	O
.	O

There	O
are	O
also	O
extremely	O
significant	O
improvements	O
such	O
as	O
on	O
NoReC	B-DatasetName
Fine	I-DatasetName
(	O
↑6.2	B-MetricValue
)	O
and	O
on	O
MultiB	B-DatasetName
CA	I-DatasetName
(	O
↑5.6	B-MetricValue
)	O
,	O
it	O
proves	O
the	O
capacity	O
of	O
our	O
model	O
in	O
exact	O
prediction	O
of	O
target	O
and	O
the	O
corresponding	O
polar	O
.	O

As	O
for	O
the	O
Sentiment	O
Graph	O
metrics	O
,	O
which	O
are	O
important	O
for	O
comprehensively	O
examining	O
span	O
,	O
relation	O
and	O
polar	O
predictions	O
,	O
our	O
TGLS	B-MethodName
model	O
achieves	O
superior	O
performance	O
throughout	O
all	O
datasets	O
in	O
both	O
NSF1	B-MetricName
and	O
SF1	B-MetricName
score	O
,	O
especially	O
on	O
NoReC	B-DatasetName
Fine	I-DatasetName
(	O
↑7.2	B-MetricValue
and	O
↑6.4	B-MetricValue
)	O
.	O

And	O
the	O
average	O
improvement	O
(	O
↑4.5	B-MetricValue
)	O
in	O
SF1	B-MetricName
score	O
verifies	O
the	O
excellent	O
ability	O
of	O
our	O
model	O
in	O
the	O
end	O
-	O
to	O
-	O
end	O
sentiment	O
tuple	O
extraction	O
.	O

In	O
this	O
section	O
,	O
we	O
conduct	O
extensive	O
ablation	O
studies	O
on	O
NoReC	B-DatasetName
Fine	I-DatasetName
to	O
better	O
understand	O
independent	O
contributions	O
of	O
different	O
components	O
in	O
terms	O
of	O
span	B-MetricName
overall	I-MetricName
F1	I-MetricName
,	O
targeted	B-MetricName
F1	I-MetricName
and	O
SF1	B-MetricName
scores	O
.	O

As	O
we	O
assumed	O
,	O
the	O
span	O
graph	O
makes	O
more	O
contribution	O
to	O
the	O
performance	O
of	O
span	O
extraction	O
(	O
Span	B-MetricName
Overall	I-MetricName
F1	I-MetricName
)	O
while	O
the	O
relation	O
graph	O
contributes	O
more	O
to	O
end	O
-	O
toend	O
sentiment	O
tuple	O
extraction	O
(	O
SF1	B-MetricName
)	O
.	O

And	O
we	O
also	O
observe	O
that	O
the	O
vanilla	O
GAT	O
graph	O
makes	O
considerable	O
improvement	O
in	O
SF1	B-MetricName
score	O
.	O

Then	O
,	O
we	O
test	O
the	O
effectiveness	O
of	O
the	O
Rotary	O
Position	O
Embedding	O
(	O
RoPE	O
)	O
(	O
Su	O
et	O
al	O
.	O
,	O

The	O
results	O
in	O
Table	O
3	O
demonstrate	O
that	O
RoPE	O
can	O
make	O
our	O
model	O
more	O
sensitive	O
to	O
the	O
relative	O
positional	O
information	O
since	O
it	O
significantly	O
improves	O
the	O
performance	O
of	O
exact	O
target	O
extraction	O
(	O
Targeted	B-MetricName
F1).Last	I-MetricName
,	O
we	O
replace	O
the	O
adaptive	O
threshold	O
with	O
fixed	O
global	O
threshold	O
,	O
and	O
we	O
observe	O
that	O
the	O
performance	O
drops	O
drastically	O
in	O
all	O
three	O
metrics	O
,	O
it	O
suggests	O
that	O
the	O
adaptive	O
thresholding	O
mechanism	O
is	O
very	O
crucial	O
for	O
our	O
model	O
since	O
the	O
flexibility	O
can	O
allow	O
our	O
model	O
to	O
selectively	O
learn	O
more	O
useful	O
information	O
for	O
SSA	B-TaskName
task	O
from	O
whole	O
labels	O
.	O

In	O
this	O
section	O
we	O
perform	O
a	O
deeper	O
analysis	O
on	O
the	O
models	O
in	O
order	O
to	O
answer	O
three	O
research	O
questions:6.1	O
Does	O
our	O
modeling	O
strategy	O
mitigate	O
the	O
label	O
imbalance	O
problem	O
in	O
span	O
prediction	O
and	O
span	O
relation	O
prediction?Experimental	O
results	O
in	O
Table	O
2	O
show	O
that	O
our	O
model	O
performs	O
significantly	O
better	O
in	O
the	O
SF1	B-MetricName
score	O
,	O
which	O
to	O
some	O
extent	O
proves	O
that	O
our	O
model	O
can	O
ensure	O
the	O
efficiency	O
of	O
relation	O
extraction	O
.	O

13?In	O
this	O
section	O
,	O
we	O
experiment	O
on	O
five	O
datasets	O
to	O
heuristically	O
search	O
for	O
the	O
appropriate	O
value	O
of	O
hyperparameter	O
α	B-HyperparameterName
(	O
cf	O
.	O

13	O
)	O
)	O
.	O

Figure	O
4	O
shows	O
that	O
all	O
datasets	O
achieve	O
higher	O
SF1	B-MetricName
score	O
with	O
α	B-HyperparameterName
between	O
0.1	B-HyperparameterValue
and	O
0.5	B-HyperparameterValue
.	O

We	O
ended	O
up	O
fixing	O
alpha	B-HyperparameterName
to	O
0.25	B-HyperparameterValue
,	O
since	O
most	O
datasets	O
yield	O
optimal	O
results	O
around	O
this	O
value	O
.	O

In	O
addition	O
,	O
it	O
is	O
worth	O
noting	O
that	O
when	O
α	B-HyperparameterName
is	O
set	O
to	O
0	B-HyperparameterValue
,	O
which	O
means	O
that	O
the	O
whole	O
labels	O
are	O
completely	O
removed	O
,	O
the	O
performance	O
drops	O
a	O
lot	O
,	O
which	O
once	O
again	O
proves	O
the	O
effectiveness	O
of	O
learning	O
whole	O
labels	O
in	O
the	O
hidden	O
layer.6.3	O
Is	O
the	O
whole	O
label	O
set	O
helpful	O
for	O
long	O
span	O
identification?In	O
this	O
section	O
,	O
we	O
experiment	O
on	O
NoReC	B-DatasetName
Fine	I-DatasetName
to	O
further	O
explore	O
whether	O
whole	O
labels	O
contribute	O
to	O
long	O
span	O
identification	O
.	O

Figure	O
5(a	O
)	O
evaluates	O
the	O
Expression	B-MetricName
F1	I-MetricName
scores	O
regarding	O
to	O
different	O
expression	O
lengths	O
,	O
we	O
can	O
find	O
that	O
whole	O
labels	O
helps	O
most	O
on	O
those	O
expressions	O
with	O
longer	O
length	O
.	O

In	O
Figure	O
5(b	O
)	O
,	O
we	O
also	O
report	O
the	O
SF1	B-MetricName
scores	O
regarding	O
to	O
different	O
distances	O
,	O
that	O
is	O
,	O
from	O
the	O
leftmost	O
token	O
in	O
a	O
tuple	O
to	O
the	O
rightmost	O
token	O
,	O
which	O
shows	O
a	O
similar	O
conclusion	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
token	O
graph	O
model	O
with	O
a	O
novel	O
labeling	O
strategy	O
,	O
consisting	O
of	O
the	O
whole	O
and	O
essential	O
label	O
sets	O
,	O
to	O
extract	O
sentiment	O
tuples	O
for	O
structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
.	O

More	O
importantly	O
,	O
our	O
modeling	O
strategy	O
is	O
able	O
to	O
alleviate	O
the	O
label	O
imbalance	O
problem	O
when	O
using	O
token	O
-	O
graph	O
-	O
based	O
approaches	O
for	O
SSA	B-TaskName
.	O

Experimental	O
results	O
show	O
that	O
our	O
model	O
overwhelmingly	O
outperforms	O
SOTA	O
baselines	O
and	O
improves	O
the	O
performance	O
of	O
identifying	O
the	O
sentiment	O
components	O
with	O
long	O
spans	O
.	O

Spoken	O
language	O
understanding	O
(	O
SLU	O
)	O
tasks	O
involve	O
mapping	O
from	O
speech	O
signals	O
to	O
semantic	O
labels	O
.	O

Given	O
the	O
complexity	O
of	O
such	O
tasks	O
,	O
good	O
performance	O
is	O
expected	O
to	O
require	O
large	O
labeled	O
datasets	O
,	O
which	O
are	O
difficult	O
to	O
collect	O
for	O
each	O
new	O
task	O
and	O
domain	O
.	O

In	O
this	O
work	O
,	O
we	O
focus	O
on	O
low	O
-	O
resource	O
spoken	B-TaskName
named	I-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	O
)	O
and	O
address	O
the	O
question	O
:	O
Beyond	O
self	O
-	O
supervised	O
pre	O
-	O
training	O
,	O
how	O
can	O
we	O
use	O
external	O
speech	O
and/or	O
text	O
data	O
that	O
are	O
not	O
annotated	O
for	O
the	O
task	O
?	O
We	O
consider	O
selftraining	O
,	O
knowledge	O
distillation	O
,	O
and	O
transfer	O
learning	O
for	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
(	O
E2E	B-MethodName
)	O
and	O
pipeline	B-MethodName
(	O
speech	O
recognition	O
followed	O
by	O
text	B-MethodName
NER	I-MethodName
)	O
approaches	O
.	O

Compared	O
to	O
prior	O
work	O
,	O
we	O
find	O
relative	O
improvements	O
in	O
F1	B-MetricName
of	O
up	O
to	O
16	B-MetricValue
%	I-MetricValue
.	O

While	O
the	O
best	O
baseline	O
model	O
is	O
a	O
pipeline	B-MethodName
approach	O
,	O
the	O
best	O
performance	O
using	O
external	O
data	O
is	O
ultimately	O
achieved	O
by	O
an	O
E2E	B-MethodName
model	O
.	O

We	O
provide	O
detailed	O
comparisons	O
and	O
analyses	O
,	O
developing	O
insights	O
on	O
,	O
for	O
example	O
,	O
the	O
effects	O
of	O
leveraging	O
external	O
data	O
on	O
(	O
i	O
)	O
different	O
categories	O
of	O
NER	O
errors	O
and	O
(	O
ii	O
)	O
the	O
switch	O
in	O
performance	O
trends	O
between	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
models	O
.	O

Pipeline	B-MethodName
"	O
refers	O
to	O
approaches	O
consisting	O
of	O
speech	O
recognition	O
followed	O
by	O
a	O
text	B-MethodName
NER	I-MethodName
model	O
;	O
"	O
E2E	B-MethodName
"	O
refers	O
to	O
approaches	O
that	O
directly	O
map	O
from	O
speech	O
to	O
NER	O
-	O
tagged	O
text	O
.	O

The	O
"	O
Baseline	O
"	O
and	O
"	O
Text	B-MethodName
NER	I-MethodName
"	O
numbers	O
are	O
from	O
previously	O
established	O
baselines	O
(	O
Shon	O
et	O
al	O
.	O
,	O

2022).swering	O
(	O
Chen	O
et	O
al	O
.	O
,	O

Spoken	B-TaskName
NER	I-TaskName
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
not	O
as	O
well	O
-	O
studied	O
.	O

It	O
has	O
the	O
added	O
challenges	O
of	O
continuous	O
-	O
valued	O
and	O
longer	O
input	O
sequences	O
and	O
,	O
at	O
the	O
same	O
time	O
,	O
provides	O
opportunities	O
to	O
take	O
advantage	O
of	O
acoustic	O
cues	O
in	O
the	O
input	O
.	O

A	O
recent	O
study	O
(	O
Shon	O
et	O
al	O
.	O
,	O

2022	O
)	O
shows	O
that	O
there	O
is	O
still	O
10	B-MetricValue
-	I-MetricValue
20	I-MetricValue
%	I-MetricValue
absolute	O
degradation	O
in	O
the	O
F1	B-MetricName
scores	O
of	O
spoken	B-TaskName
NER	I-TaskName
models	O
compared	O
to	O
text	O
-	O
based	O
NER	O
using	O
gold	O
transcripts	O
(	O
see	O
Figure	O
1	O
)	O
despite	O
using	O
large	O
pre	O
-	O
trained	O
speech	O
representation	O
models	O
.	O

We	O
benchmark	O
our	O
findings	O
against	O
recently	O
published	O
baselines	O
for	O
NER	O
on	O
the	O
VoxPopuli	B-DatasetName
dataset	O
of	O
European	O
Parliament	O
speech	O
recordings	O
(	O
Shon	O
et	O
al	O
.	O
,	O

Our	O
analysis	O
also	O
quantifies	O
the	O
pros	O
and	O
cons	O
of	O
the	O
pipeline	B-MethodName
(	O
speech	O
recognition	O
followed	O
by	O
text	B-MethodName
NER	I-MethodName
)	O
and	O
end	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
(	I-MethodName
E2E	I-MethodName
)	I-MethodName
approaches	O
.	O

Specific	O
contributions	O
include	O
:	O
(	O
i	O
)	O
Unlike	O
previous	O
work	O
,	O
we	O
devote	O
equal	O
effort	O
to	O
improving	O
both	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
approaches	O
.	O
(	O

ii	O
)	O
We	O
present	O
experiments	O
using	O
various	O
external	O
data	O
types	O
and	O
modeling	O
approaches	O
.	O
(	O

iii	O
)	O
Overall	O
,	O
we	O
obtain	O
F1	B-MetricName
improvements	O
of	O
up	O
to	O
16	B-MetricValue
%	I-MetricValue
for	O
the	O
E2E	B-MethodName
model	O
and	O
6	B-MetricValue
%	I-MetricValue
for	O
the	O
pipeline	B-MethodName
model	O
over	O
previously	O
published	O
baselines	O
,	O
setting	O
a	O
new	O
state	O
of	O
the	O
art	O
for	O
NER	O
on	O
this	O
dataset	O
.	O
(	O

SSR	O
gives	O
relative	O
improvements	O
of	O
36%/31	B-MetricValue
%	B-MetricValue
for	O
pipeline	B-MethodName
/	O
E2E	B-MethodName
models	O
,	O
respectively	O
.	O

v	O
)	O
We	O
establish	O
that	O
E2E	B-MethodName
models	O
outperform	O
pipeline	B-MethodName
approaches	O
on	O
this	O
task	O
,	O
given	O
access	O
to	O
external	O
data	O
,	O
while	O
the	O
baseline	O
models	O
without	O
the	O
external	O
data	O
have	O
the	O
opposite	O
relationship	O
.	O
(	O

vi	O
)	O
We	O
provide	O
a	O
detailed	O
analysis	O
of	O
model	O
behavior	O
,	O
including	O
differences	O
in	O
error	O
types	O
between	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
approaches	O
and	O
the	O
reasoning	O
for	O
the	O
superiority	O
of	O
E2E	B-MethodName
over	O
pipeline	B-MethodName
models	O
when	O
using	O
external	O
data	O
but	O
not	O
in	O
the	O
baseline	O
setting.2	O
Related	O
work	O
Relatively	O
little	O
work	O
has	O
been	O
conducted	O
on	O
spoken	B-TaskName
NER	I-TaskName
(	O
Kim	O
and	O
Woodland	O
,	O
2000;Sudoh	O
et	O
al	O
.	O
,	O

While	O
spoken	B-TaskName
NER	I-TaskName
is	O
commonly	O
done	O
through	O
a	O
pipeline	B-MethodName
approach	O
(	O
Sudoh	O
et	O
al	O
.	O
,	O

2015	O
)	O
,	O
there	O
is	O
rising	O
interest	O
in	O
E2E	B-MethodName
approaches	O
in	O
the	O
speech	O
community	O
Caubrière	O
et	O
al	O
.	O
,	O

2.An	O
early	O
E2E	B-MethodName
spoken	B-TaskName
NER	I-TaskName
model	O
was	O
introduced	O
by	O
.	O

2016	O
)	O
architecture	O
,	O
with	O
the	O
addition	O
of	O
special	O
characters	O
for	O
NER	O
labels	O
around	O
the	O
named	O
entities	O
in	O
the	O
transcription	O
,	O
and	O
is	O
trained	O
with	O
character	O
-	O
level	O
connectionist	O
temporal	O
classification	O
(	O
CTC	O
)	O
(	O
Graves	O
et	O
al	O
.	O
,	O

Yadav	O
et	O
al	O
.	O
(	O

2020	O
)	O
introduced	O
an	O
English	O
speech	O
NER	O
dataset	O
and	O
proposed	O
an	O
E2E	B-MethodName
approach	O
similar	O
to	O
.	O

They	O
show	O
that	O
LM	O
fusion	O
improves	O
the	O
performance	O
of	O
the	O
E2E	B-MethodName
approach	O
.	O

2020	O
)	O
provided	O
a	O
detailed	O
comparison	O
between	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
models	O
;	O
however	O
,	O
they	O
focused	O
on	O
small	O
RNN	O
/	O
CNN	O
models	O
and	O
did	O
not	O
use	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
SSR	O
models	O
.	O

These	O
previous	O
efforts	O
have	O
shown	O
that	O
E2E	B-MethodName
models	O
can	O
outperform	O
pipeline	B-MethodName
approaches	O
in	O
a	O
fully	O
supervised	O
setting	O
.	O

However	O
,	O
these	O
studies	O
do	O
not	O
account	O
for	O
improvements	O
in	O
NLP	O
from	O
self	O
-	O
supervised	O
text	O
representations	O
for	O
their	O
pipeline	B-MethodName
counterparts	O
.	O

2022	O
)	O
introduced	O
and	O
worked	O
with	O
a	O
low	O
-	O
resource	O
NER	O
corpus	O
and	O
showed	O
that	O
E2E	B-MethodName
models	O
still	O
do	O
not	O
rival	O
pipeline	B-MethodName
approaches	O
when	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
pre	O
-	O
trained	O
models	O
are	O
used	O
.	O

When	O
using	O
pre	O
-	O
trained	O
representations	O
,	O
E2E	B-MethodName
models	O
are	O
at	O
a	O
disadvantage	O
since	O
the	O
pipeline	B-MethodName
model	O
also	O
has	O
access	O
to	O
a	O
text	O
model	O
trained	O
on	O
>	O
50	O
GB	O
of	O
text	O
,	O
in	O
addition	O
to	O
the	O
same	O
speech	O
representation	O
model	O
as	O
E2E.	B-MethodName
This	O
inspires	O
us	O
to	O
study	O
the	O
benefits	O
of	O
using	O
additional	O
unlabeled	O
data	O
.	O

We	O
choose	O
to	O
work	O
with	O
the	O
NER	O
-	O
annotated	O
Vox	B-DatasetName
-	I-DatasetName
Populi	I-DatasetName
corpus	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2021a;Shon	O
et	O
al	O
.	O
,	O

VoxPopuli	B-DatasetName
consists	O
of	O
naturally	O
spoken	O
speech	O
,	O
unlike	O
Bastianelli	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
and	O
is	O
annotated	O
manually	O
,	O
unlike	O
Yadav	O
et	O
al	O
.	O
(	O

The	O
SLUE	O
benchmark	O
(	O
Shon	O
et	O
al	O
.	O
,	O

However	O
,	O
it	O
is	O
not	O
yet	O
clear	O
how	O
universal	O
these	O
pre	O
-	O
trained	O
representations	O
are	O
for	O
speech	O
tasks	O
,	O
particularly	O
for	O
semantic	O
understanding	O
tasks	O
like	O
NER	O
.	O

2021	O
)	O
.	O

However	O
,	O
these	O
pre	O
-	O
trained	O
models	O
have	O
not	O
yet	O
been	O
tested	O
on	O
a	O
broad	O
range	O
of	O
challenging	O
understanding	O
tasks	O
.	O

We	O
believe	O
our	O
work	O
is	O
the	O
first	O
to	O
quantify	O
the	O
improvements	O
from	O
SSR	O
,	O
specifically	O
on	O
spoken	B-TaskName
NER	I-TaskName
.	O

Self	O
-	O
training	O
has	O
been	O
observed	O
to	O
improve	O
ASR	O
(	O
Parthasarathi	O
and	O
Strom	O
,	O
2019;Xu	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
this	O
is	O
the	O
first	O
work	O
to	O
introduce	O
it	O
to	O
spoken	B-TaskName
NER	I-TaskName
while	O
also	O
studying	O
its	O
effects	O
on	O
both	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
approaches	O
.	O

Knowledge	O
distillation	O
is	O
widely	O
used	O
in	O
model	O
compression	O
research	O
.	O

In	O
this	O
approach	O
,	O
some	O
intermediate	O
output	O
from	O
a	O
teacher	O
model	O
is	O
used	O
to	O
train	O
a	O
smaller	O
student	O
model	O
(	O
Hinton	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
including	O
E2E	O
spoken	B-TaskName
NER	I-TaskName
Caubrière	O
et	O
al	O
.	O
,	O

Specifically	O
for	O
NER	O
,	O
ASR	O
pre	O
-	O
training	O
is	O
expected	O
to	O
help	O
since	O
the	O
accuracy	O
of	O
decoded	O
texts	O
can	O
directly	O
affect	O
the	O
final	O
NER	O
predictions	O
.	O

Spoken	B-TaskName
NER	I-TaskName
involves	O
detecting	O
the	O
entity	O
phrases	O
in	O
a	O
spoken	O
utterance	O
along	O
with	O
their	O
tags	O
.	O

The	O
annotations	O
include	O
the	O
text	O
transcripts	O
for	O
the	O
audio	O
and	O
the	O
entity	O
phrases	O
with	O
their	O
corresponding	O
tags	O
.	O

Spoken	B-TaskName
NER	I-TaskName
,	O
like	O
any	O
other	O
SLU	O
task	O
,	O
is	O
typically	O
tackled	O
using	O
one	O
of	O
two	O
types	O
of	O
approaches	O
:	O
(	O
i	O
)	O
Pipeline	B-MethodName
and	O
(	O
ii	O
)	O
End	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
end	I-MethodName
(	O
E2E	B-MethodName
)	O
.	O

2	O
,	O
a	O
pipeline	B-MethodName
approach	O
decodes	O
speech	O
to	O
text	O
using	O
ASR	O
and	O
then	O
passes	O
the	O
decoded	O
text	O
through	O
a	O
text	B-MethodName
NER	I-MethodName
module	O
,	O
whereas	O
an	O
E2E	B-MethodName
system	O
directly	O
maps	O
the	O
input	O
speech	O
to	O
the	O
output	O
task	O
labels	O
.	O

Pipeline	B-MethodName
systems	O
can	O
enjoy	O
the	O
individual	O
advances	O
from	O
both	O
the	O
speech	O
and	O
the	O
text	O
research	O
communities	O
,	O
whereas	O
combining	O
two	O
modules	O
increases	O
inference	O
time	O
,	O
and	O
propagation	O
of	O
ASR	O
errors	O
can	O
have	O
unexpected	O
detrimental	O
effects	O
on	O
the	O
text	B-MethodName
NER	I-MethodName
module	O
performance	O
.	O

On	O
the	O
other	O
hand	O
,	O
E2E	B-MethodName
models	O
directly	O
optimize	O
a	O
task	O
-	O
specific	O
objective	O
and	O
tend	O
to	O
have	O
faster	O
inference	O
.	O

This	O
can	O
be	O
seen	O
from	O
previous	O
papers	O
on	O
E2E	B-MethodName
NER	O
(	O
Yadav	O
et	O
al	O
.	O
,	O

The	O
baselines	O
we	O
use	O
for	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
models	O
are	O
taken	O
from	O
Shon	O
et	O
al	O
.	O
(	O

2020	O
)	O
,	O
we	O
formulate	O
E2E	B-MethodName
NER	O
as	O
character	O
-	O
level	O
prediction	O
with	O
tag	O
-	O
specific	O
special	O
characters	O
delimiting	O
entity	O
phrases	O
.	O

For	O
example	O
,	O
the	O
phrases	O
"	O
irish	O
"	O
and	O
"	O
eu	O
"	O
are	O
tagged	O
as	O
NORP	O
1	O
(	O
$	O
)	O
and	O
GPE	O
2	O
(	O
%	O
)	O
respectively	O
in	O
"	O
the	O
$	O
irish	O
]	O
system	O
works	O
within	O
a	O
legal	O
and	O
regulatory	O
policy	O
directive	O
framework	O
dictated	O
by	O
the	O
%	O
eu	O
]	O
"	O
.The	O
E2E	B-MethodName
NER	O
and	O
ASR	O
modules	O
are	O
initialized	O
with	O
the	O
wav2vec2.0	O
base	O
(	O
Baevski	O
et	O
al	O
.	O
,	O

2020	O
)	O
pretrained	O
speech	O
representation	O
,	O
while	O
the	O
text	B-MethodName
NER	I-MethodName
module	O
is	O
pre	O
-	O
trained	O
with	O
DeBERTa	O
base	O
(	O
He	O
et	O
al	O
.	O
,	O

2021	O
)	O
.	O

Since	O
text	O
transcripts	O
are	O
typically	O
a	O
part	O
of	O
the	O
NER	O
annotations	O
,	O
we	O
can	O
also	O
train	O
an	O
NER	O
model	O
that	O
uses	O
the	O
ground	O
-	O
truth	O
text	O
as	O
input	O
.	O

This	O
text	B-MethodName
NER	I-MethodName
model	O
serves	O
roughly	O
as	O
a	O
topline	O
and	O
is	O
further	O
used	O
in	O
experiments	O
with	O
external	O
data	O
.	O

The	O
E2E	B-MethodName
NER	O
and	O
ASR	O
models	O
are	O
trained	O
with	O
a	O
character	O
-	O
level	O
CTC	O
objective	O
.	O

The	O
text	B-MethodName
NER	I-MethodName
model	O
is	O
trained	O
for	O
token	O
-	O
level	O
classification	O
with	O
cross	O
-	O
entropy	O
loss	O
.	O

In	O
order	O
to	O
quantify	O
the	O
benefits	O
of	O
the	O
pre	O
-	O
trained	O
representations	O
in	O
our	O
setting	O
,	O
we	O
also	O
report	O
the	O
performance	O
of	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
baselines	O
that	O
are	O
trained	O
from	O
scratch	O
,	O
not	O
utilizing	O
any	O
pre	O
-	O
trained	O
models	O
.	O

2020	O
)	O
,	O
we	O
evaluate	O
performance	O
using	O
micro	B-MetricName
-	I-MetricName
averaged	I-MetricName
F1	I-MetricName
scores	O
on	O
an	O
unordered	O
list	O
of	O
tuples	O
of	O
named	O
entity	O
phrase	O
and	O
tag	O
pairs	O
predicted	O
for	O
each	O
sentence	O
.	O

Spoken	B-TaskName
NER	I-TaskName
introduces	O
an	O
added	O
variability	O
to	O
the	O
possible	O
model	O
errors	O
due	O
to	O
speech	O
-	O
to	O
-	O
text	O
conversion	O
.	O

We	O
report	O
word	B-MetricName
error	I-MetricName
rate	I-MetricName
(	O
WER	B-MetricName
)	O
to	O
evaluate	O
this	O
aspect	O
.	O

WER	B-MetricName
is	O
the	O
word	O
-	O
level	O
Levenshtein	O
distance	O
between	O
the	O
ground	O
-	O
truth	O
text	O
and	O
the	O
decoded	O
text	O
generated	O
by	O
the	O
model	O
.	O

Additionally	O
,	O
to	O
get	O
an	O
idea	O
of	O
the	O
errors	O
made	O
by	O
the	O
model	O
specifically	O
on	O
named	O
entities	O
,	O
we	O
also	O
evaluate	O
NE	B-MetricName
ACC	I-MetricName
,	O
the	O
proportion	O
of	O
entity	O
phrases	O
correctly	O
decoded	O
in	O
the	O
speech	O
-	O
to	O
-	O
text	O
conversion	O
.	O

Next	O
,	O
we	O
describe	O
our	O
approaches	O
that	O
use	O
data	O
external	O
to	O
the	O
task	O
-	O
specific	O
labeled	O
data	O
to	O
improve	O
both	O
the	O
pipeline	B-MethodName
and	O
the	O
E2E	B-MethodName
models	O
for	O
spoken	B-TaskName
NER	I-TaskName
.	O

Method	O
Target	O
modelUn	O
-	O
Sp	O
SelfTrain	O
-	O
ASR	O
ASR	O
Un	O
-	O
Txt	O
SelfTrain	B-MethodName
-	I-MethodName
txtNER	I-MethodName
text	O
NER	O
Sp	O
-	O
TxtPre	B-MethodName
-	I-MethodName
ASR	I-MethodName
ASR	O
The	O
majority	O
of	O
techniques	O
we	O
consider	O
involve	O
labeling	O
the	O
external	O
data	O
with	O
a	O
labeling	O
model	O
(	O
typically	O
one	O
of	O
the	O
baseline	O
models	O
)	O
to	O
produce	O
pseudo	O
-	O
labels	O
.	O

Tables	O
1	O
and	O
2	O
present	O
a	O
detailed	O
list	O
of	O
all	O
methods	O
we	O
consider	O
for	O
improving	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
models	O
respectively	O
.	O

The	O
fourth	O
kind	O
,	O
external	O
text	O
-	O
based	O
NER	O
data	O
,	O
is	O
used	O
in	O
experiments	O
attempting	O
to	O
improve	O
the	O
text	B-MethodName
NER	I-MethodName
model	O
;	O
since	O
it	O
does	O
not	O
succeed	O
(	O
Sec	O
.	O

5.2.1	O
)	O
,	O
this	O
data	O
source	O
is	O
not	O
explored	O
further	O
for	O
the	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
models	O
.	O

2020Xu	O
et	O
al	O
.	O
,	O
,	O

In	O
our	O
setting	O
,	O
a	O
word	O
-	O
level	O
language	O
model	O
(	O
LM	O
)	O
is	O
used	O
for	O
decoding	O
both	O
the	O
ASR	O
and	O
E2E	B-MethodName
NER	O
models	O
.	O

Shon	O
et	O
al	O
.	O
(	O

So	O
we	O
may	O
expect	O
self	O
-	O
training	O
from	O
pseudo	O
-	O
labels	O
to	O
improve	O
the	O
target	O
models	O
by	O
distilling	O
the	O
LM	O
information	O
into	O
all	O
model	O
layers	O
.	O

Among	O
the	O
baseline	O
models	O
,	O
the	O
pipeline	B-MethodName
performs	O
better	O
than	O
E2E	B-MethodName
approaches	O
,	O
presumably	O
since	O
the	O
former	O
uses	O
strong	O
pre	O
-	O
trained	O
text	O
representations	O
.	O

So	O
,	O
for	O
instance	O
,	O
distilling	O
from	O
the	O
pipeline	B-MethodName
(	O
labeling	O
model	O
)	O
into	O
the	O
E2E	B-MethodName
model	O
(	O
target	O
model	O
)	O
is	O
expected	O
to	O
boost	O
the	O
performance	O
of	O
the	O
E2E	B-MethodName
model	O
.	O

All	O
the	O
ASR	O
experiments	O
use	O
language	O
models	O
trained	O
on	O
the	O
TED	O
-	O
LIUM	O
3	O
LM	O
corpus	O
(	O
Hernandez	O
et	O
al	O
.	O
,	O

The	O
language	O
model	O
used	O
in	O
baseline	O
E2E	B-MethodName
NER	O
experiments	O
is	O
trained	O
on	O
the	O
15hr	O
fine	O
-	O
tune	O
set	O
(	O
ftune	O
4	O
-	O
gram	O
)	O
.	O

The	O
generated	O
pseudo	O
-	O
labels	O
also	O
provide	O
additional	O
annotated	O
data	O
for	O
LM	O
training	O
,	O
which	O
can	O
be	O
used	O
in	O
E2E	B-MethodName
models	O
.	O

These	O
are	O
referred	O
to	O
as	O
plabel	O
4gram	O
)	O
(	O
for	O
"	O
pseudo	O
-	O
label	O
4	O
-	O
gram").Unlabeled	O
speech	O
:	O
The	O
unlabeled	O
speech	O
is	O
used	O
to	O
improve	O
the	O
ASR	O
module	O
of	O
the	O
pipeline	B-MethodName
approach	O
via	O
self	O
-	O
training	O
(	O
SelfTrain	O
-	O
ASR).For	O
improving	O
the	O
E2E	B-MethodName
model	O
,	O
the	O
improved	O
pipeline	B-MethodName
can	O
be	O
used	O
as	O
the	O
labeling	O
model	O
,	O
followed	O
by	O
training	O
the	O
E2E	B-MethodName
model	O
on	O
the	O
generated	O
pseudo	O
-	O
labels	O
(	O
Distill	B-MethodName
-	I-MethodName
Pipeline	I-MethodName
)	O
.	O

Alternatively	O
,	O
the	O
unlabeled	O
audio	O
can	O
be	O
directly	O
used	O
to	O
improve	O
the	O
E2E	B-MethodName
model	O
via	O
self	O
-	O
training	O
(	O
SelfTrain	O
-	O
E2E).Unlabeled	B-MethodName
text	O
:	O
The	O
text	B-MethodName
NER	I-MethodName
module	O
in	O
the	O
pipeline	B-MethodName
approach	O
is	O
improved	O
by	O
self	O
-	O
training	O
using	O
the	O
unlabeled	O
text	O
data	O
(	O
SelfTrain	O
-	O
txtNER	O
)	O
.	O

The	O
E2E	B-MethodName
model	O
uses	O
the	O
pseudo	O
labels	O
generated	O
from	O
the	O
text	B-MethodName
NER	I-MethodName
baseline	O
module	O
on	O
the	O
unlabeled	O
text	O
to	O
update	O
the	O
LM	O
used	O
for	O
decoding	O
(	O
Distill	B-MethodName
-	I-MethodName
txtNER	I-MethodName
-	O
lm).Transcribed	O
speech	O
:	O
The	O
pipeline	B-MethodName
approach	O
is	O
improved	O
by	O
using	O
the	O
additional	O
transcribed	O
speech	O
data	O
to	O
improve	O
the	O
ASR	O
module	O
(	O
Pre	O
-	O
ASR	O
)	O
.	O

The	O
E2E	B-MethodName
model	O
uses	O
this	O
updated	O
ASR	O
as	O
an	O
initialization	O
in	O
a	O
typical	O
transfer	O
learning	O
setup	O
.	O

Alternatively	O
,	O
for	O
paired	O
speech	O
text	O
data	O
,	O
the	O
pseudolabels	O
generated	O
from	O
the	O
text	B-MethodName
NER	I-MethodName
model	O
can	O
be	O
paired	O
with	O
audio	O
and	O
used	O
for	O
training	O
the	O
E2E	B-MethodName
model	O
,	O
thus	O
distilling	O
information	O
from	O
a	O
stronger	O
text	B-MethodName
NER	I-MethodName
model	O
into	O
it	O
(	O
Distill	B-MethodName
-	I-MethodName
txtNER).Text	I-MethodName
NER	B-MethodName
data	O
:	O
In	O
addition	O
to	O
improving	O
the	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
models	O
using	O
the	O
approaches	O
mentioned	O
above	O
,	O
we	O
also	O
look	O
for	O
any	O
possible	O
improvements	O
in	O
the	O
text	B-MethodName
NER	I-MethodName
model	O
by	O
leveraging	O
a	O
larger	O
external	O
annotated	O
text	B-MethodName
NER	I-MethodName
corpus	O
.	O

We	O
use	O
the	O
OntoNotes5.0	O
(	O
Pradhan	O
et	O
al	O
.	O
,	O

2013	O
)	O
corpus	O
,	O
whose	O
labeling	O
scheme	O
inspired	O
that	O
of	O
VoxPopuli	B-DatasetName
(	O
Shon	O
et	O
al	O
.	O
,	O

3	O
for	O
more	O
information	O
on	O
OntoNotes5.0	O
.	O

We	O
closely	O
follow	O
the	O
setup	O
for	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
baselines	O
in	O
Shon	O
et	O
al	O
.	O
(	O

The	O
model	O
processes	O
the	O
spectrogram	O
features	O
through	O
two	B-HyperparameterValue
2	I-HyperparameterValue
-	I-HyperparameterValue
D	I-HyperparameterValue
convolutional	B-HyperparameterName
layers	I-HyperparameterName
followed	O
by	O
five	B-HyperparameterValue
bidirectional	I-HyperparameterValue
2048	I-HyperparameterValue
-	I-HyperparameterValue
dim	I-HyperparameterValue
LSTM	B-HyperparameterName
layers	I-HyperparameterName
and	O
a	O
softmax	O
layer	O
.	O

We	O
use	O
fairseq	O
library	O
(	O
Ott	O
et	O
al	O
.	O
,	O

2019	O
)	O
to	O
finetune	O
wav2vec	O
2.0	O
models	O
for	O
the	O
E2E	B-MethodName
NER	O
and	O
ASR	O
tasks	O
.	O

The	O
text	B-MethodName
NER	I-MethodName
model	O
(	O
which	O
uses	O
ground	O
-	O
truth	O
transcripts	O
)	O
is	O
far	O
better	O
than	O
the	O
pipeline	O
method	O
,	O
which	O
is	O
better	O
than	O
the	O
E2E	B-MethodName
model	O
.	O

We	O
report	O
F1	B-MetricName
scores	O
on	O
the	O
dev	O
set	O
using	O
different	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
approaches	O
in	O
Tables	O
5	O
and	O
6	O
spectively	O
.	O

1	O
presents	O
key	O
results	O
when	O
using	O
each	O
external	O
data	O
type	O
for	O
both	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
models	O
.	O

The	O
key	O
findings	O
are	O
:	O
(	O
i	O
)	O
Using	O
external	O
data	O
reduces	O
the	O
gap	O
between	O
spoken	B-TaskName
NER	I-TaskName
baselines	O
and	O
text	B-MethodName
NER.(ii	I-MethodName
)	O
With	O
access	O
to	O
either	O
unlabeled	O
speech	O
or	O
transcribed	O
speech	O
,	O
E2E	B-MethodName
models	O
outperform	O
pipeline	B-MethodName
models	O
,	O
whereas	O
,	O
for	O
the	O
baselines	O
,	O
the	O
opposite	O
holds.(iii	O
)	O
Using	O
unlabeled	O
text	O
gives	O
the	O
smallest	O
boost	O
among	O
the	O
three	O
types	O
of	O
external	O
data	O
,	O
and	O
the	O
pipeline	B-MethodName
approach	O
performs	O
better	O
in	O
that	O
setting	O
.	O

We	O
try	O
to	O
improve	O
the	O
text	B-MethodName
NER	I-MethodName
model	O
by	O
using	O
the	O
OntoNotes5.0	O
NER	O
corpus	O
(	O
Pradhan	O
et	O
al	O
.	O
,	O

2013	O
)	O
.	O

Fine	O
-	O
tuning	O
DeBERTa	O
-	O
base	O
on	O
OntoNotes5.0	O
produces	O
an	O
F1	B-MetricName
of	O
60	B-MetricValue
%	I-MetricValue
on	O
the	O
VoxPopuli	B-DatasetName
dev	O
set	O
.	O

Finetuning	O
it	O
further	O
on	O
VoxPopuli	B-DatasetName
gives	O
F1	B-MetricName
86	B-MetricValue
%	I-MetricValue
on	O
the	O
dev	O
set	O
.	O

Since	O
we	O
do	O
not	O
see	O
any	O
boost	O
over	O
the	O
existing	O
vanilla	O
approach	O
(	O
86	B-MetricValue
%	I-MetricValue
,	O
see	O
Tab	O
.	O

4	O
)	O
,	O
we	O
retain	O
the	O
original	O
text	B-MethodName
NER	I-MethodName
model	O
using	O
only	O
in	O
-	O
domain	O
data	O
and	O
do	O
not	O
perform	O
further	O
experiments	O
using	O
the	O
OntoNotes	O
-	O
finetuned	O
model	O
.	O

The	O
baseline	O
results	O
are	O
not	O
surprising	O
:	O
The	O
limited	O
labeled	O
data	O
is	O
not	O
enough	O
for	O
the	O
baseline	O
E2E	B-MethodName
approach	O
,	O
but	O
the	O
pipeline	B-MethodName
model	O
can	O
leverage	O
a	O
strong	O
text	O
representation	O
model	O
,	O
which	O
gives	O
it	O
an	O
edge	O
.	O

When	O
using	O
external	O
data	O
with	O
the	O
E2E	B-MethodName
model	O
,	O
the	O
best	O
performing	O
methods	O
use	O
either	O
(	O
a	O
)	O
external	O
unlabeled	O
speech	O
(	O
Distill	B-MethodName
-	I-MethodName
Pipeline	I-MethodName
)	O
or	O
(	O
b	O
)	O
transcribed	O
speech	O
(	O
Distill	B-MethodName
-	I-MethodName
txtNER	I-MethodName
)	O
.	O

The	O
labeling	O
models	O
have	O
a	O
stronger	O
semantic	O
component	O
than	O
the	O
E2E	B-MethodName
baseline	O
in	O
both	O
of	O
these	O
scenarios	O
because	O
of	O
their	O
strong	O
text	B-MethodName
NER	I-MethodName
module	O
.	O

The	O
same	O
can	O
not	O
be	O
said	O
for	O
the	O
other	O
competing	O
approaches	O
for	O
these	O
external	O
data	O
categories	O
,	O
SelfTrain	O
-	O
NER	O
and	O
Pre	O
-	O
ASR	O
,	O
which	O
provide	O
much	O
lower	O
improvements	O
.	O

SelfTrain	O
-	O
NER	O
distills	O
information	O
from	O
the	O
LM	O
into	O
the	O
model	O
layers	O
,	O
but	O
the	O
n	O
-	O
gram	O
LM	O
is	O
much	O
less	O
powerful	O
than	O
the	O
transformer	O
-	O
based	O
text	B-MethodName
NER	I-MethodName
module	O
used	O
in	O
Distill	B-MethodName
-	I-MethodName
Pipeline	I-MethodName
.	O

Note	O
that	O
the	O
baseline	O
E2E	B-MethodName
model	O
parameters	O
do	O
not	O
change	O
,	O
unlike	O
when	O
using	O
the	O
other	O
two	O
types	O
of	O
external	O
data	O
.	O

The	O
baseline	O
pipeline	B-MethodName
model	O
already	O
takes	O
advantage	O
of	O
the	O
text	B-MethodName
NER	I-MethodName
module	O
,	O
which	O
leaves	O
little	O
room	O
for	O
improvement	O
in	O
the	O
semantic	O
understanding	O
component	O
.	O

Specifically	O
,	O
using	O
unlabeled	O
text	O
data	O
to	O
improve	O
the	O
text	B-MethodName
NER	I-MethodName
module	O
(	O
SelfTrain	B-MethodName
-	I-MethodName
txtNER	I-MethodName
)	O
gives	O
a	O
small	O
boost	O
of	O
0.4	B-MetricValue
%	I-MetricValue
.	O

For	O
comparison	O
,	O
note	O
that	O
the	O
improvement	O
from	O
using	O
unlabeled	O
speech	O
is	O
2.5	B-MetricValue
%	I-MetricValue
over	O
baseline	O
.	O

So	O
,	O
the	O
hope	O
with	O
pipeline	B-MethodName
models	O
is	O
for	O
the	O
external	O
data	O
to	O
improve	O
the	O
speech	O
-	O
to	O
-	O
text	O
conversion	O
,	O
which	O
can	O
then	O
help	O
reduce	O
error	O
propagation	O
between	O
the	O
independent	O
pipeline	B-MethodName
modules	O
.	O

Almost	O
all	O
experiments	O
produce	O
a	O
larger	O
improvement	O
when	O
using	O
500	O
hours	O
of	O
external	O
data	O
than	O
100	O
hours	O
.	O

We	O
hypothesize	O
that	O
methods	O
for	O
balancing	O
between	O
the	O
effects	O
of	O
manually	O
annotated	O
and	O
pseudo	O
-	O
labeled	O
examples	O
could	O
help	O
tackle	O
this	O
issue	O
(	O
Park	O
et	O
al	O
.	O
,	O

2020	O
)	O
.	O

Un	O
Fig	O
.	O

3	O
presents	O
the	O
NE	B-MetricName
accuracy	I-MetricName
and	O
word	O
error	O
rates	O
(	O
WER	B-MetricName
)	O
.	O

We	O
strip	O
off	O
the	O
tag	O
-	O
specific	O
special	O
character	O
tokens	O
when	O
evaluating	O
WER	B-MetricName
for	O
the	O
E2E	B-MethodName
NER	O
models	O
.	O

Note	O
that	O
we	O
report	O
100−	O
WER	B-MetricName
so	O
that	O
higher	O
is	O
better	O
in	O
both	O
plots	O
.	O

We	O
observe	O
that	O
the	O
ASR	O
used	O
in	O
pipeline	B-MethodName
models	O
typically	O
performs	O
better	O
than	O
the	O
speech	O
-	O
to	O
-	O
text	O
conversion	O
of	O
E2E	B-MethodName
models	O
,	O
even	O
when	O
the	O
former	O
has	O
a	O
poorer	O
F1	B-MetricName
(	O
Fig	O
.	O

This	O
may	O
lead	O
us	O
to	O
hypothesize	O
that	O
the	O
E2E	B-MethodName
model	O
recognizes	O
NE	O
words	O
better	O
while	O
doing	O
worse	O
for	O
other	O
words	O
.	O

However	O
,	O
this	O
hypothesis	O
is	O
not	O
supported	O
by	O
the	O
NE	B-MetricName
-	I-MetricName
ACC	I-MetricName
results	O
(	O
Fig	O
.	O

3).Next	O
,	O
we	O
look	O
at	O
the	O
breakdown	O
of	O
F1	B-MetricName
into	O
precision	B-MetricName
and	O
recall	B-MetricName
(	O
Fig	O
.	O

We	O
see	O
that	O
pipeline	B-MethodName
models	O
have	O
worse	O
precision	O
,	O
thus	O
suggesting	O
that	O
these	O
suffer	O
from	O
a	O
higher	O
false	O
-	O
positive	O
rate	O
than	O
the	O
E2E	B-MethodName
models	O
.	O

This	O
explains	O
why	O
NE	B-MetricName
-	I-MetricName
ACC	I-MetricName
is	O
not	O
predictive	O
of	O
F1	B-MetricName
;	O
the	O
former	O
can	O
inform	O
us	O
about	O
errors	O
due	O
to	O
false	O
negatives	O
,	O
but	O
not	O
false	O
positives	O
.	O

We	O
focus	O
on	O
four	O
major	O
categories	O
showing	O
noteworthy	O
differences	O
between	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
approaches	O
.	O

We	O
provide	O
this	O
analysis	O
for	O
the	O
baselines	O
,	O
Distill	B-MethodName
-	I-MethodName
Pipeline	I-MethodName
,	O
and	O
SelfTrain	O
-	O
ASR	O
models	O
using	O
external	O
unlabeled	O
speech	O
data	O
.	O

We	O
observe	O
that	O
:	O
(	O
i	O
)	O
False	O
detections	O
are	O
1.5	O
times	O
more	O
common	O
in	O
pipeline	B-MethodName
models	O
than	O
in	O
E2E	B-MethodName
models	O
,	O
as	O
expected	O
based	O
on	O
the	O
lower	O
precision	O
for	O
the	O
former	O
.	O

This	O
happens	O
even	O
when	O
the	O
falsely	O
detected	O
text	O
is	O
not	O
a	O
speech	O
-	O
to	O
-	O
text	O
conversion	O
error	O
.	O
(	O

ii	O
)	O
Over	O
-	O
detections	O
are	O
3.5	O
to	O
4	O
times	O
more	O
common	O
in	O
the	O
pipeline	B-MethodName
models	O
even	O
when	O
the	O
entity	O
phrase	O
is	O
decoded	O
correctly	O
.	O
(	O

iii	O
)	O
Missed	O
detections	O
for	O
the	O
E2E	B-MethodName
Distill	O
-	O
Pipeline	B-MethodName
model	O
are	O
drastically	O
reduced	O
compared	O
to	O
the	O
E2E	B-MethodName
baseline	O
.	O

Missed	O
detections	O
refer	O
to	O
cases	O
where	O
the	O
entity	O
phrases	O
are	O
correctly	O
transcribed	O
but	O
are	O
not	O
labeled	O
as	O
named	O
entities	O
.	O

The	O
improvement	O
here	O
therefore	O
suggests	O
that	O
Distill	B-MethodName
-	I-MethodName
Pipeline	I-MethodName
improves	O
the	O
understanding	O
capability	O
of	O
the	O
E2E	B-MethodName
model	O
,	O
in	O
addition	O
to	O
its	O
speech	O
-	O
to	O
-	O
text	O
capability	O
.	O

Also	O
,	O
note	O
that	O
the	O
pipeline	B-MethodName
model	O
does	O
not	O
enjoy	O
the	O
same	O
benefit	O
from	O
unlabeled	O
speech	O
since	O
this	O
only	O
involves	O
self	O
-	O
training	O
(	O
instead	O
of	O
knowledge	O
distillation	O
from	O
a	O
much	O
richer	O
model	O
for	O
E2E).Overall	B-MethodName
,	O
the	O
pipeline	B-MethodName
models	O
suffer	O
disproportionately	O
from	O
false	O
positives	O
.	O

This	O
seems	O
to	O
stem	O
The	O
predicted	O
entity	O
phrase	O
is	O
not	O
in	O
the	O
GT	O
transcript	O
,	O
and	O
...	O
(	O
ORG	O
,	O
'	O
ssn	O
'	O
)	O
in	O
predictedGround	O
-	O
truth	O
Predicted	O
monetary	O
policy	O
i	O
saw	O
that	O
according	O
to	O
the	O
recent	O
poll	O
the	O
majority	O
of	O
icelanders	O
still	O
oppose	O
eu	O
membership	O
since	O
sixty	O
seven	O
are	O
against	O
and	O
only	O
thirty	O
three	O
in	O
favour	O
of	O
accession	O
[	O
(	O
'	O
NORP	O
'	O
,	O
'	O
icelanders	O
'	O
)	O
,	O
(	O
'	O
PLACE	O
'	O
,	O
'	O
eu	O
'	O
)	O
,	O
(	O
'	O
QUANT	O
'	O
,	O
'	O
sixty	O
seven	O
'	O
)	O
,	O
(	O
'	O
QUANT	O
'	O
,	O
'	O
thirty	O
three	O
'	O
)	O
]	O
monetary	O
policy	O
i	O
saw	O
that	O
according	O
to	O
the	O
recent	O
poll	O
the	O
majority	O
of	O
iceland	O
still	O
oppose	O
eu	O
membership	O
since	O
sixty	O
seven	O
are	O
against	O
and	O
only	O
thirty	O
three	O
in	O
favour	O
of	O
ssn	O
[	O
(	O
'	O
PLACE	O
'	O
,	O
'	O
iceland	O
'	O
)	O
,	O
(	O
'	O
PLACE	O
'	O
,	O
'	O
eu	O
'	O
)	O
,	O
(	O
'	O
QUANT	O
'	O
,	O
'	O
only	O
thirty	O
three	O
'	O
)	O
,	O
(	O
'	O
LAW	O
'	O
,	O
'	O
monetary	O
'	O
)	O
,	O
(	O
'	O
ORG	O
'	O
,	O
'	O
ssn	O
'	O
)	O
]	O
from	O
the	O
text	B-MethodName
NER	I-MethodName
model	O
,	O
which	O
has	O
even	O
higher	O
over	O
-	O
detection	O
and	O
false	O
detection	O
rates	O
than	O
the	O
pipeline	B-MethodName
baseline	O
models	O
(	O
Fig	O
.	O

The	O
reasons	O
behind	O
this	O
difference	O
between	O
E2E	B-MethodName
and	O
pipeline	B-MethodName
models	O
need	O
further	O
investigation	O
.	O

We	O
have	O
explored	O
various	O
ways	O
to	O
use	O
different	O
external	O
data	O
types	O
that	O
improve	O
both	O
pipeline	B-MethodName
and	O
E2E	B-MethodName
methods	O
for	O
spoken	B-TaskName
NER	I-TaskName
.	O

The	O
bestperforming	O
model	O
when	O
using	O
external	O
data	O
is	O
an	O
E2E	B-MethodName
approach	O
.	O

This	O
is	O
one	O
of	O
the	O
few	O
results	O
in	O
the	O
literature	O
thus	O
far	O
showing	O
better	O
performance	O
for	O
E2E	B-MethodName
over	O
pipeline	B-MethodName
methods	O
that	O
use	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
modules	O
for	O
spoken	O
language	O
understanding	O
.	O

We	O
develop	O
some	O
insights	O
into	O
this	O
difference	O
;	O
we	O
notice	O
that	O
pipeline	B-MethodName
models	O
are	O
adversely	O
affected	O
by	O
false	O
positives	O
and	O
that	O
leveraging	O
external	O
data	O
improves	O
the	O
semantic	O
understanding	O
capability	O
of	O
the	O
E2E	B-MethodName
models	O
.	O

We	O
hope	O
that	O
our	O
work	O
provides	O
guiding	O
principles	O
for	O
researchers	O
working	O
on	O
SLU	O
tasks	O
in	O
similar	O
low	O
-	O
resource	O
domains	O
when	O
some	O
form	O
of	O
external	O
data	O
is	O
abundant	O
.	O

5	O
and	O
6	O
)	O
,	O
which	O
suggests	O
the	O
question	O
:	O
What	O
is	O
the	O
smallest	O
amount	O
of	O
external	O
data	O
needed	O
to	O
obtain	O
significant	O
improvements	O
in	O
NER	O
performance	O
?	O
Additionally	O
,	O
one	O
preliminary	O
experiment	O
with	O
external	O
,	O
out	O
-	O
of	O
-	O
domain	O
text	B-MethodName
NER	I-MethodName
data	O
(	O
OntoNotes	O
5.0	O
)	O
fails	O
to	O
improve	O
the	O
text	B-MethodName
NER	I-MethodName
performance	O
,	O
suggesting	O
the	O
challenges	O
of	O
dealing	O
with	O
out	O
-	O
of	O
-	O
domain	O
datasets	O
.	O

From	O
the	O
modeling	O
perspective	O
,	O
better	O
fine	O
-	O
tuning	O
strategies	O
for	O
wav2vec2.0	O
in	O
low	O
supervision	O
settings	O
have	O
been	O
proposed	O
for	O
ASR	O
(	O
Pasad	O
et	O
al	O
.	O
,	O

2021	O
)	O
;	O
it	O
would	O
be	O
interesting	O
to	O
explore	O
how	O
these	O
findings	O
may	O
transfer	O
to	O
an	O
SLU	O
task	O
.	O

We	O
obtain	O
test	O
set	O
results	O
for	O
our	O
best	O
-	O
performing	O
models	O
,	O
by	O
submitting	O
model	O
outputs	O
following	O
the	O
SLUE	O
instructions	O
.	O

1	O
)	O
.	O

7	O
,	O
that	O
our	O
analytical	O
conclusions	O
about	O
the	O
pipeline	B-MethodName
model	O
performing	O
poorly	O
due	O
to	O
false	O
positives	O
are	O
consistent	O
across	O
these	O
two	O
splits	O
.	O

A	O
neural	O
multimodal	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
MMT	B-TaskName
)	O
system	O
is	O
one	O
that	O
aims	O
to	O
perform	O
better	O
translation	O
by	O
extending	O
conventional	O
textonly	O
translation	O
models	O
with	O
multimodal	O
information	O
.	O

Many	O
recent	O
studies	O
report	O
improvements	O
when	O
equipping	O
their	O
models	O
with	O
the	O
multimodal	O
module	O
,	O
despite	O
the	O
controversy	O
of	O
whether	O
such	O
improvements	O
indeed	O
come	O
from	O
the	O
multimodal	O
part	O
.	O

We	O
revisit	O
the	O
contribution	O
of	O
multimodal	O
information	O
in	O
MMT	B-TaskName
by	O
devising	O
two	O
interpretable	O
MMT	B-TaskName
models	O
.	O

We	O
report	O
empirical	O
findings	O
that	O
highlight	O
the	O
importance	O
of	O
MMT	B-TaskName
models	O
'	O
interpretability	O
,	O
and	O
discuss	O
how	O
our	O
findings	O
will	O
benefit	O
future	O
research	O
.	O

Multimodal	B-TaskName
Machine	I-TaskName
Translation	I-TaskName
(	O
MMT	B-TaskName
)	O
aims	O
at	O
designing	O
better	O
translation	O
systems	O
by	O
extending	O
conventional	O
text	O
-	O
only	O
translation	O
systems	O
to	O
take	O
into	O
account	O
multimodal	O
information	O
,	O
especially	O
from	O
visual	O
modality	O
Wang	O
et	O
al	O
.	O
,	O

Despite	O
many	O
previous	O
success	O
in	O
MMT	B-TaskName
that	O
report	O
improvements	O
when	O
models	O
are	O
equipped	O
with	O
visual	O
information	O
I	O
ve	O
et	O
al	O
.	O
,	O

2020	O
;	O
,	O
there	O
have	O
been	O
continuing	O
debates	O
on	O
the	O
need	O
for	O
visual	O
context	O
in	O
MMT.In	B-TaskName
particular	O
,	O
;	O
;	O
Barrault	O
et	O
al	O
.	O
(	O

2018a	O
)	O
provide	O
further	O
evidence	O
by	O
showing	O
that	O
MMT	B-TaskName
models	O
are	O
,	O
in	O
fact	O
,	O
insensitive	O
to	O
visual	O
input	O
and	O
can	O
translate	O
without	O
significant	O
performance	O
losses	O
even	O
in	O
the	O
presence	O
of	O
features	O
derived	O
from	O
unrelated	O
images	O
.	O

But	O
it	O
remains	O
unclear	O
where	O
the	O
gains	O
of	O
MMT	B-TaskName
methods	O
come	O
from	O
,	O
when	O
the	O
textual	O
context	O
is	O
complete	O
.	O

The	O
main	O
tool	O
utilized	O
in	O
prior	O
discussion	O
is	O
adversarial	O
model	O
comparison	O
-explaining	O
the	O
behavior	O
of	O
complex	O
and	O
black	O
-	O
box	O
MMT	B-TaskName
models	O
by	O
comparing	O
performance	O
changes	O
when	O
given	O
adversarial	O
input	O
(	O
e.g.	O
,	O
random	O
images	O
)	O
.	O

Although	O
such	O
an	O
opaque	O
tool	O
is	O
an	O
acceptable	O
beginning	O
to	O
investigate	O
the	O
need	O
for	O
visual	O
context	O
in	O
MMT	B-TaskName
,	O
they	O
provide	O
rather	O
indirect	O
evidence	O
(	O
Hessel	O
and	O
Lee	O
,	O
2020	O
)	O
.	O

2019).From	O
these	O
perspectives	O
,	O
we	O
revisit	O
the	O
need	O
for	O
visual	O
context	O
in	O
MMT	B-TaskName
by	O
designing	O
two	O
interpretable	O
models	O
.	O

To	O
our	O
surprise	O
,	O
while	O
our	O
models	O
are	O
shown	O
to	O
be	O
effective	O
on	O
Multi30k	B-DatasetName
and	O
VaTex	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

Repeating	O
the	O
evaluation	O
under	O
limited	O
textual	O
context	O
further	O
substantiates	O
our	O
findings	O
and	O
complements	O
previous	O
analysis	O
(	O
Caglayan	O
et	O
al	O
.	O
,	O

First	O
,	O
we	O
revisit	O
the	O
need	O
for	O
visual	O
context	O
in	O
the	O
popular	O
task	O
of	O
multimodal	B-TaskName
machine	I-TaskName
translation	I-TaskName
and	O
find	O
that	O
:	O
(	O
1	O
)	O
under	O
sufficient	O
textual	O
context	O
,	O
the	O
MMT	B-TaskName
models	O
'	O
improvements	O
over	O
text	O
-	O
only	O
counterparts	O
result	O
from	O
the	O
regularization	O
effect	O
(	O
Section	O
5.2	O
)	O
.	O
(	O

2	O
)	O
under	O
limited	O
textual	O
context	O
,	O
MMT	B-TaskName
models	O
can	O
leverage	O
visual	O
context	O
to	O
help	O
translation	O
(	O
Section	O
5.3	O
)	O
.	O

Our	O
findings	O
highlight	O
the	O
importance	O
of	O
MMT	B-TaskName
models	O
'	O
interpretability	O
and	O
the	O
need	O
for	O
a	O
new	O
benchmark	O
to	O
advance	O
the	O
community	O
.	O

Second	O
,	O
for	O
the	O
MMT	B-TaskName
task	O
,	O
we	O
provide	O
a	O
strong	O
text	O
-	O
only	O
baseline	O
implementation	O
and	O
two	O
models	O
with	O
interpretable	O
components	O
that	O
replicate	O
similar	O
gains	O
as	O
reported	O
in	O
previous	O
works	O
.	O

Different	O
from	O
adversarial	O
model	O
comparison	O
methods	O
,	O
our	O
models	O
are	O
interpretable	O
due	O
to	O
the	O
specifically	O
designed	O
model	O
structure	O
and	O
can	O
serve	O
as	O
standard	O
baselines	O
for	O
future	O
interpretable	O
MMT	B-TaskName
studies	O
.	O

One	O
can	O
broadly	O
categorize	O
MMT	B-TaskName
systems	O
into	O
two	O
types	O
:	O
(	O
1	O
)	O
Conventional	O
MMT	B-TaskName
,	O
where	O
there	O
is	O
gold	O
alignment	O
between	O
the	O
source	O
(	O
target	O
)	O
sentence	O
pair	O
and	O
a	O
relevant	O
image	O
and	O
(	O
2	O
)	O
Retrieval	O
-	O
based	O
MMT	B-TaskName
,	O
where	O
systems	O
retrieve	O
relevant	O
images	O
from	O
an	O
image	O
corpus	O
as	O
additional	O
clues	O
to	O
assist	O
translation	O
.	O

Conventional	O
MMT	B-TaskName
Most	O
MMT	B-TaskName
systems	O
require	O
datasets	O
consist	O
of	O
images	O
with	O
bilingual	O
annotations	O
for	O
both	O
training	O
and	O
inference	O
.	O

2016	O
;	O
.	O

2019;Lin	O
et	O
al	O
.	O
,	O

For	O
instance	O
,	O
Doubly	B-MethodName
-	I-MethodName
ATT	I-MethodName
Arslan	O
et	O
al	O
.	O
,	O

To	O
this	O
end	O
,	O
Yao	O
and	O
Wan	O
(	O
2020	O
)	O
and	O
replace	O
the	O
vanilla	O
Transformer	B-MethodName
encoder	O
with	O
a	O
multi	O
-	O
modal	O
encoder	O
.	O

Besides	O
the	O
exploration	O
on	O
network	O
structure	O
,	O
researchers	O
also	O
propose	O
to	O
leverage	O
the	O
benefits	O
of	O
multi	O
-	O
tasking	O
to	O
improve	O
MMT	B-TaskName
(	O
Elliott	O
and	O
Kádár	O
,	O
2017;Zhou	O
et	O
al	O
.	O
,	O

The	O
Imagination	B-MethodName
architecture	O
(	O
Elliott	O
and	O
Kádár	O
,	O
2017	O
;	O
decomposes	O
multimodal	O
translation	O
into	O
two	O
subtasks	O
:	O
translation	O
task	O
and	O
an	O
auxiliary	O
visual	O
reconstruction	O
task	O
,	O
which	O
encourages	O
the	O
model	O
to	O
learn	O
a	O
visually	O
grounded	O
source	O
sentence	O
representation	O
.	O

Retrieval	O
-	O
based	O
MMT	B-TaskName
The	O
effectiveness	O
of	O
conventional	O
MMT	B-TaskName
heavily	O
relies	O
on	O
the	O
availability	O
of	O
images	O
with	O
bilingual	O
annotations	O
.	O

2020	O
)	O
propose	O
UVR	B-MethodName
-	I-MethodName
NMT	I-MethodName
that	O
integrates	O
a	O
retrieval	O
component	O
into	O
MMT	B-TaskName
.	O

This	O
creates	O
image	O
-	O
bilingual	O
-	O
annotation	O
instances	O
for	O
training	O
.	O

Retrieval	O
-	O
based	O
models	O
have	O
been	O
shown	O
to	O
improve	O
performance	O
across	O
a	O
variety	O
of	O
NLP	O
tasks	O
besides	O
MMT	B-TaskName
,	O
such	O
as	O
question	O
answering	O
(	O
Guu	O
et	O
al	O
.	O
,	O

In	O
this	O
section	O
we	O
introduce	O
two	O
interpretable	O
MMT	B-TaskName
models	O
:	O
(	O
1	O
)	O
Gated	B-MethodName
Fusion	I-MethodName
for	O
conventional	O
MMT	B-TaskName
and	O
(	O
2	O
)	O
Dense	B-MethodName
-	I-MethodName
Retrieval	I-MethodName
-	I-MethodName
augmented	I-MethodName
MMT	I-MethodName
(	O
RMMT	B-MethodName
)	O
for	O
retrieval	O
-	O
based	O
MMT	B-TaskName
.	O

Given	O
a	O
source	O
sentence	O
x	O
of	O
length	O
T	O
and	O
an	O
associated	O
image	O
z	O
,	O
we	O
compute	O
the	O
probability	O
of	O
generating	O
target	O
sentence	O
y	O
of	O
length	O
N	O
by	O
:	O
p(y|x	O
,	O
z	O
)	O
=	O
N	O
i	O
p	O
θ	O
(	O
y	O
i	O
|	O
x	O
,	O
z	O
,	O
y	O
<	O
i	O
)	O
,	O
(	O
1)where	O
p	O
θ	O
(	O
y	O
i	O
|	O
x	O
,	O
z	O
,	O
y	O
<	O
i	O
)	O
is	O
implemented	O
with	O
a	O
Transformer	B-MethodName
-	O
based	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

Specifically	O
,	O
we	O
first	O
feed	O
x	O
into	O
a	O
vanilla	O
Transformer	B-MethodName
encoder	O
to	O
obtain	O
a	O
textual	O
representation	O
H	O
text	O
∈	O
R	O
T	O
×d	O
,	O
which	O
is	O
then	O
fused	O
with	O
visual	O
representation	O
Embed	O
image	O
(	O
z	O
)	O
before	O
fed	O
into	O
the	O
Transformer	B-MethodName
decoder	O
.	O

Note	O
that	O
this	O
gating	O
mechanism	O
has	O
been	O
a	O
building	O
block	O
for	O
many	O
recent	O
MMT	B-TaskName
systems	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2020;Lin	O
et	O
al	O
.	O
,	O

We	O
are	O
,	O
however	O
,	O
the	O
first	O
to	O
focus	O
on	O
its	O
interpretability	O
.	O

Finally	O
,	O
we	O
generate	O
the	O
output	O
vector	O
H	O
by	O
:	O
H	O
=	O
H	O
text	O
+	O
Λ	O
Embed	O
image	O
(	O
z).(3)H	O
is	O
then	O
fed	O
into	O
the	O
decoder	O
directly	O
for	O
translation	O
as	O
in	O
vanilla	O
Transformer	B-MethodName
.	O

RMMT	B-MethodName
consists	O
of	O
two	O
sequential	O
components:(1	O
)	O
an	O
image	O
retriever	O
p(z|x	O
)	O
that	O
takes	O
x	O
as	O
input	O
and	O
returns	O
Top	O
-	O
K	O
most	O
relevant	O
images	O
from	O
an	O
image	O
database;(2	O
)	O
a	O
multi	O
-	O
modal	O
translator	O
p(y|x	O
,	O
Z	O
)	O
=	O
N	O
i	O
p	O
θ	O
(	O
y	O
i	O
|	O
x	O
,	O
Z	O
,	O
y	O
<	O
i	O
)	O
that	O
generates	O
each	O
y	O
i	O
conditioned	O
on	O
the	O
input	O
sentence	O
x	O
,	O
the	O
image	O
set	O
Z	O
returned	O
by	O
the	O
retriever	O
,	O
and	O
the	O
previously	O
generated	O
tokens	O
y	O
<	O
i	O
.Image	O
Retriever	O
Based	O
on	O
the	O
TF	O
-	O
IDF	O
model	O
,	O
searching	O
in	O
existing	O
retrieval	O
-	O
based	O
MMT	B-TaskName
(	O
Zhang	O
et	O
al	O
.	O
,	O

To	O
improve	O
the	O
recall	O
of	O
our	O
image	O
retriever	O
,	O
we	O
compute	O
the	O
similarity	O
between	O
a	O
sentence	O
x	O
and	O
an	O
image	O
z	O
with	O
inner	O
product	O
:	O
sim(x	O
,	O
z	O
)	O
=	O
Embed	O
text	O
(	O
x	O
)	O
Embed	O
image	O
(	O
z),where	O
Embed	O
text	O
(	O
x	O
)	O
and	O
Embed	O
image	O
(	O
z	O
)	O
are	O
ddimensional	O
representations	O
of	O
x	O
and	O
z	O
,	O
respectively	O
.	O

Multimodal	O
Translator	O
Different	O
from	O
Gated	B-MethodName
Fusion	I-MethodName
,	O
p(y|x	O
,	O
Z	O
)	O
now	O
is	O
conditioning	O
on	O
a	O
set	O
of	O
images	O
rather	O
than	O
one	O
single	O
image	O
.	O

We	O
use	O
a	O
transformation	O
layer	O
f	O
θ	O
(	O
*	O
)	O
to	O
extract	O
salient	O
features	O
from	O
Embed	O
image	O
(	O
Z	O
)	O
and	O
obtain	O
a	O
compressed	O
representation	O
R	O
d	O
of	O
Z.	O
After	O
the	O
transformation	O
,	O
ideally	O
,	O
we	O
can	O
implement	O
p(y|x	O
,	O
Z	O
)	O
using	O
any	O
existing	O
MMT	B-TaskName
models	O
.	O

For	O
interpretability	O
,	O
we	O
follow	O
the	O
Gated	B-MethodName
Fusion	I-MethodName
model	O
to	O
fuse	O
the	O
textual	O
and	O
visual	O
representations	O
with	O
a	O
learnable	O
gating	O
matrix	O
Λ	O
:	O
H	O
=	O
H	O
text	O
+	O
Λf	O
θ	O
(	O
Embed	O
image	O
(	O
Z	O
)	O
)	O
.	O
(	O

In	O
this	O
section	O
,	O
we	O
evaluate	O
our	O
models	O
on	O
the	O
Multi30k	B-DatasetName
and	O
VaTex	B-DatasetName
benchmark	O
.	O

We	O
perform	O
experiments	O
on	O
the	O
widely	O
-	O
used	O
MMT	B-TaskName
datasets	O
:	O
Multi30k	B-DatasetName
.	O

We	O
follow	O
a	O
standard	O
split	O
of	O
29,000	B-HyperparameterValue
instances	O
for	O
training	B-HyperparameterName
,	O
1,014	B-HyperparameterValue
for	O
validation	B-HyperparameterName
and	O
1,000	B-HyperparameterValue
for	O
testing	B-HyperparameterName
(	O
Test2016	O
)	O
.	O

We	O
merge	O
the	O
source	O
and	O
target	O
sentences	O
in	O
the	O
officially	O
preprocessed	O
version	O
of	O
Multi30k	B-DatasetName
2	O
to	O
build	O
a	O
joint	O
vocabulary	O
.	O

2016	O
)	O
with	O
10,000	B-HyperparameterValue
merging	B-HyperparameterName
operations	I-HyperparameterName
to	O
segment	O
words	O
into	O
subwords	O
,	O
which	O
generates	O
a	O
vocabulary	O
of	O
9,712	O
(	O
9,544	O
)	O
tokens	O
for	O
En	O
-	O
De	O
(	O
En	O
-	O
Fr	O
)	O
.	O

2015	O
)	O
that	O
has	O
overlapping	O
instances	O
with	O
Multi30k	B-DatasetName
removed	O
.	O

We	O
use	O
Multi30k	B-DatasetName
's	O
validation	O
set	O
to	O
evaluate	O
the	O
retriever	O
.	O

We	O
measure	O
the	O
performance	O
by	O
recall	B-MetricName
-	I-MetricName
at	I-MetricName
-	I-MetricName
K	I-MetricName
(	O
R@K	B-MetricName
)	O
,	O
which	O
is	O
defined	O
as	O
the	O
fraction	O
of	O
queries	O
whose	O
closest	O
K	O
images	O
retrieved	O
contain	O
the	O
correct	O
images	O
.	O

The	O
pre	O
-	O
trained	O
retriever	O
achieves	O
R@1	B-MetricName
of	O
22.8	B-MetricValue
%	I-MetricValue
and	O
R@5	B-MetricName
of	O
39.6	B-MetricValue
%	I-MetricValue
.	O

Base	O
is	O
a	O
widely	O
-	O
used	O
model	O
configuration	O
for	O
Transformer	B-MethodName
in	O
both	O
text	O
-	O
only	O
translation	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
and	O
MMT	B-TaskName
(	O
Grönroos	O
et	O
al	O
.	O
,	O

However	O
,	O
for	O
small	O
datasets	O
like	O
Multi30k	B-DatasetName
,	O
training	O
such	O
a	O
large	O
model	O
(	O
about	O
50	O
million	O
parameters	O
)	O
could	O
cause	O
overfitting	O
.	O

2019	O
)	O
,	O
can	O
still	O
overfit	O
on	O
Multi30k	B-DatasetName
.	O

We	O
therefore	O
perform	O
grid	O
search	O
on	O
the	O
En→De	O
validation	O
set	O
in	O
Multi30k	B-DatasetName
and	O
obtain	O
a	O
Tiny	O
configuration	O
that	O
works	O
surprisingly	O
well	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
with	B-HyperparameterName
β	I-HyperparameterName
1	I-HyperparameterName
=	O
0.9	B-HyperparameterValue
,	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
for	O
model	O
optimization	O
.	O

We	O
start	O
training	O
with	O
a	O
warmup	B-HyperparameterName
phase	O
(	O
2,000	B-HyperparameterValue
steps	I-HyperparameterValue
)	O
where	O
we	O
linearly	O
increase	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
from	O
10	B-HyperparameterValue
−7	I-HyperparameterValue
to	O
0.005	B-HyperparameterValue
.	O

Each	O
training	O
batch	O
contains	O
at	O
most	O
4,096	B-HyperparameterValue
source	B-HyperparameterName
/	I-HyperparameterName
target	I-HyperparameterName
tokens	I-HyperparameterName
.	O

We	O
set	O
label	B-HyperparameterName
smoothing	I-HyperparameterName
weight	I-HyperparameterName
to	B-HyperparameterValue
0.1	I-HyperparameterValue
,	O
dropout	B-HyperparameterName
to	O
0.3	B-HyperparameterValue
.	O

2020	O
)	O
to	O
early	B-HyperparameterName
-	I-HyperparameterName
stop	I-HyperparameterName
the	O
training	O
if	O
validation	O
loss	O
does	O
not	O
improve	O
for	O
ten	B-HyperparameterValue
epochs	I-HyperparameterValue
.	O

We	O
perform	O
beam	O
search	O
with	O
beam	B-HyperparameterName
size	I-HyperparameterName
set	O
to	O
5	B-HyperparameterValue
.	O

We	O
report	O
4	B-MetricName
-	I-MetricName
gram	I-MetricName
BLEU	I-MetricName
and	O
METEOR	B-MetricName
scores	O
for	O
all	O
test	O
sets	O
.	O

All	O
models	O
are	O
trained	O
and	O
evaluated	O
on	O
one	O
single	O
machine	O
with	O
two	O
Titan	O
P100	O
GPUs	O
.	O

Our	O
baselines	O
can	O
be	O
categorized	O
into	O
three	O
types:•	O
The	O
text	O
-	O
only	O
Transformer;•	B-MethodName
The	O
conventional	O
MMT	B-TaskName
models	O
:	O
Doubly	B-MethodName
-	I-MethodName
ATT	I-MethodName
and	O
Imagination;•	B-MethodName
The	O
retrieval	O
-	O
based	O
MMT	B-TaskName
models	O
:	O
UVR	O
-	O
NMT.Details	O
of	O
these	O
methods	O
can	O
be	O
found	O
in	O
Section	O
2	O
.	O

We	O
use	O
top-5	O
retrieved	O
images	O
for	O
both	O
UVR	B-MethodName
-	I-MethodName
NMT	I-MethodName
and	O
our	O
RMMT	B-MethodName
.	O

We	O
also	O
consider	O
two	O
more	O
recent	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
conventional	O
methods	O
for	O
reference	O
:	O
GMNMT	B-MethodName
and	O
DCCN	B-MethodName
(	O
Lin	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
whose	O
results	O
are	O
reported	O
as	O
in	O
their	O
papers	O
.	O

Note	O
that	O
most	O
MMT	B-TaskName
methods	O
are	O
difficult	O
(	O
or	O
even	O
impossible	O
)	O
to	O
interpret	O
.	O

While	O
there	O
exist	O
some	O
interpretable	O
methods	O
(	O
e.g.	O
,	O
UVR	B-MethodName
-	I-MethodName
NMT	I-MethodName
)	O
that	O
contain	O
gated	B-MethodName
fusion	I-MethodName
layers	O
similar	O
to	O
ours	O
,	O
they	O
perform	O
sophisticated	O
transformations	O
on	O
visual	O
representation	O
before	O
fusion	O
,	O
which	O
lowers	O
the	O
interpretability	O
of	O
the	O
gating	O
matrix	O
.	O

For	O
example	O
,	O
in	O
the	O
gated	B-MethodName
fusion	I-MethodName
layer	O
of	O
UVR	B-MethodName
-	I-MethodName
NMT	I-MethodName
,	O
we	O
observe	O
that	O
the	O
visual	O
vector	O
is	O
order	O
-	O
of	O
-	O
magnitude	O
smaller	O
than	O
the	O
textual	O
vector	O
.	O

Table	O
1	O
shows	O
the	O
BLEU	B-MetricName
scores	O
of	O
these	O
methods	O
on	O
the	O
Multi30k	B-DatasetName
dataset	O
.	O

From	O
the	O
table	O
,	O
we	O
see	O
that	O
although	O
we	O
can	O
replicate	O
similar	O
BLEU	B-MetricName
scores	O
of	O
Transformer	B-MethodName
-	I-MethodName
Base	I-MethodName
as	O
reported	O
in	O
(	O
Grönroos	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
these	O
scores	O
(	O
Row	O
1	O
)	O
are	O
significantly	O
outperformed	O
by	O
Transformer	B-MethodName
-	I-MethodName
Small	I-MethodName
and	O
Transformer	B-MethodName
-	I-MethodName
Tiny	I-MethodName
,	O
which	O
have	O
fewer	O
parameters	O
.	O

This	O
shows	O
that	O
Transformer	B-MethodName
-	I-MethodName
Base	I-MethodName
could	O
overfit	O
the	O
Multi30k	B-DatasetName
dataset	O
.	O

Transformer	B-MethodName
-	O
Tiny	O
,	O
whose	O
number	O
of	O
parameters	O
is	O
about	O
20	O
times	O
smaller	O
than	O
that	O
of	O
Transformer	B-MethodName
-	I-MethodName
Base	I-MethodName
,	O
is	O
more	O
robust	O
and	O
efficient	O
in	O
our	O
test	O
cases	O
.	O

We	O
therefore	O
use	O
it	O
as	O
the	O
base	O
model	O
for	O
all	O
our	O
MMT	B-TaskName
systems	O
in	O
the	O
following	O
discussion	O
.	O

Based	O
on	O
the	O
Transformer	B-MethodName
-	I-MethodName
tiny	I-MethodName
model	O
,	O
both	O
our	O
proposed	O
models	O
(	O
Gated	B-MethodName
Fusion	I-MethodName
and	O
RMMT	B-MethodName
)	O
and	O
baseline	O
MMT	B-TaskName
models	O
(	O
Doubly	B-MethodName
-	I-MethodName
ATT	I-MethodName
,	O
Imagination	B-MethodName
and	O
UVR	B-MethodName
-	I-MethodName
NMT	I-MethodName
)	O
significantly	O
outperform	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
(	O
GMNMT	B-MethodName
and	O
DCCN	B-MethodName
)	O
on	O
En→De	O
translation	O
.	O

However	O
,	O
the	O
improvement	O
of	O
all	O
these	O
methods	O
(	O
Rows	O
4	O
-	O
10	O
)	O
over	O
the	O
base	O
Transformer	B-MethodName
-	I-MethodName
Tiny	I-MethodName
model	O
(	O
Row	O
3	O
)	O
is	O
very	O
marginal	O
.	O

We	O
further	O
evaluate	O
all	O
the	O
methods	O
on	O
the	O
ME	B-MetricName
-	I-MetricName
TEOR	I-MetricName
scores	O
(	O
see	O
Appendix	O
C	O
)	O
.	O

We	O
also	O
run	O
experiments	O
on	O
the	O
VaTex	B-DatasetName
dataset	O
(	O
see	O
Appendix	O
B	O
)	O
.	O

Although	O
various	O
MMT	B-TaskName
systems	O
have	O
been	O
proposed	O
recently	O
,	O
a	O
well	O
-	O
tuned	O
model	O
that	O
uses	O
text	O
only	O
remain	O
competitive	O
.	O

This	O
motivates	O
us	O
to	O
revisit	O
the	O
importance	O
of	O
visual	O
context	O
for	O
translation	O
in	O
MMT	B-TaskName
models	O
.	O

To	O
explore	O
the	O
need	O
for	O
visual	O
context	O
in	O
our	O
models	O
,	O
we	O
focus	O
on	O
the	O
interpretable	O
component	O
:	O
the	O
gated	B-MethodName
fusion	I-MethodName
layer	O
(	O
see	O
Equation	O
3	O
sual	O
context	O
to	O
perform	O
better	O
translation	O
.	O

We	O
first	O
study	O
models	O
'	O
behavior	O
after	O
convergence	O
.	O

In	O
other	O
words	O
,	O
visual	O
context	O
may	O
not	O
be	O
as	O
important	O
for	O
translation	O
as	O
previously	O
thought	O
.	O

The	O
Gated	B-MethodName
Fusion	I-MethodName
's	O
training	O
process	O
also	O
shed	O
some	O
light	O
on	O
how	O
the	O
model	O
accommodates	O
the	O
visual	O
information	O
during	O
training	O
.	O

We	O
find	O
that	O
,	O
Gated	B-MethodName
Fusion	I-MethodName
starts	O
with	O
a	O
relatively	O
high	O
Λ	O
(	O
>	O
0.5	O
)	O
,	O
but	O
quickly	O
decreases	O
to	O
≈	O
0.48	O
after	O
the	O
first	O
epoch	O
.	O

As	O
the	O
training	O
continues	O
,	O
Λ	O
gradually	O
decreases	O
to	O
roughly	O
zero	O
.	O

Compared	O
with	O
text	O
-	O
only	O
NMT	O
,	O
utilizing	O
visual	O
features	O
lowers	O
MMT	B-TaskName
models	O
'	O
trust	O
in	O
the	O
hidden	O
representations	O
generated	O
from	O
the	O
textual	O
encoders	O
.	O

As	O
the	O
training	O
continues	O
,	O
the	O
textual	O
encoder	O
learns	O
to	O
represent	O
source	O
text	O
better	O
and	O
the	O
importance	O
of	O
visual	O
context	O
gradually	O
decreases	O
.	O

In	O
the	O
previous	O
section	O
,	O
we	O
hypothesize	O
that	O
the	O
gains	O
of	O
MMT	B-TaskName
systems	O
come	O
from	O
some	O
regularization	O
effects	O
.	O

To	O
verify	O
our	O
hypothesis	O
,	O
we	O
conduct	O
experiments	O
based	O
on	O
two	O
widely	O
used	O
regularization	O
techniques	O
:	O
random	O
noise	O
injection	O
and	O
weight	O
decay	O
(	O
Hanson	O
and	O
Pratt	O
,	O
1989	O
)	O
.	O

A	O
MMT	B-TaskName
model	O
equipped	O
with	O
ResNet	O
features	O
is	O
denoted	O
as	O
a	O
ResNet	O
-	O
based	O
model	O
,	O
while	O
the	O
same	O
model	O
with	O
random	O
initialization	O
is	O
denoted	O
as	O
a	O
noise	O
-	O
based	O
model	O
.	O

Note	O
that	O
values	O
in	O
parentheses	O
indicate	O
the	O
performance	O
gap	O
between	O
the	O
ResNetbased	O
model	O
and	O
its	O
noise	O
-	O
based	O
adversary	O
.	O

Table	O
3	O
shows	O
BLEU	B-MetricName
scores	O
on	O
the	O
Multi30k	B-DatasetName
dataset	O
.	O

From	O
the	O
table	O
,	O
we	O
observe	O
that	O
,	O
among	O
18	O
(	O
3	O
methods	O
×	O
3	O
test	O
sets	O
×	O
2	O
tasks	O
)	O
contests	O
with	O
the	O
Transformer	B-MethodName
model	O
(	O
row	O
1	O
)	O
,	O
noise	O
-	O
based	O
models	O
(	O
rows	O
2	O
-	O
4	O
)	O
achieve	O
better	O
performance	O
13	O
times	O
,	O
while	O
ResNet	O
-	O
based	O
models	O
win	O
14	O
cases	O
.	O

We	O
observe	O
similar	O
results	O
when	O
repeating	O
above	O
evaluation	O
using	O
METEOR	B-MetricName
(	O
Tabel	O
9	O
)	O
and	O
on	O
VaTex	B-DatasetName
(	O
Table	O
7	O
)	O
.	O

In	O
MMT	B-TaskName
systems	O
,	O
adding	O
random	O
noise	O
or	O
visual	O
context	O
can	O
help	O
reduce	O
overfitting	O
when	O
translating	O
sentences	O
in	O
Multi30k	B-DatasetName
,	O
which	O
are	O
short	O
and	O
repetitive	O
(	O
Caglayan	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
2	O
norm	O
of	O
model	O
weights	O
in	O
ResNet	O
-	O
based	O
Gated	B-MethodName
Fusion	I-MethodName
and	O
noise	O
-	O
based	O
Gated	B-MethodName
Fusion	I-MethodName
are	O
only	O
97.7	O
%	O
and	O
95.2	O
%	O
of	O
that	O
in	O
Transformer	B-MethodName
on	O
En→De	O
,	O
respectively	O
.	O

Further	O
,	O
we	O
regularize	O
the	O
models	O
with	O
weight	B-HyperparameterName
decay	I-HyperparameterName
.	O

We	O
consider	O
three	O
models	O
:	O
the	O
text	O
-	O
only	O
Trans-	B-MethodName
former	I-MethodName
,	O
the	O
representative	O
existing	O
MMT	B-TaskName
method	O
Doubly	B-MethodName
-	I-MethodName
ATT	I-MethodName
,	O
and	O
our	O
Gated	B-MethodName
Fusion	I-MethodName
method	O
.	O

Figure	O
2	O
and	O
3	O
(	O
in	O
Appendix	O
C	O
)	O
show	O
the	O
BLEU	B-MetricName
and	O
METEOR	B-MetricName
scores	O
of	O
these	O
methods	O
on	O
En→De	O
translation	O
as	O
weight	O
decay	O
rate	O
changes	O
,	O
respectively	O
.	O

We	O
see	O
that	O
the	O
best	O
results	O
of	O
the	O
text	O
-	O
only	O
Transformer	B-MethodName
model	O
with	O
fine	O
-	O
tuned	O
weight	O
decay	O
are	O
comparable	O
or	O
even	O
better	O
than	O
that	O
of	O
the	O
MMT	B-TaskName
models	O
Doubly	B-MethodName
-	I-MethodName
ATT	I-MethodName
and	O
Gated	B-MethodName
Fusion	I-MethodName
that	O
utilize	O
visual	O
context	O
.	O

2019	O
)	O
experimentally	O
show	O
that	O
,	O
with	O
limited	O
textual	O
context	O
(	O
e.g.	O
,	O
masking	O
some	O
input	O
tokens	O
)	O
,	O
MMT	B-TaskName
models	O
will	O
utilize	O
the	O
visual	O
input	O
for	O
translation	O
.	O

This	O
further	O
motivates	O
us	O
to	O
investigate	O
when	O
visual	O
context	O
is	O
needed	O
in	O
MMT	B-TaskName
models	O
.	O

2019	O
)	O
.	O

A	O
visually	O
grounded	O
token	O
is	O
the	O
one	O
that	O
has	O
more	O
than	O
30	O
occurrences	O
in	O
the	O
Multi30k	B-DatasetName
dataset	O
with	O
stop	O
words	O
removed	O
.	O

Masking	O
all	O
visually	O
grounded	O
tokens	O
will	O
affect	O
around	O
45	O
%	O
of	O
tokens	O
in	O
Multi30k	B-DatasetName
.	O

In	O
particular	O
,	O
we	O
select	O
Transformer	O
,	O
Gated	B-MethodName
Fusion	I-MethodName
and	O
RMMT	B-MethodName
as	O
representative	O
methods	O
.	O

From	O
the	O
table	O
,	O
we	O
see	O
that	O
random	O
noise	O
injection	O
(	O
row	O
5,6	O
)	O
and	O
weight	O
decay	O
(	O
row	O
2	O
)	O
can	O
only	O
bring	O
marginal	O
improvement	O
over	O
the	O
text	O
-	O
only	O
Transformer	B-MethodName
model	O
.	O

For	O
example	O
,	O
RMMT	B-MethodName
achieves	O
almost	O
50	O
%	O
gain	O
over	O
the	O
Transformer	B-MethodName
on	O
the	O
BLEU	B-MetricName
score	O
.	O

Moreover	O
,	O
both	O
Gated	B-MethodName
Fusion	I-MethodName
and	O
RMMT	B-MethodName
using	O
ResNet	O
features	O
lead	O
to	O
a	O
larger	O
Λ	O
value	O
than	O
that	O
when	O
textual	O
context	O
is	O
sufficient	O
as	O
shown	O
in	O
Table	O
2	O
.	O

Those	O
results	O
further	O
suggest	O
that	O
visual	O
context	O
is	O
needed	O
when	O
textual	O
context	O
is	O
insufficient	O
.	O

Therefore	O
,	O
to	O
fully	O
exert	O
the	O
power	O
of	O
MMT	B-TaskName
systems	O
,	O
we	O
emphasize	O
the	O
need	O
for	O
a	O
new	O
MMT	B-TaskName
benchmark	O
,	O
in	O
which	O
visual	O
context	O
is	O
deemed	O
necessary	O
to	O
generate	O
correct	O
translation	O
.	O

Interestingly	O
,	O
even	O
with	O
ResNet	O
features	O
,	O
we	O
observe	O
a	O
significant	O
drop	O
in	O
both	O
BLEU	B-MetricName
and	O
ME	B-MetricName
-	I-MetricName
TEOR	I-MetricName
scores	O
compared	O
with	O
those	O
in	O
Table	O
1	O
and	O
8	O
,	O
similar	O
to	O
that	O
reported	O
in	O
(	O
Chowdhury	O
and	O
Elliott	O
,	O
2019	O
)	O
.	O

For	O
example	O
,	O
in	O
Table	O
5	O
(	O
a	O
)	O
,	O
although	O
Gated	B-MethodName
Fusion	I-MethodName
can	O
successfully	O
identify	O
the	O
main	O
objects	O
in	O
the	O
image	O
(	O
"	O
little	O
boys	O
pose	O
with	O
a	O
puppy	O
"	O
)	O
,	O
it	O
fails	O
to	O
generate	O
the	O
more	O
abstract	O
concept	O
"	O
family	O
picture	O
"	O
.	O

For	O
example	O
,	O
in	O
Table	O
5	O
(	O
b	O
)	O
,	O
we	O
see	O
that	O
Gated	B-MethodName
Fusion	I-MethodName
incorrectly	O
generates	O
the	O
word	O
frauen	O
(	O
women	O
)	O
because	O
it	O
captures	O
the	O
woman	O
at	O
the	O
top	O
-	O
right	O
corner	O
of	O
the	O
image	O
.	O

We	O
use	O
underline	O
to	O
denote	O
masked	O
tokens	O
,	O
and	O
strikethrough	O
(	O
bold	O
)	O
font	O
to	O
denote	O
incorrect	O
(	O
correct	O
)	O
lexical	O
choices	O
.	O

We	O
use	O
Gated	B-MethodName
Fusion	I-MethodName
for	O
analysis	O
.	O

Finally	O
,	O
we	O
discuss	O
how	O
our	O
findings	O
might	O
benefit	O
future	O
MMT	B-TaskName
research	O
.	O

First	O
,	O
a	O
benchmark	O
that	O
requires	O
more	O
visual	O
information	O
than	O
Multi30k	B-DatasetName
to	O
solve	O
is	O
desired	O
.	O

As	O
shown	O
in	O
Section	O
5.2	O
,	O
sentences	O
in	O
Multi30k	B-DatasetName
are	O
rather	O
simple	O
and	O
easy	O
-	O
tounderstand	O
.	O

While	O
the	O
MSCOCO	O
test	O
set	O
in	O
Multi30k	B-DatasetName
contains	O
ambiguous	O
verbs	O
and	O
encourages	O
models	O
to	O
use	O
image	O
sources	O
for	O
disambiguation	O
,	O
we	O
still	O
lack	O
a	O
corresponding	O
training	O
set	O
.	O

Third	O
,	O
we	O
find	O
that	O
visual	O
feature	O
selection	O
is	O
also	O
critical	O
for	O
MMT	B-TaskName
's	O
performance	O
.	O

Therefore	O
,	O
a	O
more	O
effective	O
end	O
-	O
to	O
-	O
end	O
visual	O
feature	O
selection	O
technique	O
is	O
needed	O
,	O
which	O
can	O
be	O
further	O
integrated	O
into	O
MMT	B-TaskName
systems	O
to	O
improve	O
performance	O
.	O

In	O
this	O
paper	O
we	O
devise	O
two	O
interpretable	O
models	O
that	O
exhibit	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
widely	O
adopted	O
MMT	B-TaskName
datasets	O
-Multi30k	B-DatasetName
and	O
the	O
new	O
video	O
-	O
based	O
dataset	O
-VaTex	B-DatasetName
.	O

Our	O
analysis	O
on	O
the	O
proposed	O
models	O
,	O
as	O
well	O
as	O
on	O
other	O
existing	O
MMT	B-TaskName
systems	O
,	O
suggests	O
that	O
visual	O
context	O
helps	O
MMT	B-TaskName
in	O
the	O
similar	O
vein	O
as	O
regularization	O
methods	O
(	O
e.g.	O
,	O
weight	O
decay	O
)	O
,	O
under	O
sufficient	O
textual	O
context	O
.	O

Those	O
empirical	O
findings	O
,	O
however	O
,	O
should	O
not	O
be	O
understood	O
as	O
us	O
downplaying	O
the	O
importance	O
existing	O
datasets	O
and	O
models	O
;	O
we	O
believe	O
that	O
sophisticated	O
MMT	B-TaskName
models	O
are	O
necessary	O
for	O
effective	O
grounding	O
of	O
visual	O
context	O
into	O
translation	O
.	O

Our	O
goal	O
,	O
rather	O
,	O
is	O
to	O
(	O
1	O
)	O
provide	O
additional	O
clarity	O
on	O
the	O
remaining	O
shortcomings	O
of	O
current	O
dataset	O
and	O
stress	O
the	O
need	O
for	O
new	O
datasets	O
to	O
move	O
the	O
field	O
forward	O
;	O
(	O
2	O
)	O
emphasise	O
the	O
importance	O
of	O
interpretability	O
in	O
MMT	B-TaskName
research	O
.	O

We	O
observe	O
that	O
although	O
most	O
MMT	B-TaskName
systems	O
show	O
improvement	O
over	O
the	O
Transformer	B-MethodName
baseline	O
,	O
the	O
gains	O
are	O
quite	O
marginal	O
.	O

Indicating	O
that	O
although	O
imagebased	O
MMT	B-TaskName
models	O
can	O
be	O
directly	O
applied	O
to	O
video	O
-	O
based	O
MMT	B-TaskName
,	O
there	O
is	O
still	O
room	O
for	O
improvement	O
due	O
to	O
the	O
challenge	O
of	O
video	O
understanding	O
.	O

We	O
also	O
note	O
that	O
(	O
a	O
)	O
regularize	O
the	O
text	O
-	O
only	O
Transformer	B-MethodName
with	O
weight	O
decay	O
demonstrates	O
similar	O
gains	O
as	O
injecting	O
video	O
information	O
into	O
the	O
models	O
;	O
(	O
b	O
)	O
replacing	O
video	O
features	O
with	O
random	O
noise	O
replicate	O
comparable	O
performance	O
,	O
which	O
further	O
supports	O
our	O
findings	O
in	O
Section	O
5.2	O
.	O

We	O
also	O
report	O
our	O
results	O
based	O
on	O
ME	B-MetricName
-	I-MetricName
TEOR	I-MetricName
(	O
Banerjee	O
and	O
Lavie	O
,	O
2005	O
)	O
,	O
which	O
consistently	O
demonstrates	O
higher	O
correlation	O
with	O
human	O
judgments	O
than	O
BLEU	B-MetricName
does	O
in	O
independent	O
evaluations	O
such	O
as	O
in	O
EMNLP	O
WMT	O
2011	O
3	O
.	O

From	O
Table	O
8	O
,	O
we	O
can	O
see	O
that	O
on	O
En	O
-	O
Fr	O
translation	O
,	O
MMT	B-TaskName
systems	O
demonstrate	O
similar	O
improvements	O
over	O
text	O
-	O
only	O
baselines	O
in	O
both	O
METEOR	B-MetricName
and	O
BLEU(see	B-MetricName
Table	O
1	O
)	O
.	O

On	O
En	O
-	O
De	O
translation	O
,	O
however	O
,	O
MMT	B-TaskName
systems	O
are	O
mostly	O
on	O
-	O
par	O
with	O
Transformer	B-MethodName
-	I-MethodName
tiny	I-MethodName
on	O
METEOR	B-MetricName
and	O
do	O
not	O
show	O
consistent	O
gains	O
as	O
BLEU	B-MetricName
.	O

We	O
hypothesis	O
the	O
reason	O
being	O
that	O
En	O
-	O
De	O
sets	O
are	O
created	O
in	O
a	O
imageblind	O
fashion	O
,	O
in	O
which	O
the	O
crowd	O
-	O
sourcing	O
workers	O
produce	O
translations	O
without	O
seeing	O
the	O
images	O
.	O

Such	O
that	O
source	O
sentence	O
can	O
already	O
provide	O
sufficient	O
context	O
for	O
translation	O
.	O

Although	O
BLEU	B-MetricName
is	O
unable	O
to	O
elicit	O
this	O
difference	O
,	O
evaluation	O
based	O
on	O
ME	B-MetricName
-	I-MetricName
TEOR	I-MetricName
captured	O
it	O
and	O
confirmed	O
previous	O
research	O
.	O

We	O
also	O
compute	O
METEOR	B-MetricName
scores	O
for	O
our	O
experiments	O
that	O
regularize	O
models	O
with	O
random	O
noise	O
(	O
see	O
Table	O
9	O
)	O
and	O
weight	B-HyperparameterName
decay	I-HyperparameterName
(	O
see	O
Figure	O
3	O
)	O
.	O

The	O
results	O
are	O
consistent	O
with	O
those	O
evaluated	O
using	O
BLEU	B-MetricName
and	O
further	O
complement	O
our	O
early	O
findings	O
.	O

We	O
also	O
evaluate	O
the	O
retrieval	O
-	O
based	O
model	O
RMMT	B-MethodName
on	O
text	O
-	O
only	O
corpus	O
-IWSLT'14	O
.	O

The	O
number	O
of	O
BPE	B-HyperparameterName
operations	I-HyperparameterName
is	O
set	O
to	O
20,000	B-HyperparameterValue
.	O

We	O
use	O
the	O
Small	O
configuration	O
in	O
all	O
our	O
experiments	O
.	O

The	O
dropout	B-HyperparameterName
and	O
label	B-HyperparameterName
smoothing	I-HyperparameterName
rate	I-HyperparameterName
are	O
set	O
to	O
0.3	B-HyperparameterValue
and	O
0.1	B-HyperparameterValue
,	O
respectively	O
.	O

2020	O
)	O
and	O
retrieve	O
top-5	O
images	O
from	O
Multi30	B-DatasetName
K	I-DatasetName
corpus	O
.	O

From	O
Table	O
10	O
,	O
we	O
see	O
that	O
Transformer	B-MethodName
without	O
weight	O
decay	O
is	O
marginally	O
outperformed	O
by	O
RMMT	B-MethodName
,	O
but	O
achieves	O
slightly	O
higher	O
BLEU	B-MetricName
scores	O
when	O
trained	O
with	O
a	O
0.0001	B-HyperparameterValue
weight	B-HyperparameterName
decay	I-HyperparameterName
.	O

Template	O
-	O
based	O
QG	O
uses	O
linguistically	O
-	O
informed	O
heuristics	O
to	O
transform	O
declarative	O
sentences	O
into	O
interrogatives	O
,	O
whereas	O
supervised	O
QG	O
uses	O
existing	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	B-TaskName
)	O
datasets	O
to	O
train	O
a	O
system	O
to	O
generate	O
a	O
question	O
given	O
a	O
passage	O
and	O
an	O
answer	O
.	O

A	O
disadvantage	O
of	O
the	O
heuristic	O
approach	O
is	O
that	O
the	O
generated	O
questions	O
are	O
heavily	O
tied	O
to	O
their	O
declarative	O
counterparts	O
.	O

A	O
disadvantage	O
of	O
the	O
supervised	O
approach	O
is	O
that	O
they	O
are	O
heavily	O
tied	O
to	O
the	O
domain	O
/	O
language	O
of	O
the	O
QA	B-TaskName
dataset	O
used	O
as	O
training	O
data	O
.	O

The	O
resulting	O
questions	O
are	O
then	O
combined	O
with	O
the	O
original	O
news	O
articles	O
to	O
train	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
QG	O
model	O
.	O

We	O
extrinsically	O
evaluate	O
our	O
approach	O
using	O
unsupervised	B-TaskName
QA	I-TaskName
:	O
our	O
QG	O
model	O
is	O
used	O
to	O
generate	O
synthetic	O
QA	B-TaskName
pairs	O
for	O
training	O
a	O
QA	B-TaskName
model	O
.	O

Experimental	O
results	O
show	O
that	O
,	O
trained	O
with	O
only	O
20k	O
English	O
Wikipedia	O
-	O
based	O
synthetic	O
QA	B-TaskName
pairs	O
,	O
the	O
QA	B-TaskName
model	O
substantially	O
outperforms	O
previous	O
unsupervised	O
models	O
on	O
three	O
in	O
-	O
domain	O
datasets	O
(	O
SQuAD1.1	B-DatasetName
,	O
Natural	B-DatasetName
Questions	I-DatasetName
,	O
TriviaQA	B-DatasetName
)	O
and	O
three	O
out	O
-	O
of	O
-	O
domain	O
datasets	O
(	O
NewsQA	B-DatasetName
,	O
BioASQ	B-DatasetName
,	O
DuoRC	B-DatasetName
)	O
,	O
demonstrating	O
the	O
transferability	O
of	O
the	O
approach	O
.	O

Additionally	O
,	O
QG	O
can	O
be	O
applied	O
to	O
Question	B-TaskName
Answering	I-TaskName
(	O
QA	B-TaskName
)	O
for	O
the	O
purpose	O
of	O
data	O
augmentation	O
(	O
Puri	O
et	O
al	O
.	O
,	O

2020	O
)	O
where	O
labeled	O
<	O
passage	O
,	O
answer	O
,	O
ques	O
-	O
tion	O
>	O
triples	O
are	O
combined	O
with	O
synthetic	O
<	O
passage	O
,	O
answer	O
,	O
question	O
>	O
triples	O
produced	O
by	O
a	O
QG	O
system	O
to	O
train	O
a	O
QA	B-TaskName
system	O
,	O
and	O
unsupervised	O
QA	B-TaskName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
in	O
which	O
only	O
the	O
QG	O
system	O
output	O
is	O
used	O
to	O
train	O
the	O
QA	B-TaskName
system	O
.	O

Early	O
work	O
on	O
QG	O
focused	O
on	O
template	O
or	O
rulebased	O
approaches	O
,	O
employing	O
syntactic	O
knowledge	O
to	O
manipulate	O
constituents	O
in	O
declarative	O
sentences	O
to	O
form	O
interrogatives	O
Smith	O
,	O
2009	O
,	O
2010	O
)	O
.	O

Although	O
template	O
-	O
based	O
methods	O
are	O
capable	O
of	O
generating	O
linguistically	O
correct	O
questions	O
,	O
the	O
resulting	O
questions	O
often	O
lack	O
variety	O
and	O
incur	O
high	O
lexical	O
overlap	O
with	O
corresponding	O
declarative	O
sentences	O
.	O

For	O
example	O
,	O
the	O
question	O
generated	O
from	O
the	O
sentence	O
Stephen	O
Hawking	O
announced	O
the	O
party	O
in	O
the	O
morning	O
,	O
with	O
Stephen	O
Hawking	O
as	O
the	O
candidate	O
answer	O
span	O
,	O
could	O
be	O
Who	O
announced	O
the	O
party	O
in	O
the	O
morning	O
?	O
,	O
with	O
a	O
high	O
level	O
of	O
lexical	O
overlap	O
between	O
the	O
generated	O
question	O
and	O
the	O
declarative	O
sentence	O
.	O

This	O
is	O
undesirable	O
in	O
a	O
QA	B-TaskName
system	O
(	O
Hong	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
and	O
are	O
commonly	O
trained	O
with	O
<	O
passage	O
,	O
answer	O
,	O
question	O
>	O
triples	O
taken	O
from	O
human	O
-	O
created	O
QA	B-TaskName
datasets	O
(	O
Dzendzik	O
et	O
al	O
.	O
,	O

Crucially	O
,	O
however	O
,	O
they	O
remain	O
semantically	O
close	O
to	O
the	O
passages	O
since	O
the	O
summaries	O
by	O
definition	O
contain	O
the	O
most	O
important	O
information	O
contained	O
in	O
the	O
passages	O
.	O

A	O
second	O
advantage	O
of	O
this	O
QG	O
approach	O
is	O
that	O
it	O
does	O
not	O
rely	O
on	O
the	O
existence	O
of	O
a	O
QA	B-TaskName
dataset	O
,	O
and	O
it	O
is	O
arguably	O
easier	O
to	O
obtain	O
summary	O
data	O
in	O
a	O
given	O
language	O
than	O
equivalent	O
QA	B-TaskName
data	O
since	O
summary	O
data	O
is	O
created	O
for	O
many	O
purposes	O
(	O
e.g.	O
news	O
,	O
review	O
and	O
thesis	O
summaries	O
)	O
whereas	O
many	O
QA	B-TaskName
datasets	O
are	O
created	O
specifically	O
for	O
training	O
a	O
QA	B-TaskName
system	O
.	O

We	O
employ	O
our	O
QG	O
model	O
to	O
generate	O
synthetic	O
QA	B-TaskName
data	O
to	O
train	O
a	O
QA	B-TaskName
model	O
in	O
an	O
unsupervised	O
setting	O
and	O
test	O
the	O
approach	O
with	O
six	O
English	O
QA	B-TaskName
datasets	O
:	O
SQuAD1.1	B-DatasetName
,	O
Natural	B-DatasetName
Questions	I-DatasetName
,	O
TriviaQA	B-DatasetName
,	O
NewsQA	B-DatasetName
,	O
BioASQ	B-DatasetName
and	O
DuoRC	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016;Kwiatkowski	O
et	O
al	O
.	O
,	O

Experiment	O
results	O
show	O
that	O
our	O
approach	O
substantially	O
improves	O
over	O
previous	O
unsupervised	B-TaskName
QA	I-TaskName
models	O
even	O
when	O
trained	O
on	O
sub	O
-	O
stantially	O
fewer	O
synthetic	O
QA	B-TaskName
examples	O
.	O

We	O
employ	O
our	O
QG	O
model	O
to	O
generate	O
synthetic	O
QA	B-TaskName
data	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
even	O
at	O
low	O
volumes	O
of	O
synthetic	O
training	O
data	O
.	O

Neural	O
seq2seq	O
generation	O
models	O
have	O
additionally	O
been	O
widely	O
employed	O
in	O
QG	O
,	O
with	O
QG	O
data	O
usually	O
borrowed	O
from	O
existing	O
QA	B-TaskName
datasets	O
(	O
Du	O
et	O
al	O
.	O
,	O

2017;Sun	O
et	O
al	O
.	O
,	O

2018	O
;	O
.	O

Consequently	O
,	O
there	O
has	O
been	O
a	O
shift	O
in	O
focus	O
to	O
evaluating	O
QG	O
using	O
an	O
extrinsic	O
evaluation	O
that	O
generates	O
synthetic	O
QA	B-TaskName
pairs	O
for	O
the	O
purpose	O
of	O
evaluating	O
their	O
effectiveness	O
as	O
a	O
data	O
augmentation	O
or	O
unsupervised	B-TaskName
QA	I-TaskName
approach	O
Puri	O
et	O
al	O
.	O
,	O

Unsupervised	B-TaskName
QA	I-TaskName
In	O
unsupervised	B-TaskName
QA	I-TaskName
,	O
the	O
QA	B-TaskName
model	O
is	O
trained	O
using	O
synthetic	O
data	O
based	O
on	O
a	O
QG	O
model	O
instead	O
of	O
an	O
existing	O
QA	B-TaskName
dataset	O
.	O

Instead	O
of	O
resorting	O
to	O
existing	O
QA	B-TaskName
datasets	O
,	O
unsupervised	O
QG	O
methods	O
have	O
been	O
employed	O
,	O
such	O
as	O
Unsupervised	B-MethodName
Neural	I-MethodName
Machine	I-MethodName
Translation	I-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

Fabbri	O
et	O
al	O
.	O
(	O

2020	O
)	O
and	O
propose	O
template	B-MethodName
/	I-MethodName
rule	I-MethodName
-	I-MethodName
based	I-MethodName
methods	O
for	O
generating	O
questions	O
and	O
employ	O
retrieved	O
paragraphs	O
and	O
cited	O
passages	O
as	O
source	O
passages	O
to	O
alleviate	O
the	O
problems	O
of	O
lexical	O
similarities	O
between	O
passages	O
and	O
questions	O
.	O
;	O

2020	O
)	O
additionally	O
employ	O
existing	O
QA	B-TaskName
datasets	O
to	O
train	O
a	O
QG	O
model	O
.	O

Although	O
related	O
,	O
this	O
work	O
falls	O
outside	O
the	O
scope	O
of	O
unsupervised	B-TaskName
QA	I-TaskName
.	O

Diverging	O
from	O
supervised	O
neural	O
question	O
generation	O
models	O
trained	O
on	O
existing	O
QA	B-TaskName
datasets	O
,	O
the	O
approach	O
we	O
propose	O
employs	O
synthetic	O
QG	O
data	O
,	O
that	O
we	O
create	O
from	O
summary	O
data	O
using	O
a	O
number	O
of	O
heuristics	O
,	O
to	O
train	O
a	O
QG	O
model	O
.	O

We	O
then	O
employ	O
the	O
trained	O
QG	O
model	O
to	O
generate	O
synthetic	O
QA	B-TaskName
data	O
that	O
is	O
further	O
employed	O
to	O
train	O
an	O
unsupervised	B-TaskName
QA	I-TaskName
model	O
.	O

DP	O
is	O
firstly	O
employed	O
as	O
a	O
means	O
of	O
identifying	O
the	O
main	O
verb	O
(	O
root	O
verb	O
)	O
,	O
in	O
addition	O
to	O
other	O
constituents	O
such	O
as	O
auxiliaries	O
.	O

NER	O
is	O
then	O
responsible	O
for	O
tagging	O
all	O
entities	O
in	O
the	O
summary	O
sentence	O
to	O
facilitate	O
discovery	O
of	O
the	O
most	O
appropriate	O
question	O
words	O
to	O
generate	O
.	O

We	O
first	O
ob-	O
tain	O
all	O
dependency	O
edges	O
and	O
labels	O
(	O
dps	O
)	O
,	O
NER	O
tags	O
(	O
ners	O
)	O
and	O
SRL	O
frames	O
(	O
srl_f	O
rames	O
)	O
of	O
a	O
summary	O
sentence	O
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
1	O
,	O
a	O
single	O
summary	O
sentence	O
generates	O
multiple	O
questions	O
when	O
its	O
SRL	O
frame	O
has	O
multiple	O
arguments	O
.	O

By	O
using	O
this	O
QG	O
data	O
to	O
train	O
a	O
neural	O
generation	O
model	O
,	O
we	O
expect	O
the	O
model	O
to	O
learn	O
a	O
combination	O
of	O
summarization	O
and	O
question	O
generation	O
.	O

To	O
train	O
the	O
question	O
generation	O
model	O
,	O
we	O
concatenate	O
each	O
passage	O
and	O
answer	O
to	O
form	O
a	O
sequence	O
:	O
passage	O
<	O
SEP	O
>	O
answer	O
<	O
SEP	O
>	O
,	O
where	O
<	O
SEP	O
>	O
is	O
a	O
special	O
token	O
used	O
to	O
separate	O
the	O
passage	O
and	O
answer	O
.	O

In	O
our	O
experiments	O
,	O
we	O
use	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

2020	O
)	O
for	O
generation	O
,	O
which	O
is	O
optimized	O
by	O
the	O
following	O
negative	O
log	O
likelihood	O
loss	O
function	O
:	O
L	O
=	O
−	O
N	O
i=1	O
logP	O
(	O
q	O
i	O
|C	O
,	O
A	O
)	O
(	O
1)where	O
q	O
i	O
is	O
the	O
i	O
-	O
th	O
token	O
in	O
the	O
question	O
,	O
and	O
C	O
and	O
A	O
are	O
context	O
and	O
answer	O
,	O
respectively	O
.	O

We	O
test	O
our	O
idea	O
of	O
using	O
summaries	O
in	O
question	O
generation	O
by	O
applying	O
the	O
questions	O
generated	O
by	O
our	O
QG	O
system	O
in	O
unsupervised	B-TaskName
QA	I-TaskName
.	O

We	O
describe	O
the	O
details	O
of	O
our	O
experiment	O
setup	O
,	O
followed	O
by	O
our	O
unsupervised	B-TaskName
QA	I-TaskName
results	O
on	O
six	O
English	O
benchmark	O
extractive	O
QA	B-TaskName
datasets	O
.	O

2017	O
)	O
to	O
obtain	O
dependency	B-MethodName
trees	I-MethodName
,	O
named	O
entities	O
and	O
semantic	O
role	O
labels	O
for	O
summary	O
sentences	O
,	O
before	O
further	O
employing	O
this	O
knowledge	O
to	O
generate	O
questions	O
from	O
summaries	O
following	O
the	O
algorithm	O
described	O
in	O
Section	O
3.1	O
.	O

We	O
remove	O
any	O
generated	O
<	O
passage	O
-	O
answer	O
-	O
question	O
>	O
triples	O
that	O
meet	O
one	O
or	O
more	O
of	O
the	O
following	O
three	O
conditions:1	O
.	O

Articles	O
longer	O
than	O
480	O
tokens	O
(	O
exceeding	O
the	O
maximum	O
BART	B-MethodName
input	O
length	O
)	O
;	O
2	O
.	O

For	O
training	O
the	O
QG	O
model	O
,	O
we	O
employ	O
implementations	O
of	O
BART	B-MethodName
(	O
Lewis	O
et	O
al	O
.	O
,	O

The	O
QG	O
model	O
we	O
employ	O
is	O
BART	B-MethodName
-	I-MethodName
base	I-MethodName
.	O

We	O
train	O
the	O
QG	O
model	O
on	O
the	O
QG	O
data	O
for	O
3	B-HyperparameterValue
epochs	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
,	O
using	O
the	O
AdamW	B-HyperparameterValue
optimizer	B-HyperparameterName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
.	O

Datasets	O
We	O
carry	O
out	O
experiments	O
on	O
six	O
extractive	O
QA	B-TaskName
datasets	O
,	O
namely	O
,	O
SQuAD1.1	B-DatasetName
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
NewsQA	B-DatasetName
(	O
Trischler	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
Natural	B-DatasetName
Questions	I-DatasetName
(	O
Kwiatkowski	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
Triv	B-DatasetName
-	I-DatasetName
iaQA	I-DatasetName
(	O
Joshi	O
et	O
al	O
.	O
,	O

2017	O
)	O
,	O
BioASQ	B-DatasetName
(	O
Tsatsaronis	O
et	O
al	O
.	O
,	O

2015	O
)	O
and	O
DuoRC	B-DatasetName
(	O
Saha	O
et	O
al	O
.	O
,	O

We	O
employ	O
the	O
official	O
data	O
of	O
SQuAD1.1	B-DatasetName
,	O
NewsQA	B-DatasetName
and	O
TriviaQA	B-DatasetName
and	O
for	O
Natural	B-DatasetName
Questions	I-DatasetName
,	O
BioASQ	B-DatasetName
and	O
DuoRC	B-DatasetName
,	O
we	O
employ	O
the	O
pre	O
-	O
processed	O
data	O
released	O
by	O
MRQA	O
(	O
Fisch	O
et	O
al	O
.	O
,	O

2019).Unsupervised	B-TaskName
QA	I-TaskName
Training	O
Details	O
To	O
generate	O
synthetic	O
QA	B-TaskName
training	O
data	O
,	O
we	O
make	O
use	O
of	O
Wikidumps	O
4	O
by	O
firstly	O
removing	O
all	O
HTML	O
tags	O
and	O
reference	O
links	O
,	O
then	O
extracting	O
paragraphs	O
that	O
are	O
longer	O
than	O
500	O
characters	O
,	O
resulting	O
in	O
60k	O
paragraphs	O
sampled	O
from	O
all	O
paragraphs	O
of	O
Wikidumps	O
.	O

Paragraphs	O
and	O
answers	O
are	O
concatenated	O
to	O
form	O
sequences	O
of	O
the	O
form	O
passage	O
<	O
SEP	O
>	O
answer	O
<	O
SEP	O
>	O
,	O
before	O
being	O
fed	O
into	O
the	O
trained	O
BART	B-MethodName
-	I-MethodName
QG	I-MethodName
model	O
to	O
obtain	O
corresponding	O
questions	O
.	O

This	O
results	O
in	O
20k	O
synthetic	O
QA	B-TaskName
pairs	O
,	O
which	O
are	O
then	O
employed	O
to	O
train	O
an	O
unsupervised	B-TaskName
QA	I-TaskName
model	O
.	O

The	O
QA	B-TaskName
model	O
we	O
employ	O
is	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
-	I-MethodName
wholeword	I-MethodName
-	I-MethodName
masking	I-MethodName
(	O
which	O
we	O
henceforth	O
refer	O
to	O
as	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
for	O
ease	O
of	O
reference	O
)	O
.	O

Document	B-HyperparameterName
length	I-HyperparameterName
and	O
stride	B-HyperparameterName
length	I-HyperparameterName
are	O
364	B-HyperparameterValue
and	O
128	B-HyperparameterValue
respectively	O
,	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
set	O
to	O
1	B-HyperparameterValue
×	I-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
.	O

Evaluation	O
metrics	O
for	O
unsupervised	B-TaskName
QA	I-TaskName
are	O
Exact	B-MetricName
Match	I-MetricName
(	O
EM	B-MetricName
)	O
and	O
F-1	B-MetricName
score	O
.	O

We	O
use	O
the	O
20k	O
generated	O
synthetic	O
QA	B-TaskName
pairs	O
to	O
train	O
a	O
BERT	B-MethodName
QA	B-TaskName
model	O
and	O
first	O
validate	O
its	O
performance	O
on	O
the	O
development	O
sets	O
of	O
three	O
benchmark	O
QA	B-TaskName
datasets	O
based	O
on	O
Wikipedia	O
-SQuAD1.1	B-DatasetName
,	O
Natural	B-DatasetName
Questions	I-DatasetName
and	O
TriviaQA	B-DatasetName
.	O

The	O
unsupervised	O
baselines	O
we	O
compare	O
with	O
are	O
as	O
follows:1	O
.	O

2019	O
)	O
employ	O
unsupervised	B-MethodName
neural	I-MethodName
machine	I-MethodName
translation	I-MethodName
(	O
Artetxe	O
et	O
al	O
.	O
,	O

2018	O
)	O
to	O
train	O
a	O
QG	O
model	O
;	O
4	O
M	O
synthetic	O
QA	B-TaskName
examples	O
were	O
generated	O
to	O
train	O
a	O
QA	B-TaskName
model	O
;	O
2	O
.	O

employ	O
dependency	B-MethodName
trees	I-MethodName
to	O
generate	O
questions	O
and	O
employed	O
cited	O
documents	O
as	O
passages	O
.	O

For	O
comparison	O
,	O
we	O
also	O
show	O
the	O
results	O
of	O
some	O
supervised	O
models	O
fine	O
-	O
tuned	O
on	O
the	O
correspond-	O
ing	O
training	O
sets	O
:	O
Match	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Wang	O
and	O
Jiang	O
)	O
,	O
BiDAF	B-MethodName
(	O
Seo	O
et	O
al	O
.	O
,	O

2016	O
)	O
,	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
and	O
BERTlarge	B-MethodName
.	O

SQuAD1.1	B-DatasetName
results	O
are	O
shown	O
in	O
Table	O
1	O
.	O

As	O
can	O
be	O
seen	O
from	O
results	O
in	O
Table	O
1	O
,	O
our	O
proposed	O
method	O
outperforms	O
all	O
unsupervised	O
baselines	O
,	O
and	O
even	O
exceeds	O
the	O
performance	O
of	O
one	O
supervised	O
model	O
,	O
Match	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Wang	O
and	O
Jiang).Results	O
for	O
Natural	B-DatasetName
Questions	I-DatasetName
and	O
TriviaQA	B-DatasetName
are	O
shown	O
in	O
Table	O
2	O
.	O

The	O
results	O
of	O
all	O
baseline	O
models	O
were	O
produced	O
using	O
the	O
released	O
synthetic	O
QA	B-TaskName
data	O
to	O
finetune	O
a	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
model	O
.	O

Our	O
method	O
outperforms	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
methods	O
by	O
a	O
substantial	O
margin	O
,	O
obtaining	O
relative	O
improvements	O
over	O
the	O
best	O
unsupervised	O
baseline	O
model	O
of	O
47	B-MetricValue
%	I-MetricValue
with	O
respect	O
to	O
EM	B-MetricName
,	O
10	B-MetricValue
%	I-MetricValue
F-1	B-MetricName
on	O
Natural	O
Questions	O
,	O
and	O
by	O
34	B-MetricValue
%	I-MetricValue
EM	B-MetricName
and	O
12	B-MetricValue
%	I-MetricValue
F-1	B-MetricName
on	O
Trivi	O
-	O
aQA.In	O
summary	O
,	O
our	O
method	O
achieves	O
the	O
best	O
performance	O
(	O
both	O
in	O
terms	O
of	O
EM	B-MetricName
and	O
F-1	B-MetricName
)	O
out	O
of	O
three	O
unsupervised	O
models	O
on	O
all	O
three	O
tested	O
datasets	O
.	O

Compared	O
to	O
previous	O
work	O
,	O
this	O
is	O
approximately	O
less	O
than	O
10	O
%	O
of	O
the	O
training	O
data	O
employed	O
.Transferability	O
of	O
Our	O
Generated	O
Synthetic	O
QA	B-TaskName
Data	O
We	O
also	O
validate	O
our	O
method	O
's	O
efficacy	O
on	O
three	O
out	O
-	O
of	O
-	O
domain	O
QA	B-TaskName
datasets	O
:	O
NewsQA	B-DatasetName
created	O
from	O
news	O
articles	O
,	O
BioASQ	B-DatasetName
created	O
from	O
biomedical	O
articles	O
,	O
and	O
DuoRC	B-DatasetName
created	O
from	O
movie	O
plots	O
,	O
for	O
the	O
purpose	O
of	O
evaluating	O
the	O
transferability	O
of	O
the	O
Wikipedia	O
-	O
based	O
synthetic	O
data	O
.	O

Results	O
in	O
Table	O
3	O
show	O
that	O
our	O
proposed	O
method	O
additionally	O
outperforms	O
the	O
unsupervised	O
baseline	O
models	O
on	O
the	O
out	O
-	O
of	O
-	O
domain	O
datasets	O
,	O
achieving	O
F1	B-MetricName
improve-	O
ments	O
over	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
by	O
3.8	B-MetricValue
,	O
4.5	B-MetricValue
and	O
5.4	B-MetricValue
points	O
respectively	O
.	O

Experiment	O
results	O
additionally	O
indicate	O
that	O
our	O
generated	O
synthetic	O
data	O
transfers	O
well	O
to	O
domains	O
distinct	O
from	O
that	O
of	O
the	O
original	O
summary	O
data	O
.	O

In	O
the	O
unsupervised	B-TaskName
QA	I-TaskName
experiments	O
,	O
we	O
extracted	O
answers	O
from	O
Wikipedia	O
passages	O
before	O
feeding	O
them	O
into	O
our	O
QG	O
model	O
to	O
obtain	O
questions	O
.	O

These	O
<	O
passage	O
,	O
answer	O
,	O
question	O
>	O
triples	O
constitute	O
the	O
synthetic	O
data	O
employed	O
to	O
train	O
the	O
QA	B-TaskName
model	O
.	O

Additionally	O
,	O
we	O
wish	O
to	O
consider	O
what	O
might	O
happen	O
if	O
we	O
instead	O
employ	O
passages	O
and	O
answers	O
taken	O
directly	O
from	O
the	O
QA	B-TaskName
training	O
data	O
?	O
Doing	O
this	O
would	O
mean	O
that	O
the	O
QA	B-TaskName
system	O
is	O
no	O
longer	O
considered	O
unsupervised	O
but	O
we	O
carry	O
out	O
this	O
experiment	O
in	O
order	O
to	O
provide	O
insight	O
into	O
the	O
degree	O
to	O
which	O
there	O
may	O
be	O
room	O
for	O
improvement	O
in	O
terms	O
of	O
our	O
NER	O
-	O
based	O
automatic	O
answer	O
extraction	O
method	O
(	O
described	O
in	O
Section	O
4.1.2	O
)	O
.	O

Results	O
of	O
the	O
two	O
additional	O
settings	O
are	O
shown	O
in	O
Table	O
5	O
-answer	O
extraction	O
has	O
quite	O
a	O
large	O
effect	O
on	O
the	O
quality	O
of	O
generated	O
synthetic	O
QA	B-TaskName
data	O
.	O

When	O
we	O
employ	O
the	O
answers	O
from	O
the	O
training	O
set	O
,	O
the	O
performance	O
of	O
the	O
QA	B-TaskName
model	O
is	O
improved	O
by	O
5	B-MetricValue
F-1	B-MetricName
points	O
for	O
SQuAD1.1	B-DatasetName
,	O
and	O
over	O
10	B-MetricValue
F-1	B-MetricName
points	O
for	O
Natural	B-DatasetName
Questions	I-DatasetName
and	O
TriviaQA	B-DatasetName
.	O

Recall	O
that	O
the	O
QG	O
data	O
is	O
employed	O
to	O
train	O
a	O
question	O
generator	O
which	O
is	O
then	O
employed	O
to	O
generate	O
synthetic	O
QA	B-TaskName
data	O
for	O
unsupervised	B-TaskName
QA.The	I-TaskName
heuristics	O
are	O
defined	O
as	O
follows:•	O
Naive	O
-	O
QG	O
only	O
employs	O
summary	O
sentences	O
as	O
passages	O
(	O
instead	O
of	O
the	O
original	O
articles	O
)	O
and	O
generates	O
trivial	O
questions	O
in	O
which	O
only	O
the	O
answer	O
spans	O
are	O
replaced	O
with	O
the	O
appropriate	O
question	O
words	O
.	O

Summary	O
-	O
QG	O
makes	O
use	O
of	O
the	O
original	O
news	O
articles	O
of	O
the	O
summaries	O
as	O
passages	O
rather	O
than	O
summary	O
sentences	O
to	O
avoid	O
high	O
lexical	O
overlap	O
between	O
the	O
passage	O
and	O
question	O
.	O
-	O

We	O
employ	O
the	O
QG	O
data	O
generated	O
by	O
these	O
heuristics	O
to	O
train	O
QG	O
models	O
,	O
which	O
leads	O
to	O
six	O
BART	B-MethodName
-	I-MethodName
QG	I-MethodName
models	O
.	O

We	O
then	O
employ	O
these	O
six	O
models	O
to	O
further	O
generate	O
synthetic	O
QA	B-TaskName
data	O
based	O
on	O
the	O
same	O
Wikipedia	O
data	O
and	O
compare	O
their	O
performances	O
on	O
the	O
SQuAD1.1	B-DatasetName
dev	O
set	O
.	O

The	O
results	O
in	O
Table	O
6	O
show	O
that	O
using	O
articles	O
as	O
passages	O
to	O
avoid	O
lexical	O
overlap	O
with	O
their	O
summarygenerated	O
questions	O
greatly	O
improves	O
QA	B-TaskName
performance	O
.	O

Summary	O
-	O
QG	O
outperforms	O
Naive	O
-	O
QG	O
by	O
roughly	O
20	B-MetricValue
EM	B-MetricName
points	O
and	O
16	B-MetricValue
F-1	B-MetricName
points	O
.	O

The	O
results	O
for	O
the	O
other	O
heuristics	O
show	O
that	O
they	O
continuously	O
improve	O
the	O
performance	O
,	O
especially	O
Wh	O
-	O
Movement	O
and	O
Decomp	O
-	O
Verb	O
which	O
make	O
the	O
questions	O
in	O
the	O
QG	O
data	O
more	O
similar	O
to	O
the	O
questions	O
in	O
the	O
QA	B-TaskName
dataset	O
.	O

We	O
investigate	O
the	O
effects	O
of	O
varying	O
the	O
quantity	O
of	O
synthetic	O
QA	B-TaskName
data	O
.	O

Results	O
in	O
Figure	O
3	O
show	O
that	O
our	O
synthetic	O
data	O
allows	O
the	O
QA	B-TaskName
model	O
to	O
achieve	O
competitive	O
performance	O
even	O
with	O
fewer	O
than	O
20k	O
examples	O
,	O
which	O
suggests	O
that	O
our	O
synthetic	O
data	O
contains	O
sufficient	O
QA	B-TaskName
knowledge	O
to	O
enable	O
models	O
to	O
correctly	O
answer	O
a	O
question	O
with	O
less	O
synthetic	O
data	O
compared	O
to	O
previous	O
unsupervised	O
methods	O
.	O

The	O
data	O
-	O
efficiency	O
of	O
our	O
approach	O
increases	O
the	O
feasibility	O
of	O
training	O
a	O
QA	B-TaskName
system	O
for	O
a	O
target	O
domain	O
where	O
there	O
is	O
no	O
labeled	O
QA	B-TaskName
data	O
available	O
.	O

We	O
conduct	O
experiments	O
in	O
a	O
few	O
-	O
shot	O
learning	O
setting	O
,	O
in	O
which	O
we	O
employ	O
a	O
limited	O
number	O
of	O
labeled	O
QA	B-TaskName
examples	O
from	O
the	O
training	O
set	O
.	O

We	O
take	O
the	O
model	O
trained	O
with	O
our	O
synthetic	O
QA	B-TaskName
data	O
,	O
the	O
model	O
trained	O
with	O
the	O
synthetic	O
QA	B-TaskName
data	O
of	O
and	O
a	O
vanilla	O
BERT	B-MethodName
model	O
,	O
with	O
all	O
QA	B-TaskName
models	O
employing	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
.	O

We	O
train	O
these	O
models	O
using	O
progressively	O
increasing	O
amounts	O
of	O
labeled	O
QA	B-TaskName
samples	O
from	O
Natural	B-DatasetName
Questions	I-DatasetName
(	O
NQ	B-DatasetName
)	O
and	O
SQuAD1.1	B-DatasetName
and	O
assess	O
their	O
performance	O
on	O
corresponding	O
dev	O
sets	O
.	O

Results	O
are	O
shown	O
in	O
Figure	O
4	O
where	O
with	O
only	O
a	O
small	O
amount	O
of	O
labeled	O
data	O
(	O
less	O
than	O
5,000	O
examples	O
)	O
,	O
our	O
method	O
outperforms	O
and	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
,	O
clearly	O
demonstrating	O
the	O
efficacy	O
of	O
our	O
approach	O
in	O
a	O
few	O
-	O
shot	O
learning	O
setting	O
.	O

We	O
manually	O
examine	O
50	O
randomly	O
selected	O
questions	O
,	O
31	O
(	O
62	O
%	O
)	O
of	O
which	O
were	O
deemed	O
high	O
quality	O
questions	O
.	O

The	O
remaining	O
19	O
contain	O
various	O
errors	O
with	O
some	O
questions	O
containing	O
more	O
than	O
one	O
error	O
,	O
including	O
mismatched	O
wh	O
-	O
word	O
and	O
answer	O
(	O
12	O
%	O
)	O
,	O
missing	O
information	O
needed	O
to	O
locate	O
the	O
answer	O
(	O
8	O
%	O
)	O
,	O
factual	O
errors	O
(	O
10	O
%	O
)	O
and	O
grammatical	O
errors	O
(	O
8)	O
(	O
16	O
%	O
)	O
Typical	O
examples	O
are	O
shown	O
in	O
Table	O
4.We	O
propose	O
an	O
unsupervised	O
question	O
generation	O
method	O
which	O
uses	O
summarization	O
data	O
to	O
1	O
)	O
minimize	O
the	O
lexical	O
overlap	O
between	O
passage	O
and	O
question	O
and	O
2	O
)	O
provide	O
a	O
QA	B-TaskName
-	O
dataset	O
-	O
independent	O
way	O
of	O
generating	O
questions	O
.	O

Our	O
unsupervised	B-TaskName
QA	I-TaskName
extrinsic	O
evaluation	O
on	O
SQuAD1.1	B-DatasetName
,	O
NQ	B-DatasetName
and	O
TriviaQA	B-DatasetName
using	O
synthetic	O
QA	B-TaskName
data	O
generated	O
by	O
our	O
method	O
shows	O
that	O
our	O
method	O
substantially	O
outperforms	O
previous	O
methods	O
for	O
generating	O
synthetic	O
QA	B-TaskName
for	O
unsupervised	B-TaskName
QA	I-TaskName
.	O

Furthermore	O
,	O
our	O
synthetic	O
QA	B-TaskName
data	O
transfers	O
well	O
to	O
the	O
out	O
-	O
of	O
-	O
domain	O
datasets	O
.	O

A.1	O
Effects	O
of	O
Different	O
Beam	O
Size	O
We	O
also	O
study	O
the	O
effects	O
of	O
different	O
beam	O
size	O
in	O
generating	O
synthetic	O
questions	O
to	O
the	O
performance	O
of	O
downstream	O
QA	B-TaskName
task	O
.	O

Experiments	O
are	O
conducted	O
on	O
SQuAD1.1	B-DatasetName
dev	O
set	O
using	O
BERT	B-MethodName
-	I-MethodName
large	I-MethodName
,	O
questions	O
in	O
the	O
synthetic	O
QA	B-TaskName
data	O
are	O
generated	O
with	O
different	O
beam	O
size	O
using	O
the	O
same	O
BART	B-MethodName
-	I-MethodName
QG	I-MethodName
model	O
.	O

The	O
experimental	O
results	O
in	O
Figure	O
5	O
show	O
that	O
the	O
beam	O
size	O
is	O
an	O
important	O
factor	O
affecting	O
the	O
performance	O
of	O
unsupervised	B-TaskName
QA	I-TaskName
,	O
the	O
largest	O
margin	O
between	O
the	O
highest	O
score	O
(	O
beam-15	O
)	O
and	O
the	O
lowest	O
score	O
(	O
beam-1	O
)	O
in	O
Figure	O
5	O
is	O
close	O
to	O
4	B-MetricValue
points	O
on	O
EM	B-MetricName
and	O
F-1	B-MetricName
score	O
.	O

A.3	O
Generated	O
QA	B-TaskName
Examples	O
Some	O
Wikipedia	O
-	O
based	O
<	O
passage	O
,	O
answer	O
,	O
ques	O
-	O
tion	O
>	O
examples	O
generated	O
by	O
our	O
BART	B-MethodName
-	I-MethodName
QG	I-MethodName
model	O
are	O
shown	O
in	O
Table	O
7	O
,	O
Table	O
8	O
and	O
Table	O
9	O
Answer	O
Question	O
In	O
March	O
2008	O
as	O
part	O
of	O
the	O
annual	O
budget	O
,	O
the	O
government	O
introduced	O
several	O
laws	O
to	O
amend	O
the	O
Immigration	O
and	O
Refugee	O
Protection	O
Act	O
.	O

In	O
the	O
spring	O
of	O
1998	O
the	O
cause	O
was	O
finally	O
detected	O
;	O
Paddy	O
had	O
a	O
brain	O
tumor	O
as	O
well	O
as	O
lung	O
cancer	O
.	O

Colchester	O
artisans	O
included	O
clockmakers	O
,	O
who	O
maintained	O
clocks	O
in	O
church	O
towers	O
across	O
north	O
Essex	O
and	O
Suffolk.north	O
Essex	O
where	O
were	O
hundreds	O
of	O
clocks	O
made	O
by	O
local	O
artisans	O
?	O
Badge	O
numbers	O
for	O
Sheriffs	O
and	O
Deputies	O
consist	O
of	O
a	O
prefix	O
number	O
,	O
which	O
represents	O
the	O
county	O
number	O
,	O
followed	O
by	O
a	O
one	O
to	O
three	O
digit	O
number	O
,	O
which	O
represents	O
the	O
Sheriff	O
's	O
or	O
Deputy	O
's	O
number	O
within	O
that	O
specific	O
office	O
.	O

In	O
the	O
former	O
war	O
the	O
Germanic	O
tribes	O
of	O
the	O
Cimbri	O
and	O
the	O
Teutones	O
migrated	O
around	O
Europe	O
and	O
invaded	O
territories	O
of	O
allies	O
of	O
Rome	O
,	O
particularly	O
in	O
southern	O
France	O
,	O
and	O
routed	O
the	O
Romans	O
in	O
several	O
battles	O
until	O
their	O
final	O
defeat	O
.	O

Calpurnius	O
Piso	O
who	O
was	O
sent	O
to	O
the	O
south	O
of	O
italy	O
to	O
fight	O
for	O
the	O
roman	O
empire	O
?	O
The	O
parish	O
churches	O
of	O
Sempringham	O
,	O
Birthorpe	O
,	O
Billingborough	O
,	O
and	O
Kirkby	O
were	O
already	O
appropriated	O
.	O

Among	O
the	O
lyricists	O
,	O
Sauvo	O
Puhtila	O
,	O
Reino	O
Helismaa	O
(	O
died	O
1965	O
)	O
and	O
Veikko	O
"	O
Vexi	O
"	O
Salmi	O
are	O
a	O
few	O
of	O
the	O
most	O
notable	O
writers	O
.	O

The	O
composer	O
and	O
bandleader	O
Jimi	O
Tenor	O
is	O
well	O
known	O
for	O
his	O
brand	O
of	O
retro	O
-	O
funk	O
music	O
.	O

We	O
also	O
thank	O
the	O
reviewers	O
for	O
their	O
insightful	O
and	O
helpful	O
comments	O
.	O

Structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
attempts	O
to	O
extract	O
full	O
opinion	O
tuples	O
from	O
a	O
text	O
,	O
but	O
over	O
time	O
this	O
task	O
has	O
been	O
subdivided	O
into	O
smaller	O
and	O
smaller	O
sub	O
-	O
tasks	O
,	O
e.g.	O
,	O
target	O
extraction	O
or	O
targeted	O
polarity	O
classification	O
.	O

We	O
cast	O
the	O
structured	B-TaskName
sentiment	I-TaskName
problem	I-TaskName
as	O
dependency	O
graph	O
parsing	O
,	O
where	O
the	O
nodes	O
are	O
spans	O
of	O
sentiment	O
holders	O
,	O
targets	O
and	O
expressions	O
,	O
and	O
the	O
arcs	O
are	O
the	O
relations	O
between	O
them	O
.	O

We	O
perform	O
experiments	O
on	O
five	O
datasets	O
in	O
four	O
languages	O
(	O
English	O
,	O
Norwegian	O
,	O
Basque	O
,	O
and	O
Catalan	O
)	O
and	O
show	O
that	O
this	O
approach	O
leads	O
to	O
strong	O
improvements	O
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
.	O

Our	O
analysis	O
shows	O
that	O
refining	O
the	O
sentiment	O
graphs	O
with	O
syntactic	O
dependency	O
information	O
further	O
improves	O
results	O
.	O

Structured	B-TaskName
1	I-TaskName
sentiment	I-TaskName
analysis	I-TaskName
,	O
i.e.	O
,	O
the	O
task	O
of	O
predicting	O
a	O
structured	O
sentiment	O
graph	O
like	O
the	O
ones	O
in	O
Figure	O
1	O
,	O
can	O
be	O
theoretically	O
cast	O
as	O
an	O
information	O
extraction	O
problem	O
in	O
which	O
one	O
attempts	O
to	O
find	O
all	O
of	O
the	O
opinion	O
tuples	O
O	O
=	O
O	O
i	O
,	O
.	O
.	O
.	O
,	O

Each	O
opinion	O
O	O
i	O
is	O
a	O
tuple	O
(	O
h	O
,	O
t	O
,	O
e	O
,	O
p	O
)	O
where	O
h	O
is	O
a	O
holder	O
who	O
expresses	O
a	O
polarity	O
p	O
towards	O
a	O
target	O
t	O
through	O
a	O
sentiment	O
expression	O
e	O
,	O
implicitly	O
defining	O
pairwise	O
relationships	O
between	O
elements	O
of	O
the	O
same	O
tuple	O
.	O

Liu	O
(	O
2012	O
)	O
argues	O
that	O
all	O
of	O
these	O
elements	O
2	O
are	O
essential	O
to	O
fully	O
resolve	O
the	O
sentiment	B-TaskName
analysis	I-TaskName
problem	O
.	O

However	O
,	O
most	O
research	O
on	O
sentiment	B-TaskName
analysis	I-TaskName
focuses	O
either	O
on	O
a	O
variety	O
of	O
sub	O
-	O
tasks	O
,	O
which	O
avoids	O
performing	O
the	O
full	O
task	O
,	O
or	O
on	O
simplified	O
and	O
idealized	O
tasks	O
,	O
e.g.	O
,	O
sentence	O
-	O
level	O
binary	O
polarity	O
classification	O
.	O

We	O
argue	O
that	O
the	O
division	O
of	O
structured	B-TaskName
sentiment	I-TaskName
into	O
these	O
sub	O
-	O
tasks	O
has	O
become	O
counterproductive	O
,	O
as	O
reported	O
experiments	O
are	O
often	O
not	O
sensitive	O
to	O
whether	O
a	O
given	O
addition	O
to	O
the	O
pipeline	O
improves	O
the	O
overall	O
resolution	O
of	O
sentiment	O
,	O
or	O
do	O
not	O
take	O
into	O
account	O
the	O
inter	O
-	O
dependencies	O
of	O
the	O
various	O
sub	O
-	O
tasks	O
.	O

As	O
such	O
,	O
we	O
propose	O
a	O
unified	O
approach	O
to	O
structured	B-TaskName
sentiment	I-TaskName
which	O
jointly	O
predicts	O
all	O
elements	O
of	O
an	O
opinion	O
tuple	O
and	O
their	O
relations	O
.	O

Moreover	O
,	O
we	O
cast	O
sentiment	B-TaskName
analysis	I-TaskName
as	O
a	O
dependency	O
graph	O
parsing	O
problem	O
,	O
where	O
the	O
sentiment	O
expression	O
is	O
the	O
root	O
node	O
,	O
and	O
the	O
other	O
elements	O
have	O
arcs	O
which	O
model	O
the	O
relationships	O
between	O
them	O
.	O

We	O
aim	O
to	O
answer	O
RQ1	O
:	O
whether	O
graph	O
-	O
based	O
approaches	O
to	O
structured	B-TaskName
sentiment	I-TaskName
outperform	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
sequence	O
labeling	O
approaches	O
,	O
and	O
RQ2	O
:	O
how	O
to	O
best	O
encode	O
structured	B-TaskName
sentiment	I-TaskName
as	O
parsing	O
graphs	O
.	O

We	O
perform	O
experiments	O
on	O
five	O
standard	O
datasets	O
in	O
four	O
languages	O
(	O
English	O
,	O
Norwegian	O
,	O
Basque	O
,	O
Catalan	O
)	O
and	O
show	O
that	O
graph	O
-	O
based	O
approaches	O
outperform	O
state	O
-	O
ofthe	O
-	O
art	O
baselines	O
on	O
all	O
datasets	O
on	O
several	O
standard	O
metrics	O
,	O
as	O
well	O
as	O
our	O
proposed	O
novel	O
(	O
unlabeled	O
and	O
labeled	O
)	O
sentiment	O
graph	O
metrics	O
.	O

We	O
further	O
propose	O
methods	O
to	O
inject	O
linguistic	O
structure	O
into	O
the	O
sentiment	O
graphs	O
using	O
syntactic	O
dependencies	O
.	O

CL	O
]	O
Figure	O
1	O
:	O
A	O
structured	B-TaskName
sentiment	I-TaskName
graph	O
is	O
composed	O
of	O
a	O
holder	O
,	O
target	O
,	O
sentiment	O
expression	O
,	O
their	O
relationships	O
and	O
a	O
polarity	O
attribute	O
.	O

Structured	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
can	O
be	O
broken	O
down	O
into	O
five	O
sub	O
-	O
tasks	O
:	O
i	O
)	O
sentiment	O
expression	O
extraction	O
,	O
ii	O
)	O
sentiment	O
target	O
extraction	O
,	O
iii	O
)	O
sentiment	O
holder	O
extraction	O
,	O
iv	O
)	O
defining	O
the	O
relationship	O
between	O
these	O
elements	O
,	O
and	O
v	O
)	O
assigning	O
polarity	O
.	O

Previous	O
work	O
on	O
information	O
extraction	O
has	O
used	O
pipeline	O
methods	O
which	O
first	O
extract	O
the	O
holders	O
,	O
targets	O
,	O
and	O
expressions	O
(	O
tasks	O
iiii	O
)	O
and	O
subsequently	O
predict	O
their	O
relations	O
(	O
task	O
iv	O
)	O
,	O
mostly	O
on	O
the	O
MPQA	B-DatasetName
dataset	O
(	O
Wiebe	O
et	O
al	O
.	O
,	O

CRFs	B-MethodName
and	O
a	O
number	O
of	O
external	O
resources	O
(	O
sentiment	B-MethodName
lexicons	I-MethodName
,	O
dependency	B-MethodName
parsers	I-MethodName
,	O
named	B-MethodName
-	I-MethodName
entity	I-MethodName
taggers	I-MethodName
)	O
(	O
Choi	O
et	O
al	O
.	O
,	O

Given	O
the	O
small	O
size	O
of	O
the	O
training	O
data	O
and	O
the	O
complicated	O
task	O
,	O
these	O
techniques	O
often	O
still	O
outperform	O
neural	O
models	O
,	O
such	O
as	O
BiLSTMs	B-MethodName
(	O
Katiyar	O
and	O
Cardie	O
,	O
2016	O
)	O
.	O

Transition	O
-	O
based	O
end	O
-	O
toend	O
approaches	O
have	O
shown	O
some	O
potential	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

Targeted	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
only	O
concentrates	O
on	O
extracting	O
sentiment	O
targets	O
(	O
task	O
ii	O
)	O
and	O
classifying	O
the	O
polarity	O
directed	O
towards	O
them	O
(	O
task	O
iv	O
)	O
(	O
Jiang	O
et	O
al	O
.	O
,	O

Recent	O
shared	O
tasks	O
on	O
Aspect	B-TaskName
-	I-TaskName
Based	I-TaskName
Sentiment	I-TaskName
Analysis	I-TaskName
(	O
ABSA	B-TaskName
)	O
(	O
Pontiki	O
et	O
al	O
.	O
,	O

2019	O
)	O
can	O
also	O
lead	O
to	O
improvements	O
on	O
the	O
ABSA	B-TaskName
data	O
(	O
Li	O
et	O
al	O
.	O
,	O

2019b).End2End	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
is	O
a	O
recently	O
proposed	O
subtask	O
which	O
combines	O
targeted	B-TaskName
sentiment	I-TaskName
(	O
tasks	O
ii	O
and	O
v	O
)	O
and	O
sentiment	B-TaskName
expression	I-TaskName
extraction	I-TaskName
(	O
task	O
i	O
)	O
,	O
without	O
requiring	O
the	O
resolution	O
of	O
relationships	O
between	O
targets	O
and	O
expressions	O
.	O

2016	O
)	O
augment	O
the	O
ABSA	B-TaskName
datasets	O
with	O
sentiment	O
expressions	O
,	O
but	O
provide	O
no	O
details	O
on	O
the	O
annotation	O
process	O
or	O
any	O
inter	O
-	O
annotator	O
agreement	O
.	O

2019	O
)	O
make	O
use	O
of	O
this	O
data	O
and	O
propose	O
a	O
multi	B-MethodName
-	I-MethodName
layer	I-MethodName
CNN	I-MethodName
(	O
IMN	B-MethodName
)	O
to	O
create	O
hidden	O
representations	O
h	O
which	O
are	O
then	O
fed	O
to	O
a	O
target	O
and	O
opinion	B-MethodName
extraction	I-MethodName
module	I-MethodName
(	O
AE	O
)	O
,	O
which	O
is	O
also	O
a	O
multi	O
-	O
layer	O
CNN	O
.	O

This	O
module	O
predictsŷ	O
ae	O
,	O
a	O
sequence	O
of	O
BIO	O
tags	O
4	O
that	O
predict	O
the	O
presence	O
or	O
absence	O
of	O
targets	O
and	O
expressions	O
.	O

After	O
jointly	O
predicting	O
the	O
targets	O
and	O
expressions	O
,	O
a	O
second	O
multi	O
-	O
layer	O
CNN	O
with	O
a	O
final	O
self	O
-	O
attention	O
network	O
is	O
used	O
to	O
classify	O
the	O
polarity	O
,	O
again	O
as	O
sequence	O
labeling	O
task	O
(	O
AS	O
)	O
.	O

This	O
second	O
module	O
combines	O
the	O
information	O
from	O
h	O
andŷ	O
ae	O
by	O
incorporating	O
the	O
predicted	O
probability	O
of	O
a	O
token	O
to	O
be	O
a	O
target	O
in	O
the	O
formulation	O
of	O
self	O
-	O
attention	O
.	O

Chen	O
and	O
Qian	O
(	O
2020	O
)	O
instead	O
propose	O
Relation	B-MethodName
-	I-MethodName
Aware	I-MethodName
Collaborative	I-MethodName
Learning	I-MethodName
(	O
RACL	B-MethodName
)	O
.	O

This	O
model	O
creates	O
task	O
specific	O
representations	O
by	O
first	O
embedding	O
a	O
sentence	O
,	O
passing	O
through	O
a	O
shared	O
feed	O
-	O
forward	O
network	O
and	O
finally	O
a	O
task	O
-	O
specific	O
CNN	O
.	O

This	O
approach	O
then	O
models	O
interactions	O
between	O
each	O
pair	O
of	O
sub	O
-	O
tasks	O
(	O
target	O
extraction	O
,	O
expression	O
extraction	O
,	O
sentiment	O
classification	O
)	O
by	O
creating	O
pairwise	O
weighted	O
attention	O
representations	O
.	O

The	O
authors	O
finally	O
stack	O
several	O
RACL	B-MethodName
layers	O
,	O
using	O
the	O
output	O
from	O
the	O
previous	O
layer	O
as	O
input	O
for	O
the	O
next	O
.	O

Both	O
models	O
perform	O
well	O
on	O
the	O
augmented	O
Se	B-DatasetName
-	I-DatasetName
mEval	I-DatasetName
data	O
,	O
but	O
it	O
is	O
unlikely	O
that	O
these	O
annotations	O
are	O
adequate	O
for	O
full	O
structured	O
sentiment	O
,	O
as	O
Wang	O
et	O
al	O
.	O
(	O

2016	O
)	O
only	O
provide	O
expression	O
annotations	O
for	O
sentences	O
that	O
have	O
targets	O
,	O
generally	O
only	O
include	O
sentiment	O
-	O
bearing	O
words	O
(	O
not	O
phrases	O
)	O
,	O
and	O
do	O
not	O
specify	O
the	O
relationship	O
between	O
target	O
and	O
expression	O
.	O

2014(Oepen	O
et	O
al	O
.	O
,	O
,	O

The	O
largest	O
available	O
structured	O
sentiment	O
dataset	O
is	O
the	O
NoReC	B-DatasetName
Fine	I-DatasetName
dataset	O
(	O
Øvrelid	O
et	O
al	O
.	O
,	O

MultiB	B-DatasetName
EU	I-DatasetName
and	O
MultiB	B-DatasetName
CA	I-DatasetName
(	O
Barnes	O
et	O
al	O
.	O
,	O

MPQA	B-DatasetName
(	O
Wiebe	O
et	O
al	O
.	O
,	O

Finally	O
,	O
DS	B-DatasetName
Unis	I-DatasetName
(	O
Toprak	O
et	O
al	O
.	O
,	O

Regarding	O
holders	O
,	O
MPQA	B-DatasetName
has	O
the	O
most	O
(	O
2,054	O
)	O
and	O
DS	B-DatasetName
Unis	I-DatasetName
has	O
the	O
fewest	O
(	O
94	O
)	O
,	O
whereas	O
NoReC	B-DatasetName
Fine	I-DatasetName
has	O
the	O
largest	O
proportion	O
of	O
targets	O
(	O
8,923	O
)	O
and	O
expressions	O
(	O
11,115	O
)	O
.	O

The	O
average	O
length	O
of	O
holders	O
(	O
2.6	O
tokens	O
)	O
and	O
targets	O
(	O
6.1	O
tokens	O
)	O
in	O
MPQA	B-DatasetName
is	O
also	O
considerably	O
higher	O
than	O
the	O
others	O
.	O

It	O
is	O
also	O
worth	O
pointing	O
out	O
that	O
MPQA	B-DatasetName
and	O
DS	B-DatasetName
Unis	I-DatasetName
additionally	O
include	O
neutral	O
polarity	O
.	O

In	O
the	O
case	O
of	O
MPQA	B-DatasetName
the	O
neutral	O
class	O
refers	O
to	O
verbs	O
which	O
are	O
subjective	O
but	O
do	O
not	O
convey	O
polarity	O
,	O
e.g.	O
,	O
'	O
say	O
'	O
,	O
'	O
opt	O
for	O
'	O
.	O

In	O
DS	B-DatasetName
Unis	I-DatasetName
,	O
however	O
,	O
the	O
neutral	O
label	O
tends	O
to	O
indicate	O
expressions	O
that	O
could	O
entail	O
mixed	O
polarity	O
or	O
are	O
polar	O
under	O
the	O
right	O
conditions	O
,	O
e.g.	O
,	O
'	O
the	O
classes	O
were	O
not	O
easy	O
'	O
is	O
considered	O
neutral	O
,	O
as	O
it	O
is	O
possible	O
for	O
difficult	O
classes	O
to	O
be	O
desirable	O
at	O
a	O
university	O
.	O

MultiB	B-DatasetName
EU	I-DatasetName
,	O
and	O
MultiB	B-DatasetName
CA	I-DatasetName
also	O
have	O
labels	O
for	O
strong	O
positive	O
and	O
strong	O
negative	O
,	O
which	O
we	O
map	O
to	O
positive	O
and	O
negative	O
,	O
respectively	O
.	O

Finally	O
,	O
NoReC	B-DatasetName
Fine	I-DatasetName
includes	O
intensity	O
annotations	O
(	O
strong	O
,	O
normal	O
,	O
slight	O
)	O
,	O
which	O
we	O
disregard	O
for	O
the	O
purposes	O
of	O
these	O
experiments	O
.	O

Structured	O
sentiment	O
graphs	O
as	O
in	O
Figure	O
1	O
are	O
directed	O
graphs	O
,	O
that	O
are	O
made	O
up	O
of	O
a	O
set	O
of	O
labeled	O
nodes	O
and	O
a	O
set	O
of	O
unlabeled	O
edges	O
connecting	O
pairs	O
of	O
nodes	O
.	O

Similarly	O
to	O
the	O
source	O
structures	O
,	O
the	O
graphs	O
can	O
have	O
multiple	O
roots	O
and	O
nodes	O
can	O
have	O
multiple	O
or	O
no	O
incoming	O
arcs	O
.	O

We	O
here	O
propose	O
two	O
simple	O
parsing	O
graph	O
representations	O
:	O
head	O
-	O
first	O
and	O
head	O
-	O
final	O
,	O
which	O
are	O
shown	O
in	O
Figure	O
2	O
.	O

For	O
head	O
-	O
first	O
,	O
we	O
set	O
the	O
first	O
token	O
of	O
the	O
sentiment	O
expression	O
as	O
a	O
root	O
node	O
,	O
and	O
similarly	O
set	O
the	O
first	O
token	O
in	O
each	O
holder	O
and	O
token	O
span	O
as	O
the	O
head	O
of	O
the	O
span	O
with	O
all	O
other	O
tokens	O
within	O
that	O
span	O
as	O
dependents	O
.	O

Head	O
-	O
final	O
is	O
similar	O
,	O
but	O
instead	O
sets	O
the	O
final	O
token	O
of	O
spans	O
as	O
the	O
heads	O
,	O
and	O
the	O
final	O
token	O
of	O
the	O
sentiment	O
expression	O
as	O
the	O
root	O
node	O
.	O

2020	O
)	O
for	O
negation	O
resolution	O
.	O

The	O
base	O
of	O
the	O
network	O
structure	O
is	O
a	O
bidirectional	B-MethodName
LSTM	I-MethodName
(	O
BiLSTM	B-MethodName
)	O
,	O
that	O
processes	O
the	O
input	O
sentence	O
both	O
from	O
left	O
-	O
toright	O
and	O
right	O
-	O
to	O
-	O
left	O
,	O
to	O
create	O
contextualized	O
representations	O
c	O
1	O
,	O
.	O
.	O
.	O
,	O

w	O
n	O
)	O
where	O
w	O
i	O
is	O
the	O
concatenation	O
of	O
a	O
word	O
embedding	O
,	O
POS	O
tag	O
embedding	O
,	O
lemma	O
embedding	O
,	O
and	O
character	O
embedding	O
created	O
by	O
a	O
character	O
-	O
based	O
LSTM	B-MethodName
for	O
the	O
ith	O
token	O
.	O

In	O
our	O
experiments	O
,	O
we	O
further	O
augment	O
the	O
token	O
representations	O
with	O
pretrained	O
contextualized	O
embeddings	O
from	O
multilingual	B-MethodName
BERT	I-MethodName
.	O

We	O
use	O
multilingual	B-MethodName
BERT	I-MethodName
as	O
several	O
languages	O
did	O
not	O
have	O
available	O
monolingual	B-MethodName
BERT	I-MethodName
models	O
at	O
the	O
time	O
of	O
the	O
experiments	O
(	O
Catalan	O
,	O
Norwegian).The	O
contextualized	O
embeddings	O
are	O
then	O
processed	O
by	O
two	O
feedforward	O
neural	O
networks	O
(	O
FNN	O
)	O
,	O
creating	O
specialized	O
representations	O
for	O
potential	O
heads	O
and	O
dependents	O
,	O
h	O
i	O
=	O
FNN	O
head	O
(	O
c	O
i	O
)	O
and	O
d	O
i	O
=	O
FNN	O
dep	O
(	O
c	O
i	O
)	O
.	O

Its	O
inner	O
dimension	O
corresponds	O
to	O
the	O
number	O
of	O
sentiment	O
graph	O
labels	O
plus	O
a	O
special	O
NONE	O
label	O
,	O
indicating	O
the	O
ab	O
-	O
sence	O
of	O
an	O
arc	O
,	O
which	O
allows	O
the	O
model	O
to	O
predict	O
arcs	O
and	O
labels	O
jointly	O
,	O
score(h	O
i	O
,	O
d	O
j	O
)	O
=	O
h	O
i	O
U	O
d	O
j	O
.	O

We	O
compare	O
our	O
proposed	O
graph	O
prediction	O
approach	O
with	O
three	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
5	O
for	O
extracting	O
targets	O
and	O
expressions	O
and	O
predicting	O
the	O
polarity	O
:	O
IMN	B-MethodName
6	O
,	O
RACL	B-MethodName
7	O
,	O
as	O
well	O
as	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
,	O
which	O
also	O
incorporates	O
contextualized	O
embeddings	O
.	O

Instead	O
of	O
using	O
BERT	B-MethodName
Large	I-MethodName
,	O
we	O
use	O
the	O
cased	B-MethodName
BERT	I-MethodName
-	I-MethodName
multilingual	I-MethodName
-	I-MethodName
base	I-MethodName
in	O
order	O
to	O
fairly	O
compare	O
with	O
our	O
own	O
models	O
.	O

Note	O
,	O
however	O
,	O
that	O
our	O
model	O
does	O
not	O
update	O
the	O
mBERT	B-MethodName
representations	O
,	O
putting	O
it	O
at	O
a	O
disadvantage	O
to	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
.	O

The	O
main	O
metrics	O
we	O
use	O
to	O
rank	O
models	O
are	O
Targeted	B-MetricName
F	I-MetricName
1	I-MetricName
and	O
Sentiment	B-MetricName
Graph	I-MetricName
F	I-MetricName
1	I-MetricName
.	O

Table	O
3	O
:	O
Experiments	O
comparing	O
our	O
sentiment	O
graph	O
approaches	O
(	O
Head	O
-	O
first	O
/	O
Head	O
-	O
final	O
)	O
using	O
mBERT	B-MethodName
with	O
the	O
sequence	O
-	O
labeling	O
baselines	O
(	O
IMN	B-MethodName
,	O
RACL	B-MethodName
,	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
)	O
.	O

indicates	O
results	O
that	O
are	O
not	O
comparable	O
,	O
as	O
they	O
were	O
calculated	O
with	O
10	B-HyperparameterValue
-	O
fold	O
cross	O
-	O
validation	O
.	O

Token	B-MetricName
-	I-MetricName
level	I-MetricName
F	I-MetricName
1	I-MetricName
for	O
Holders	O
,	O
Targets	O
,	O
and	O
Expressions	O
To	O
easily	O
compare	O
our	O
models	O
to	O
pipeline	O
models	O
,	O
we	O
evaluate	O
how	O
well	O
these	O
models	O
are	O
able	O
to	O
identify	O
the	O
elements	O
of	O
a	O
sentiment	O
graph	O
with	O
token	B-MetricName
-	I-MetricName
level	I-MetricName
F	I-MetricName
1	I-MetricName
.Targeted	B-MetricName
F	I-MetricName
1	I-MetricName
This	O
is	O
a	O
common	O
metric	O
in	O
targeted	B-TaskName
sentiment	I-TaskName
analysis	I-TaskName
(	O
also	O
referred	O
to	O
as	O
F	B-MetricName
1	I-MetricName
-i	I-MetricName
(	O
He	O
et	O
al	O
.	O
,	O

2019	O
)	O
or	O
ABSA	B-MetricName
F	I-MetricName
1	I-MetricName
(	O
Chen	O
and	O
Qian	O
,	O
2020)).A	O
true	O
positive	O
requires	O
the	O
combination	O
of	O
exact	O
extraction	O
of	O
the	O
sentiment	O
target	O
,	O
and	O
the	O
correct	O
polarity	O
.	O

Parsing	O
graph	O
metrics	O
We	O
additionally	O
compute	O
graph	O
-	O
level	O
metrics	O
to	O
determine	O
how	O
well	O
the	O
models	O
predict	O
the	O
unlabeled	O
and	O
labeled	O
arcs	O
of	O
the	O
parsing	O
graphs	O
:	O
Unlabeled	B-MetricName
F	I-MetricName
1	I-MetricName
(	O
UF	B-MetricName
1	I-MetricName
)	O
,	O
Labeled	B-MetricName
F	I-MetricName
1	I-MetricName
(	O
LF	B-MetricName
1	I-MetricName
)	O
.	O

These	O
measure	O
the	O
amount	O
of	O
(	O
in)correctly	O
predicted	O
arcs	O
and	O
labels	O
,	O
as	O
the	O
harmonic	O
mean	O
of	O
precision	B-MetricName
and	O
recall	B-MetricName
(	O
Oepen	O
et	O
al	O
.	O
,	O

2014	O
)	O
.	O

The	O
two	O
metrics	O
that	O
measure	O
how	O
well	O
a	O
model	O
is	O
able	O
to	O
capture	O
the	O
full	O
sentiment	O
graph	O
(	O
see	O
Figure	O
1	O
)	O
are	O
Non	B-MetricName
-	I-MetricName
polar	I-MetricName
Sentiment	I-MetricName
Graph	I-MetricName
F	I-MetricName
1	I-MetricName
(	O
NSF	B-MetricName
1	I-MetricName
)	O
and	O
Sentiment	B-MetricName
Graph	I-MetricName
F	I-MetricName
1	I-MetricName
(	O
SF	B-MetricName
1	I-MetricName
)	O
.	O

For	O
NSF	B-MetricName
1	I-MetricName
,	O
each	O
sentiment	O
graph	O
is	O
a	O
tuple	O
of	O
(	O
holder	O
,	O
target	O
,	O
expression	O
)	O
,	O
while	O
for	O
SF	B-MetricName
1	I-MetricName
we	O
include	O
polarity	O
(	O
holder	O
,	O
target	O
,	O
expression	O
,	O
polarity	O
)	O
.	O

For	O
precision	B-MetricName
we	O
weight	O
the	O
number	O
of	O
correctly	O
predicted	O
tokens	O
divided	O
by	O
the	O
total	O
number	O
of	O
predicted	O
tokens	O
(	O
for	O
recall	B-MetricName
,	O
we	O
divide	O
instead	O
by	O
the	O
number	O
of	O
gold	O
tokens	O
)	O
.	O

All	O
sentiment	O
graph	O
models	O
use	O
token	O
-	O
level	O
mBERT	B-MethodName
representations	O
in	O
addition	O
to	O
word2vec	B-MethodName
skip	I-MethodName
-	I-MethodName
gram	I-MethodName
embeddings	I-MethodName
openly	O
available	O
from	O
the	O
NLPL	O
vector	O
repository	O
8	O
(	O
Fares	O
et	O
al	O
.	O
,	O

We	O
train	O
all	O
models	O
for	O
100	B-HyperparameterValue
epochs	B-HyperparameterName
and	O
keep	O
the	O
model	O
that	O
performs	O
best	O
regarding	O
LF	B-MetricName
1	I-MetricName
on	O
the	O
dev	O
set	O
(	O
Targeted	B-MetricName
F	I-MetricName
1	I-MetricName
for	O
the	O
baselines	O
)	O
.	O

On	O
NoReC	B-DatasetName
Fine	I-DatasetName
,	O
the	O
baselines	O
IMN	B-MethodName
,	O
RACL	B-MethodName
,	O
and	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
perform	O
well	O
at	O
extracting	B-TaskName
targets	I-TaskName
(	O
35.9	B-MetricValue
,	O
45.6	B-MetricValue
,	O
and	O
47.2	B-MetricValue
F	B-MetricName
1	I-MetricName
,	O
respectively	O
)	O
and	O
expressions	B-TaskName
(	O
48.7/55.4/56.3	B-MetricValue
)	O
,	O
but	O
struggle	O
with	O
the	O
full	B-TaskName
targeted	I-TaskName
sentiment	I-TaskName
task	O
(	O
18.0/20.1/30.3	B-MetricName
)	O
.	O

The	O
graphbased	O
models	O
extract	O
targets	O
better	O
(	O
50.1/54.8	B-MetricName
)	O
and	O
have	O
comparable	O
scores	O
for	O
expressions	B-TaskName
(	O
54.4/55.5	B-MetricName
)	O
.	O

The	O
holder	O
extraction	O
scores	O
have	O
a	O
similar	O
range	O
(	O
51.1/60.4	B-MetricValue
)	O
.	O

These	O
patterns	O
hold	O
throughout	O
the	O
other	O
datasets	O
,	O
where	O
the	O
proposed	O
graph	O
models	O
nearly	O
always	O
perform	O
best	O
on	O
extracting	O
spans	O
,	O
although	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
achieves	O
the	O
best	O
score	O
on	O
extracting	O
targets	O
on	O
DS	B-DatasetName
Unis	I-DatasetName
(	O
44.6	B-MetricValue
vs.	O
42.1	B-MetricValue
)	O
.	O

The	O
graph	O
models	O
also	O
outperform	O
the	O
strongest	O
baseline	O
(	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
)	O
on	O
targeted	O
sentiment	O
on	O
all	O
5	O
datasets	O
,	O
although	O
this	O
difference	O
is	O
often	O
not	O
statistically	O
significant	O
(	O
NoReC	B-DatasetName
Fine	I-DatasetName
Head	O
-	O
first	O
,	O
MultiB	B-DatasetName
EU	I-DatasetName
Head	I-DatasetName
-	O
final	O
)	O
and	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
is	O
better	O
than	O
Head	O
-	O
first	O
on	O
DS	B-DatasetName
Unis	I-DatasetName
.Regarding	O
the	O
Graph	O
metrics	O
,	O
the	O
results	O
depend	O
highly	O
on	O
the	O
dataset	O
,	O
with	O
UF	B-MetricName
1	I-MetricName
and	O
LF	B-MetricName
1	I-MetricName
ranging	O
from	O
35.3/31.4	B-MetricValue
(	O
DS	B-DatasetName
Unis	I-DatasetName
Head	O
-	O
first	O
)	O
to	O
66.8/62.1	B-MetricValue
(	O
MultiB	B-DatasetName
CA	I-DatasetName
Head	O
-	O
first	O
)	O
.	O

Sentiment	O
Graph	O
metrics	O
NSF	B-MetricName
1	I-MetricName
and	O
SF	B-MetricName
1	I-MetricName
have	O
a	O
similar	O
,	O
though	O
slightly	O
lower	O
range	O
(	O
24.5/17.7	B-MetricName
-62.0/56.8	B-MetricName
)	O
.	O

The	O
graph	O
and	O
sentiment	O
graph	O
metrics	O
do	O
not	O
correlate	O
perfectly	O
,	O
however	O
,	O
as	O
UF	B-MetricName
1	I-MetricName
and	O
LF	B-MetricName
1	I-MetricName
on	O
MPQA	B-DatasetName
are	O
relatively	O
good	O
8	O
Nordic	O
Language	O
Processing	O
Laboratory	O
vector	O
repo	O
.	O
:	O

and	O
100dimensional	O
embeddings	O
trained	O
on	O
the	O
2017	O
CoNLL	B-DatasetName
corpora	O
for	O
all	O
others	O
;	O
Basque	O
(	O
i	O
d	O
32	O
)	O
,	O
Catalan	O
(	O
i	O
d	O
34	O
)	O
,	O
and	O
Norwegian	O
Bokmål	O
(	O
i	O
d	O
58	O
)	O
.	O
(	O

40.0/36.9	B-MetricValue
and	O
41.4/38.0	B-MetricValue
for	O
Head	O
-	O
first	O
and	O
Headfinal	O
,	O
respectively	O
)	O
,	O
but	O
the	O
NSF	B-MetricName
1	I-MetricName
and	O
SF	B-MetricName
1	I-MetricName
are	O
poor	O
(	O
24.5/17.4	B-MetricValue
and	O
26.1/18.8).On	B-MetricValue
average	O
IMN	B-MethodName
is	O
the	O
weakest	O
baseline	O
,	O
followed	O
by	O
RACL	B-MethodName
and	O
then	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
.	O

The	O
main	O
improvement	O
that	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
gives	O
over	O
RACL	B-MethodName
on	O
these	O
datasets	O
is	O
seen	O
in	O
the	O
Targeted	O
metric	O
,	O
i.e.	O
,	O
the	O
contextualized	O
representations	O
improve	O
the	O
polarity	O
classification	O
more	O
than	O
the	O
extraction	O
task	O
.	O

Head	O
-	O
first	O
performs	O
better	O
on	O
MultiB	B-DatasetName
CA	I-DatasetName
and	O
slightly	O
better	O
on	O
MultiB	B-DatasetName
EU	I-DatasetName
,	O
while	O
for	O
the	O
others	O
(	O
NoReC	B-DatasetName
Fine	I-DatasetName
,	O
MPQA	B-DatasetName
,	O
and	O
DS	B-DatasetName
Unis	I-DatasetName
)	O
Head	O
-	O
final	O
is	O
better	O
.	O

This	O
suggests	O
that	O
the	O
main	O
benefit	O
is	O
the	O
joint	O
prediction	O
of	O
all	O
spans	O
and	O
relationships	O
,	O
and	O
that	O
the	O
specific	O
graph	O
representation	O
matters	O
less	O
.	O

Our	O
two	O
baseline	O
graph	O
representations	O
,	O
Head	O
-	O
first	O
and	O
Head	O
-	O
final	O
,	O
are	O
crude	O
approximations	O
of	O
linguistic	O
structure	O
.	O

As	O
there	O
can	O
be	O
more	O
than	O
one	O
such	O
edge	O
,	O
we	O
default	O
to	O
the	O
first	O
.	O

Therefore	O
,	O
we	O
suggest	O
a	O
final	O
approach	O
,	O
Dep	O
.	O

It	O
also	O
generally	O
improves	O
the	O
graph	O
scores	O
UF	B-MetricName
1	I-MetricName
and	O
LF	B-MetricName
1	I-MetricName
on	O
the	O
non	O
-	O
English	O
datasets	O
.	O

Dep	O
.	O

edges	O
has	O
the	O
strongest	O
positive	O
effect	O
on	O
the	O
NSF	B-MetricName
1	I-MetricName
and	O
SF	B-MetricName
1	I-MetricName
(	O
an	O
avg	O
.	O

2.52	B-MetricValue
and	O
2.22	B-MetricValue
percentage	O
point	O
(	O
pp	O
)	O
over	O
Head	O
-	O
final	O
,	O
respectively	O
)	O
.	O

Removing	O
these	O
two	O
,	O
the	O
average	O
benefit	O
is	O
5.2	B-MetricValue
and	O
4.2	B-MetricValue
for	O
NSF	B-MetricName
1	I-MetricName
and	O
SF	B-MetricName
1	I-MetricName
,	O
respectively	O
.	O

On	O
span	O
extraction	O
and	O
targeted	O
sentiment	O
,	O
however	O
,	O
Dep	O
.	O

Therefore	O
,	O
we	O
create	O
a	O
subset	O
of	O
the	O
test	O
data	O
containing	O
sentences	O
with	O
multiple	O
targets	O
and	O
reevaluate	O
Head	O
-	O
first	O
,	O
Head	O
-	O
final	O
,	O
and	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
the	O
target	O
extraction	O
task	O
.	O

Table	O
4	O
shows	O
the	O
number	O
of	O
sentences	O
with	O
multiple	O
targets	O
and	O
the	O
Target	O
span	O
extraction	O
score	O
for	O
each	O
model	O
.	O

On	O
this	O
subset	O
,	O
Head	O
-	O
first	O
and	O
Head	O
-	O
final	O
outperform	O
RACL	B-MethodName
-	I-MethodName
BERT	I-MethodName
on	O
9	O
of	O
10	O
experiments	O
,	O
confirming	O
the	O
hypothesis	O
that	O
the	O
graph	O
models	O
improve	O
on	O
examples	O
with	O
multiple	O
targets	O
.	O

We	O
also	O
perform	O
experiments	O
without	O
mBERT	B-MethodName
(	O
shown	O
in	O
Table	O
7	O
in	O
the	O
Appendix	O
)	O
and	O
show	O
the	O
average	O
gains	O
(	O
over	O
all	O
6	O
graph	O
setups	O
)	O
of	O
including	O
it	O
in	O
Table	O
5	O
.	O

Adding	O
the	O
mBERT	B-MethodName
features	O
leads	O
to	O
average	O
improvements	O
in	O
all	O
experiments	O
:	O
for	O
extracting	O
spans	O
an	O
average	O
gain	O
of	O
4.1	B-MetricValue
pp	O
for	O
holders	O
,	O
3.4	B-MetricValue
for	O
targets	O
,	O
and	O
3.1	B-MetricValue
for	O
expressions	O
.	O

For	O
targeted	O
sentiment	O
there	O
is	O
a	O
larger	O
gain	O
of	O
4.2	B-MetricValue
pp	O
,	O
while	O
for	O
the	O
parsing	O
graph	O
metrics	O
UF	B-MetricName
1	I-MetricName
and	O
lF	B-MetricName
1	I-MetricName
the	O
gains	O
are	O
more	O
limited	O
(	O
3.3	B-MetricValue
pp/	O
3.8	B-MetricValue
pp	O
)	O
and	O
similarly	O
for	O
NSF	B-MetricName
1	I-MetricName
and	O
SF	B-MetricName
1	I-MetricName
(	O
3.6	B-MetricValue
pp/	O
3.9	B-MetricValue
pp	O
)	O
.	O

The	O
gains	O
are	O
6	B-MetricValue
:	O
Polarity	B-MetricName
F	I-MetricName
1	I-MetricName
scores	O
(	O
unweighted	O
and	O
weighted	O
)	O
of	O
models	O
augmented	O
with	O
mBERT	B-MethodName
on	O
the	O
head	O
-	O
final	O
setup	O
.	O

We	O
report	O
average	O
and	O
standard	O
deviation	O
over	O
5	O
runs.largest	O
for	O
the	O
English	O
datasets	O
(	O
MPQA	B-DatasetName
,	O
DS	B-DatasetName
Unis	I-DatasetName
)	O
followed	O
by	O
NoReC	B-DatasetName
Fine	O
,	O
and	O
finally	O
MultiB	B-DatasetName
CA	I-DatasetName
and	O
MultiB	B-DatasetName
EU	I-DatasetName
.	O

In	O
this	O
section	O
we	O
zoom	O
in	O
on	O
polarity	O
,	O
in	O
order	O
to	O
quantify	O
how	O
well	O
models	O
perform	O
at	O
predicting	O
only	O
polarity	O
.	O

As	O
the	O
polarity	O
annotations	O
are	O
bound	O
to	O
the	O
expressions	O
,	O
we	O
consider	O
true	O
positives	O
to	O
be	O
any	O
expression	O
that	O
overlaps	O
the	O
gold	O
expression	O
and	O
has	O
the	O
same	O
polarity	O
.	O

Table	O
6	O
shows	O
that	O
the	O
polarity	O
predictions	O
are	O
best	O
on	O
MultiB	B-DatasetName
EU	I-DatasetName
and	O
MultiB	B-DatasetName
CA	I-DatasetName
,	O
followed	O
by	O
NoReC	B-DatasetName
Fine	I-DatasetName
and	O
DS	B-DatasetName
Unis	I-DatasetName
,	O
and	O
finally	O
MPQA	B-DatasetName
.	O

NoReC	O
Fine	O
contains	O
many	O
domains	O
and	O
has	O
longer	O
expressions	O
,	O
while	O
MPQA	B-DatasetName
contains	O
many	O
highly	O
ambiguous	O
polar	O
expressions	O
,	O
e.g.	O
,	O
'	O
said	O
'	O
,	O
'	O
asked	O
'	O
,	O
which	O
have	O
different	O
polarity	O
depending	O
on	O
the	O
context	O
.	O

2020	O
)	O
or	O
by	O
multi	O
-	O
task	O
learning	O
to	O
dependency	O
parse	O
.	O

As	O
the	O
pandemic	O
is	O
a	O
global	O
problem	O
,	O
it	O
is	O
worth	O
creating	O
COVID-19	O
related	O
datasets	O
for	O
languages	O
other	O
than	O
English	O
.	O

Particularly	O
,	O
our	O
dataset	O
is	O
annotated	O
for	O
the	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
task	O
with	O
newly	O
-	O
defined	O
entity	O
types	O
that	O
can	O
be	O
used	O
in	O
other	O
future	O
epidemics	O
.	O

Our	O
dataset	O
also	O
contains	O
the	O
largest	O
number	O
of	O
entities	O
compared	O
to	O
existing	O
Vietnamese	B-DatasetName
NER	I-DatasetName
datasets	O
.	O

We	O
empirically	O
conduct	O
experiments	O
using	O
strong	O
baselines	O
on	O
our	O
dataset	O
,	O
and	O
find	O
that	O
:	O
automatic	B-MethodName
Vietnamese	I-MethodName
word	I-MethodName
segmentation	I-MethodName
helps	O
improve	O
the	O
NER	B-TaskName
results	O
and	O
the	O
highest	O
performances	O
are	O
obtained	O
by	O
finetuning	O
pre	O
-	O
trained	O
language	O
models	O
where	O
the	O
monolingual	O
model	O
PhoBERT	B-MethodName
for	O
Vietnamese	O
(	O
Nguyen	O
and	O
Nguyen	O
,	O
2020	O
)	O
produces	O
higher	O
results	O
than	O
the	O
multilingual	O
model	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
As	O
of	O
early	O
November	O
2020	O
,	O
the	O
total	O
number	O
of	O
COVID-19	O
cases	O
worldwide	O
has	O
surpassed	O
50M.	O
1	O
The	O
world	O
is	O
once	O
again	O
hit	O
by	O
a	O
new	O
wave	O
of	O
COVID-19	O
infection	O
with	O
record	O
-	O
breaking	O
numbers	O
of	O
new	O
cases	O
reported	O
everyday	O
.	O

Along	O
with	O
the	O
outbreak	O
of	O
the	O
pandemic	O
,	O
information	O
about	O
the	O
COVID-19	O
is	O
aggregated	O
rapidly	O
through	O
different	O
types	O
of	O
texts	O
in	O
different	O
languages	O
(	O
Aizawa	O
et	O
al	O
.	O
,	O

One	O
of	O
the	O
first	O
steps	O
to	O
develop	O
such	O
systems	O
is	O
to	O
recognize	O
relevant	O
named	O
entities	O
mentioned	O
in	O
the	O
texts	O
,	O
which	O
is	O
also	O
known	O
as	O
the	O
NER	B-TaskName
task	O
.	O

Compared	O
to	O
other	O
languages	O
,	O
data	O
resources	O
for	O
the	O
Vietnamese	B-TaskName
NER	I-TaskName
task	O
are	O
limited	O
,	O
including	O
only	O
two	O
public	O
datasets	O
from	O
the	O
VLSP	B-DatasetName
2016	I-DatasetName
and	O
2018	B-DatasetName
NER	I-DatasetName
shared	O
tasks	O
(	O
Huyen	O
and	O
Luong	O
,	O
2016;Nguyen	O
et	O
al	O
.	O
,	O

Here	O
,	O
the	O
VLSP-2018	B-DatasetName
NER	I-DatasetName
dataset	O
is	O
an	O
extension	O
of	O
the	O
VLSP-2016	B-DatasetName
NER	I-DatasetName
dataset	O
with	O
more	O
data	O
.	O

Thus	O
,	O
making	O
them	O
difficult	O
to	O
adapt	O
to	O
the	O
context	O
of	O
extracting	O
key	B-TaskName
entity	I-TaskName
information	I-TaskName
related	O
to	O
COVID-19	O
patients	O
.	O

This	O
leads	O
to	O
our	O
work	O
's	O
main	O
goals	O
that	O
are	O
:	O
(	O
i	O
)	O
To	O
develop	O
a	O
NER	B-TaskName
task	O
in	O
the	O
COVID-19	O
specified	O
domain	O
,	O
that	O
potentially	O
impacts	O
research	O
and	O
downstream	O
applications	O
,	O
and	O
(	O
ii	O
)	O
To	O
provide	O
the	O
research	O
community	O
with	O
a	O
new	O
dataset	O
for	O
recognizing	O
COVID-19	O
related	O
named	O
entities	O
in	O
Vietnamese	O
.	O

The	O
dataset	O
contains	O
informative	O
sentences	O
related	O
to	O
COVID-19	O
,	O
extracted	O
from	O
articles	O
crawled	O
from	O
reputable	O
Vietnamese	O
online	O
news	O
sites	O
.	O

Compared	O
to	O
the	O
VLSP-2016	B-DatasetName
and	O
VLSP-2018	B-DatasetName
Vietnamese	O
NER	B-TaskName
datasets	O
,	O
our	O
dataset	O
has	O
the	O
largest	O
number	O
of	O
entities	O
,	O
consisting	O
of	O
35	O
K	O
entities	O
over	O
10	O
K	O
sentences.•	O
We	O
empirically	O
investigate	O
strong	O
baselines	O
on	O
our	O
dataset	O
,	O
including	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
and	O
the	O
pre	O
-	O
trained	O
language	O
models	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	O
Conneau	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PhoBERT	B-MethodName
(	O
Nguyen	O
and	O
Nguyen	O
,	O
2020	O
)	O
.	O

We	O
find	O
that	O
:	O
(	O
i	O
)	O
Automatic	B-MethodName
Vietnamese	I-MethodName
word	I-MethodName
segmentation	I-MethodName
helps	O
improve	O
the	O
NER	B-TaskName
results	O
,	O
and	O
(	O
ii	O
)	O
The	O
highest	O
results	O
are	O
obtained	O
by	O
fine	O
-	O
tuning	O
the	O
pre	O
-	O
trained	O
language	O
models	O
,	O
where	O
PhoBERT	B-MethodName
does	O
better	O
than	O
XLM	B-MethodName
-	I-MethodName
R.•	I-MethodName
We	O
publicly	O
release	O
our	O
dataset	O
for	O
research	O
or	O
educational	O
purposes	O
.	O

We	O
hope	O
that	O
our	O
dataset	O
can	O
serve	O
as	O
a	O
starting	O
point	O
for	O
future	O
COVID-19	O
related	O
Vietnamese	O
NLP	O
research	O
and	O
applications	O
.	O

The	O
first	O
one	O
is	O
scientific	O
publications	O
,	O
including	O
the	O
datasets	O
CORD-19	B-DatasetName
(	O
Wang	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
LitCovid	B-DatasetName
(	O
Chen	O
et	O
al	O
.	O
,	O

2020	O
)	O
fine	O
-	O
tune	O
a	O
BERT	B-MethodName
-	O
based	O
NER	O
model	O
on	O
the	O
CRAFT	B-DatasetName
corpus	O
(	O
Verspoor	O
et	O
al	O
.	O
,	O

2012	O
)	O
to	O
recognize	O
and	O
then	O
normalize	O
biomedical	O
ontology	O
and	O
terminology	O
entities	O
in	O
LitCovid	B-DatasetName
.	O

In	O
general	O
,	O
these	O
entity	O
types	O
can	O
be	O
used	O
in	O
the	O
context	O
of	O
not	O
only	O
the	O
COVID-19	O
pandemic	O
but	O
also	O
in	O
other	O
future	O
epidemics	O
.	O

2020	O
)	O
,	O
we	O
utilize	O
F	B-MetricName
1	I-MetricName
score	O
to	O
measure	O
the	O
interannotator	O
agreement	O
between	O
the	O
two	O
annotators	O
at	O
the	O
entity	O
span	O
level	O
,	O
resulting	O
in	O
an	O
F1	B-MetricName
score	O
of	O
0.88	B-MetricValue
.	O

Each	O
subset	O
contains	O
100	O
sentences	O
from	O
the	O
pilot	O
set	O
from	O
the	O
first	O
annotation	O
phase	O
.	O

Annotation	O
quality	O
of	O
each	O
annotator	O
is	O
measured	O
by	B-MetricName
F	I-MetricName
1	I-MetricName
calculated	O
over	O
the	O
100	O
sentences	O
that	O
already	O
have	O
gold	O
annotations	O
from	O
the	O
pilot	O
set	O
.	O

All	O
annotators	O
are	O
asked	O
to	O
revise	O
their	O
annotations	O
until	O
they	O
achieve	O
an	O
F	B-MetricName
1	I-MetricName
of	O
at	O
least	O
0.92	B-MetricValue
.	O

Note	O
that	O
when	O
written	O
in	O
Vietnamese	O
texts	O
,	O
in	O
addition	O
to	O
marking	O
word	O
boundaries	O
,	O
white	O
space	O
is	O
also	O
used	O
to	O
separate	O
syllables	O
that	O
constitute	O
words	O
.	O

To	O
obtain	O
a	O
word	O
-	O
level	O
variant	O
of	O
the	O
dataset	O
,	O
we	O
apply	O
the	O
RDRSegmenter	O
to	O
perform	O
automatic	O
Vietnamese	O
word	O
segmentation	O
,	O
e.g.	O
a	O
4syllable	O
written	O
text	O
"	O
bệnh	O
viện	O
Đà	O
Nẵng	O
"	O
(	O
Da	O
Nang	O
hospital	O
)	O
is	O
word	O
-	O
segmented	O
into	O
a	O
2	O
-	O
word	O
text	O
"	O
bệnh_viện	O
hospital	O
Đà_Nẵng	O
Da_Nang	O
"	O
.	O

Here	O
,	O
automatic	O
Vietnamese	O
word	O
segmentation	O
outputs	O
do	O
not	O
affect	O
gold	O
boundaries	O
of	O
entity	O
mentions	O
.	O

Statistics	O
of	O
our	O
dataset	O
is	O
presented	O
in	O
Table	O
2	O
.	O

We	O
formulate	O
the	O
COVID-19	O
NER	B-TaskName
task	O
for	O
Vietnamese	O
as	O
a	O
sequence	O
labeling	O
problem	O
with	O
the	O
BIO	O
tagging	O
scheme	O
.	O

The	O
baselines	O
include	O
:	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Ma	O
and	O
Hovy	O
,	O
2016	O
)	O
and	O
the	O
pre	O
-	O
trained	O
language	O
models	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	O
Conneau	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
PhoBERT	B-MethodName
(	O
Nguyen	O
and	O
Nguyen	O
,	O
2020	O
)	O
.	B-MethodName

XLM	I-MethodName
-	I-MethodName
R	I-MethodName
is	O
a	O
multi	O
-	O
lingual	O
variant	O
of	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
pre	O
-	O
trained	O
on	O
a	O
2.5	O
TB	O
multilingual	O
dataset	O
that	O
contains	O
137	O
GB	O
of	O
syllable	O
-	O
level	O
Vietnamese	O
texts	O
.	O

PhoBERT	B-MethodName
is	O
a	O
monolingual	O
variant	O
of	O
RoBERTa	B-MethodName
,	O
pre	O
-	O
trained	O
on	O
a	O
20	O
GB	O
word	O
-	O
level	O
Vietnamese	O
dataset	O
.	O

We	O
employ	O
the	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
implementation	O
from	O
AllenNLP	O
(	O
Gardner	O
et	O
al	O
.	O
,	O

Training	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
requires	O
input	O
pretrained	O
syllable	O
-	O
and	O
word	O
-	O
level	O
embeddings	O
for	O
the	O
syllable	O
-	O
and	O
word	O
-	O
level	O
settings	O
,	O
respectively	O
.	O

Thus	O
we	O
employ	O
the	O
pre	O
-	O
trained	O
Word2Vec	B-MethodName
syllable	I-MethodName
and	I-MethodName
word	I-MethodName
embeddings	I-MethodName
for	O
Vietnamese	O
from	O
Nguyen	O
et	O
al	O
.	O
(	O

Optimal	O
hyper	O
-	O
parameters	O
that	O
we	O
gridsearched	O
for	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
are	O
presented	O
in	O
Table	O
3	O
.	O

2020	O
)	O
to	O
fine	O
-	O
tune	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
and	O
PhoBERT	B-MethodName
for	O
the	O
syllable	O
-	O
and	O
word	O
-	O
level	O
settings	O
,	O
respectively	O
,	O
using	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
with	O
a	O
fixed	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
5.e-5	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
32	B-HyperparameterValue
(	O
Liu	O
et	O
al	O
.	O
,	O

2019).The	O
baselines	O
are	O
trained	O
/	O
fine	O
-	O
tuned	O
for	O
30	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

We	O
evaluate	O
the	O
Micro	B-MetricName
-	I-MetricName
average	I-MetricName
F	I-MetricName
1	I-MetricName
score	O
after	O
each	O
epoch	B-HyperparameterName
on	O
the	O
validation	O
set	O
(	O
here	O
,	O
we	O
apply	O
early	O
stopping	O
if	O
we	O
find	O
no	O
performance	O
improvement	O
after	O
5	O
continuous	O
epochs	O
)	O
.	O

Note	O
that	O
each	O
F	B-MetricName
1	I-MetricName
score	O
reported	O
is	O
an	O
average	O
over	O
5	O
runs	O
with	O
different	O
random	O
seeds	O
.	O

Table	O
4	O
shows	O
the	O
final	O
entity	O
-	O
level	O
NER	B-MethodName
results	O
of	O
the	O
baselines	O
on	O
the	O
test	O
set	O
.	O

In	O
addition	O
to	O
the	O
standard	O
Micro	B-MetricName
-	I-MetricName
average	I-MetricName
F	I-MetricName
1	O
score	O
,	O
we	O
also	O
report	O
the	O
Macro	O
-	O
average	O
F	O
1	O
score	O
.	O

We	O
find	O
that	O
the	O
performances	O
of	O
word	O
-	O
level	O
models	O
are	O
higher	O
than	O
their	O
syllable	O
-	O
level	O
counterparts	O
,	O
showing	O
that	O
automatic	O
Vietnamese	O
word	O
segmentation	O
helps	O
improve	O
NER	B-TaskName
,	O
e.g.	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
improves	O
from	O
0.906	B-MetricValue
to	O
0.910	B-MetricValue
Micro	B-MetricName
-	I-MetricName
F	I-MetricName
1	I-MetricName
and	O
from	O
0.858	B-MetricValue
to	O
0.875	B-MetricValue
Macro	B-MetricName
-	I-MetricName
F	I-MetricName
1	I-MetricName
.We	O
also	O
find	O
that	O
fine	O
-	O
tuning	O
the	O
pre	O
-	O
trained	O
language	O
models	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
and	O
PhoBERT	B-MethodName
helps	O
produce	O
better	O
performances	O
than	O
BiLSTM	B-MethodName
-	I-MethodName
CNN	I-MethodName
-	I-MethodName
CRF	I-MethodName
.	O

Here	O
,	O
PhoBERT	B-MethodName
outperforms	O
XLM	B-MethodName
-	I-MethodName
R	I-MethodName
(	O
Micro	B-MetricName
-	I-MetricName
F	I-MetricName
1	I-MetricName
:	O
0.945	B-MetricValue
vs.	O
0.938	B-MetricValue
;	O
Macro	B-MetricName
-	I-MetricName
F	I-MetricName
1	I-MetricName
:	O
0.931	B-MetricValue
vs.	O
0.911	B-MetricValue
)	O
,	O
thus	O
reconfirming	O
the	O
effectiveness	O
of	O
pre	O
-	O
trained	O
monolingual	O
language	O
models	O
on	O
the	O
language	O
-	O
specific	O
downstream	O
tasks	O
(	O
Nguyen	O
and	O
Nguyen	O
,	O
2020	O
)	O
.	O

We	O
perform	O
an	O
error	O
analysis	O
using	O
the	O
best	O
performing	O
model	O
PhoBERT	B-MethodName
large	O
that	O
produces	O
353	O
incorrect	O
predictions	O
in	O
total	O
on	O
the	O
validation	O
set	O
.	O

The	O
second	O
error	O
group	O
contains	O
65/353	O
instances	O
with	O
inexact	O
spans	O
overlapped	O
with	O
gold	O
spans	O
but	O
having	O
correct	O
entity	O
labels	O
.	O

Particularly	O
in	O
the	O
case	O
of	O
LOCATION	O
,	O
where	O
generic	O
mentions	O
,	O
such	O
as	O
"	O
Bệnh	O
viện	O
tỉnh	O
"	O
(	O
province	O
hospital	O
)	O
,	O
"	O
Trạm	O
y	O
tế	O
xã	O
"	O
(	O
commune	O
medical	O
station	O
)	O
,	O
"	O
chung	O
cư	O
"	O
(	O
apartment	O
)	O
,	O
are	O
recognized	O
as	O
entities	O
,	O
while	O
in	O
fact	O
,	O
they	O
are	O
not	O
.	O

We	O
empirically	O
conduct	O
experiments	O
on	O
our	O
dataset	O
to	O
compare	O
strong	O
baselines	O
and	O
find	O
that	O
the	O
input	O
representations	O
and	O
the	O
pre	O
-	O
trained	O
language	O
models	O
all	O
have	O
influences	O
on	O
this	O
COVID-19	O
related	O
NER	B-MethodName
task	O
.	O

We	O
hope	O
that	O
our	O
dataset	O
can	O
serve	O
as	O
the	O
starting	O
point	O
for	O
further	O
Vietnamese	O
NLP	O
research	O
and	O
applications	O
in	O
fighting	O
the	O
COVID-19	O
and	O
other	O
future	O
epidemics	O
.	O

We	O
have	O
two	O
principles	O
for	O
selecting	O
the	O
ten	O
entity	O
types	O
:	O
(	O
i	O
)	O
Entities	O
should	O
contain	O
key	O
information	O
related	O
to	O
the	O
COVID-19	O
patients	O
(	O
here	O
,	O
the	O
information	O
should	O
be	O
helpful	O
in	O
the	O
context	O
of	O
contact	O
tracing	O
and	O
monitoring	O
the	O
growth	O
of	O
the	O
pandemic	O
)	O
;	O
and	O
(	O
ii	O
)	O
The	O
availability	O
of	O
entity	O
types	O
in	O
the	O
text	O
,	O
i.e.	O
,	O
how	O
frequent	O
does	O
each	O
of	O
the	O
entity	O
types	O
appear	O
.	O

For	O
example	O
,	O
when	O
a	O
patient	O
is	O
presented	O
at	O
an	O
organization	O
,	O
we	O
refer	O
to	O
that	O
organization	O
as	O
a	O
location	O
if	O
we	O
can	O
infer	O
its	O
specific	O
location	O
on	O
the	O
map	O
.	O

On	O
the	O
other	O
hand	O
,	O
in	O
Example	O
2	O
,	O
the	O
entity	O
mention	O
"	O
Bệnh	O
viện	O
Bệnh	O
Nhiệt	O
đới	O
TP	O
HCM	O
"	O
(	O
Ho	O
Chi	O
Minh	O
City	O
Hospital	O
for	O
Tropical	O
Diseases	O
)	O
is	O
labeled	O
as	O
ORGANIZA	O
-	O
TION	O
because	O
it	O
acts	O
as	O
the	O
subject	O
executing	O
a	O
specific	O
action	O
(	O
i.e.	O
reporting	O
a	O
test	O
result).For	O
OCCUPATION	O
,	O
AGE	O
and	O
GENDER	O
entities	O
,	O
we	O
only	O
tag	O
them	O
if	O
we	O
can	O
link	O
the	O
corresponding	O
entity	O
mentions	O
to	O
a	O
specific	O
entity	O
with	O
NAME	O
or	O
PATIENT_ID	O
label	O
within	O
the	O
same	O
sentence	O
.	O

It	O
has	O
been	O
shown	O
that	B-TaskName
named	I-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
could	O
benefit	O
from	O
incorporating	O
the	O
long	O
-	O
distance	O
structured	O
information	O
captured	O
by	O
dependency	O
trees	O
.	O

However	O
,	O
existing	O
approaches	O
largely	O
focused	O
on	O
stacking	O
the	O
LSTM	B-MethodName
and	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
such	O
as	O
graph	B-MethodName
convolutional	I-MethodName
networks	I-MethodName
(	O
GCNs	B-MethodName
)	O
for	O
building	O
improved	O
NER	B-TaskName
models	O
,	O
where	O
the	O
exact	O
interaction	O
mechanism	O
between	O
the	O
two	O
different	O
types	O
of	O
features	O
is	O
not	O
very	O
clear	O
,	O
and	O
the	O
performance	O
gain	O
does	O
not	O
appear	O
to	O
be	O
significant	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
and	O
robust	O
solution	O
to	O
incorporate	O
both	O
types	O
of	O
features	O
with	O
our	O
Synergized	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
)	O
,	O
which	O
clearly	O
captures	O
how	O
the	O
two	O
types	O
of	O
features	O
interact	O
.	O

1	O
Named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
(	O
NER	B-TaskName
)	O
is	O
one	O
of	O
the	O
most	O
fundamental	O
and	O
important	O
tasks	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
.	O

2018;Akbik	O
et	O
al	O
.	O
,	O

2019	O
)	O
largely	O
focuses	O
on	O
training	O
deep	O
language	O
models	O
to	O
improve	O
the	O
contextualized	O
word	O
representations	O
,	O
previous	O
studies	O
show	O
that	O
the	O
structured	O
information	O
such	O
as	O
interactions	O
between	O
non	O
-	O
adjacent	O
words	O
can	O
also	O
be	O
important	O
for	O
NER	B-TaskName
(	O
Finkel	O
et	O
al	O
.	O
,	O

2005;Jie	O
et	O
al	O
.	O
,	O

For	O
instance	O
,	O
Figure	O
1	O
(	O
top	O
)	O
shows	O
one	O
type	O
of	O
structured	O
information	O
in	O
NER	B-TaskName
.	O

2019	O
)	O
have	O
been	O
using	O
the	O
parse	B-MethodName
trees	I-MethodName
(	O
Chomsky	O
,	O
1956(Chomsky	O
,	O
,	O
1969Sandra	O
and	O
Taft	O
,	O
2014	O
)	O
to	O
incorporate	O
such	O
structured	O
information	O
.	O

Incorporating	O
the	O
dependency	O
information	O
can	O
be	O
done	O
with	O
graph	B-MethodName
neural	I-MethodName
networks	I-MethodName
(	O
GNNs	B-MethodName
)	O
such	O
as	O
graph	B-MethodName
convolutional	I-MethodName
networks	I-MethodName
(	O
GCNs	B-MethodName
)	O
(	O
Kipf	O
and	O
Welling	O
,	O
2017	O
)	O
.	O

However	O
,	O
simply	O
stacking	O
the	O
LSTM	B-MethodName
and	O
GCN	B-MethodName
architectures	O
for	O
NER	B-TaskName
can	O
only	O
provide	O
us	O
with	O
modest	O
improvements	O
;	O
sometimes	O
,	O
it	O
decreases	O
performance	O
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
.	O

Based	O
on	O
the	O
depen	O
-	O
dency	O
path	O
in	O
Figure	O
1	O
,	O
it	O
requires	O
a	O
5	O
-	O
layer	O
GCN	B-MethodName
to	O
capture	O
the	O
connections	O
between	O
these	O
two	O
entities	O
.	O

However	O
,	O
deep	O
GCN	B-MethodName
architectures	O
often	O
face	O
training	O
difficulties	O
,	O
which	O
cause	O
a	O
performance	O
drop	O
(	O
Hamilton	O
et	O
al	O
.	O
,	O

Directly	O
stacking	O
GCN	B-MethodName
and	O
LSTM	B-MethodName
has	O
difficulties	O
in	O
modeling	O
the	O
interaction	O
between	O
dependency	O
trees	O
and	O
contextual	O
information	O
.	O

To	O
address	O
the	O
above	O
limitations	O
,	O
we	O
propose	O
the	O
Synergized	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
)	O
,	O
a	O
new	O
recurrent	O
neural	O
network	O
architecture	O
that	O
considers	O
an	O
additional	O
graph	O
-	O
encoded	O
representation	O
to	O
update	O
the	O
memory	O
and	O
hidden	O
states	O
,	O
as	O
shown	O
in	O
Figure	O
2	O
.	O

More	O
specifically	O
,	O
the	O
graph	O
-	O
encoded	O
representation	O
for	O
each	O
word	O
can	O
be	O
obtained	O
with	O
GCNs	B-MethodName
.	O

Our	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
allows	O
the	O
cell	O
to	O
receive	O
the	O
structured	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
.	O

With	O
the	O
newly	O
designed	O
gating	O
mechanism	O
,	O
our	O
model	O
is	O
able	O
to	O
make	O
independent	O
assessments	O
on	O
the	O
amounts	O
of	O
information	O
to	O
be	O
retrieved	O
from	O
the	O
word	O
representation	O
and	O
the	O
graph	O
-	O
encoded	O
representation	O
respectively	O
.	O

Our	O
contributions	O
can	O
be	O
summarized	O
as:•	O
We	O
propose	O
a	O
simple	O
and	O
robust	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
model	O
to	O
better	O
incorporate	O
the	O
structured	O
information	O
conveyed	O
by	O
dependency	O
trees	O
.	O

The	O
output	O
of	O
the	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
cell	O
is	O
jointly	O
determined	O
by	O
both	O
contextual	O
and	O
structured	O
information	O
.	O

We	O
adopt	O
the	O
classic	O
conditional	B-MethodName
random	I-MethodName
fields	I-MethodName
(	O
CRF	B-MethodName
)	O
(	O
Lafferty	O
et	O
al	O
.	O
,	O

2001	O
)	O
on	O
top	O
of	O
the	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
for	O
NER.•	B-TaskName
We	O
conduct	O
extensive	O
experiments	O
on	O
several	O
standard	O
datasets	O
across	O
four	O
languages	O
.	O

Our	O
further	O
analysis	O
statistically	O
demonstrates	O
the	O
proposed	O
gating	O
mechanism	O
is	O
able	O
to	O
aggregate	O
the	O
structured	O
information	O
selectively.2	O
Synergized	B-MethodName
-	I-MethodName
LSTM	I-MethodName
To	O
incorporate	O
the	O
long	O
-	O
range	O
dependencies	O
,	O
we	O
consider	O
an	O
additional	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
(	O
Figure	O
2	O
)	O
as	O
the	O
model	O
input	O
to	O
integrateσ	O
σ	O
σ	O
tanh	O
tanh	O
σ	O
×	O
+	O
×	O
+	O
×	O
×	O
tanh	O
c	O
t-1Previous	O
Cell	O
contextual	O
and	O
structured	O
information	O
.	O

The	O
graphencoded	O
representation	O
g	O
t	O
can	O
be	O
derived	O
from	O
Graph	B-MethodName
Neural	I-MethodName
Networks	I-MethodName
(	O
GNNs	B-MethodName
)	O
such	O
as	O
GCN	B-MethodName
(	O
Kipf	O
and	O
Welling	O
,	O
2017	O
)	O
,	O
which	O
are	O
capable	O
of	O
bringing	O
in	O
structured	O
information	O
through	O
graph	O
structure	O
(	O
Hamilton	O
et	O
al	O
.	O
,	O

One	O
naive	O
approach	O
is	O
to	O
use	O
a	O
deep	O
GNN	B-MethodName
to	O
capture	O
such	O
information	O
along	O
multiple	O
dependency	O
arcs	O
between	O
two	O
words	O
,	O
which	O
could	O
mess	O
up	O
information	O
and	O
lead	O
to	O
training	O
difficulties	O
.	O

A	O
straightforward	O
solution	O
is	O
to	O
integrate	O
both	O
structured	O
and	O
contextual	O
information	O
via	O
LSTM	B-MethodName
.	O

As	O
shown	O
in	O
Figure	O
1	O
(	O
Hybrid	O
Paths	O
)	O
,	O
the	O
structured	O
information	O
can	O
be	O
passed	O
to	O
neighbors	O
or	O
context	O
,	O
which	O
allows	O
a	O
model	O
to	O
use	O
less	O
number	O
of	O
GNN	B-MethodName
layers	O
and	O
alleviate	O
such	O
issues	O
for	O
long	O
-	O
range	O
dependencies	O
.	O

The	O
input	O
to	O
the	O
LSTM	B-MethodName
can	O
simply	O
be	O
the	O
concatenation	O
of	O
word	O
representation	O
x	O
t	O
and	O
g	O
t	O
at	O
each	O
position	O
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
2	O
.	O

Thus	O
,	O
we	O
need	O
to	O
design	O
a	O
new	O
approach	O
to	O
incorporate	O
both	O
types	O
of	O
information	O
from	O
x	O
t	O
and	O
g	O
t	O
with	O
a	O
more	O
explicit	O
interaction	O
mechanism	O
,	O
with	O
which	O
we	O
hope	O
to	O
alleviate	O
the	O
above	O
issues	O
.	O

We	O
propose	O
the	O
Synergized	B-MethodName
-	I-MethodName
LSTM	I-MethodName
(	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
)	O
to	O
better	O
integrate	O
the	O
contextual	O
and	O
structured	O
information	O
to	O
address	O
the	O
above	O
limitations	O
.	O

The	O
inputs	O
of	O
the	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
cell	O
include	O
previous	O
cell	O
state	O
c	O
t−1	O
,	O
previous	O
hidden	O
state	O
h	O
t−1	O
,	O
current	O
cell	O
input	O
x	O
t	O
,	O
and	O
an	O
additional	O
graph	O
-	O
encoded	O
representation	O
g	O
t	O
.	O

The	O
outputs	O
of	O
the	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
cell	O
include	O
current	O
cell	O
state	O
c	O
t	O
and	O
current	O
hidden	O
state	O
h	O
t	O
.	O

The	O
cell	O
state	O
c	O
t	O
and	O
hidden	O
state	O
h	O
t	O
are	O
computed	O
as	O
follows	O
:	O
f	O
t	O
=	O
σ(W	O
(	O
f	O
)	O
x	O
t	O
+	O
U	O
(	O
f	O
)	O
h	O
t−1	O
+	O
Q	O
(	O
f	O
)	O
g	O
t	O
+	O
b	O
(	O
f	O
)	O
)	O
(	O
1	O
)	O
o	O
t	O
=	O
σ(W	O
(	O
o	O
)	O
x	O
t	O
+	O
U	O
(	O
o	O
)	O
h	O
t−1	O
+	O
Q	O
(	O
o	O
)	O
g	O
t	O
+	O
b	O
(	O
o	O
)	O
)	O
(	O
2	O
)	O
i	O
t	O
=	O
σ(W	O
(	O
i	O
)	O
x	O
t	O
+	O
U	O
(	O
i	O
)	O
h	O
t−1	O
+	O
b	O
(	O
i	O
)	O
)	O
(	O
3)m	O
t	O
=	O
σ(W	O
(	O
m	O
)	O
g	O
t	O
+	O
U	O
(	O
m	O
)	O
h	O
t−1	O
+	O
b	O
(	O
m	O
)	O
)	O
(	O
4)c	O
t	O
=	O
tanh(W	O
(	O
u	O
)	O
x	O
t	O
+	O
U	O
(	O
u	O
)	O
h	O
t−1	O
+	O
b	O
(	O
u	O
)	O
)	O
(	O
5)s	O
t	O
=	O
tanh(W	O
(	O
n	O
)	O
g	O
t	O
+	O
U	O
(	O
n	O
)	O
h	O
t−1	O
+	O
b	O
(	O
n	O
)	O
)	O
(	O
6	O
)	O
c	O
t	O
=	O
f	O
t	O
c	O
t−1	O
+	O
i	O
t	O
c	O
t	O
+	O
m	O
t	O
s	O
t	O
(	O
7	O
)	O
h	O
t	O
=	O
o	O
t	O
tanh(c	O
t	O
)	O
(	O
8)where	O
σ	O
is	O
the	O
sigmoid	O
function	O
,	O
W	O
(	O
•	O
)	O
,	O
U	O
(	O
•	O
)	O
,	O
Q	O
(	O
•	O
)	O
and	O
b	O
(	O
•	O
)	O
are	O
learnable	O
parameters	O
.	O

With	O
the	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
,	O
the	O
structured	O
information	O
captured	O
by	O
the	O
dependency	O
trees	O
can	O
be	O
passed	O
to	O
each	O
cell	O
,	O
and	O
the	O
additional	O
gate	O
m	O
t	O
is	O
able	O
to	O
control	O
how	O
much	O
structured	O
information	O
can	O
be	O
incorporated	O
.	O

The	O
additional	O
gate	O
enables	O
the	O
model	O
to	O
feed	O
the	O
contextual	O
and	O
structured	O
information	O
into	O
the	O
LSTM	B-MethodName
cell	O
separately	O
.	O

Such	O
a	O
mechanism	O
allows	O
our	O
model	O
to	O
aggregate	O
the	O
information	O
from	O
linear	O
sequence	O
and	O
dependency	O
trees	O
selectively	O
.	O

The	O
goal	O
of	O
named	B-TaskName
entity	I-TaskName
recognition	I-TaskName
is	O
to	O
predict	O
the	O
label	O
sequence	O
y	O
=	O
{	O
y	O
1	O
,	O
y	O
2	O
,	O
...	O
,	O
y	O
n	O
}	O
given	O
the	O
input	O
sequence	O
w	O
=	O
{	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
}	O
,	O
where	O
w	O
t	O
represents	O
the	O
t	O
-	O
th	O
word	O
and	O
n	O
is	O
the	O
number	O
of	O
words	O
.	O

Furthermore	O
,	O
previous	O
methods	O
)	O
use	O
embeddings	O
of	O
part	B-MethodName
-	I-MethodName
ofspeech	I-MethodName
(	O
POS	B-MethodName
)	O
tags	B-MethodName
as	O
additional	O
input	O
representation	O
.	O

For	O
experiments	O
with	O
the	O
contextualized	O
representations	O
(	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

For	O
our	O
task	O
,	O
we	O
employ	O
the	O
graph	B-MethodName
convolutional	I-MethodName
network	I-MethodName
(	O
Kipf	O
and	O
Welling	O
,	O
2017;Zhang	O
et	O
al	O
.	O
,	O

The	O
input	O
and	O
output	O
representations	O
of	O
the	O
l	O
-	O
th	O
layer	O
GCN	B-MethodName
at	O
t	O
-	O
th	O
position	O
are	O
denoted	O
as	O
g	O
l−1	O
t	O
and	O
g	O
l	O
t	O
respectively	O
.	O

Similar	O
to	O
the	O
work	O
by	O
Zhang	O
et	O
al	O
.	O
(	O

The	O
GCN	B-MethodName
operation	O
is	O
defined	O
as	O
:	O
g	O
l	O
t	O
=	O
ReLU	O
(	O
n	O
j=1	O
A	O
t	O
,	O
j	O
W	O
l	O
g	O
l−1	O
t	O
/d	O
t	O
+	O
b	O
l	O
)	O
(	O
13)where	O
W	O
l	O
is	O
a	O
linear	O
transformation	O
and	O
b	O
l	O
is	O
a	O
bias	O
.	O

The	O
initial	O
g	O
0	O
t	O
is	O
the	O
concatenation	O
of	O
word	O
embedding	O
v	O
t	O
,	O
character	O
embedding	O
e	O
t	O
,	O
and	O
dependency	O
relation	O
embedding	O
r	O
t	O
:	O
←	O
−	O
h	O
t	O
from	O
backward	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
to	O
form	O
the	O
contextual	O
representation	O
of	O
t	O
-	O
th	O
token	O
:	O
g	O
0	O
t	O
=	O
[	O
v	O
t	O
;	O
e	O
t	O
;	O
r	O
t	O
]	O
.	O

Bi	O
-	O
directionalh	O
t	O
=	O
[	O
−	O
→	O
h	O
t	O
;	O
←	O
−	O
h	O
t	O
]	O
.CRF	B-MethodName
Layer	O
The	O
CRF	B-MethodName
(	O
Lafferty	O
et	O
al	O
.	O
,	O

2001	O
)	O
is	O
widely	O
used	O
in	O
NER	B-TaskName
tasks	O
as	O
it	O
is	O
capable	O
of	O
capturing	O
the	O
structured	O
correlations	O
between	O
adjacent	O
output	O
labels	O
.	O

Datasets	O
The	O
proposed	O
model	O
is	O
evaluated	O
on	O
four	O
benchmark	O
datasets	O
:	O
SemEval	B-DatasetName
2010	I-DatasetName
Task	I-DatasetName
1	I-DatasetName
(	O
Recasens	O
et	O
al	O
.	O
,	O

2010	O
)	O
Catalan	B-DatasetName
and	O
Spanish	B-DatasetName
datasets	O
,	O
and	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
(	O
Weischedel	O
et	O
al	O
.	O
,	O

2013	O
)	O
English	B-DatasetName
and	O
Chinese	B-DatasetName
datasets	O
.	O

For	O
SemEval	B-DatasetName
2010	I-DatasetName
Task	I-DatasetName
1	I-DatasetName
datasets	O
,	O
there	O
are	O
4	O
entity	O
types	O
:	O
PER	B-DatasetName
,	O
LOC	B-DatasetName
and	O
ORG	B-DatasetName
and	O
MISC	B-DatasetName
.	O

For	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
datasets	O
,	O
there	O
are	O
18	O
entity	O
types	O
in	O
total	O
.	O

Following	O
the	O
work	O
by	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
,	O
we	O
transform	O
the	O
parse	O
trees	O
into	O
the	O
Stanford	O
dependency	O
trees	O
(	O
De	O
Marneffe	O
and	O
Manning	O
,	O
2008	O
)	O
by	O
using	O
Stanford	O
CoreNLP	O
.	O

Experimental	O
Setup	O
For	O
Catalan	B-DatasetName
,	O
Spanish	B-DatasetName
,	O
and	O
Chinese	B-DatasetName
,	O
we	O
use	O
the	O
FastText	O
(	O
Grave	O
et	O
al	O
.	O
,	O

For	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	I-DatasetName
,	O
we	O
adopt	O
the	O
publicly	O
available	O
GloVE	O
(	O
Pennington	O
et	O
al	O
.	O
,	O

For	O
experiments	O
with	O
the	O
contextualized	O
representation	O
,	O
we	O
adopt	O
the	O
pre	O
-	O
trained	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
for	O
the	O
four	O
datasets	O
.	O

Specifically	O
,	O
we	O
use	O
bert	B-MethodName
-	I-MethodName
as	I-MethodName
-	I-MethodName
service	I-MethodName
(	O
Xiao	O
,	O
2018	O
)	O
to	O
generate	O
the	O
contextualized	O
word	O
representation	O
without	O
fine	O
-	O
tuning	O
.	O

2020	O
)	O
,	O
we	O
use	O
the	O
cased	O
version	O
of	O
BERT	B-MethodName
large	O
model	O
for	O
the	O
experiments	O
on	O
the	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	I-DatasetName
data	O
.	O

We	O
use	O
the	O
cased	O
version	O
of	O
BERT	B-MethodName
base	I-MethodName
model	O
for	O
the	O
experiments	O
on	O
the	O
other	O
three	O
datasets	O
.	O

For	O
the	O
character	O
embedding	O
,	O
we	O
randomly	O
initialize	O
the	O
character	O
embeddings	O
and	O
set	O
the	O
dimension	B-HyperparameterName
as	O
30	B-HyperparameterValue
,	O
and	O
set	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	I-HyperparameterName
character	I-HyperparameterName
-	I-HyperparameterName
level	I-HyperparameterName
BiLSTM	I-HyperparameterName
as	O
50	B-HyperparameterValue
.	O

The	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	I-HyperparameterName
GCN	I-HyperparameterName
and	I-HyperparameterName
Syn	I-HyperparameterName
-	I-HyperparameterName
LSTM	I-HyperparameterName
is	O
set	O
as	O
200	B-HyperparameterValue
,	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
GCN	I-HyperparameterName
layer	I-HyperparameterName
is	O
2	B-HyperparameterValue
.	O

We	O
adopt	O
stochastic	B-HyperparameterValue
gradient	I-HyperparameterValue
descent	I-HyperparameterValue
(	O
SGD	B-HyperparameterValue
)	O
to	O
optimize	O
our	O
model	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
100	B-HyperparameterValue
,	B-HyperparameterValue
L2	I-HyperparameterValue
regularization	B-HyperparameterName
10	B-HyperparameterValue
−8	I-HyperparameterValue
,	O
initial	B-HyperparameterName
learning	I-HyperparameterName
rate	I-HyperparameterName
lr	B-HyperparameterName
0.2	B-HyperparameterValue
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
is	O
decayed	O
4	B-HyperparameterValue
with	O
respect	O
to	O
the	O
number	O
of	O
epoch	O
.	O

We	O
use	O
the	O
bootstrapping	B-HyperparameterValue
t	I-HyperparameterValue
-	I-HyperparameterValue
test	I-HyperparameterValue
to	O
compare	O
the	O
results	O
.	O

Baselines	O
We	O
compare	O
our	O
model	O
with	O
several	O
baselines	O
with	O
or	O
without	O
dependency	O
tree	O
information	O
.	O

The	O
first	O
one	O
is	O
BERT	B-MethodName
-	I-MethodName
CRF	I-MethodName
,	O
where	O
we	O
apply	O
a	O
CRF	B-MethodName
layer	O
on	O
top	O
of	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

Secondly	O
,	O
we	O
compare	O
with	O
the	O
BERT	B-MethodName
implementation	O
by	O
HuggingFace	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

For	O
models	O
with	O
dependency	O
trees	O
,	O
we	O
take	O
the	O
models	O
BiLSTM	B-MethodName
-	I-MethodName
GCN	I-MethodName
-	I-MethodName
CRF	I-MethodName
and	O
dependency-	O
4	O
We	O
set	O
the	O
decay	B-HyperparameterName
as	O
0.1	B-HyperparameterValue
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
each	O
epoch	O
equals	O
to	O
lr/(1	B-HyperparameterValue
+	I-HyperparameterValue
decay	I-HyperparameterValue
*	I-HyperparameterValue
(	I-HyperparameterValue
epoch	I-HyperparameterValue
−	I-HyperparameterValue
1	I-HyperparameterValue
)	I-HyperparameterValue
)	I-HyperparameterValue
.	O

5	O
The	O
experimental	O
results	O
on	O
the	O
dev	O
set	O
and	O
other	O
experimental	O
details	O
can	O
be	O
found	O
in	O
the	O
Appendix	O
.	O

2018	O
)	O
,	O
but	O
we	O
also	O
implement	O
it	O
with	O
BERT	B-MethodName
.	O

SemEval	B-DatasetName
2010	I-DatasetName
Task	I-DatasetName
1	I-DatasetName
Table	O
2	O
shows	O
comparisons	O
of	O
our	O
model	O
with	O
baseline	O
models	O
on	O
the	O
SemEval	B-DatasetName
2010	I-DatasetName
Task	I-DatasetName
1	I-DatasetName
Catalan	I-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
datasets	O
.	O

Our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
outperforms	O
all	O
existing	O
models	O
with	O
F	B-MetricName
1	I-MetricName
82.76	B-MetricValue
and	O
85.09	B-MetricValue
(	O
p	B-HyperparameterName
<	O
10	B-HyperparameterValue
−5	I-HyperparameterValue
)	O
compared	O
to	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
on	O
Catalan	B-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
datasets	O
when	O
FastText	O
word	O
embeddings	O
are	O
used	O
.	O

Our	O
model	O
outperforms	O
the	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
by	O
13.25	B-MetricValue
and	O
11.22	B-MetricValue
F	B-MetricName
1	I-MetricName
points	O
,	O
and	O
outperforms	O
BiLSTM	B-MethodName
-	I-MethodName
GCN	I-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
model	O
by	O
4.64	B-MetricValue
and	O
3.16	B-MetricValue
on	O
Catalan	B-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
.	O

The	O
large	O
performance	O
gap	O
between	O
BiLSTM	B-MethodName
-	I-MethodName
GCN	I-MethodName
-	I-MethodName
CRF	I-MethodName
and	O
our	O
model	O
indicates	O
that	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
shows	O
better	O
compatibility	O
with	O
GCN	B-MethodName
,	O
and	O
this	O
confirms	O
that	O
simply	O
stacking	O
GCN	B-MethodName
on	O
top	O
of	O
the	O
BiLSTM	B-MethodName
does	O
not	O
perform	O
well	O
.	O

Our	O
method	O
outperforms	O
GCN	B-MethodName
-	I-MethodName
BiLSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
by	O
5.33	B-MetricValue
and	O
3.24	B-MetricValue
F	B-MetricName
1	I-MetricName
points	O
on	O
Catalan	B-DatasetName
and	O
Spanish	B-DatasetName
.	O

Furthermore	O
,	O
our	O
proposed	O
method	O
brings	O
1.12	B-MetricValue
and	O
1.62	B-MetricValue
F	B-MetricName
1	I-MetricName
points	O
improvement	O
on	O
Catalan	B-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
datasets	O
compare	O
to	O
the	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
.	O

The	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
employs	O
2	O
-	O
layer	O
dependency	O
guided	O
BiLSTM	B-MethodName
to	O
capture	O
grandchild	O
dependencies	O
,	O
which	O
leads	O
to	O
longer	O
training	O
time	O
and	O
more	O
model	O
parameters	O
.	O

However	O
,	O
our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
is	O
able	O
to	O
get	O
better	O
performance	O
with	O
fewer	O
model	O
parameters	O
and	O
shorter	O
training	O
time	O
because	O
of	O
the	O
fewer	O
LSTM	O
layers	O
.	O

Such	O
results	O
demonstrate	O
that	O
our	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
manages	O
to	O
capture	O
structured	O
information	O
effectively	O
.	O

Furthermore	O
,	O
with	O
the	O
contextualized	O
word	O
representation	O
,	O
the	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
achieves	O
much	O
higher	O
performance	O
improvement	O
than	O
any	O
other	O
method	O
.	O

Our	O
model	O
outperforms	O
the	O
strong	O
baseline	O
model	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
by	O
4.83	B-MetricValue
and	O
2.54	B-MetricValue
in	O
terms	O
of	O
F	B-MetricName
1	I-MetricName
(	O
p	B-HyperparameterName
<	O
10	B-HyperparameterValue
−5	I-HyperparameterValue
)	O
on	O
Catalan	B-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
,	O
respectively	O
.	O

OntoNotes	B-DatasetName
5.0	I-DatasetName
English	I-DatasetName
To	O
understand	O
the	O
generalizability	O
of	O
our	O
model	O
,	O
we	O
evaluate	O
the	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
on	O
large	O
scale	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
datasets	O
.	O

Table	O
3	O
shows	O
comparisons	O
of	O
our	O
model	O
with	O
baseline	O
models	O
on	O
English	B-DatasetName
.	O

Our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
outperforms	O
all	O
existing	O
methods	O
with	O
89.04	B-MetricValue
in	O
terms	O
of	O
F	B-MetricName
1	I-MetricName
score	O
(	B-HyperparameterName
p	I-HyperparameterName
<	O
0.01	B-HyperparameterValue
)	O
compared	O
to	B-MethodName
DGLSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
,	O
when	O
GloVE	O
word	O
embeddings	O
are	O
used	O
.	O

Our	O
model	O
outperforms	O
the	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
by	O
1.97	B-MetricValue
in	O
F	B-MetricName
1	I-MetricName
,	O
BiLSTM	B-MethodName
-	I-MethodName
GCN	I-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
model	O
by	O
0.86	B-MetricValue
.	O

Note	O
that	O
our	O
implemented	O
GCN	B-MethodName
-	I-MethodName
BiLSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
outperforms	O
the	O
previous	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
by	O
0.14	B-MetricValue
in	O
F	B-MetricName
1	I-MetricName
.	O

Our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
further	O
brings	O
the	O
improvement	O
to	O
0.52	B-MetricValue
.	O

Moreover	O
,	O
with	O
the	O
contextualized	O
word	O
representation	O
BERT	B-MethodName
,	O
our	O
method	O
achieves	O
an	O
F	B-MetricName
1	I-MetricName
score	O
of	O
90.85	B-MetricValue
(	O
p	B-HyperparameterName
<	O
10	B-HyperparameterValue
−5	I-HyperparameterValue
)	O
compared	O
to	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
.	O

Our	O
method	O
outperforms	O
the	O
previous	O
model	O
(	O
Luo	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
which	O
relies	O
on	O
document	O
-	O
level	O
information	O
,	O
by	O
0.55	B-MetricValue
in	O
F	B-MetricName
1	I-MetricName
.	O

This	O
shows	O
that	O
the	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
is	O
able	O
to	O
extract	O
more	O
entities	O
.	O

There	O
are	O
also	O
other	O
methods	O
(	O
Li	O
et	O
al	O
.	O
,	O

2020	O
)	O
use	O
document	O
-	O
level	O
information	O
to	O
encode	O
the	O
sentence	O
,	O
which	O
are	O
not	O
direct	O
comparisons	O
to	O
ours.forms	O
the	O
baseline	O
models	O
,	O
specifically	O
by	O
2.04	B-MetricValue
in	O
F	B-MetricName
1	I-MetricName
compared	O
to	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
,	O
by	O
2.39	B-MetricValue
compared	O
to	O
BiLSTM	B-MethodName
-	I-MethodName
GCN	I-MethodName
-	I-MethodName
CRF	I-MethodName
,	O
by	O
1.86	B-MetricValue
compared	O
to	O
GCN	B-MethodName
-	I-MethodName
BILSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
and	O
by	O
1.11	B-MetricValue
(	O
p	B-HyperparameterName
<	B-HyperparameterValue
10	I-HyperparameterValue
−5	I-HyperparameterValue
)	O
compared	O
to	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
when	O
FastText	O
is	O
used	O
.	O

Note	O
that	O
the	O
baseline	O
BiLSTM	B-MethodName
-	I-MethodName
GCN	I-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
is	O
0.35	B-MetricValue
points	O
worse	O
than	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
.	O

Such	O
results	O
further	O
confirm	O
the	O
effectiveness	O
of	O
our	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
for	O
incorporating	O
structured	O
information	O
.	O

We	O
find	O
a	O
similar	O
behavior	O
when	O
the	O
contextualized	O
word	O
representation	O
BERT	B-MethodName
is	O
used	O
.	O

With	O
the	O
contextualized	O
word	O
representation	O
,	O
we	O
achieve	O
a	O
higher	O
F	B-MetricName
1	I-MetricName
score	O
of	O
80.20	B-MetricValue
.	O

Table	O
8	O
presents	O
the	O
comparisons	O
between	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
and	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
with	O
given	O
,	O
predicted	O
and	O
random	O
dependency	O
trees	O
.	O

Our	O
performance	O
differences	O
with	O
the	O
given	O
parse	O
trees	O
are	O
relatively	O
smaller	O
than	O
the	O
corresponding	O
differences	O
in	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
.	O

It	O
is	O
worthwhile	O
to	O
note	O
that	O
,	O
with	O
the	O
predicted	O
dependencies	O
,	O
our	O
proposed	O
Syn	O
-	O
LSTM	O
-	O
CRF	O
+	O
BERT	O
is	O
still	O
able	O
to	O
outperform	O
the	O
strong	O
baseline	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
even	O
with	O
the	O
given	O
parse	O
trees	O
on	O
Catalan	B-DatasetName
,	O
English	B-DatasetName
,	I-DatasetName
and	I-DatasetName
Chinese	I-DatasetName
datasets	O
.	O

We	O
statistically	O
plot	O
the	O
number	O
of	O
words	O
with	O
respect	O
to	O
different	O
gate	O
value	O
ranges	O
(	O
m	O
t	O
)	O
.	O

Figure	O
4	O
shows	O
the	O
comparison	O
between	O
the	O
models	O
of	O
using	O
random	O
trees	O
and	O
given	O
trees	O
on	O
Catalan	B-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
7	I-DatasetName
.	O

Such	O
behavior	O
demonstrates	O
that	O
our	O
proposed	O
model	O
can	O
selectively	O
aggregate	O
the	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
.	O

We	O
compare	O
the	O
performance	O
of	O
our	O
Syn	O
-	O
LSTM	O
-	O
CRF	O
+	O
BERT	O
with	O
BiLSTM	O
-	O
CRF	O
+	O
BERT	O
and	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
models	O
with	O
respect	O
to	O
sentence	O
length	O
,	O
and	O
the	O
results	O
are	O
shown	O
in	O
Figure	O
5	O
.	O

We	O
observe	O
that	O
the	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
model	O
consistently	O
outperforms	O
the	O
two	O
baseline	O
models	O
on	O
the	O
four	O
languages	O
8	O
.	O

In	O
particular	O
,	O
although	O
the	O
performance	O
tends	O
to	O
drop	O
as	O
the	O
sentence	O
length	O
increases	O
,	O
our	O
proposed	O
model	O
shows	O
relatively	O
better	O
performance	O
when	O
the	O
sentence	B-HyperparameterName
length	I-HyperparameterName
is	O
≥	B-HyperparameterValue
60	I-HyperparameterValue
.	O

This	O
confirms	O
that	O
the	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
is	O
able	O
to	O
effectively	O
incorporate	O
structured	O
information	O
.	O

Note	O
that	O
our	O
2	O
-	O
layer	O
GCN	B-MethodName
is	O
computed	O
based	O
on	O
the	O
dependency	O
trees	O
,	O
which	O
include	O
both	O
short	O
-	O
range	O
dependencies	O
and	O
long	O
-	O
range	O
dependencies	O
.	O

With	O
the	O
graph	O
-	O
encoded	O
representation	O
and	O
the	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
,	O
the	O
individual	O
word	O
representation	O
is	O
enhanced	O
by	O
both	O
contextual	O
and	O
structured	O
information	O
.	O

The	O
significant	O
performance	O
improvements	O
on	O
the	O
four	O
datasets	O
show	O
the	O
capability	O
of	O
our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
to	O
capture	O
the	O
structured	O
information	O
despite	O
the	O
sentence	B-HyperparameterName
length	I-HyperparameterName
.	O

We	O
conduct	O
another	O
evaluation	O
on	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
,	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
,	O
and	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
models	O
with	O
respect	O
to	O
entity	B-HyperparameterName
length	I-HyperparameterName
∈	O
{	O
1	B-HyperparameterValue
,	I-HyperparameterValue
2	I-HyperparameterValue
,	I-HyperparameterValue
3	I-HyperparameterValue
,	I-HyperparameterValue
4	I-HyperparameterValue
,	I-HyperparameterValue
5	I-HyperparameterValue
,	I-HyperparameterValue
≥	I-HyperparameterValue
6	I-HyperparameterValue
}	O
on	O
the	O
four	O
languages	O
.	O

Table	O
6	O
shows	O
the	O
performance	O
comparison	O
of	O
two	O
models	O
with	O
respect	O
to	O
entity	O
length	O
.	O

With	O
the	O
structured	O
information	O
,	O
both	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
and	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
achieve	O
better	O
performance	O
compared	O
to	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
.	O

When	O
the	O
length	B-HyperparameterName
of	I-HyperparameterName
entity	I-HyperparameterName
is	O
≤	B-HyperparameterValue
3	I-HyperparameterValue
,	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
achieves	O
better	O
results	O
compared	O
to	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
.	O

Our	O
model	O
consistently	O
outperforms	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
,	O
and	O
the	O
performance	O
tends	O
to	O
have	O
more	O
improvements	O
when	O
entities	O
are	O
getting	O
longer	O
except	O
on	O
the	O
Chinese	B-DatasetName
dataset	O
.	O

As	O
mentioned	O
by	O
Jie	O
and	O
Lu	O
(	O
2019	O
)	O
,	O
the	O
percentage	O
of	O
entities	O
that	O
are	O
able	O
to	O
perfectly	O
form	O
a	O
sub	O
-	O
tree	O
is	O
only	O
92.9	B-MetricValue
%	I-MetricValue
for	O
OntoNotes	B-DatasetName
Chinese	I-DatasetName
,	O
as	O
compared	O
to	O
98.5	B-MetricValue
%	I-MetricValue
,	O
100	B-MetricValue
%	I-MetricValue
,	O
100	B-MetricValue
%	I-MetricValue
for	O
OntoNotes	B-DatasetName
English	I-DatasetName
,	O
SemEval	B-DatasetName
Catalan	B-DatasetName
and	I-DatasetName
Spanish	I-DatasetName
.	O

Furthermore	O
,	O
the	O
ratio	O
of	O
long	O
entities	O
is	O
much	O
higher	O
for	O
Catalan	B-DatasetName
and	O
Spanish	B-DatasetName
compared	O
to	O
English	B-DatasetName
and	O
Chinese	B-DatasetName
.	O

The	O
experimental	O
results	O
on	O
Catalan	B-DatasetName
and	O
Spanish	B-DatasetName
datasets	O
show	O
significant	O
improvements	O
for	O
long	O
entities	O
.	O

To	O
fully	O
explore	O
the	O
impact	O
of	O
the	O
number	O
of	O
GCN	O
layers	O
,	O
we	O
conduct	O
another	O
experiment	O
on	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
model	O
with	O
the	O
number	B-HyperparameterName
of	I-HyperparameterName
GCN	I-HyperparameterName
layers	I-HyperparameterName
∈	O
{	O
1	B-HyperparameterValue
,	I-HyperparameterValue
2	I-HyperparameterValue
,	I-HyperparameterValue
3	I-HyperparameterValue
}	O
,	O
and	O
Figure	O
6	O
shows	O
the	O
performance	O
on	O
the	O
dev	O
set	O
of	O
the	O
four	O
languages	O
.	O

Therefore	O
,	O
we	O
evaluate	O
our	O
proposed	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
model	O
with	O
2	B-HyperparameterValue
-	I-HyperparameterValue
layer	I-HyperparameterValue
GCN	B-HyperparameterName
.	O

Ablation	O
Study	O
To	O
understand	O
the	O
contribution	O
of	O
each	O
component	O
,	O
we	O
conduct	O
an	O
ablation	O
study	O
on	O
the	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	I-DatasetName
dataset	O
,	O
and	O
Table	O
7	O
presents	O
the	O
detailed	O
results	O
of	O
our	O
model	O
with	O
contextualized	O
representation	O
.	O

We	O
find	O
that	O
the	O
performance	O
drops	O
by	O
0.24	B-MetricValue
F	B-MetricName
1	I-MetricName
score	O
when	O
we	O
only	O
use	O
1	B-HyperparameterValue
-	O
layer	B-HyperparameterName
GCN	I-HyperparameterName
.	O

Without	O
GCN	O
at	O
all	O
,	O
the	O
score	O
drops	O
by	O
1.13	B-MetricValue
F	B-MetricName
1	I-MetricName
.	O

The	O
original	O
dependency	O
contributes	O
0.27	B-MetricValue
F	B-MetricName
1	I-MetricName
score	O
.	O

Removing	O
the	O
dependency	O
relation	O
embedding	O
also	O
decreases	O
the	O
performance	O
by	O
0.27	B-MetricValue
F	B-MetricName
1	I-MetricName
.	O

When	O
we	O
remove	O
the	O
POS	O
tags	O
embedding	O
,	O
the	O
result	O
drops	O
by	O
0.39	B-MetricValue
F	B-MetricName
1	O
.	O

LSTM	O
LSTM	O
has	O
demonstrated	O
its	O
great	O
effectiveness	O
in	O
many	O
NLP	O
tasks	O
and	O
becomes	O
a	O
standard	O
module	O
for	O
many	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
(	O
Wen	O
et	O
al	O
.	O
,	O

However	O
,	O
the	O
sequential	O
nature	O
of	O
the	O
LSTM	O
makes	O
it	O
challenging	O
to	O
capture	O
long	O
-	O
range	O
dependencies	O
.	O

2020	O
)	O
mutually	O
gates	O
the	O
current	O
input	O
and	O
the	O
previous	O
output	O
to	O
enhance	O
the	O
interaction	O
between	O
the	O
input	O
and	O
the	O
context	O
.	O

2018	O
)	O
propose	O
ON	B-MethodName
-	I-MethodName
LSTM	I-MethodName
to	O
add	O
a	O
hierarchical	O
bias	O
to	O
allow	O
the	O
neurons	O
to	O
be	O
updated	O
by	O
following	O
certain	O
order	O
.	O

While	O
the	O
ON	B-MethodName
-	I-MethodName
LSTM	I-MethodName
is	O
learning	O
the	O
latent	O
constituency	O
parse	O
trees	O
,	O
we	O
focus	O
on	O
incorporating	O
the	O
explicit	O
structured	O
information	O
conveyed	O
by	O
the	O
dependency	O
parse	O
trees	O
.	O

NER	B-TaskName
Early	O
work	O
(	O
Sasano	O
and	O
Kurohashi	O
,	O
2008	O
)	O
uses	O
syntactic	O
dependency	O
features	O
to	O
improve	O
the	O
SVM	B-MethodName
performance	O
on	O
Japanese	O
NER	B-TaskName
task	O
.	O

2016;Chiu	O
and	O
Nichols	O
,	O
2016b	O
)	O
focus	O
on	O
using	O
neural	O
networks	O
to	O
extract	O
features	O
and	O
achieved	O
the	O
stateof	O
-	O
the	O
-	O
art	O
performance	O
.	O

The	O
pre	O
-	O
trained	O
language	O
models	O
(	O
e.g.	O
,	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

In	O
language	O
model	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
we	O
select	O
the	O
18	O
th	O
layer	O
of	O
the	O
cased	O
version	O
of	O
BERT	B-MethodName
large	O
model	O
for	O
the	O
experiments	O
on	O
the	O
OntoNotes	O
5.0	O
English	O
data	O
.	O

We	O
use	O
the	O
the	O
9	O
th	O
layer	O
of	O
cased	O
version	O
of	O
BERT	B-MethodName
base	O
model	O
for	O
the	O
experiments	O
on	O
the	O
rest	O
three	O
datasets	O
.	O

For	O
the	O
character	O
embedding	O
,	O
we	O
randomly	O
initialize	O
the	O
character	O
embeddings	O
and	O
set	O
the	O
dimension	B-HyperparameterName
as	O
30	B-HyperparameterValue
,	O
and	O
set	O
the	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	I-HyperparameterName
character	I-HyperparameterName
-	I-HyperparameterName
level	I-HyperparameterName
BiLSTM	I-HyperparameterName
as	O
50	B-HyperparameterValue
.	O

The	O
hidden	B-HyperparameterName
size	I-HyperparameterName
of	I-HyperparameterName
GCN	I-HyperparameterName
and	I-HyperparameterName
Syn	I-HyperparameterName
-	I-HyperparameterName
LSTM	I-HyperparameterName
is	O
set	O
as	O
200	B-HyperparameterValue
.	O

Note	O
that	O
we	O
only	O
use	O
one	O
layer	O
of	O
bi	O
-	O
directional	O
Syn	O
-	O
LSTM	O
for	O
our	O
experiments	O
.	O

Dropout	B-HyperparameterName
is	O
set	O
to	O
0.5	B-HyperparameterValue
for	O
input	O
embeddings	O
and	O
hidden	O
states	O
.	O

We	O
adopt	O
stochastic	B-HyperparameterValue
gradient	I-HyperparameterValue
descent	I-HyperparameterValue
(	O
SGD	B-HyperparameterValue
)	O
to	O
optimize	O
our	O
model	O
with	O
batch	B-HyperparameterName
size	I-HyperparameterName
100	B-HyperparameterValue
,	O
L2	B-HyperparameterName
regularization	I-HyperparameterName
10	B-HyperparameterValue
−8	I-HyperparameterValue
,	O
learning	B-HyperparameterName
rate	I-HyperparameterName
0.2	B-HyperparameterValue
and	O
the	O
learning	O
rate	O
is	O
decayed	O
with	O
respect	O
to	O
the	O
number	O
of	O
epoch	O
9	O
.	O

Table	O
8	O
presents	O
the	O
performance	O
of	O
dependency	O
parser	O
.	O

9	O
We	O
set	O
the	O
decay	B-HyperparameterName
as	O
0.1	B-HyperparameterValue
and	O
the	O
learning	B-HyperparameterName
rate	I-HyperparameterName
for	O
each	O
epoch	O
equals	O
to	O
learning_rate/(1	B-HyperparameterValue
+	I-HyperparameterValue
decay	I-HyperparameterValue
*	I-HyperparameterValue
(	I-HyperparameterValue
epoch	I-HyperparameterValue
−	I-HyperparameterValue
1	I-HyperparameterValue
)	I-HyperparameterValue
)	I-HyperparameterValue
.	O

Table	O
9	O
shows	O
the	O
statistics	O
of	O
the	O
number	O
of	O
entities	O
with	O
respect	O
to	O
entity	O
length	O
for	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	I-DatasetName
and	O
Chinese	B-DatasetName
,	O
SemEval	B-DatasetName
2010	I-DatasetName
Task	I-DatasetName
1	I-DatasetName
Catalan	B-DatasetName
and	O
Spanish	B-DatasetName
datasets	O
.	O

Figure	O
7	O
shows	O
the	O
comparisons	O
of	O
the	O
models	O
of	O
using	O
random	O
trees	O
and	O
given	O
trees	O
on	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	B-DatasetName
and	O
Chinese	B-DatasetName
datasets	O
.	O

We	O
compare	O
the	O
performance	O
of	O
our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
with	O
BiLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
BERT	I-MethodName
and	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
+	I-MethodName
ELMO	I-MethodName
models	O
with	O
respect	O
to	O
sentence	O
length	O
,	O
and	O
the	O
results	O
are	O
shown	O
in	O
Figure	O
8	O
.	O

The	O
example	O
is	O
selected	O
from	O
OntoNotes	B-DatasetName
5.0	I-DatasetName
English	I-DatasetName
dataset	O
.	O

Even	O
though	O
the	O
DGLSTM	B-MethodName
-	I-MethodName
CRF	I-MethodName
(	O
Jie	O
and	O
Lu	O
,	O
2019	O
)	O
model	O
is	O
able	O
to	O
recognize	O
"	O
Tianshui	O
"	O
as	O
a	O
named	O
entity	O
,	O
it	O
predicts	O
a	O
wrong	O
entity	O
type	O
as	O
PERSON	O
while	O
the	O
true	O
type	O
is	O
GPE	O
.	O

If	O
only	O
looking	O
at	O
the	O
first	O
half	O
of	O
the	O
sentence	O
,	O
it	O
is	O
possible	O
to	O
predict	O
"	O
Tianshui	O
"	O
as	O
PERSON	O
because	O
of	O
the	O
local	O
information	O
"	O
age	O
"	O
.	O

However	O
,	O
the	O
second	O
half	O
of	O
the	O
sentence	O
confirms	O
that	O
the	O
entity	O
type	O
of	O
During	O
Tanshui	O
's	O
golden	O
age	O
,	O
large	O
and	O
small	O
boats	O
were	O
constantly	O
coming	O
and	O
going	O
in	O
the	O
harbor	O
,	O
and	O
it	O
was	O
not	O
usual	O
to	O
see	O
enormous	O
steamships	O
.ROOT	O
Figure	O
9	O
:	O
An	O
example	O
of	O
dependency	O
tree	O
.	O

With	O
the	O
non	O
-	O
local	O
information	O
from	O
the	O
graph	O
-	O
encoded	O
representation	O
,	O
our	O
Syn	B-MethodName
-	I-MethodName
LSTM	I-MethodName
-	I-MethodName
CRF	I-MethodName
successfully	O
predicts	O
the	O
right	O
entity	O
type	O
.	O

This	O
research	O
is	O
partially	O
supported	O
by	O
Ministry	O
of	O
Education	O
,	O
Singapore	O
,	O
under	O
its	O
Academic	O
Research	O
Fund	O
(	O
AcRF	O
)	O
Tier	O
2	O
Programme	O
(	O
MOE	O
AcRF	O
Tier	O
2	O
Award	O
No	O
:	O
MOE2017	O
-	O
T2	O
-	O
1	O
-	O
156	O
)	O
.	O

Any	O
opinions	O
,	O
findings	O
and	O
conclusions	O
or	O
recommendations	O
expressed	O
in	O
this	O
material	O
are	O
those	O
of	O
the	O
authors	O
and	O
do	O
not	O
reflect	O
the	O
views	O
of	O
the	O
Ministry	O
of	O
Education	O
,	O
Singapore	O
.	O

The	O
total	O
number	O
of	O
parameters	O
is	O
11M.	O
Table	O
10	O
shows	O
the	O
performance	O
of	O
our	O
model	O
on	O
the	O
dev	O
sets	O
of	O
OntoNotes	O
5.0	O
English	O
and	O
Chinese	O
,	O
SemEval	O
2010	O
Task	O
1	O
Catalan	O
and	O
Spanish	O
.	O

2018	O
)	O
300	O
dimensional	O
embeddings	O
to	O
initialize	O
the	O
word	O
embeddings	O
for	O
Catalan	O
,	O
Spanish	O
,	O
and	O
Chinese	O
.	O

In	O
this	O
work	O
,	O
we	O
aim	O
to	O
build	O
a	O
many	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
many	I-TaskName
translation	I-TaskName
system	I-TaskName
with	I-TaskName
an	I-TaskName
emphasis	I-TaskName
on	I-TaskName
the	I-TaskName
quality	I-TaskName
of	I-TaskName
non	I-TaskName
-	I-TaskName
English	I-TaskName
language	I-TaskName
directions	I-TaskName
.	O

To	O
this	O
end	O
,	O
we	O
propose	O
mRASP2	B-MethodName
,	O
a	O
training	O
method	O
to	O
obtain	O
a	O
single	O
unified	O
multilingual	O
translation	O
model	O
.	O

mRASP2	B-MethodName
is	O
empowered	O
by	O
two	O
techniques	O
:	O
a	O
)	O
a	O
contrastive	O
learning	O
scheme	O
to	O
close	O
the	O
gap	O
among	O
representations	O
of	O
different	O
languages	O
,	O
and	O
b	O
)	O
data	O
augmentation	O
on	O
both	O
multiple	O
parallel	O
and	O
monolingual	O
data	O
to	O
further	O
align	O
token	O
representations	O
.	O

For	O
English	O
-	O
centric	O
directions	O
,	O
mRASP2	B-MethodName
outperforms	O
existing	O
best	O
unified	O
model	O
and	O
achieves	O
competitive	O
or	O
even	O
better	O
performance	O
than	O
the	O
pre	O
-	O
trained	O
and	O
fine	O
-	O
tuned	O
model	O
mBART	B-MethodName
on	O
tens	O
of	O
WMT	B-DatasetName
's	O
translation	O
directions	O
.	O

For	O
non	O
-	O
English	O
directions	O
,	O
mRASP2	B-MethodName
achieves	O
an	O
improvement	O
of	O
average	O
10	B-MetricValue
+	I-MetricValue
BLEU	B-MetricName
compared	O
with	O
the	O
multilingual	O
Transformer	O
baseline	O
.	O

com	O
/	O
PANXiao1994	O
/	O
mRASP2	B-MethodName
.	O

Transformer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2019;Siddhant	O
et	O
al	O
.	O
,	O

Multilingual	B-TaskName
translation	I-TaskName
models	O
are	O
appealing	O
for	O
two	O
reasons	O
.	O

2019;Ji	O
et	O
al	O
.	O
,	O

Despite	O
these	O
benefits	O
,	O
challenges	O
still	O
remain	O
in	O
multilingual	B-TaskName
NMT	I-TaskName
.	O

First	O
,	O
previous	O
work	O
on	O
multilingual	B-TaskName
NMT	I-TaskName
does	O
not	O
always	O
perform	O
well	O
as	O
their	O
corresponding	O
bilingual	O
baseline	O
especially	O
on	O
rich	O
resource	O
language	O
pairs	O
.	O

Such	O
performance	O
gap	O
becomes	O
larger	O
with	O
the	O
increasing	O
number	O
of	O
accommodated	O
languages	O
for	B-TaskName
multilingual	I-TaskName
NMT	I-TaskName
,	O
as	O
model	O
capacity	O
necessarily	O
must	O
be	O
split	O
between	O
many	O
languages	O
(	O
Arivazhagan	O
et	O
al	O
.	O
,	O

In	O
addition	O
,	O
an	O
optimal	O
setting	O
for	O
multilingual	B-TaskName
NMT	I-TaskName
should	O
be	O
effective	O
for	O
any	O
language	O
pairs	O
,	O
while	O
most	O
previous	O
work	O
focus	O
on	O
improv	O
-	O
ing	O
English	O
-	O
centric	O
1	O
directions	O
(	O
Johnson	O
et	O
al	O
.	O
,	O

2017;Aharoni	O
et	O
al	O
.	O
,	O

A	O
few	O
recent	O
exceptions	O
are	O
and	O
,	O
who	O
trained	O
many	O
-	O
to	O
-	O
many	O
systems	O
with	O
introducing	O
more	O
non	O
-	O
English	O
corpora	O
,	O
through	O
data	O
mining	O
or	O
back	O
translation	O
.	O

In	O
this	O
work	O
,	O
we	O
take	O
a	O
step	O
towards	O
a	O
unified	O
many	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
many	I-TaskName
multilingual	I-TaskName
NMT	I-TaskName
with	O
only	O
English	O
-	O
centric	O
parallel	O
corpora	O
and	O
additional	O
monolingual	O
corpora	O
.	O

As	O
such	O
,	O
many	B-TaskName
-	I-TaskName
to	I-TaskName
-	I-TaskName
many	I-TaskName
translations	I-TaskName
can	O
make	O
the	O
most	O
of	O
the	O
knowledge	O
from	O
all	O
supervised	O
directions	O
and	O
the	O
model	O
can	O
perform	O
well	O
for	O
both	O
English	O
-	O
centric	O
and	O
non	O
-	O
English	O
settings	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
multilingual	B-MethodName
COntrastive	I-MethodName
Learning	I-MethodName
framework	I-MethodName
for	I-MethodName
Translation	I-MethodName
(	O
mCOLT	B-MethodName
or	O
mRASP2	B-MethodName
)	O
to	O
reduce	O
the	O
representation	O
gap	O
of	O
different	O
languages	O
,	O
as	O
shown	O
in	O
Figure	O
1.The	O
objective	O
of	O
mRASP2	B-MethodName
ensures	O
the	O
model	O
to	O
represent	O
similar	O
sentences	O
across	O
languages	O
in	O
a	O
shared	O
space	O
by	O
training	O
the	O
encoder	O
to	O
minimize	O
the	O
representation	O
distance	O
of	O
similar	O
sentences	O
.	O

In	O
addition	O
,	O
we	O
also	O
boost	O
mRASP2	B-MethodName
by	O
leveraging	O
monolingual	O
data	O
to	O
further	O
improve	O
multilingual	O
translation	O
quality	O
.	O

We	O
introduce	O
an	O
effective	O
aligned	O
augmentation	O
technique	O
by	O
extending	O
RAS	B-MethodName
(	O
Lin	O
et	O
al	O
.	O
,	O

Simple	O
yet	O
effective	O
,	O
mRASP2	B-MethodName
achieves	O
consistent	O
translation	O
performance	O
improvements	O
for	O
both	O
English	O
-	O
centric	O
and	O
non	O
-	O
English	O
directions	O
on	O
a	O
wide	O
range	O
of	O
benchmarks	O
.	O

For	O
Englishcentric	O
directions	O
,	O
mRASP2	B-MethodName
outperforms	O
a	O
strong	O
multilingual	O
baseline	O
in	O
20	O
translation	O
directions	O
on	O
WMT	B-DatasetName
testsets	O
.	O

On	O
10	O
WMT	B-DatasetName
translation	O
benchmarks	O
,	O
mRASP2	B-MethodName
even	O
obtains	O
better	O
results	O
than	O
the	O
strong	O
bilingual	B-MethodName
mBART	I-MethodName
model	O
.	O

For	O
zeroshot	O
and	O
unsupervised	O
directions	O
,	O
mRASP2	B-MethodName
obtains	O
surprisingly	O
strong	O
results	O
on	O
36	O
translation	O
directions	O
2	O
,	O
with	O
10	B-MetricValue
+	I-MetricValue
BLEU	B-MetricName
improvements	O
on	O
average.1	O
"	O
English	O
-	O
centric	O
"	O
means	O
that	O
having	O
English	O
as	O
the	O
source	O
or	O
target	O
language	O
2	O
6	O
unsupervised	O
directions	O
+	O
30	O
zero	O
-	O
shot	O
directions	O
2	O
Methodology	O
mRASP2	B-MethodName
unifies	O
both	O
parallel	O
corpora	O
and	O
monolingual	O
corpora	O
with	O
contrastive	B-MethodName
learning	I-MethodName
.	O

This	O
section	O
will	O
explain	O
our	O
proposed	O
mRASP2	B-MethodName
.	O

The	O
overall	O
framework	O
is	O
illustrated	O
in	O
Figure	O
1	O
2	O
A	O
multilingual	B-MethodName
neural	I-MethodName
machine	I-MethodName
translation	I-MethodName
model	O
learns	O
a	O
many	O
-	O
to	O
-	O
many	O
mapping	O
function	O
f	O
to	O
translate	O
from	O
one	O
language	O
to	O
another	O
.	O

The	O
base	O
architecture	O
of	O
mRASP2	B-MethodName
is	O
the	O
state	O
-	O
of	O
-	O
theart	O
Transformer	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

The	O
model	B-HyperparameterName
dimension	I-HyperparameterName
is	O
1024	B-HyperparameterValue
on	O
16	B-HyperparameterValue
heads	B-HyperparameterName
.	O

To	O
ease	O
the	O
training	O
of	O
the	O
deep	O
model	O
,	O
we	O
apply	O
Layer	B-HyperparameterValue
Normalization	I-HyperparameterValue
for	O
word	O
embedding	O
and	O
pre	O
-	O
norm	O
residual	O
connection	O
following	O
Wang	O
et	O
al	O
.	O
(	O

Therefore	O
,	O
our	O
multilingual	B-TaskName
NMT	I-TaskName
baseline	O
is	O
much	O
stronger	O
than	O
that	O
of	O
Transformer	O
big	O
model	O
.	O

The	O
training	O
loss	O
is	O
cross	O
entropy	O
defined	O
as	O
:	O
L	O
ce	O
=	O
x	O
i	O
,	O
x	O
j	O
∈D	O
−	O
log	O
P	O
θ	O
(	O
x	O
i	O
|x	O
j	O
)	O
(	O
1)where	O
x	O
i	O
represents	O
a	O
sentence	O
in	O
language	O
L	O
i	O
,	O
and	O
θ	O
is	O
the	O
parameter	O
of	O
multilingual	O
Transformer	O
model	O
.	O

mRASP2	B-MethodName
introduces	O
contrastive	O
loss	O
to	O
explicitly	O
bring	O
different	O
languages	O
to	O
map	O
a	O
shared	O
semantic	O
space	O
.	O

The	O
key	O
idea	O
of	O
contrastive	B-MethodName
learning	I-MethodName
is	O
to	O
minimize	O
the	O
representation	O
gap	O
of	O
similar	O
sentences	O
and	O
maximize	O
that	O
of	O
irrelevant	O
sentences	O
.	O

In	O
our	O
experiments	O
,	O
it	O
is	O
set	O
to	O
0.1	O
.	O

The	O
similarity	O
of	O
two	O
sentences	O
is	O
calculated	O
with	O
the	O
cosine	B-HyperparameterValue
similarity	I-HyperparameterValue
of	O
the	O
averagepooled	O
encoded	O
output	O
.	O

Since	O
L	O
ctr	O
is	O
calculated	O
on	O
the	O
sentencelevel	O
and	O
L	O
ce	O
is	O
calculated	O
on	O
the	O
token	O
-	O
level	O
,	O
therefore	O
L	O
ctr	O
should	O
be	O
multiplied	O
by	O
the	O
averaged	O
sequence	O
length	O
|s|	O
.	O

We	O
then	O
will	O
introduce	O
how	O
to	O
improve	O
mRASP2	B-MethodName
with	O
data	O
augmentation	O
methods	O
,	O
including	O
the	O
introduction	O
of	O
noised	O
bilingual	O
and	O
noised	O
monolingual	O
data	O
for	O
multilingual	B-TaskName
NMT	I-TaskName
.	O

2020	O
)	O
propose	O
Random	B-MethodName
Aligned	I-MethodName
Substitution	I-MethodName
technique	I-MethodName
(	O
or	O
RAS	B-MethodName
5	I-MethodName
)	O
that	O
builds	O
codeswitched	O
sentence	O
pairs	O
(	O
C(x	O
i	O
)	O
,	O
x	O
j	O
)	O
for	O
multilingual	O
pre	O
-	O
training	O
.	O

In	O
this	O
paper	O
,	O
we	O
extend	O
it	O
to	O
Aligned	B-MethodName
Augmentation	I-MethodName
(	O
AA	B-MethodName
)	O
,	O
which	O
can	O
also	O
be	O
applied	O
to	O
monolingual	O
data	O
.	O

C(x	O
i	O
)	O
,	O
x	O
j	O
)	O
and	O
(	O
C(x	O
i	O
)	O
,	O
x	O
i	O
)	O
is	O
then	O
used	O
in	O
the	O
training	O
by	O
calculating	O
both	O
the	O
translation	O
loss	O
and	O
contrastive	O
loss	O
.	O

2020	O
)	O
tains	O
a	O
large	O
public	O
parallel	O
corpora	O
of	O
32	O
Englishcentric	O
language	O
pairs	O
.	O

The	O
total	O
number	B-HyperparameterName
of	I-HyperparameterName
sentence	I-HyperparameterName
pairs	I-HyperparameterName
is	O
97.6	B-HyperparameterValue
million	I-HyperparameterValue
.	O

We	O
apply	O
AA	O
on	O
PC32	B-DatasetName
by	O
randomly	O
replacing	O
words	O
in	O
the	O
source	O
side	O
sentences	O
with	O
synonyms	O
from	O
an	O
arbitrary	O
bilingual	O
dictionary	O
provided	O
by	O
(	O
Lample	O
et	O
al	O
.	O
,	O

For	O
words	O
in	O
the	O
dictionaries	O
,	O
we	O
replace	O
them	O
into	O
one	O
of	O
the	O
synonyms	O
with	O
a	O
probability	O
of	O
90	B-HyperparameterValue
%	I-HyperparameterValue
and	O
keep	O
them	O
unchanged	O
otherwise	O
.	O

We	O
apply	O
this	O
augmentation	O
in	O
the	O
pre	O
-	O
processing	O
step	O
before	O
training	O
.	O

We	O
create	O
a	O
dataset	O
MC24	B-DatasetName
with	O
monolingual	O
text	O
in	O
24	O
languages	O
9	O
.	O

It	O
is	O
a	O
subset	O
of	O
the	O
Newscrawl	O
10	O
dataset	O
by	O
retaining	O
only	O
those	O
languages	O
in	O
PC32	B-DatasetName
,	O
plus	O
three	O
additional	O
languages	O
that	O
are	O
not	O
in	O
PC32	B-DatasetName
(	O
Nl	O
,	O
Pl	O
,	O
Pt	O
)	O
.	O

In	O
order	O
to	O
balance	O
the	O
volume	O
across	O
different	O
languages	O
,	O
we	O
apply	O
temperature	O
sam	O
-	O
plingñ	O
i	O
=	O
n	O
i	O
/	O
j	O
n	O
j	O
1	O
/	O
Twith	O
T	O
=	O
5	O
over	O
the	O
dataset	O
,	O
where	O
n	O
i	O
is	O
the	O
number	O
of	O
sentences	O
in	O
ith	O
language	O
.	O

Then	O
we	O
apply	O
AA	B-MethodName
on	O
monolingual	O
data	O
.	O

The	O
total	O
number	O
of	O
sentences	O
in	O
MC24	B-DatasetName
is	O
1.01	O
billion	O
.	O

We	O
apply	O
AA	B-MethodName
on	O
MC24	B-DatasetName
by	O
randomly	O
replacing	O
words	O
in	O
the	O
source	O
side	O
sentences	O
with	O
synonyms	O
from	O
a	O
multilingual	O
dictionary	O
.	O

The	O
replace	B-HyperparameterName
probability	I-HyperparameterName
is	O
also	O
set	O
to	O
90	B-HyperparameterValue
%	I-HyperparameterValue
.	O

Evaluation	O
Datasets	O
For	O
supervised	O
directions	O
,	O
most	O
of	O
our	O
evaluation	O
datasets	O
are	O
from	O
WMT	B-DatasetName
and	O
IWSLT	B-DatasetName
benchmarks	O
,	O
for	O
pairs	O
that	O
are	O
not	O
available	O
in	O
WMT	B-DatasetName
or	O
IWSLT	B-DatasetName
,	O
we	O
use	O
OPUS-100	B-DatasetName
instead	O
.	O

For	O
zero	O
-	O
shot	O
directions	O
,	O
we	O
follow	O
and	O
use	O
their	O
proposed	O
OPUS-100	B-DatasetName
zero	I-DatasetName
-	I-DatasetName
shot	I-DatasetName
testset	O
.	O

We	O
report	O
de	B-MetricName
-	I-MetricName
tokenized	I-MetricName
BLEU	I-MetricName
with	O
Sacre-	B-MetricName
BLEU	I-MetricName
(	O
Post	O
,	O
2018	O
)	O
.	O

For	O
tokenized	B-MetricName
BLEU	I-MetricName
,	O
we	O
tokenize	O
both	O
reference	O
and	O
hypothesis	O
using	O
Sacremoses	O
11	O
toolkit	O
then	O
report	O
BLEU	B-MetricName
using	O
the	O
multi-bleu.pl	O
script	O
12	O
.	O

For	O
Chinese	O
(	O
Zh	O
)	O
,	O
BLEU	B-MetricName
score	O
is	O
calculated	O
on	O
character	O
-	O
level	O
.	O

We	O
use	O
the	O
Transformer	O
model	O
in	O
our	O
experiments	O
,	O
with	O
12	B-HyperparameterValue
encoder	B-HyperparameterName
layers	I-HyperparameterName
and	O
12	B-HyperparameterValue
decoder	B-HyperparameterName
layers	I-HyperparameterName
.	O

The	O
embedding	B-HyperparameterName
size	I-HyperparameterName
and	O
FFN	B-HyperparameterName
dimension	I-HyperparameterName
are	O
set	O
to	O
1024	B-HyperparameterValue
.	O

We	O
use	O
dropout	B-HyperparameterName
=	O
0.1	B-HyperparameterValue
,	O
as	O
well	O
as	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
3e-4	B-HyperparameterValue
with	O
polynomial	B-HyperparameterValue
decay	I-HyperparameterValue
scheduling	B-HyperparameterName
and	O
a	O
warm	B-HyperparameterName
-	I-HyperparameterName
up	I-HyperparameterName
step	I-HyperparameterName
of	O
10000	B-HyperparameterValue
.	O

For	O
optimization	O
,	O
we	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
with	O
=	O
1e-6	B-HyperparameterValue
and	O
β	B-HyperparameterName
2	I-HyperparameterName
=	O
0.98	B-HyperparameterValue
.	O

To	O
stabilize	O
training	O
,	O
we	O
set	O
the	O
threshold	B-HyperparameterName
of	I-HyperparameterName
gradient	I-HyperparameterName
norm	I-HyperparameterName
to	O
be	O
5.0	B-HyperparameterValue
and	O
clip	O
all	O
gradients	O
with	O
a	O
larger	O
norm	O
.	O

We	O
set	O
the	O
hyper	O
-	O
parameter	O
λ	B-HyperparameterName
=	O
1.0	B-HyperparameterValue
in	O
Eq.3	O
during	O
training	O
.	O

The	O
vocabulary	O
contains	O
64808	O
tokens	O
.	O

After	O
adding	O
59	O
language	O
tokens	O
,	O
the	O
total	B-HyperparameterName
size	I-HyperparameterName
of	I-HyperparameterName
vocabulary	I-HyperparameterName
is	O
64867.11	B-HyperparameterValue
https://github.com/alvations/sacremoses	O
12	O
https://github.com/moses-smt/mosesdecoder	O
This	O
section	O
shows	O
that	O
mRASP2	B-MethodName
provides	O
consistent	O
performance	O
gains	O
for	O
supervised	O
and	O
unsupervised	O
English	O
-	O
centric	O
translation	O
directions	O
as	O
well	O
as	O
for	O
non	O
-	O
English	O
directions	O
.	O

Supervised	O
Directions	O
As	O
shown	O
in	O
Furthermore	O
,	O
mRASP2	B-MethodName
achieves	O
reasonable	O
BLEU	O
scores	O
on	O
Nl↔Pt	O
directions	O
even	O
though	O
it	O
has	O
only	O
been	O
trained	O
on	O
monolingual	O
data	O
of	O
both	O
sides	O
.	O

This	O
indicates	O
that	O
by	O
simply	O
incorporating	O
monolingual	O
data	O
with	O
parallel	O
data	O
in	O
the	O
unified	O
framework	O
,	O
mRASP2	B-MethodName
successfully	O
enables	O
unsupervised	O
translation	O
through	O
its	O
unified	B-DatasetName
multilingual	I-DatasetName
representation	I-DatasetName
.	O

Previous	O
work	O
shows	O
that	O
the	O
multilingual	B-DatasetName
NMT	I-DatasetName
model	O
can	O
do	O
zero	O
-	O
shot	O
translation	O
directly	O
.	O

We	O
evaluate	O
mRASP2	B-MethodName
on	O
the	O
OPUS-100	B-TaskName
zero	I-TaskName
-	I-TaskName
shot	I-TaskName
test	O
set	O
,	O
which	O
contains	O
6	O
languages	O
14	O
and	O
30	O
translation	O
directions	O
in	O
total	O
.	O

mRASP2	B-MethodName
w/o	I-MethodName
AA	I-MethodName
only	O
adopt	O
contrastive	O
learning	O
on	O
the	O
basis	O
of	O
m	O
-	O
Transformer	O
.	O

mRASP2	B-MethodName
w/o	I-MethodName
MC24	I-MethodName
excludes	O
monolingual	O
data	O
from	O
mRASP2.The	B-MethodName
evaluation	O
results	O
are	O
listed	O
in	O
Appendix	O
and	O
we	O
summarize	O
them	O
in	O
Table	O
3	O
.	O

We	O
find	O
that	O
our	O
mRASP2	B-MethodName
significantly	O
outperforms	O
m	O
-	O
Transformer	O
and	O
substantially	O
narrows	O
the	O
gap	O
with	O
pivot	O
-	O
based	O
model	O
.	O

This	O
is	O
in	O
line	O
with	O
our	O
intuition	O
that	O
bridging	O
the	O
representation	O
gap	O
of	O
different	O
languages	O
can	O
improve	O
the	O
zero	O
-	O
shot	O
translation	O
.	O

It	O
is	O
worth	O
noting	O
that	O
,	O
achieves	O
BLEU	B-MetricName
score	O
improvements	O
on	O
zero	O
-	O
shot	O
translations	O
at	O
sacrifice	O
of	O
about	O
0.5	B-MetricValue
BLEU	B-MetricName
score	O
loss	O
on	O
English	O
-	O
centric	O
directions	O
.	O

By	O
contrast	O
,	O
mRASP2	B-MethodName
improves	O
zero	O
-	O
shot	O
translation	O
by	O
a	O
large	O
margin	O
without	O
losing	O
performance	O
on	O
English	O
-	O
Centric	O
directions	O
.	O

Therefore	O
,	O
mRASP2	B-MethodName
has	O
a	O
great	O
potential	O
to	O
serve	O
many	B-DatasetName
-	I-DatasetName
to	I-DatasetName
-	I-DatasetName
many	I-DatasetName
translations	I-DatasetName
,	O
including	O
both	O
English	O
-	O
centric	O
and	O
non	O
-	O
English	O
directions	O
.	O

First	O
we	O
summarize	O
and	O
analyze	O
the	O
performance	O
of	O
mRASP2	B-MethodName
in	O
different	O
scenarios	O
.	O

Second	O
we	O
adopt	O
the	O
sentence	O
representation	O
of	O
mRASP2	B-MethodName
to	O
retrieve	O
similar	O
sentences	O
across	O
languages	O
.	O

This	O
is	O
to	O
verify	O
our	O
argument	O
that	O
the	O
improvements	O
come	O
from	O
the	O
universal	O
language	O
representation	O
learned	O
by	O
mRASP2	B-MethodName
.	O

Finally	O
we	O
visualize	O
the	O
sentence	O
representations	O
,	O
mRASP2	B-MethodName
indeed	O
draws	O
the	O
representations	O
closer	O
.	O

To	O
make	O
a	O
better	O
understanding	O
of	O
the	O
effectiveness	O
of	O
mRASP2	B-MethodName
,	O
we	O
evaluate	O
models	O
of	O
different	O
settings	O
.	O

We	O
summarize	O
the	O
experiment	O
results	O
in	O
Table	O
4:•	O
1	O
v.s.	O
3	O
:	O
3	O
performs	O
comparably	O
with	O
m	O
-	O
Transformer	O
in	O
supervised	O
and	O
unsupervised	O
scenarios	O
,	O
whereas	O
achieves	O
a	O
substantial	O
BLEU	B-MetricName
improvement	O
for	O
zero	O
-	O
shot	O
translation	O
.	O

This	O
means	O
contrastive	O
loss	O
is	O
crucial	O
for	O
the	O
performance	O
in	O
zero	O
-	O
shot	O
directions.•	O
5	O
:	O
mRASP2	B-MethodName
further	O
improves	O
BLEU	B-MetricName
in	O
all	O
of	O
the	O
three	O
scenarios	O
,	O
especially	O
in	O
unsupervised	O
directions	O
.	O

Therefore	O
it	O
is	O
safe	O
to	O
conjecture	O
that	O
by	O
accomplishing	O
with	O
monolingual	O
data	O
,	O
mRASP2	B-MethodName
learns	O
a	O
better	O
representation	O
space	O
.	O

In	O
order	O
to	O
verify	O
whether	O
mRASP2	B-MethodName
learns	O
a	O
better	O
representation	O
space	O
,	O
we	O
conduct	O
a	O
set	O
of	O
similarity	O
search	O
experiments	O
.	O

We	O
argue	O
that	O
mRASP2	B-MethodName
benefits	O
this	O
task	O
in	O
the	O
sense	O
that	O
it	O
bridges	O
the	O
representation	O
gap	O
across	O
languages	O
.	O

Therefore	O
we	O
use	O
the	O
accuracy	B-MetricName
of	O
similarity	O
search	O
tasks	O
as	O
a	O
quantitative	O
indicator	O
of	O
cross	O
-	O
lingual	O
representation	O
alignment	O
.	O

We	O
conducted	O
comprehensive	O
experiments	O
to	O
support	O
our	O
argument	O
and	O
experiment	O
on	O
mRASP2	B-MethodName
and	O
mRASP2	B-MethodName
w/o	I-MethodName
AA	I-MethodName
.We	O
divide	O
the	O
experiments	O
into	O
two	O
scenarios	O
:	O
First	O
we	O
evaluate	O
our	O
method	O
on	O
Tatoeba	O
dataset	O
(	O
Artetxe	O
and	O
Schwenk	O
,	O
2019	O
)	O
,	O
which	O
is	O
English	O
-	O
centric	O
.	O

Following	O
Tran	O
et	O
al	O
.	O
(	O

We	O
detect	O
two	O
trends	O
:	O
(	O
i	O
)	O
The	O
overall	B-MetricName
accuracy	I-MetricName
follows	O
the	O
rule	O
:	O
m	O
-	O
Transformer	O
<	O
mRASP2	B-MethodName
w/o	I-MethodName
AA	I-MethodName
<	O
mRASP2	B-MethodName
.	O
(	O

ii	O
)	O
mRASP2	B-MethodName
brings	O
more	O
significant	O
improvements	O
for	O
languages	O
with	O
less	O
data	O
volume	O
in	O
PC32	B-DatasetName
.	O

The	O
two	O
trends	O
mean	O
that	O
mRASP2	B-MethodName
increases	O
translation	O
BLEU	B-MetricName
score	O
in	O
a	O
sense	O
that	O
it	O
bridges	O
the	O
representation	O
gap	O
across	O
languages	O
.	O

Non	O
-	O
English	O
:	O
Ted	O
-	O
M	O
It	O
will	O
be	O
more	O
convincing	O
to	O
argue	O
that	O
mRASP2	B-MethodName
indeed	O
bridges	O
the	O
representation	O
gap	O
if	O
similarity	O
search	O
accuracy	O
increases	O
on	O
zero	O
-	O
shot	O
directions	O
.	O

We	O
list	O
the	O
averaged	B-MetricName
top-1	I-MetricName
accuracy	I-MetricName
of	O
210	O
non	O
-	O
English	O
directions	O
17	O
in	O
Table	O
6	O
that	O
our	O
method	O
generally	O
narrows	O
the	O
representation	O
gap	O
across	O
languages	O
.	O

To	O
better	O
understanding	O
the	O
specifics	O
beyond	O
the	O
averaged	B-MetricName
accuracy	I-MetricName
,	O
we	O
plot	O
the	O
accuracy	B-MetricName
improvements	O
in	O
the	O
heat	O
map	O
in	O
Figure	O
3	O
.	O

mRASP2	B-MethodName
w/o	I-MethodName
AA	I-MethodName
brings	O
general	O
improvements	O
over	O
m	O
-	O
Transformer	O
.	O

mRASP2	B-MethodName
especially	O
improves	O
on	O
Dutch(Nl	O
)	O
.	O

This	O
is	O
because	O
mRASP2	B-MethodName
introduces	O
monolingual	O
data	O
of	O
Dutch	O
while	O
mRASP2	B-MethodName
w/o	I-MethodName
AA	I-MethodName
includes	O
no	O
Dutch	O
data	O
.	O

By	O
contrast	O
,	O
mRASP2	B-MethodName
draws	O
the	O
representations	O
across	O
3	O
languages	O
much	O
closer	O
.	O

While	O
initial	O
research	O
on	O
NMT	O
starts	O
with	O
build	O
-	O
ing	O
translation	O
systems	O
between	O
two	O
languages	O
,	O
Dong	O
et	O
al	O
.	O
(	O

2018;Choi	O
et	O
al	O
.	O
,	O

Recent	O
efforts	O
mainly	O
focuses	O
on	O
designing	O
language	O
specific	O
components	O
for	O
multilingual	O
NMT	O
to	O
enhance	O
the	O
model	O
performance	O
on	O
rich	O
-	O
resource	O
languages	O
Kim	O
et	O
al	O
.	O
,	O

2019;Wang	O
et	O
al	O
.	O
,	O

Different	O
from	O
these	O
approaches	O
,	O
mRASP2	B-MethodName
proposes	O
to	O
explicitly	O
close	O
the	O
semantic	O
representation	O
of	O
different	O
languages	O
and	O
make	O
the	O
most	O
of	O
cross	O
lingual	O
transfer	O
.	O

2017	O
)	O
shows	O
that	O
a	O
multilingual	B-TaskName
NMT	I-TaskName
system	O
enables	O
zero	O
-	O
shot	O
translation	O
without	O
explicitly	O
introducing	O
pivot	O
methods	O
.	O

2020	O
)	O
;	O
Liu	O
et	O
al	O
.	O
(	O

2020;He	O
et	O
al	O
.	O
,	O

Inspired	O
by	O
these	O
studies	O
,	O
we	O
apply	O
contrastive	O
learning	O
for	O
multilingual	B-TaskName
NMT.Cross	I-TaskName
-	O
lingual	O
Representation	O
Cross	O
-	O
lingual	O
representation	O
learning	O
has	O
been	O
intensively	O
studied	O
in	O
order	O
to	O
improve	O
cross	O
-	O
lingual	O
understanding	O
(	O
XLU	O
)	O
tasks	O
.	O

Combined	O
with	O
additional	O
unsupervised	O
monolingual	O
data	O
,	O
we	O
achieve	O
substantial	O
improvements	O
on	O
all	O
translation	O
directions	O
of	O
multilingual	B-TaskName
NMT	I-TaskName
.	O

As	O
such	O
,	O
a	O
single	O
model	O
can	O
handle	O
more	O
than	O
100	O
languages	O
and	O
outperforms	O
the	O
corresponding	O
bilingual	O
baseline	O
.	O

Detailed	O
results	O
on	B-DatasetName
OPUS-100	I-DatasetName
zero	I-DatasetName
-	I-DatasetName
shot	I-DatasetName
evaluation	O
set	O
are	O
listed	O
in	O
Table	O
8	O
B.2	O
Results	O
on	O
WMT	B-DatasetName
Detailed	O
results	O
on	O
WMT	B-DatasetName
evaluation	O
set	O
are	O
listed	O
in	O
Table	O
9	O
C	O
Example	O
of	O
AA	O

Augmenting	O
parametric	O
neural	O
network	O
models	O
with	O
non	O
-	O
parametric	O
memory	O
(	O
Khandelwal	O
et	O
al	O
.	O
,	O

2020;Lewis	O
et	O
al	O
.	O
,	O

2019;Brown	O
et	O
al	O
.	O
,	O

In	O
general	O
,	O
TM	O
is	O
a	O
database	O
that	O
stores	O
pairs	O
of	O
source	O
text	O
and	O
its	O
corresponding	O
translations	O
.	O

Like	O
for	O
human	O
translation	O
,	O
early	O
work	O
(	O
Koehn	O
and	O
Senellart	O
,	O
2010;He	O
et	O
al	O
.	O
,	O

These	O
models	O
perform	O
translation	O
in	O
two	O
stages	O
:	O
In	O
the	O
retrieval	O
stage	O
,	O
a	O
retriever	O
searches	O
for	O
nearest	O
neighbors	O
(	O
i.e.	O
,	O
source	O
-	O
target	O
pairs	O
)	O
from	O
the	O
training	O
corpus	O
based	O
on	O
source	O
-	O
side	O
similarity	O
such	O
as	O
lexical	O
overlaps	O
(	O
Gu	O
et	O
al	O
.	O
,	O

2021	O
)	O
or	O
directly	O
concatenating	O
them	O
to	O
the	O
source	O
input	O
(	O
Bulte	O
and	O
Tezcan	O
,	O
2019;Xu	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
or	O
biasing	O
the	O
word	O
distribution	O
during	O
decoding	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

2020	O
)	O
propose	O
a	O
token	O
-	O
level	O
nearest	O
neighbor	O
search	O
using	O
complete	O
translation	O
context	O
,	O
i.e.	O
,	O
both	O
the	O
source	O
-	O
side	O
input	O
and	O
target	O
-	O
side	O
prefix	O
.	O

Second	O
,	O
the	O
memory	O
retriever	O
is	O
non	O
-	O
learnable	O
,	O
not	O
end	O
-	O
to	O
-	O
end	O
optimized	O
,	O
and	O
lacks	O
for	O
the	O
ability	O
to	O
adapt	O
to	O
specific	O
downstream	O
NMT	O
models	O
.	O

Concretely	O
,	O
current	O
retrieval	O
mechanisms	O
(	O
e.g.	O
,	O
BM25	O
)	O
are	O
generic	O
similarity	O
search	O
,	O
adopting	O
a	O
simple	O
heuristic	O
.	O

Although	O
this	O
observation	O
is	O
true	O
,	O
the	O
most	O
similar	O
one	O
does	O
not	O
necessarily	O
serve	O
the	O
best	O
for	O
NMT	O
models	O
.	O

In	O
this	O
work	O
,	O
we	O
propose	O
to	O
augment	O
NMT	O
models	O
with	O
monolingual	O
TM	O
and	O
a	O
learnable	O
crosslingual	O
memory	O
retriever	O
.	O

Experiments	O
show	O
that	O
(	O
1	O
)	O
Our	O
model	O
leads	O
to	O
significant	O
improvements	O
over	O
non	O
-	O
TM	O
baseline	O
NMT	O
model	O
,	O
even	O
outperforming	O
strong	O
TM	O
-	O
augmented	O
baselines	O
.	O

2020	O
)	O
ran	O
existing	O
translation	O
model	O
on	O
large	O
bi	O
-	O
text	O
corpora	O
and	O
recorded	O
all	O
hidden	O
states	O
for	O
later	O
nearest	O
neighbor	O
search	O
at	O
each	O
decoding	O
step	O
,	O
which	O
is	O
very	O
compute	O
-	O
intensive	O
.	O

It	O
can	O
be	O
observed	O
that	O
there	O
is	O
a	O
shift	O
from	O
using	O
off	O
-	O
the	O
-	O
shelf	O
search	O
engines	O
to	O
learning	O
task	O
-	O
specific	O
retrievers	O
.	O

Our	O
work	O
draws	O
inspiration	O
from	O
this	O
line	O
of	O
research	O
.	O

NMT	O
using	O
Monolingual	O
Data	O
To	O
our	O
knowledge	O
,	O
the	O
integration	O
of	O
monolingual	O
data	O
for	O
NMT	O
was	O
first	O
investigated	O
by	O
Gulcehre	O
et	O
al	O
.	O
(	O

Jean	O
et	O
al	O
.	O
(	O

2016	O
)	O
,	O
where	O
a	O
reverse	O
translation	O
model	O
is	O
used	O
to	O
translate	O
monolingual	O
sentences	O
from	O
the	O
target	O
language	O
to	O
the	O
source	O
language	O
to	O
generate	O
synthetic	O
parallel	O
sentences	O
.	O

2019	O
)	O
showed	O
that	O
self	O
-	O
training	O
,	O
where	O
the	O
synthetic	O
parallel	O
sentences	O
are	O
created	O
by	O
translating	O
monolingual	O
sentences	O
in	O
the	O
source	O
language	O
,	O
is	O
also	O
helpful	O
.	O

We	O
start	O
by	O
formalizing	O
the	O
translation	O
task	O
as	O
a	O
retrieve	O
-	O
then	O
-	O
generate	O
process	O
in	O
§	O
3.1	O
.	O

z	O
M	O
,	O
f	O
(	O
x	O
,	O
z	O
M	O
)	O
)	O
.	O

This	O
could	O
involve	O
measuring	O
the	O
relevance	O
scores	O
between	O
the	O
source	O
sentence	O
and	O
millions	O
of	O
candidate	O
target	O
sentences	O
,	O
which	O
poses	O
a	O
serious	O
computational	O
challenge	O
.	O

1993	O
)	O
such	O
that	O
the	O
selection	O
of	O
the	O
most	O
relevant	O
sentences	O
can	O
be	O
reduced	O
to	O
Maximum	O
Inner	O
Product	O
Search	O
(	O
MIPS	O
)	O
.	O

E	O
src	O
(	O
x	O
)	O
=	O
normalize(W	O
src	O
Trans	O
src	O
(	O
x	O
)	O
)	O
E	O
tgt	O
(	O
z	O
)	O
=	O
normalize(W	O
tgt	O
Trans	O
tgt	O
(	O
z))The	O
normalized	O
vectors	O
have	O
zero	O
means	O
and	O
unit	O
lengths	O
.	O

Therefore	O
,	O
the	O
relevance	O
scores	O
always	O
fall	O
in	O
the	O
interval	O
[	O
−1	O
,	O
1	O
]	O
.	O

Given	O
a	O
source	O
sentence	O
x	O
,	O
a	O
small	O
set	O
of	O
relevant	O
TM	O
{	O
z	O
i	O
}	O
M	O
i=1	O
,	O
and	O
relevance	O
scores	O
{	O
f	O
(	O
x	O
,	O
z	O
i	O
)	O
}	O
M	O
i=1	O
,	O
the	O
translation	O
model	O
defines	O
the	O
conditional	O
proba	O
-	O
bility	O
p(y|x	O
,	O
z	O
1	O
,	O
f	O
(	O
x	O
,	O
z	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O

z	O
M	O
,	O
f	O
(	O
x	O
,	O
z	O
M	O
)	O
)	O
.Our	O
translation	O
model	O
is	O
built	O
upon	O
the	O
standard	O
encoder	O
-	O
decoder	O
NMT	O
model	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O

z	O
M	O
,	O
f	O
(	O
x	O
,	O
z	O
M	O
)	O
)	O
,	O
where	O
y	O
*	O
refers	O
to	O
the	O
reference	O
translation	O
.	O

Let	O
X	O
and	O
Z	O
be	O
the	O
(	O
B	O
×	O
d	O
)	O
matrix	O
of	O
the	O
source	O
and	O
target	O
vectors	O
encoded	O
by	O
E	O
src	O
and	O
E	O
tgt	O
respectively	O
.	O

S	O
=	O
XZ	O
T	O
is	O
a	O
(	O
B	O
×	O
B	O
)	O
matrix	O
of	O
relevance	O
scores	O
,	O
where	O
each	O
row	O
corresponds	O
to	O
a	O
source	O
sentence	O
and	O
each	O
column	O
corresponds	O
to	O
a	O
target	O
sentence	O
.	O

2002	O
)	O
as	O
the	O
evaluation	O
metric	O
.	B-Doc-Begin

We	O
build	O
our	O
model	O
using	O
Transformer	O
blocks	O
with	O
the	O
same	O
configuration	O
as	O
Transformer	O
Base	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
(	O
8	B-HyperparameterValue
attention	B-HyperparameterName
heads	I-HyperparameterName
,	O
512	B-HyperparameterValue
dimensional	I-HyperparameterValue
hidden	B-HyperparameterName
state	I-HyperparameterName
,	O
and	O
2048	B-HyperparameterValue
dimensional	I-HyperparameterValue
feed	B-HyperparameterName
-	I-HyperparameterName
forward	I-HyperparameterName
state	I-HyperparameterName
)	O
.	O

The	O
number	O
of	O
Transformer	B-HyperparameterName
blocks	I-HyperparameterName
is	O
3	B-HyperparameterValue
for	O
the	O
retrieval	O
model	O
,	O
4	B-HyperparameterValue
for	O
the	O
memory	O
encoder	O
in	O
the	O
translation	O
model	O
,	O
and	O
6	B-HyperparameterValue
for	O
the	O
encoder	O
-	O
decoder	O
architecture	O
in	O
the	O
translation	O
model	O
.	O

We	O
retrieve	O
the	O
top	O
5	O
TM	O
sentences	O
.	O

The	O
FAISS	B-HyperparameterName
index	I-HyperparameterName
code	I-HyperparameterName
is	O
"	B-HyperparameterValue
IVF1024	I-HyperparameterValue
HNSW32,SQ8	I-HyperparameterValue
"	I-HyperparameterValue
and	O
the	O
search	B-HyperparameterName
depth	I-HyperparameterName
is	O
64	B-HyperparameterValue
.	O

2017	O
)	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
(	O
Kingma	O
and	O
Ba	O
,	O
2014	O
)	O
and	O
train	O
models	O
with	O
up	O
to	O
100	O
K	O
Xia	O
et	O
al	O
.	O
(	O

When	O
trained	O
with	O
asynchronous	O
index	O
refresh	O
,	O
the	O
re	O
-	O
indexing	O
interval	O
is	O
3	B-HyperparameterValue
K	I-HyperparameterValue
training	B-HyperparameterName
steps	I-HyperparameterName
.	B-Doc-Begin

The	O
JRC	O
-	O
Acquis	O
corpus	O
contains	O
the	O
total	O
body	O
of	O
European	O
Union	O
(	O
EU	O
)	O
law	O
applicable	O
to	O
the	O
EU	O
member	O
states	O
.	O

Detailed	O
data	O
statistics	O
are	O
shown	O
in	O
Table	O
1.Models	O
To	O
study	O
the	O
effect	O
of	O
each	O
model	O
component	O
,	O
we	O
implement	O
a	O
series	O
of	O
model	O
variants	O
(	O
model	O
#	O
1	O
to	O
#	O
5	O
in	O
Table	O
2).1	O
.	O

To	O
measure	O
the	O
help	O
from	O
TM	O
,	O
we	O
remove	O
the	O
model	O
components	O
related	O
to	O
TM	O
(	O
including	O
the	O
retrieval	O
model	O
and	O
the	O
memory	O
encoder	O
)	O
,	O
and	O
only	O
employ	O
the	O
encoder	O
-	O
decoder	O
architecture	O
for	O
NMT	O
.	O

We	O
attribute	O
the	O
success	O
,	O
again	O
,	O
to	O
the	O
endto	O
-	O
end	O
adaptability	O
of	O
our	O
cross	O
-	O
lingual	O
retriever	O
.	O

This	O
is	O
manifested	O
by	O
the	O
fact	O
that	O
model	O
#	O
3	O
even	O
slightly	O
underperforms	O
model	O
#	O
2	O
in	O
some	O
of	O
translation	O
tasks	O
.	O

In	O
fact	O
,	O
we	O
can	O
see	O
that	O
our	O
translation	O
model	O
using	O
traditional	O
similarity	O
search	O
(	O
model	O
#	O
2	O
)	O
already	O
outperforms	O
the	O
best	O
previously	O
reported	O
results	O
,	O
which	O
reveals	O
that	O
the	O
architectural	O
design	O
of	O
our	O
translation	O
model	O
is	O
surprisingly	O
effective	O
despite	O
its	O
simplicity	O
.	O

This	O
motivates	O
us	O
to	O
conduct	O
experiments	O
in	O
low	O
-	O
resource	O
scenarios	O
,	O
where	O
we	O
use	O
extra	O
monolingual	O
data	O
in	O
the	O
target	O
language	O
to	O
boost	O
translation	O
quality	O
.	O

The	O
general	O
patterns	O
are	O
consistent	O
across	O
all	O
experiments	O
:	O
the	O
larger	O
the	O
TM	O
becomes	O
,	O
the	O
better	O
translation	O
performance	O
the	O
model	O
achieves	O
.	O

When	O
using	O
all	O
available	O
monolingual	O
data	O
(	O
4/4	O
)	O
,	O
the	O
translation	O
quality	O
is	O
boosted	O
significantly	O
.	O

We	O
also	O
observe	O
that	O
when	O
the	O
training	O
pairs	O
are	O
very	O
scarce	O
(	O
only	O
1/4	O
bilingual	O
pairs	O
are	O
available	O
)	O
,	O
a	O
small	O
size	O
of	O
TM	O
even	O
hurts	O
the	O
model	O
performance	O
.	O

The	O
reason	O
could	O
be	O
overfitting	O
.	O

We	O
speculate	O
that	O
better	O
results	O
would	O
be	O
obtained	O
by	O
tuning	O
the	O
model	O
hyper	O
-	O
parameters	O
according	O
to	O
different	O
TM	O
sizes	O
.	O

We	O
can	O
see	O
that	O
when	O
only	O
using	O
the	O
bilingual	O
data	O
,	O
the	O
TM	O
-	O
augmented	O
model	O
obtains	O
higher	O
BLEU	O
scores	O
in	O
domains	O
with	O
less	O
data	O
but	O
slightly	O
lower	O
scores	O
in	O
other	O
domains	O
compared	O
to	O
the	O
non	O
-	O
TM	O
baseline	O
.	O

We	O
show	O
that	O
a	O
task	O
-	O
specific	O
cross	O
-	O
lingual	O
memory	O
retriever	O
can	O
be	O
learned	O
by	O
end	O
-	O
to	O
-	O
end	O
MT	O
training	O
.	O

However	O
,	O
we	O
observe	O
that	O
these	O
methods	O
tend	O
to	O
predict	O
polarities	O
based	O
on	O
the	O
literal	O
meaning	O
of	O
aspect	O
and	O
opinion	O
terms	O
and	O
mainly	O
consider	O
relations	O
implicitly	O
among	O
subtasks	O
at	O
the	O
word	O
level	O
.	O

CL	O
]	O
7	O
Jun	O
2021	O
opinions	O
but	O
also	O
correctly	O
predict	O
each	O
polarity	O
of	O
the	O
aspect	O
(	O
E3).To	O
address	O
the	O
aforementioned	O
issues	O
,	O
we	O
propose	O
Deep	O
Contextualized	O
Relation	O
-	O
Aware	O
Network	O
(	O
DCRAN	O
)	O
for	O
ABSA	O
.	O

2	O
)	O
We	O
propose	O
novel	O
selfsupervised	O
strategies	O
for	O
ABSA	O
,	O
which	O
are	O
highly	O
effective	O
in	O
dealing	O
with	O
multiple	O
aspects	O
and	O
considering	O
deep	O
contextualized	O
information	O
with	O
the	O
aspect	O
and	O
opinion	O
terms	O
.	O

We	O
represent	O
the	O
parameters	O
of	O
the	O
shared	O
encoder	O
as	O
Θ	O
s	O
.	O

The	O
objective	O
of	O
aspect	O
term	O
extraction	O
is	O
minimizing	O
the	O
negative	O
log	O
-	O
likelihood	O
(	O
NLL	O
)	O
loss	O
:	O
L	O
ate	O
(	O
Θ	O
s	O
,	O
Θ	O
a	O
)	O
=	O
−	O
log	O
p(Y	O
a	O
|H	O
)	O
.	O

Then	O
,	O
the	O
NLL	O
loss	O
of	O
opinion	O
term	O
extraction	O
is	O
defined	O
as	O
,	O
L	O
ote	O
(	O
Θ	O
s	O
,	O
Θ	O
o	O
)	O
=	O
−	O
log	O
p(Y	O
o	O
|H	O
)	O
.	O

Then	O
,	O
the	O
NLL	O
loss	O
of	O
the	O
typespecific	O
masked	O
term	O
discrimination	O
is	O
defined	O
as	O
:	O
L	O
tsmtd	O
(	O
Θ	O
s	O
,	O
Θ	O
m	O
)	O
=	O
−	O
log	O
p(Y	O
m	O
|H).This	O
allows	O
the	O
model	O
to	O
explicitly	O
exploit	O
sentence	O
information	O
by	O
discriminating	O
what	O
kind	O
of	O
term	O
is	O
masked	O
.	O

Pairwise	O
Relations	O
Discrimination	O
In	O
this	O
task	O
,	O
we	O
uniformly	O
replace	O
both	O
aspects	O
and	O
opinion	O
terms	O
using	O
the	O
special	O
token	O
[	O
REL	O
]	O
.	O

The	O
input	O
sequence	O
of	O
a	O
masked	O
sentence	O
is	O
represented	O
as	O
,	O
X	O
prd	O
=	O
[	O
[	O
CLS	O
]	O
w	O
1	O
...	O
[	O
REL	O
]	O
i	O
...	O
[	O
REL	O
]	O
j	O
...	O
w	O
n	O
[	O
SEP]],and	O
is	O
fed	O
into	O
pre	O
-	O
trained	O
language	O
models	O
.	O

Then	O
,	O
the	O
output	O
representation	O
of	O
[	O
CLS	O
]	O
token	O
is	O
used	O
to	O
discriminate	O
whether	O
the	O
replaced	O
tokens	O
have	O
a	O
pairwise	O
relation	O
as	O
,	O
Y	O
r	O
=	O
softmax(W	O
4	O
h	O
[	O
CLS	O
]	O
+	O
b	O
4	O
)	O
,	O
where	O
W	O
4	O
∈	O
R	O
2×d	O
h	O
represents	O
trainable	O
parameters	O
andŶ	O
r	O
∈	O
{	O
T	O
rue	O
,	O
F	O
alse	O
}	O
.	O

denotes	O
unreported	O
results	O
.	O

LAP14	O
REST14	O
REST15	O
ATE	O
-	O
F1	O
OTE	O
-	O
F1	O
ASC	O
-	O
F1	O
ABSA	O
-	O
F1	O
ATE	O
-	O
F1	O
OTE	O
-	O
F1	O
ASC	O
-	O
F1	O
ABSA	O
-	O
F1	O
ATE	O
-	O
F1	O
OTE	O
-	O
F1	O
ASC	O
-	O
F1	O
ABSA	O
-	O
F1	O
MNN	O
(	O
(	O
LAP14	O
)	O
,	O
restaurant	O
reviews	O
(	O
REST14	O
)	O
from	O
(	O
Pontiki	O
et	O
al	O
.	O
,	O

As	O
the	O
PRD	O
objective	O
is	O
discriminating	O
whether	O
the	O
replace	O
tokens	O
have	O
a	O
pairwise	O
aspect	O
-	O
opinion	O
relations	O
,	O
it	O
allows	O
the	O
model	O
to	O
more	O
exploit	O
the	O
relations	O
between	O
aspect	O
and	O
opinion	O
at	O
a	O
sentence	O
level	O
.	O

Although	O
considering	O
the	O
relations	O
between	O
aspect	O
and	O
opinion	O
implicitly	O
can	O
improve	O
performance	O
w.r.t	O
.	O

In	O
the	O
case	O
of	O
multiple	O
-	O
aspect	O
,	O
Explicit	O
Self	O
-	O
Supervised	O
Strategies	O
show	O
absolute	O
ABSA	O
-	O
F1	O
improvements	O
of	O
0.97	O
%	O
(	O
80.22	O
%	O
→	O
81.19	O
%	O
)	O
and	O
3.04	O
%	O
(	O
65.16	O
%	O
→	O
68.20	O
)	O
on	O
the	O
REST14	O
and	O
REST15	O
datasets	O
,	O
respectively	O
.	O

From	O
these	O
observations	O
,	O
we	O
demonstrate	O
that	O
our	O
proposed	O
method	O
is	O
highly	O
effective	O
for	O
the	O
case	O
when	O
the	O
sentence	O
contains	O
multiple	O
aspects	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
the	O
Deep	O
Contextualized	O
Relation	O
-	O
Aware	O
Network	O
(	O
DCRAN	O
)	O
for	O
aspectbased	O
sentiment	O
analysis	O
.	O

It	O
jointly	O
trains	O
two	O
BERT	O
-	O
MRC	O
models	O
sharing	O
parameters	O
.	O

Especially	O
,	O
E3	O
expresses	O
a	O
sarcastic	O
opinion	O
about	O
aspect	O
terms	O
throughout	O
the	O
sentence	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
,	O
Dongsuk	O
Oh	O
,	O
Jungwoo	O
Lim	O
,	O
and	O
Heuiseok	O
Lim	O
for	O
their	O
constructive	O
comments	O
.	O

2016;Wang	O
et	O
al	O
.	O
,	O

2019	O
;	O
and	O
collapsed	O
approach	O
(	O
Li	O
and	O
Lu	O
,	O
2017;Ma	O
et	O
al	O
.	O
,	O

2018	O
;	O
has	O
been	O
proposed	O
.	O

For	O
the	O
shared	O
encoder	O
,	O
we	O
adopt	O
four	B-Doc-Begin
We	O
set	O
the	O
batch	B-HyperparameterName
size	I-HyperparameterName
to	O
64	B-HyperparameterValue
for	O
the	O
base	O
model	O
,	O
12	B-HyperparameterValue
for	O
the	O
BERT	B-MethodName
large	I-MethodName
and	O
32	B-HyperparameterValue
for	O
the	O
ELECTRA	B-MethodName
large	I-MethodName
.	O

We	O
set	O
the	O
initial	O
learning	B-HyperparameterName
rate	I-HyperparameterName
to	O
5e-5	B-HyperparameterValue
for	O
BERT	B-MethodName
base	I-MethodName
and	O
ELECTRA	B-MethodName
base	I-MethodName
,	O
2e-5	B-HyperparameterValue
for	O
BERT	B-MethodName
large	I-MethodName
,	O
and	O
5e-6	B-HyperparameterValue
for	O
ELECTRA	B-MethodName
large	I-MethodName
.	O

For	O
the	O
transformer	O
decoder	O
,	O
we	O
set	O
the	O
number	O
of	O
heads	O
in	O
multi	O
-	O
head	O
attention	O
and	O
hidden	B-HyperparameterName
layers	I-HyperparameterName
to	O
2	B-HyperparameterValue
among	O
range	O
from	O
2	B-HyperparameterValue
to	O
6	B-HyperparameterValue
,	O
and	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
size	I-HyperparameterName
to	O
768	B-HyperparameterValue
.	O

In	O
the	O
case	O
of	O
α	B-HyperparameterName
,	O
we	O
obtained	O
the	O
best	O
results	O
when	O
α	B-HyperparameterName
is	O
1	B-HyperparameterValue
.	O

The	O
average	O
runtime	O
for	O
each	O
approach	O
was	O
about	O
20	O
seconds	O
for	O
BERT	B-MethodName
base	I-MethodName
and	O
ELECTRA	B-MethodName
base	I-MethodName
,	O
and	O
90	O
seconds	O
for	O
BERT	B-MethodName
large	I-MethodName
and	O
ELECTRA	B-MethodName
large	I-MethodName
.	O

We	O
train	O
our	O
models	O
using	O
AdamP	B-HyperparameterValue
(	O
Heo	O
et	O
al	O
.	O
,	O

2021	O
)	O
optimizer	B-HyperparameterName
and	O
conduct	O
experiments	O
with	O
Tesla	O
V100	O
GPU	O
for	O
all	O
the	O
experiments	B-Doc-Begin
.	I-Doc-Begin

We	O
compare	O
our	O
model	O
with	O
the	O
following	O
previous	O
works	O
2	O
.MNN	O
)	O
is	O
a	O
multi	O
-	O
task	O
model	O
for	O
ATE	O
and	O
ASC	O
using	O
attention	O
mechanisms	O
to	O
learn	O
the	O
joint	O
representation	O
of	O
aspect	O
and	O
polarity	O
relations	O
.	O

Additionally	O
,	O
it	O
introduces	O
the	O
auxiliary	O
OTE	O
task	O
without	O
explicit	O
interaction	O
.	O

We	O
benchmark	O
our	O
grounded	O
semantics	O
on	O
compositionality	O
and	O
zero	O
-	O
shot	O
inference	O
tasks	O
,	O
and	O
we	O
show	O
that	O
it	O
provides	O
better	O
results	O
and	O
better	O
generalizations	O
than	O
SOTA	O
non	O
-	O
grounded	O
models	O
,	O
such	O
as	O
word2vec	O
and	O
BERT	O
.	O

Most	O
SOTA	O
models	O
in	O
NLP	O
are	O
only	O
intratextual	O
.	O

While	O
successful	O
in	O
a	O
range	O
of	O
cases	O
,	O
this	O
approach	O
does	O
not	O
take	O
into	O
consideration	O
two	O
fundamental	O
facts	O
about	O
language	O
.	O

2019).While	O
most	O
of	O
the	O
work	O
in	O
this	O
area	O
uses	O
toy	O
worlds	O
and	O
synthetic	O
linguistic	O
data	O
,	O
we	O
explore	O
grounded	O
language	O
learning	O
offering	O
an	O
example	O
in	O
which	O
unsupervised	O
learning	O
is	O
combined	O
with	O
a	O
language	O
-	O
independent	O
grounding	O
domain	O
in	O
a	O
realworld	O
scenario	O
.	O

In	O
our	O
setting	O
,	O
users	O
produce	O
search	O
queries	O
to	O
find	O
products	O
on	O
the	O
web	O
:	O
queries	O
and	O
clicks	O
on	O
search	O
results	O
are	O
used	O
as	O
a	O
model	O
for	O
the	O
teacher	O
-	O
learner	O
dynamics	O
.	O

For	O
in	O
-	O
stance	O
,	O
the	O
denotation	O
of	O
the	O
word	O
"	O
shoes	O
"	O
is	O
constructed	O
from	O
the	O
clicks	O
produced	O
by	O
real	O
users	O
on	O
products	O
that	O
are	O
in	O
fact	O
shoes	O
after	O
having	O
performed	O
the	O
query	O
"	O
shoes	O
"	O
in	O
the	O
search	O
bar	O
.	O

Albeit	O
domain	O
specific	O
,	O
the	O
resulting	O
language	O
is	O
significantly	O
richer	O
than	O
languages	O
from	O
agent	O
-	O
based	O
models	O
of	O
language	O
acquisition	O
(	O
Słowik	O
et	O
al	O
.	O
,	O

2020;Fitzgerald	O
and	O
Tagliabue	O
,	O
2020	O
)	O
,	O
as	O
it	O
is	O
based	O
on	O
26k	O
entities	O
from	O
the	O
inventory	O
of	O
a	O
real	O
website.2	O
.	O

We	O
show	O
that	O
a	O
dense	O
domain	O
built	O
through	O
unsupervised	O
representations	O
can	O
support	O
compositionality	O
.	O

The	O
chosen	O
IR	O
domain	O
is	O
rich	O
enough	O
to	O
provide	O
a	O
wealth	O
of	O
data	O
and	O
possibly	O
to	O
see	O
practical	O
applications	O
,	O
whereas	O
at	O
the	O
same	O
time	O
it	O
is	O
sufficiently	O
self	O
-	O
contained	O
to	O
be	O
realistically	O
mastered	O
without	O
human	O
supervision	O
.	O

Following	O
our	O
informal	O
exposition	O
in	O
Section	O
1	O
,	O
we	O
distinguish	O
three	O
components	O
,	O
which	O
are	O
learned	O
separately	O
in	O
a	O
sequence	O
:	O
learning	O
a	O
languageindependent	O
grounding	O
domain	O
,	O
learning	O
noisy	O
denotation	O
from	O
search	O
logs	O
and	O
finally	O
learning	O
functional	O
composition	O
.	O

2015	O
):	O
prod2vec	O
models	O
are	O
word2vec	O
models	O
in	O
which	O
words	O
in	O
a	O
sentence	O
are	O
replaced	O
by	O
products	O
in	O
a	O
shopping	O
session	O
.	B-Doc-Begin

For	O
this	O
study	O
,	O
we	O
pick	O
CBOW	O
(	O
Mu	O
et	O
al	O
.	O
,	O

2018	O
)	O
as	O
our	O
training	O
algorithm	O
,	O
and	O
select	O
d	B-HyperparameterName
=	O
24	B-HyperparameterValue
as	O
vector	O
size	O
,	O
optimizing	O
hyperparameters	O
as	O
recommended	O
by	O
;	O
similar	O
to	O
what	O
happens	O
with	O
word2vec	O
,	O
related	O
products	O
(	O
e.g.	O
two	O
pairs	O
of	O
sneakers	O
)	O
end	O
up	O
closer	O
in	O
the	O
embedding	O
space	O
.	B-Doc-Begin

Learning	O
lexical	O
denotation	O
.	O

Our	O
functional	O
composition	O
will	O
come	O
from	O
the	O
composition	O
of	O
DeepSet	O
representations	O
,	O
where	O
we	O
want	O
to	O
learn	O
a	O
function	O
f	O
:	O
DeepSet	O
×	O
DeepSet	O
→	O
DeepSet	O
.	O

2017	O
):	O
one	O
,	O
Additive	O
Compositional	O
Model	O
(	O
ADM	O
)	O
,	O
sums	O
vectors	O
together	O
to	O
build	O
the	O
final	O
DeepSet	O
representation	O
.	O

Since	O
the	O
output	O
of	O
these	O
processes	O
is	O
always	O
a	O
DeepSet	O
,	O
both	O
models	O
can	O
be	O
recursively	O
composed	O
,	O
given	O
the	O
form	O
of	O
the	O
function	O
f	O
.	O

Data	O
.	O

2016;Jaccard	O
,	O
1912	O
)	O
.	O

We	O
focus	O
on	O
two	O
tasks	O
:	O
leave	O
-	O
one	O
-	O
brand	O
-	O
out	O
(	O
LOBO	O
)	O
and	O
zero	O
-	O
shot	O
(	O
ZT	O
)	O
.	O

7	B-Doc-Begin
We	O
run	O
every	O
model	O
15	O
times	O
and	O
report	O
average	O
results	O
;	O
RMSProp	B-HyperparameterValue
is	O
the	O
chosen	O
optimizer	B-HyperparameterName
,	O
with	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
200	B-HyperparameterValue
,	O
20	O
%	O
of	O
the	O
training	O
set	O
as	O
validation	O
set	O
and	O
early	O
stopping	O
with	O
patience	B-HyperparameterName
=	O
10.Results	B-HyperparameterValue
.	B-Doc-Begin

MDM	O
typically	O
outperforms	O
ADM	O
as	O
a	O
composition	O
method	O
,	O
except	O
for	O
GAS	O
,	O
where	O
all	O
models	O
suffer	O
from	O
gender	O
sparsity	O
;	O
in	O
that	O
case	O
,	O
the	O
best	O
model	O
is	O
ADM	O
,	O
i.e.	O
the	O
one	O
without	O
an	O
implicit	O
bias	O
from	O
the	O
training	O
.	O

2020	O
)	O
,	O
we	O
argued	O
for	O
grounding	O
linguistic	O
meaning	O
in	O
artificial	O
systems	O
through	O
experience	O
.	O

While	O
encouraging	O
,	O
our	O
results	O
are	O
still	O
preliminary	O
:	O
first	O
,	O
we	O
plan	O
on	O
extending	O
our	O
semantics	O
,	O
starting	O
with	O
Boolean	O
operators	O
(	O
e.g.	O
"	O
shoes	O
NOT	O
Nike	O
"	O
)	O
;	O
second	O
,	O
we	O
plan	O
to	O
improve	O
our	O
representational	O
capabilities	O
,	O
either	O
through	O
symbolic	O
knowledge	O
or	O
more	O
discerning	O
embedding	O
strategies	O
;	O
third	O
,	O
we	O
wish	O
to	O
explore	O
transformer	O
-	O
based	O
architectures	O
as	O
an	O
alternative	O
way	O
to	O
produce	O
set	O
-	O
like	O
representations	O
.	O

Federico	O
Bianchi	O
is	O
a	O
member	O
of	O
the	O
Bocconi	O
Institute	O
for	O
Data	O
Science	O
and	O
Analytics	O
(	O
BIDSA	O
)	O
and	O
the	O
Data	O
and	O
Marketing	O
Insights	O
(	O
DMI	O
)	O
unit	O
.	O

We	O
annotated	O
MT	O
evaluations	O
conducted	O
in	O
769	O
research	O
papers	O
published	O
from	O
2010	O
to	O
2020	O
.	O

Our	O
study	O
shows	O
that	O
practices	O
for	O
automatic	O
MT	O
evaluation	O
have	O
dramatically	O
changed	O
during	O
the	O
past	O
decade	O
and	O
follow	O
concerning	O
trends	O
.	O

MT	O
evaluations	O
in	O
recent	O
papers	O
tend	O
to	O
copy	O
and	O
compare	O
automatic	O
metric	O
scores	O
from	O
previous	O
work	O
to	O
claim	O
the	O
superiority	O
of	O
a	O
method	O
or	O
an	O
algorithm	O
without	O
confirming	O
neither	O
exactly	O
the	O
same	O
training	O
,	O
validating	O
,	O
and	O
testing	O
data	O
have	O
been	O
used	O
nor	O
the	O
metric	O
scores	O
are	O
comparable	O
.	O

While	O
such	O
comparisons	O
between	O
MT	O
systems	O
are	O
exhibited	O
in	O
the	O
large	O
majority	O
of	O
MT	O
papers	O
,	O
there	O
are	O
no	O
well	O
-	O
defined	O
guideline	O
nor	O
clear	O
prerequisites	O
under	O
which	O
a	O
comparison	O
between	O
MT	O
systems	O
is	O
considered	O
valid	O
.	O

However	O
,	O
to	O
what	O
extent	O
these	O
requirements	O
have	O
been	O
met	O
in	O
MT	O
publications	O
is	O
unclear	O
.	O

Our	O
study	O
shows	O
that	O
evaluation	O
in	O
MT	O
has	O
dramatically	O
changed	O
since	O
2010	O
.	O

1	O
To	O
identify	O
MT	O
papers	O
,	O
we	O
searched	O
the	O
ACL	O
Anthology	O
website	O
2	O
for	O
the	O
terms	O
"	O
MT	O
"	O
or	O
"	O
translation	O
"	O
in	O
their	O
titles	O
3	O
and	O
analyzed	O
among	O
them	O
the	O
769	O
papers	O
that	O
make	O
comparisons	O
of	O
translation	O
quality	O
between	O
at	O
least	O
two	O
MT	O
systems	O
.	O

We	O
did	O
not	O
list	O
variants	O
of	O
the	O
same	O
metric	O
:	O
e.g.	O
,	O
chrF3	O
and	O
chrF++	O
are	O
labeled	O
chrF	O
(	O
Popović	O
,	O
2015	O
)	O
.	O

A2	O
.	O

Whether	O
a	O
human	O
evaluation	O
of	O
the	O
translation	O
quality	O
has	O
been	O
conducted	O
:	O
yes	O
or	O
no	O
.	O

4	O
A3	O
.	O

1	O
We	O
considered	O
only	O
*	O
ACL	O
main	O
conferences	O
,	O
namely	O
ACL	O
,	O
NAACL	O
,	O
EACL	O
,	O
EMNLP	O
,	O
CoNLL	O
,	O
and	O
AACL	O
,	O
as	O
they	O
are	O
the	O
primary	O
venues	O
for	O
publishing	O
MT	O
papers.2	O
www.aclweb.org/anthology/	O
3	O
There	O
are	O
potentially	O
MT	O
papers	O
falling	O
outside	O
these	O
search	O
criteria	O
but	O
we	O
considered	O
the	O
769	O
papers	O
we	O
obtained	O
to	O
be	O
representative	O
enough	O
for	O
the	O
purpose	O
of	O
this	O
study	O
.	O

Previous	O
work	O
already	O
studied	O
pitfalls	O
in	O
human	O
evaluation	O
(	O
Läubli	O
et	O
al	O
.	O
,	O

If	O
there	O
is	O
no	O
evidence	O
that	O
the	O
scores	O
have	O
been	O
copied	O
,	O
we	O
annotated	O
these	O
papers	O
with	O
"	O
no	O
"	O
for	O
this	O
question	O
.	O

Except	O
for	O
A6	O
,	O
the	O
annotation	O
was	O
straightforward	O
since	O
most	O
papers	O
present	O
a	O
dedicated	O
section	O
for	O
experimental	O
settings	O
with	O
most	O
of	O
the	O
information	O
we	O
searched	O
for	O
.	O

Answering	O
A6	O
required	O
to	O
check	O
the	O
data	O
exploited	O
in	O
the	O
previous	O
work	O
used	O
for	O
comparison	O
.	O

Note	O
that	O
answering	O
"	O
yes	O
"	O
to	O
the	O
questions	O
from	O
A2	O
to	O
A6	O
may	O
only	O
be	O
true	O
for	O
at	O
least	O
one	O
of	O
the	O
comparisons	O
between	O
MT	O
systems	O
,	O
while	O
we	O
did	O
not	O
evaluate	O
how	O
well	O
it	O
applies	O
.	O

Based	O
on	O
previous	O
Other	O
"	O
denotes	O
all	O
other	O
automatic	O
metrics	O
.	O
"	O

This	O
is	O
probably	O
the	O
most	O
expected	O
finding	O
in	O
our	O
study	O
:	O
the	O
overwhelming	O
majority	O
of	O
MT	O
publications	O
uses	O
BLEU	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
the	O
ratio	O
of	O
papers	O
using	O
BLEU	O
remained	O
stable	O
over	O
the	O
years	O
.	O

When	O
properly	O
used	O
,	O
BLEU	O
is	O
a	O
valid	O
metric	O
for	O
evaluating	O
translation	O
quality	O
of	O
MT	O
systems	O
(	O
Callison	O
-	O
Burch	O
et	O
al	O
.	O
,	O

2006;Reiter	O
,	O
2018	O
)	O
.	O

2020	O
)	O
using	O
SacreBLEU	O
and	O
show	O
rankings	O
given	O
by	O
both	O
metrics	O
in	O
Table	O
1	O
.	O

However	O
,	O
according	O
to	O
another	O
metric	O
that	O
better	O
correlates	O
with	O
human	O
judgment	O
,	O
i.e.	O
,	O
chrF	O
,	O
this	O
does	O
not	O
hold	O
:	O
Tohoku	O
-	O
AIP	O
-	O
NTT	O
system	O
is	O
better	O
.	O

Moreover	O
,	O
BLEU	O
may	O
also	O
be	O
directly	O
requested	O
by	O
reviewers	O
,	O
or	O
even	O
worse	O
,	O
other	O
metrics	O
may	O
be	O
requested	O
to	O
be	O
dropped	O
.	O

In	O
MT	O
,	O
statistical	O
significance	O
testing	O
has	O
been	O
used	O
on	O
automatic	O
metric	O
scores	O
and	O
more	O
particularly	O
to	O
assess	O
whether	O
a	O
particular	O
difference	O
of	O
metric	O
scores	O
between	O
two	O
MT	O
9	O
For	O
both	O
Ja→En	O
and	O
Zh→En	O
tasks	O
,	O
systems	O
ranked	O
first	O
by	O
chrF	O
were	O
also	O
ranked	O
first	O
by	O
the	O
human	O
evaluation.10	O
Examples	O
of	O
such	O
requests	O
or	O
related	O
comments	O
by	O
reviewers	O
can	O
be	O
found	O
in	O
the	O
ACL	O
2017	O
review	O
corpus	O
(	O
github.com/allenai/PeerRead	O
)	O
,	O
e.g.	O
,	O
in	O
the	O
review	O
ID	O
369	O
we	O
read	O
:	O
"	O
I	O
am	O
also	O
rather	O
suspicious	O
of	O
the	O
fact	O
that	O
the	O
authors	O
present	O
only	O
METEOR	O
results	O
and	O
no	O
BLEU	O
.	O
"	O

This	O
was	O
also	O
observed	O
by	O
Dror	O
et	O
al	O
.	O
(	O

For	O
Ja→En	O
,	O
NiuTrans	O
system	O
is	O
significantly	O
better	O
in	O
BLEU	O
than	O
Tohoku	O
-	O
AIP	O
-	O
NTT	O
system	O
.	O

This	O
is	O
indicated	O
by	O
the	O
length	O
ratio	O
reported	O
by	O
SacreBLEU	O
but	O
rarely	O
shown	O
in	O
MT	O
papers.16	O
Wasserstein	O
et	O
al	O
.	O
(	O

Based	O
on	O
our	O
annotations	O
for	O
A4	O
,	O
we	O
counted	O
how	O
often	O
papers	O
copied	O
the	O
scores	O
from	O
previous	O
work	O
to	O
compare	O
them	O
with	O
their	O
own	O
scores	O
.	O

In	O
fact	O
,	O
Post	O
(	O
2018	O
)	O
pointed	O
out	O
that	O
most	O
papers	O
do	O
not	O
provide	O
enough	O
information	O
to	O
enable	O
the	O
comparability	O
of	O
their	O
scores	O
with	O
other	O
work	O
.	O

Ultimately	O
,	O
comparisons	O
with	O
copied	O
scores	O
must	O
be	O
avoided	O
.	O

Henceforth	O
,	O
we	O
denote	O
these	O
datasets	O
as	O
training	O
,	O
validating	O
,	O
and	O
testing	O
data	O
,	O
respective	O
to	O
these	O
three	O
steps	O
.	O

Here	O
,	O
we	O
also	O
excluded	O
comparisons	O
between	O
systems	O
performed	O
to	O
specifically	O
evaluate	O
the	O
impact	O
of	O
new	O
datasets	O
,	O
pre	O
-	O
processing	O
methods	O
,	O
and	O
human	O
intervention	O
or	O
feedback	O
(	O
e.g.	O
,	O
post	O
-	O
editing	O
and	O
interactive	O
MT	O
)	O
.	O

Consequently	O
,	O
our	O
estimation	O
can	O
be	O
considered	O
as	O
the	O
lower	O
bound	O
.	B-Doc-Begin

To	O
illustrate	O
the	O
impact	O
of	O
modifications	O
of	O
these	O
datasets	O
on	O
metric	O
scores	O
,	O
we	O
conducted	O
experiments	O
using	O
the	O
training	O
,	O
validating	O
,	O
and	O
testing	O
data	O
of	O
the	O
WMT20	B-DatasetName
news	I-DatasetName
translation	I-DatasetName
tasks	O
.	O

We	O
--type	O
transformer	O
--mini	B-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
-	O
fit	B-HyperparameterValue
--valid	B-HyperparameterName
-	I-HyperparameterName
freq	I-HyperparameterName
5000	B-HyperparameterValue
--save	B-HyperparameterName
-	I-HyperparameterName
freq	I-HyperparameterName
5000	B-HyperparameterValue
--workspace	O
10000	O
--disp	O
-	O
freq	O
500	O
--beam	B-HyperparameterName
-	I-HyperparameterName
size	I-HyperparameterName
12	B-HyperparameterValue
--normalize	B-HyperparameterName
1	B-HyperparameterValue
--valid	B-HyperparameterName
-	I-HyperparameterName
mini	I-HyperparameterName
-	I-HyperparameterName
batch	I-HyperparameterName
16	B-HyperparameterValue
--overwrite	O
--early	B-HyperparameterName
-	I-HyperparameterName
stopping	I-HyperparameterName
5	B-HyperparameterValue
--cost	B-HyperparameterName
-	I-HyperparameterName
type	I-HyperparameterName
ce	B-HyperparameterValue
-	I-HyperparameterValue
mean	I-HyperparameterValue
-	O
words	O
--valid	B-HyperparameterName
-	I-HyperparameterName
metrics	I-HyperparameterName
bleu	B-MetricName
--keep	O
-	O
best	O
--enc	B-HyperparameterName
-	I-HyperparameterName
depth	I-HyperparameterName
6	B-HyperparameterValue
--dec	B-HyperparameterName
-	I-HyperparameterName
depth	I-HyperparameterName
6	B-HyperparameterValue
--transformer	O
-	O
dropout	B-HyperparameterName
0.1	B-HyperparameterValue
--learn	B-HyperparameterName
-	I-HyperparameterName
rate	I-HyperparameterName
0.0003	B-HyperparameterValue
--lr	B-HyperparameterName
-	I-HyperparameterName
warmup	I-HyperparameterName
16000	B-HyperparameterValue
--lr	B-HyperparameterName
-	I-HyperparameterName
decay	I-HyperparameterName
-	O
inv	B-HyperparameterValue
-	I-HyperparameterValue
sqrt	I-HyperparameterValue
16000	I-HyperparameterValue
--lr	O
-	O
report	O
--label	B-HyperparameterName
-	I-HyperparameterName
smoothing	I-HyperparameterName
0.1	B-HyperparameterValue
--devices	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
--optimizer	B-HyperparameterName
-	I-HyperparameterName
params	I-HyperparameterName
0.9	B-HyperparameterValue
0.98	I-HyperparameterValue
1e-09	I-HyperparameterValue
--clip	B-HyperparameterName
-	I-HyperparameterName
norm	I-HyperparameterName
5	B-HyperparameterValue
--sync	O
-	O
sgd	O
--exponential	O
-	O
smoothing	O
--seed	B-HyperparameterName
1234	B-HyperparameterValue
4	O
,	O
on	O
all	O
the	O
provided	O
parallel	O
data	O
(	O
"	O
all	O
"	O
configurations	O
)	O
and	O
removed	O
sentence	O
pairs	O
based	O
on	O
their	O
length	O
(	O
"	B-HyperparameterName
Max	I-HyperparameterName
Len	I-HyperparameterName
.	O
"	O
)	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
we	O
observed	O
that	O
BLEU	B-MetricName
scores	O
vary	O
by	O
several	O
points	O
depending	O
on	O
the	O
maximum	O
length	O
used	O
for	O
filtering	O
.	O

While	O
it	O
is	O
rather	O
commonly	O
performed	O
by	O
participants	O
in	O
the	O
WMT	B-DatasetName
translation	O
shared	O
tasks	O
,	O
how	O
casing	O
is	O
handled	O
is	O
rarely	O
mentioned	O
in	O
research	O
papers	O
.	O

In	O
our	O
experiments	O
,	O
applying	O
this	O
step	O
changed	O
BLEU	B-MetricName
scores	O
by	O
more	O
than	O
0.5	B-MetricValue
points	O
.	O

Further	O
experiments	O
applying	O
language	O
identification	O
filtering	O
or	O
removing	O
one	O
corpus	O
from	O
the	O
training	O
data	O
also	O
lead	O
to	O
variations	O
in	O
metric	O
scores	O
.	O

The	O
best	O
configurations	O
according	O
to	O
metric	O
scores	O
do	O
not	O
use	O
truecasing	O
and	O
has	O
a	O
maximum	B-HyperparameterName
sentence	I-HyperparameterName
length	I-HyperparameterName
set	O
at	O
120	B-HyperparameterValue
(	O
second	O
row	O
)	B-Doc-Begin
.	I-Doc-Begin

If	O
not	O
,	O
original	O
case	O
of	O
the	O
data	O
is	O
kept	O
for	O
all	O
datasets	O
.	O
"	O

Last	O
row	O
denotes	O
that	O
we	O
remove	O
one	O
corpus	O
from	O
the	O
training	O
data	O
:	O
"	O
Rapid	O
"	O
for	O
En→De	O
and	O
"	O
OpenSubtitles	O
"	O
for	O
Ja→En	O
.	O

They	O
have	O
all	O
been	O
described	O
by	O
previous	O
work	O
.	O

Figure	O
5	O
shows	O
that	O
an	O
increasing	O
number	O
of	O
publications	O
accumulate	O
questionable	O
evaluation	O
practices	O
.	O

In	O
the	O
period	O
2019	O
-	O
2020	O
,	O
17.4	O
%	O
(	O
38	O
papers	O
)	O
of	O
the	O
annotated	O
papers	O
exclusively	O
relied	O
for	O
their	O
evaluation	O
on	O
differences	O
between	O
BLEU	O
scores	O
of	O
MT	O
systems	O
,	O
of	O
which	O
at	O
least	O
some	O
have	O
been	O
copied	O
from	O
different	O
papers	O
,	O
without	O
using	O
SacreBLEU	O
nor	O
statistical	O
significance	O
testing	O
,	O
while	O
exploiting	O
different	O
datasets	O
.	O

Thus	O
,	O
their	O
strict	O
adherence	O
can	O
only	O
guarantee	O
a	O
better	O
evaluation	O
but	O
not	O
a	O
flawless	O
evaluation	O
.	O

This	O
guideline	O
and	O
the	O
scoring	O
method	O
that	O
follows	O
are	O
proposed	O
for	O
MT	O
papers	O
that	O
rely	O
on	O
automatic	O
metric	O
scores	O
for	O
evaluating	O
translation	O
quality.1	O
.	O

Other	O
automatic	O
metrics	O
that	O
better	O
correlate	O
with	O
human	O
judgments	O
,	O
or	O
a	O
human	O
evaluation	O
,	O
may	O
be	O
used	O
in	O
addition	O
or	O
in	O
lieu	O
of	O
BLEU.2	O
.	O

Statistical	O
significance	O
testing	O
may	O
be	O
performed	O
on	O
automatic	O
metric	O
scores	O
to	O
ensure	O
that	O
the	O
difference	O
between	O
two	O
scores	O
,	O
whatever	O
its	O
amplitude	O
,	O
is	O
not	O
coincidental.3	O
.	O

Is	O
statistical	O
significance	O
testing	O
performed?3	O
.	O

Our	O
meta	O
-	O
evaluation	O
identified	O
pitfalls	O
in	O
the	O
MT	O
evaluation	O
in	O
most	O
of	O
the	O
annotated	O
papers	O
.	O

Despite	O
recent	O
progress	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
question	O
answering	O
models	O
remain	O
vulnerable	O
to	O
a	O
variety	O
of	O
adversarial	O
attacks	O
.	O

2015	O
)	O
have	O
been	O
driving	O
forces	O
in	O
natural	O
language	O
processing	O
research	O
.	O

Over	O
the	O
past	O
few	O
years	O
,	O
however	O
,	O
such	O
"	O
statically	O
collected	O
"	O
datasets	O
have	O
been	O
shown	O
to	O
suffer	O
from	O
various	O
problems	O
.	O

2018;Geva	O
et	O
al	O
.	O
,	O

2019;McCoy	O
et	O
al	O
.	O
,	O

Perhaps	O
around	O
85	O
per	O
cent	O
…	O
"	O
BART	O
<	O
s	O
>	O
…	O
settlement	O
of	O
Britain	O
<	O
/s	O
>	O
Old	O
English	O
was	O
not	O
…	O
<	O
/s	O
>	O
When	O
did	O
Old	O
English	O
begin	O
to	O
be	O
used	O
?	O
(	O
ii	O
)	O
answer	O
candidate	O
selection	O
and	O
filtering	O
by	O
model	O
confidence	O
(	O
an	O
example	O
retained	O
answer	O
shown	O
in	O
green	O
,	O
and	O
a	O
dropped	O
answer	O
candidate	O
in	O
red	O
)	O
;	O
(	O
iii	O
)	O
question	O
generation	O
using	O
BART	O
Large	O
;	O
and	O
(	O
iv	O
)	O
answer	O
re	O
-	O
labelling	O
using	O
self	O
-	O
training	O
.	O

2020	O
)	O
,	O
where	O
data	O
is	O
collected	O
with	O
both	O
humans	O
and	O
models	O
in	O
the	O
annotation	O
loop	O
.	O

Usually	O
,	O
these	O
humans	O
are	O
instructed	O
to	O
ask	O
adversarial	O
questions	O
that	O
fool	O
existing	O
models	O
.	O

2020	O
)	O
due	O
to	O
the	O
added	O
incentive	O
for	O
crowdworkers	O
to	O
provide	O
challenging	O
examples	O
.	O

2020;Nie	O
et	O
al	O
.	O
,	O

In	O
this	O
work	O
,	O
we	O
develop	O
a	O
synthetic	O
adversarial	O
data	O
generation	O
pipeline	O
,	O
making	O
novel	O
contributions	O
to	O
the	O
answer	O
selection	O
,	O
question	O
generation	O
,	O
and	O
filtering	O
and	O
re	O
-	O
labelling	O
tasks	O
.	O

The	O
collected	O
dataset	O
will	O
form	O
part	O
of	O
the	O
evaluation	O
for	O
a	O
new	O
round	O
of	O
the	O
Dynabench	O
QA	O
task	O
.	O

Adversar	O
-	O
ialQA	O
was	O
collected	O
by	O
asking	O
crowdworkers	O
to	O
write	O
extractive	O
question	O
-	O
answering	O
examples	O
that	O
three	O
different	O
models	O
-	O
in	O
-	O
the	O
-	O
loop	O
were	O
unable	O
to	O
answer	O
correctly	O
,	O
creating	O
the	O
D	O
BiDAF	O
,	O
D	O
BERT	O
,	O
and	O
D	O
RoBERTa	O
subsets	O
.	O

Other	O
datasets	O
for	O
question	O
answering	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O

2018;Dua	O
et	O
al	O
.	O
,	O

2019;Puri	O
et	O
al	O
.	O
,	O

Our	O
work	O
focuses	O
on	O
extractive	O
question	O
-	O
answering	O
(	O
QA	O
)	O
,	O
which	O
motivates	O
the	O
need	O
for	O
different	O
generative	O
models	O
.	O

In	O
self	O
-	O
training	O
,	O
a	O
model	O
is	O
trained	O
to	O
both	O
predict	O
correctly	O
on	O
labelled	O
examples	O
and	O
increase	O
its	O
confidence	O
on	O
unlabelled	O
examples	O
.	O

2020	O
)	O
.	O

In	O
our	O
setting	O
,	O
large	O
amounts	O
of	O
unlabelled	O
adversarial	O
-	O
style	O
questions	O
are	O
not	O
readily	O
available	O
,	O
which	O
motivates	O
our	O
use	O
of	O
a	O
question	O
generation	O
model	O
.	O

2015;Liu	O
et	O
al	O
.	O
,	O

Evaluation	O
with	O
real	O
humans	O
is	O
considered	O
beneficial	O
,	O
but	O
not	O
easily	O
scalable	O
,	O
and	O
as	O
such	O
is	O
rarely	O
conducted	O
in	O
-	O
the	O
-	O
loop	O
.	O

With	O
NLP	O
model	O
capabilities	O
ever	O
improving	O
,	O
adversarial	O
worst	O
case	O
evaluation	O
becomes	O
even	O
more	O
pertinent	O
.	O

We	O
develop	O
a	O
synthetic	O
data	O
generation	O
pipeline	O
for	O
QA	O
that	O
involves	O
four	O
stages	O
:	O
passage	O
selection	O
,	O
answer	O
candidate	O
selection	O
,	O
question	O
generation	O
,	O
and	O
synthetic	O
data	O
filtering	O
and	O
re	O
-	O
labelling	O
.	O

The	O
final	O
synthetic	O
data	O
generation	O
pipeline	O
consists	O
of:1	O
.	O

2019	O
)	O
.	O

The	O
next	O
step	O
is	O
to	O
identify	O
which	O
spans	O
of	O
text	O
within	O
the	O
passages	O
are	O
likely	O
to	O
be	O
answers	O
to	O
a	O
question	O
.	O

We	O
adapt	O
scaled	O
dot	O
-	O
product	O
attention	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O

2017	O
)	O
where	O
the	O
candidate	O
start	O
,	O
S	O
,	O
and	O
end	O
,	O
E	O
,	O
token	O
representations	O
are	O
analogous	O
to	O
the	O
projected	O
layer	O
input	O
queries	O
and	O
keys	O
.	O

While	O
answer	O
candidate	O
selection	O
only	O
requires	O
a	O
single	O
attention	O
head	O
,	O
the	O
multi	O
-	O
head	O
implementation	O
allows	O
application	O
to	O
any	O
labelling	O
task	O
requiring	O
span	O
modelling	O
with	O
overlaps	O
,	O
where	O
each	O
head	O
is	O
trained	O
to	O
predict	O
labels	O
for	O
each	O
class	O
,	O
such	O
as	O
for	O
nested	O
Named	O
Entity	O
Recognition	O
.	O

We	O
implement	O
this	O
in	O
Transformers	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
fine	O
-	O
tune	O
RoBERTa	O
Large	O
with	O
SAL	O
on	O
the	O
answer	O
selection	O
dataset	O
.	O

2020	O
)	O
autoregressive	O
sequence	O
generation	O
decoder	O
.	O

2	O
To	O
discourage	O
the	O
model	O
from	O
memorising	O
the	O
questions	O
in	O
the	O
SQuAD	O
training	O
set	O
and	O
directly	O
reproducing	O
these	O
,	O
we	O
train	O
on	O
a	O
subset	O
of	O
10k	O
examples	O
from	O
SQuAD	O
,	O
selected	O
such	O
that	O
they	O
correspond	O
to	O
the	O
same	O
source	O
passages	O
as	O
the	O
AdversarialQA	O
training	O
data	O
.	O

As	O
of	O
the	O
2014	O
episode	O
"	O
Dark	O
Water	O
,	O
"	O
it	O
was	O
revealed	O
that	O
the	O
Master	O
had	O
become	O
a	O
female	O
incarnation	O
or	O
"	O
Time	O
Lady	O
,	O
"	O
going	O
by	O
the	O
name	O
of	O
"	O
Missy	O
"	O
,	O
played	O
by	O
Michelle	O
Gomez	O
.	O

We	O
find	O
that	O
,	O
in	O
this	O
setting	O
,	O
the	O
best	O
source	O
data	O
for	O
the	O
generative	O
model	O
is	O
consistently	O
the	O
combination	O
of	O
SQuAD	O
and	O
AdversarialQA	O
.	O

We	O
also	O
note	O
that	O
using	O
only	O
synthetic	O
generated	O
data	O
,	O
we	O
can	O
achieve	O
good	O
performance	O
on	O
D	O
SQuAD	O
consistent	O
with	O
the	O
findings	O
of	O
Puri	O
et	O
al	O
.	O
(	O

This	O
suggests	O
that	O
while	O
we	O
can	O
do	O
well	O
on	O
SQuAD	O
using	O
synthetic	O
questions	O
alone	O
,	O
we	O
may	O
need	O
to	O
combine	O
the	O
synthetic	O
data	O
with	O
the	O
human	O
-	O
written	O
data	O
for	O
best	O
performance	O
in	O
the	O
more	O
challenging	O
adversarial	O
settings	O
.	B-Doc-Begin

Question	O
Diversity	O
In	O
order	O
to	O
provide	O
training	O
signal	O
diversity	O
to	O
the	O
downstream	O
QA	B-MethodName
model	O
,	O
we	O
experiment	O
with	O
a	O
range	O
of	O
decoding	O
techniques	O
(	O
see	O
Appendix	O
D	O
)	O
,	O
and	O
then	O
evaluate	O
these	O
by	O
downstream	O
performance	O
of	O
a	O
QA	B-MethodName
model	O
trained	O
on	O
the	O
questions	O
generated	O
in	O
each	O
setting	O
.	O

We	O
observe	O
minimal	O
variation	O
in	O
downstream	O
performance	O
as	O
a	O
result	O
of	O
question	O
decoding	B-HyperparameterName
strategy	I-HyperparameterName
,	O
with	O
the	O
best	O
downstream	O
results	O
obtained	O
using	O
nucleus	B-HyperparameterValue
sampling	I-HyperparameterValue
(	O
top	B-HyperparameterName
p	I-HyperparameterName
=	O
0.75	B-HyperparameterValue
)	O
.	O

However	O
,	O
we	O
also	O
obtain	O
similar	O
downstream	O
results	O
with	O
standard	O
beam	O
search	O
using	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
5	B-HyperparameterValue
.	B-Doc-Begin

To	O
mitigate	O
these	O
effects	O
,	O
we	O
explore	O
a	O
range	O
of	O
filtering	O
and	O
re	O
-	O
labelling	O
methods	O
.	O

Since	O
our	O
setup	O
is	O
designed	O
to	O
generate	O
questions	O
which	O
are	O
intentionally	O
challenging	O
for	O
the	O
QA	O
model	O
to	O
answer	O
,	O
we	O
attempt	O
to	O
exploit	O
the	O
observed	O
variation	O
in	O
model	O
behaviour	O
over	O
multiple	O
random	O
seeds	O
,	O
and	O
replace	O
the	O
single	O
QA	O
model	O
with	O
a	O
six	O
-	O
model	O
ensemble	O
.	O

Filtering	O
out	O
examples	O
that	O
are	O
not	O
roundtrip	O
-	O
consistent	O
can	O
help	O
eliminate	O
noisy	O
data	O
,	O
however	O
,	O
it	O
also	O
results	O
in	O
(	O
potentially	O
difficult	O
to	O
answer	O
)	O
questions	O
to	O
which	O
a	O
valid	O
answer	O
may	O
still	O
exist	O
being	O
unnecessarily	O
discarded	O
.	O

By	O
using	O
appropriate	O
filtering	O
of	O
the	O
synthetic	O
generated	O
data	O
,	O
combined	O
with	O
the	O
ability	O
to	O
scale	O
to	O
many	O
more	O
generated	O
examples	O
,	O
we	O
approach	O
the	O
performance	O
of	O
R	O
SQuAD+AQA	O
,	O
practically	O
matching	O
performance	O
on	O
SQuAD	O
and	O
reducing	O
the	O
performance	O
disparity	O
to	O
just	O
2.2F	O
1	O
on	O
D	O
BiDAF	O
,	O
6.6F	O
1	O
on	O
D	O
BERT	O
,	O
and	O
8.3F	O
1	O
on	O
D	O
RoBERTa	O
,	O
while	O
still	O
training	O
solely	O
on	O
synthetic	O
data	O
.	O

We	O
also	O
try	O
using	O
BART	O
to	O
both	O
select	O
answers	O
and	O
generate	O
questions	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
setting	O
.	O

R	O
SQuAD	O
:	O
using	O
the	O
SQuAD1.1	O
training	O
data.2	O
.	O

We	O
evaluate	O
the	O
final	O
models	O
on	O
AdversarialQA	O
,	O
with	O
results	O
shown	O
in	O
Table	O
7	O
.	O

We	O
find	O
that	O
synthetic	O
data	O
augmentation	O
yields	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
AdversarialQA	O
,	O
providing	O
performance	O
gains	O
of	O
2.3F	O
1	O
on	O
D	O
BiDAF	O
,	O
4.1F	O
1	O
on	O
D	O
BERT	O
,	O
and	O
4.9F	O
1	O
on	O
D	O
RoBERTa	O
over	O
the	O
baselines	O
while	O
retaining	O
good	O
performance	O
on	O
SQuAD	O
,	O
a	O
considerable	O
improvement	O
at	O
no	O
additional	O
annotation	O
cost	O
.	O

We	O
evaluate	O
domain	O
generalisation	O
of	O
our	O
final	O
models	O
on	O
the	O
MRQA	O
(	O
Fisch	O
et	O
al	O
.	O
,	O

Generative	O
models	O
could	O
also	O
help	O
guide	O
or	O
inspire	O
human	O
annotators	O
as	O
they	O
try	O
to	O
come	O
up	O
with	O
more	O
challenging	O
examples	O
.	O

Furthermore	O
,	O
while	O
our	O
work	O
focuses	O
on	O
improving	O
adversarial	O
robustness	O
,	O
this	O
approach	O
is	O
not	O
limited	O
to	O
the	O
adversarial	O
setting	O
.	O

We	O
collect	O
an	O
evaluation	O
dataset	O
as	O
a	O
part	O
of	O
the	O
adversarial	O
human	O
evaluation	O
process	O
.	O

2020	O
)	O
split	O
the	O
SQuAD1.1	O
dev	O
set	O
into	O
a	O
dev	O
and	O
test	O
set	O
,	O
with	O
passages	O
allocated	O
between	O
the	O
two	O
.	O

To	O
facilitate	O
this	O
process	O
,	O
we	O
provide	O
some	O
example	O
answer	O
candidates	O
of	O
each	O
of	O
the	O
methods	O
in	O
Table	O
11	O
.	B-Doc-Begin

In	O
order	O
to	O
provide	O
training	O
signal	O
diversity	O
to	O
the	O
downstream	O
QA	O
model	O
,	O
we	O
experiment	O
with	O
a	O
range	O
of	O
diversity	O
decoding	O
techniques	O
and	O
hyperparameters	O
.	O

Specifically	O
,	O
we	O
explore	O
standard	O
beam	O
search	O
with	O
beam_size	B-HyperparameterName
∈	O
{	O
1	B-HyperparameterValue
,	I-HyperparameterValue
3	I-HyperparameterValue
,	I-HyperparameterValue
5	I-HyperparameterValue
,	I-HyperparameterValue
10	I-HyperparameterValue
}	O
,	O
number	B-HyperparameterName
of	I-HyperparameterName
questions	I-HyperparameterName
to	I-HyperparameterName
generate	I-HyperparameterName
per	I-HyperparameterName
example	I-HyperparameterName
with	O
nbest	B-HyperparameterName
∈	O
{	O
1	B-HyperparameterValue
,	I-HyperparameterValue
3	I-HyperparameterValue
,	I-HyperparameterValue
5	I-HyperparameterValue
,	I-HyperparameterValue
10	I-HyperparameterValue
}	O
,	O
diverse	B-HyperparameterName
beam	I-HyperparameterName
search	I-HyperparameterName
with	O
beam_strength	B-HyperparameterName
∈	O
{	O
0.1	B-HyperparameterValue
,	I-HyperparameterValue
0.3	I-HyperparameterValue
,	I-HyperparameterValue
0.5	I-HyperparameterValue
,	I-HyperparameterValue
0.7	I-HyperparameterValue
,	I-HyperparameterValue
0.9	I-HyperparameterValue
,	I-HyperparameterValue
1.0	I-HyperparameterValue
}	O
,	O
and	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
with	O
top	B-HyperparameterName
p	I-HyperparameterName
∈	O
{	O
0.1	B-HyperparameterValue
,	I-HyperparameterValue
0.5	I-HyperparameterValue
,	I-HyperparameterValue
0.75}.We	I-HyperparameterValue
observe	O
minimal	O
variation	O
in	O
downstream	O
performance	O
(	O
see	O
Table	O
13	O
)	O
as	O
a	O
result	O
of	O
question	O
decoding	O
strategy	O
,	O
with	O
the	O
best	O
downstream	O
results	O
obtained	O
using	O
nucleus	B-HyperparameterName
sampling	I-HyperparameterName
(	O
top	B-HyperparameterName
p	I-HyperparameterName
=	O
0.75	B-HyperparameterValue
)	O
.	B-Doc-Begin

Our	O
findings	O
are	O
in	O
line	O
with	O
those	O
on	O
the	O
full	O
sets	O
of	O
generated	O
data	O
,	O
in	O
that	O
both	O
answer	O
candidate	O
selection	O
using	O
SAL	O
and	O
filtering	O
using	O
self	O
-	O
training	O
provide	O
considerable	O
downstream	O
benefits	O
.	O

In	O
fact	O
,	O
the	O
performance	O
gain	O
between	O
training	O
on	O
10k	O
instead	O
of	O
8k	O
examples	O
is	O
just	O
0.5F	O
1	O
on	O
the	O
overall	O
AdversarialQA	O
test	O
set	O
.	O

For	O
adversarial	O
human	O
evaluation	O
,	O
crowdworkers	O
are	O
required	O
to	O
be	O
based	O
in	O
Canada	O
,	O
the	O
UK	O
,	O
or	O
the	O
US	O
,	O
have	O
a	O
Human	O
Intelligence	O
Task	O
(	O
HIT	O
)	O
Approval	O
Rate	O
greater	O
than	O
98	O
%	O
,	O
and	O
have	O
previously	O
completed	O
at	O
least	O
1,000	O
HITs	O
.	O

Refined	O
Hindu	O
and	O
Buddhist	O
sculptures	O
reflect	O
the	O
influence	O
of	O
India	O
;	O
items	O
on	O
show	O
include	O
betel	O
-	O
nut	O
cutters	O
,	O
ivory	O
combs	O
and	O
bronze	O
palanquin	O
hooks	O
.	O

Q	O
:	O
What	O
material	O
is	O
on	O
display	O
with	O
Buddhist	O
sculptures	O
,	O
but	O
not	O
Tibetan	O
art	O
?	O
A	O
:	O
ivory	O
M	O
:	O
bronze	O
SynQAExt	O
C	O
:	O
.	O
.	O
.	O

2019	O
)	O
.	O

Evaluation	O
approaches	O
that	O
address	O
this	O
gap	O
were	O
recently	O
proposed	O
for	O
tasks	O
like	O
*	O
Work	O
done	O
during	O
an	O
internship	O
at	O
Google	O
Research	O
.	O

Yet	O
,	O
evaluating	O
grounded	O
dialogues	O
poses	O
additional	O
challenges	O
,	O
since	O
dialogue	O
outputs	O
may	O
refer	O
to	O
the	O
dialogue	O
history	O
and	O
include	O
personal	O
opinions	O
,	O
questions	O
to	O
the	O
user	O
,	O
and	O
general	O
"	O
chitchat	O
"	O
,	O
whose	O
consistency	O
with	O
external	O
knowledge	O
is	O
mostly	O
irrelevant	O
.	O

Q	O
2	O
first	O
takes	O
a	O
given	O
generated	O
response	O
as	O
input	O
,	O
and	O
generates	O
questions	O
whose	O
answers	O
are	O
informative	O
spans	O
in	O
the	O
response	O
,	O
using	O
a	O
QG	O
system	O
.	O

Q	O
2	O
reaches	O
significantly	O
higher	O
correlations	O
with	O
human	O
judgments	O
on	O
all	O
datasets	O
compared	O
to	O
the	O
other	O
metrics	O
,	O
demonstrating	O
its	O
potential	O
as	O
an	O
evaluation	O
framework	O
for	O
grounded	O
dialogue	O
generation	O
.	O

To	O
summarize	O
,	O
our	O
contributions	O
in	O
this	O
work	O
are	O
three	O
-	O
fold	O
:	O
(	O
1	O
)	O
We	O
develop	O
a	O
novel	O
framework	O
for	O
evaluating	O
the	O
factual	O
consistency	O
of	O
knowledgegrounded	O
,	O
open	O
-	O
domain	O
dialogue	O
systems	O
,	O
incorporating	O
question	O
generation	O
,	O
question	O
answering	O
and	O
NLI	O
models	O
.	O
(	O

2	O
)	O
We	O
construct	O
a	O
first	O
-	O
of	O
-	O
its	O
-	O
kind	O
dataset	O
of	O
knowledge	O
-	O
grounded	O
dialogue	O
system	O
outputs	O
manually	O
annotated	O
for	O
factual	O
consistency	O
,	O
fostering	O
future	O
work	O
on	O
the	O
subject	O
.	O
(	O

We	O
next	O
detail	O
each	O
component	O
in	O
our	O
metric	O
.	O

2	O
For	O
example	O
,	O
in	O
"	O
coffee	O
is	O
very	O
acidic	O
"	O
we	O
mark	O
'	O
coffee	O
'	O
as	O
an	O
informative	O
span	O
.	O

We	O
set	O
n	O
=	O
5	O
and	O
test	O
two	O
variants	O
of	O
generating	O
multiple	O
questions	O
.	O

To	O
increase	O
the	O
diversity	O
of	O
the	O
generated	O
questions	O
,	O
we	O
tried	O
sampling	O
-	O
based	O
methods	O
(	O
Fan	O
et	O
al	O
.	O
,	O

To	O
be	O
robust	O
to	O
lexical	O
variability	O
between	O
the	O
response	O
and	O
the	O
knowledge	O
,	O
e.g.	O
"	O
US	O
"	O
vs.	O
"	O
United	O
States	O
"	O
or	O
"	O
a	O
book	O
series	O
"	O
vs.	O
"	O
a	O
set	O
of	O
novels	O
"	O
,	O
we	O
measure	O
the	O
answer	O
span	O
similarity	O
using	O
an	O
NLI	O
model	O
.	O

2017).For	O
span	O
pairs	O
a	O
r	O
i	O
and	O
a	O
k	O
i	O
j	O
that	O
match	O
perfectly	O
at	O
the	O
token	O
-	O
level	O
,	O
we	O
assign	O
a	O
score	O
of	O
1	O
.	O

Our	O
use	O
of	O
NLI	O
differs	O
from	O
prior	O
use	O
of	O
NLI	O
in	O
dialogue	O
evaluation	O
,	O
where	O
it	O
was	O
applied	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
(	O
Welleck	O
et	O
al	O
.	O
,	O

2019;Pang	O
et	O
al	O
.	O
,	O

For	O
some	O
responses	O
,	O
no	O
valid	O
questions	O
are	O
generated	O
-i.e	O
.	O

Mehri	O
and	O
Eskenazi	O
(	O
2020	O
)	O
introduced	O
USR	O
,	O
an	O
evaluation	O
metric	O
that	O
measures	O
different	O
aspects	O
required	O
from	O
dialogue	O
systems	O
.	O

Using	O
this	O
dataset	O
,	O
we	O
test	O
whether	O
Q	O
2	O
can	O
measure	O
consistency	O
when	O
the	O
grounding	O
"	O
knowledge	O
"	O
is	O
a	O
persona	O
sentence	O
or	O
the	O
previous	O
dialogue	O
history	O
.	O

2017	O
)	O
.	B-Doc-Begin

For	O
both	O
systems	O
,	O
we	O
used	O
beam	B-HyperparameterValue
search	I-HyperparameterValue
decoding	I-HyperparameterValue
with	O
a	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
10	B-HyperparameterValue
,	O
a	O
beam	B-HyperparameterName
block	I-HyperparameterName
size	I-HyperparameterName
of	O
3	B-HyperparameterValue
and	O
a	O
context	B-HyperparameterName
block	I-HyperparameterName
size	I-HyperparameterName
of	O
3	B-HyperparameterValue
to	O
generate	O
responses	O
.	O

The	O
annotators	O
went	O
through	O
the	O
responses	O
until	O
150	B-HyperparameterValue
examples	B-HyperparameterName
of	O
factually	O
inconsistent	O
responses	O
were	O
annotated	O
for	O
each	O
system	O
(	O
300	O
in	O
total	O
)	O
,	O
and	O
then	O
repeated	O
the	O
process	O
and	O
annotated	O
the	O
same	O
number	O
of	O
factually	O
consistent	O
responses	O
.	B-Doc-Begin

Table	O
2	O
presents	O
the	O
Q	O
2	O
score	O
for	O
the	O
different	O
sets	O
of	O
annotated	O
system	O
responses	O
,	O
as	O
well	O
as	O
for	O
150	O
randomly	O
selected	O
system	O
responses	O
.	O

Assessing	O
answer	O
similarity	O
using	O
NLI	O
results	O
in	O
higher	O
absolute	O
scores	O
for	O
both	O
inconsistent	O
and	O
consistent	O
responses	O
,	O
and	O
by	O
a	O
larger	O
margin	O
for	O
the	O
latter	O
.	O

To	O
find	O
if	O
Q	O
2	O
can	O
be	O
used	O
to	O
automatically	O
separate	O
between	O
consistent	O
and	O
inconsistent	O
responses	O
at	O
the	O
more	O
granular	O
,	O
single	O
response	O
level	O
,	O
we	O
report	O
in	O
Figure	O
3	O
the	O
Precision	O
/	O
Recall	O
curve	O
of	O
consistent	O
responses	O
for	O
various	O
response	O
-	O
level	O
score	O
thresholds	O
for	O
each	O
evaluated	O
metric	O
on	O
the	O
WOW	O
annotated	O
data	O
.	O

The	O
results	O
are	O
detailed	O
in	O
BERTScore	O
,	O
and	O
BLEU	O
obtain	O
lower	O
correlations	O
of	O
0.9216	O
,	O
0.878	O
,	O
0.8467	O
and	O
0.3051	O
,	O
respectively	O
.	O

Table	O
5	O
presents	O
their	O
reported	O
correlation	O
results	O
for	O
the	O
"	O
Uses	O
Knowledge	O
"	O
category	O
,	O
as	O
well	O
as	O
the	O
correlation	O
of	O
Q	O
2	O
with	O
the	O
same	O
human	O
judgments	O
.	O

We	O
also	O
show	O
results	O
from	O
zero	O
-	O
shot	O
methods	O
reported	O
in	O
Welleck	O
et	O
al	O
.	O
(	O

The	O
results	O
on	O
the	O
three	O
datasets	O
demonstrate	O
Q	O
2	O
's	O
zero	O
-	O
shot	O
,	O
reference	O
-	O
response	O
-	O
free	O
capability	O
to	O
generalize	O
to	O
various	O
dialogue	O
tasks	O
that	O
require	O
evaluation	O
of	O
factual	O
consistency	O
.	O

The	O
performance	O
of	O
Q	O
2	O
depends	O
on	O
the	O
different	O
components	O
used	O
throughout	O
the	O
pipeline	O
,	O
i.e.	O
,	O
the	O
QG	O
,	O
QA	O
,	O
and	O
NLI	O
models	O
.	O

To	O
demonstrate	O
that	O
Q	O
2	O
is	O
robust	O
to	O
the	O
quality	O
of	O
these	O
models	O
,	O
we	O
exper	O
-	O
Avg	O
.	O

Table	O
8	O
presents	O
the	O
absolute	O
scores	O
of	O
the	O
smaller	O
models	O
on	O
the	O
WOW	O
dataset	O
,	O
as	O
well	O
as	O
each	O
variant	O
's	O
question	O
coverage	O
,	O
defined	O
as	O
the	O
percentage	O
of	O
responses	O
for	O
which	O
Q	O
2	O
generated	O
at	O
least	O
one	O
valid	O
question	O
,	O
not	O
resorting	O
to	O
the	O
end	O
-	O
to	O
-	O
end	O
NLI	O
fallback	O
.	O

Surprisingly	O
,	O
using	O
a	O
smaller	O
QA	O
model	O
had	O
the	O
opposite	O
outcome	O
-higher	O
Q	O
2	O
scores	O
in	O
all	O
cases	O
.	O

Regarding	O
domain	O
robustness	O
of	O
the	O
undelying	O
models	O
,	O
while	O
the	O
QG	O
and	O
QA	O
models	O
were	O
trained	O
on	O
a	O
dataset	O
collected	O
from	O
Wikipedia	O
and	O
are	O
therefore	O
suited	O
for	O
WOW	O
's	O
domain	O
,	O
these	O
models	O
work	O
well	O
even	O
when	O
the	O
grounding	O
source	O
is	O
not	O
Wikipedia	O
.	O

Other	O
than	O
the	O
final	O
score	O
,	O
Q	O
2	O
outputs	O
the	O
generated	O
questions	O
,	O
the	O
response	O
-	O
based	O
answer	O
spans	O
and	O
the	O
answers	O
the	O
QA	O
model	O
predicted	O
based	O
on	O
the	O
knowledge	O
,	O
which	O
can	O
be	O
used	O
as	O
an	O
explanation	O
to	O
the	O
assigned	O
score	O
or	O
to	O
highlight	O
the	O
potentially	O
inconsistent	O
text	O
spans	O
in	O
the	O
response	O
.	O

2002	O
)	O
or	O
METEOR	O
(	O
Banerjee	O
and	O
Lavie	O
,	O
2005	O
)	O
in	O
machine	O
translation	O
,	O
or	O
ROUGE	O
(	O
Lin	O
,	O
2004	O
)	O
in	O
summarization	O
,	O
were	O
shown	O
to	O
have	O
weak	O
or	O
no	O
correlation	O
with	O
human	O
judgements	O
for	O
dialogue	O
Lowe	O
et	O
al	O
.	O
,	O

Supervised	O
assessment	O
methods	O
learn	O
to	O
predict	O
human	O
-	O
like	O
evaluation	O
scores	O
(	O
Lowe	O
et	O
al	O
.	O
,	O

Recently	O
,	O
Mehri	O
and	O
Eskenazi	O
(	O
2020	O
)	O
and	O
Pang	O
et	O
al	O
.	O
(	O

2020).Concurrently	O
with	O
our	O
work	O
,	O
Dziri	O
et	O
al	O
.	O
(	O

2021	O
)	O
introduced	O
the	O
Benchmark	O
for	O
Evaluation	O
of	O
Grounded	O
INteraction	O
(	O
BEGIN	O
)	O
.	O

For	O
example	O
,	O
Eyal	O
et	O
al	O
.	O
(	O

2020	O
)	O
and	O
suggested	O
using	O
QG	O
and	O
QA	O
to	O
identify	O
factual	O
inconsistencies	O
in	O
abstractive	O
summaries	O
,	O
which	O
is	O
more	O
Precisionoriented	O
.	O

2021	O
)	O
suggested	O
QuestEval	O
,	O
which	O
combines	O
the	O
Recall	O
and	O
Precision	O
oriented	O
QG	O
and	O
QA	O
approaches	O
,	O
obtaining	O
a	O
more	O
robust	O
metric	O
for	O
evaluating	O
abstractive	O
summaries	O
which	O
was	O
adopted	O
in	O
the	O
GEM	O
shared	O
task	O
(	O
Bosselut	O
et	O
al	O
.	O
,	O

This	O
measure	O
reflects	O
the	O
answerability	O
independently	O
of	O
the	O
way	O
the	O
answer	O
is	O
expressed	O
,	O
but	O
does	O
not	O
take	O
into	O
account	O
possible	O
model	O
hallucinations	O
,	O
and	O
it	O
is	O
therefore	O
only	O
applied	O
for	O
the	O
Recall	O
-	O
based	O
component	O
.	O

Comparing	O
to	O
other	O
automatic	O
evaluation	O
methods	O
of	O
abstractive	O
summaries	O
,	O
the	O
QG	O
-	O
QA	O
based	O
methods	O
showed	O
higher	O
correlations	O
with	O
human	O
judgments	O
of	O
factual	O
consistency	O
.	O

We	O
presented	O
Q	O
2	O
,	O
an	O
automatic	O
evaluation	O
method	O
for	O
factual	O
consistency	O
in	O
knowledge	O
grounded	O
dialogue	O
.	O

Table	O
9	O
presents	O
the	O
results	O
of	O
two	O
ablations	O
studies	O
on	O
Q	O
2	O
.	O

First	O
,	O
we	O
experiment	O
with	O
a	O
different	O
decoding	O
strategy	O
for	O
generating	O
questions	O
.	O

Next	O
,	O
we	O
additionally	O
drop	O
the	O
filtration	O
of	O
questions	O
relating	O
to	O
personal	O
statements	O
and	O
opinionated	O
parts	O
of	O
the	O
response	O
.	O

While	O
the	O
top	O
-	O
n	O
decoding	O
seems	O
to	O
be	O
ineffective	O
in	O
terms	O
of	O
separating	O
consistent	O
responses	O
from	O
inconsistent	O
responses	O
,	O
it	O
is	O
effective	O
for	O
improving	O
the	O
question	O
coverage	O
of	O
Q	O
2	O
.Filtering	O
Questions	O
Relating	O
to	O
Personal	O
Statements	O
.	O

We	O
ran	O
each	O
experiment	O
on	O
4	O
CPUs	O
.	O

In	O
future	O
work	O
,	O
we	O
plan	O
to	O
design	O
a	O
more	O
efficient	O
version	O
of	O
Q	O
2	O
.	O

In	O
the	O
second	O
,	O
we	O
randomly	O
select	O
knowledge	O
from	O
a	O
different	O
dialogue	O
.	O

Similar	O
results	O
were	O
obtained	O
for	O
the	O
MemNet	O
system	O
.	O

Note	O
that	O
these	O
changes	O
may	O
be	O
subtle	O
.	O

Ignore	O
your	O
background	O
knowledge	O
and	O
focus	O
on	O
the	O
information	O
provided	O
to	O
the	O
bot	O
.	O

If	O
not	O
,	O
ignore	O
the	O
response.2	O
.	O

Or	O
Honovich	O
was	O
partially	O
supported	O
by	O
a	O
fellowship	O
from	O
the	O
Hebrew	O
University	O
Center	O
for	O
Interdisciplinary	O
Data	O
Science	O
Research	O
.	O

In	O
this	O
paper	O
,	O
we	O
redefine	O
every	O
subtask	O
target	O
as	O
a	O
sequence	O
mixed	O
by	O
pointer	O
indexes	O
and	O
sentiment	O
class	O
indexes	O
,	O
which	O
converts	O
all	O
ABSA	O
subtasks	O
into	O
a	O
unified	O
generative	O
formulation	O
.	O

Aspect	O
-	O
based	O
Sentiment	O
Analysis	O
(	O
ABSA	O
)	O
is	O
the	O
fine	O
-	O
grained	O
Sentiment	O
Analysis	O
(	O
SA	O
)	O
task	O
,	O
which	O
aims	O
to	O
identify	O
the	O
aspect	O
term	O
(	O
a	O
)	O
,	O
its	O
corresponding	O
sentiment	O
polarity	O
(	O
s	O
)	O
,	O
and	O
the	O
opinion	O
term	O
(	O
o	O
)	O
.	O

We	O
summarize	O
these	O
subtasks	O
in	O
Figure	O
1	O
.	O

2021	O
)	O
apply	O
the	O
pipeline	O
model	O
to	O
output	O
the	O
a	O
,	O
s	O
,	O
o	O
from	O
the	O
inside	O
sub	O
-	O
models	O
separately	O
.	O

Specifically	O
,	O
we	O
model	O
the	O
extraction	O
and	O
classification	O
tasks	O
as	O
the	O
pointer	O
indexes	O
and	O
class	O
indexes	O
generation	O
,	O
respectively	O
.	O

In	O
this	O
section	O
,	O
we	O
first	O
review	O
the	O
existing	O
studies	O
on	O
single	O
output	O
subtasks	O
,	O
and	O
then	O
turn	O
to	O
studies	O
focusing	O
on	O
the	O
compound	O
output	O
subtasks	O
.	O

2017	O
)	O
;	O
Tay	O
et	O
al	O
.	O
(	O

2019	O
)	O
and	O
they	O
propose	O
the	O
datasets	O
for	O
this	O
subtask	O
.	O

Most	O
studies	O
apply	O
sequence	O
tagging	O
method	O
for	O
this	O
subtask	O
(	O
Wu	O
et	O
al	O
.	O
,	O

2019;Chen	O
and	O
Qian	O
,	O
2020	O
)	O
to	O
avoid	O
the	O
error	O
-	O
propagation	O
problem	O
(	O
Ma	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
which	O
can	O
tackle	O
the	O
sentiment	O
inconsistency	O
problem	O
in	O
the	O
unified	O
tagging	O
schema	O
.	O

2001	O
)	O
and	O
Semi	O
-	O
Markov	O
CRF	O
(	O
Sarawagi	O
and	O
Cohen	O
,	O
2004	O
)	O
.	O

2018;Devlin	O
et	O
al	O
.	O
,	O

2020	O
)	O
as	O
our	O
backbone	O
,	O
while	O
the	O
other	O
sequence	O
-	O
to	O
-	O
sequence	O
pre	O
-	O
training	O
models	O
can	O
also	O
be	O
applied	O
in	O
our	O
architecture	O
to	O
use	O
the	O
pointer	O
mechanism	O
(	O
Vinyals	O
et	O
al	O
.	O
,	O

2015	O
)	O
,	O
such	O
as	O
MASS	O
.BART	O
is	O
a	O
strong	O
sequence	O
-	O
to	O
-	O
sequence	O
pretrained	O
model	O
for	O
Natural	O
Language	O
Generation	O
(	O
NLG	O
)	O
.	O

In	O
this	O
part	O
,	O
we	O
first	O
introduce	O
our	O
sequential	O
representation	O
for	O
each	O
ABSA	O
subtask	O
.	O

As	O
depicted	O
in	O
Figure	O
1	O
,	O
there	O
are	O
two	O
types	O
of	O
tasks	O
,	O
namely	O
the	O
extraction	O
and	O
classification	O
,	O
whose	O
target	O
can	O
be	O
represented	O
as	O
a	O
sequence	O
of	O
pointer	O
indexes	O
and	O
class	O
indexes	O
,	O
respectively	O
.	O

We	O
use	O
a	O
,	O
s	O
,	O
o	O
,	O
to	O
represent	O
the	O
aspect	O
term	O
,	O
sentiment	O
polarity	O
,	O
and	O
opinion	O
term	O
,	O
respectively	O
.	O

Encoder	O
The	O
encoder	O
part	O
is	O
to	O
encode	O
X	O
into	O
vectors	O
H	O
e	O
.	O

We	O
ignore	O
the	O
<	O
s	O
>	O
token	O
in	O
our	O
equations	O
for	O
simplicity	O
.	O

Decoder	O
The	O
decoder	O
part	O
takes	O
the	O
encoder	O
outputs	O
H	O
e	O
and	O
previous	O
decoder	O
outputs	O
Y	O
<	O
t	O
as	O
inputs	O
to	O
get	O
P	O
t	O
.	O

We	O
use	O
"	O
-	O
"	O
to	O
denote	O
the	O
missing	O
data	O
statistics	O
of	O
some	O
datasets	O
.	O

P	O
t	O
∈	O
R	O
(	O
During	O
the	O
training	O
phase	O
,	O
we	O
use	O
the	O
teacher	O
forcing	O
to	O
train	O
our	O
model	O
and	O
the	O
negative	O
loglikelihood	O
to	O
optimize	O
the	O
model	O
.	O

y	O
i	O
∈	O
[	O
1	O
,	O
n	O
+	O
|C|	O
]	O
Output	O
:	O
Target	O
span	O
set	O
L	O
=	O
{	O
(	O
a	O
s	O
1	O
,	O
a	O
e	O
1	O
,	O
o	O
s	O
1	O
,	O
o	O
e	O
1	O
,	O
s	O
1	O
)	O
,	O
...	O
,	O
(	O
a	O
s	O
i	O
,	O
a	O
e	O
i	O
,	O
o	O
s	O
i	O
,	O
o	O
e	O
i	O
,	O
s	O
i	O
)	O
,	O
...	O
}	O
1	O
:	O
L	O
=	O
{	O
}	O
,	O
e	O
=	O
[	O
]	O
,	O
i	O
=	O
1	O
2	O
:	O
while	O
i	O
<	O
=	O
m	O
do	O
3	O
:	O
y	O
i	O
=	O
Y	O
[	O
i	O
]	O
4	O
:	O
if	O
y	O
i	O
>	O
n	O
then	O
We	O
evaluate	O
our	O
method	O
on	O
four	O
ABSA	O
datasets	O
.	O

The	O
fourth	O
dataset(D	O
20b	O
)	O
from	O
is	O
the	O
revised	O
variant	O
of	O
Peng	O
et	O
al	O
.	O
(	O

Given	O
different	O
ABSA	O
subtasks	O
,	O
datasets	O
,	O
and	O
experimental	O
setups	O
,	O
existing	O
baselines	O
can	O
be	O
separated	O
into	O
three	O
groups	O
roughly	O
as	O
shown	O
in	O
Table	O
2	O
.	O

2020).The	O
baselines	O
of	O
the	O
third	O
group	O
are	O
mainly	O
conducted	O
on	O
D	O
20a	O
and	O
D	O
20b	O
datasets	O
,	O
which	O
could	O
cover	O
almost	O
all	O
the	O
ABSA	O
subtasks	O
except	O
for	O
one	O
certain	O
subtask	O
depending	O
on	O
the	O
baseline	O
structures	O
.	O

For	O
the	O
ALSC	O
subtask	O
,	O
we	O
require	O
the	O
generated	O
sentiment	O
polarity	O
of	O
the	O
given	O
aspect	O
should	O
be	O
the	O
same	O
as	O
the	O
ground	O
truth	O
.	O

We	O
report	O
the	O
precision	O
(	O
P	O
)	O
,	O
recall	O
(	O
R	O
)	O
,	O
and	O
F1	O
scores	O
for	O
all	O
experiments	O
6	O
.	O

The	O
baseline	O
results	O
with	O
"	O
†	O
"	O
are	O
retrieved	O
from	O
Mao	O
et	O
al	O
.	O
(	O

2021	O
)	O
,	O
and	O
result	O
with	O
"	O
"	O
is	O
from	O
.	O

Additionally	O
,	O
we	O
notice	O
that	O
our	O
F1	O
score	O
on	O
14lap	O
is	O
close	O
to	O
the	O
previous	O
SOTA	O
result	O
.	O

Invalid	O
size	O
0.48	O
%	O
0.77	O
%	O
1.41	O
%	O
1.40%Invalid	O
order	O
1.75	O
%	O
3.70	O
%	O
3.26	O
%	O
3.26	O
%	O
Invalid	O
token	O
0.48	O
%	O
0.78	O
%	O
1.02	O
%	O
1.02	O
%	O
This	O
paper	O
summarizes	O
the	O
seven	O
ABSA	O
subtasks	O
and	O
previous	O
studies	O
,	O
which	O
shows	O
that	O
there	O
exist	O
divergences	O
on	O
all	O
the	O
input	O
,	O
output	O
,	O
and	O
task	O
type	O
sides	O
.	O

The	O
experimental	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
method	O
.	O

Our	O
work	O
leads	O
to	O
several	O
promising	O
directions	O
,	O
such	O
as	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
on	O
other	O
tasks	O
,	O
and	O
data	O
augmentation	O
.	O

62022027	O
)	O
.	O

It	O
does	O
not	O
harm	O
anyone.(4	O
)	O
Our	O
experiments	O
do	O
not	O
need	O
a	O
lot	O
of	O
computer	O
resources	O
compared	O
to	O
pre	O
-	O
trained	O
models.(5	O
)	O
We	O
will	O
open	O
source	O
all	O
our	O
code	B-Doc-Begin
.	I-Doc-Begin

A	O
Supplemental	O
Material	O
We	O
use	O
the	O
triangular	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
warmup	I-HyperparameterName
.	O

The	O
number	O
of	O
parameters	O
is	O
as	O
follows:•	O
BART	B-MethodName
-	I-MethodName
Base	I-MethodName
model	O
:	O
12	B-HyperparameterValue
layers	B-HyperparameterName
,	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
dimensions	I-HyperparameterName
and	O
16	B-HyperparameterValue
heads	B-HyperparameterName
with	O
the	O
total	O
number	O
of	O
parameters	O
,	O
139M;•	O
BERT	O
-	O
Base	O
model	O
:	O
12	B-HyperparameterValue
layers	B-HyperparameterName
,	O
768	B-HyperparameterValue
hidden	B-HyperparameterName
dimensions	I-HyperparameterName
and	O
12	B-HyperparameterValue
heads	B-HyperparameterName
with	O
the	O
total	O
number	O
of	O
parameters	O
,	O
110M.	B-Doc-Begin
In	O
this	O
part	O
,	O
we	O
introduce	O
the	O
decoding	O
algorithm	O
we	O
used	O
to	O
convert	O
the	O
predicted	O
target	O
sequence	O
Y	O
into	O
the	O
target	O
span	O
set	O
L.	O
These	O
algorithm	O
can	O
be	O
found	O
in	O
Algorithm	O
2	O
,	O
3	O
,	O
4	O
.	O

To	O
that	O
end	O
,	O
we	O
feed	O
the	O
pre	O
-	O
defined	O
task	O
tags	O
"	O
<	O
AESC	O
>	O
"	O
and	O
"	O
<	O
OE	O
>	O
"	O
to	O
the	O
decoder	O
first	O
.	O

For	O
example	O
,	O
for	O
the	O
input	O
"	O
The	O
drinks	O
are	O
always	O
:	O
:	O
:	O
:	O
well	O
:	O
:	O
:	O
:	O
:	O
made	O
and	O
wine	O
selection	O
is	O
:	O
:	O
:	O
:	O
:	O
fairly	O
:	O
:	O
:	O
:	O
:	O
:	O
priced	O
"	O
from	O
D	O
17	O
dataset	O
,	O
we	O
i+	O
=	O
1	O
10	O
:	O
end	O
while	O
11	O
:	O
return	O
L	O
define	O
the	O
AESC	O
sequence	O
and	O
the	O
OE	O
target	O
sequence	O
as	O
"	O
<	O
AESC	O
>	O
,	O
1	O
,	O
1	O
,	O
POS	O
,	O
7	O
,	O
8	O
,	O
POS	O
,	O
<	O
/s	O
>	O
"	O
and	O
"	O
<	O
OE	O
>	O
,	O
4	O
,	O
5	O
,	O
10	O
,	O
11	O
,	O
<	O
/s>".•	O
On	O
the	O
D	O
19	O
dataset	O
,	O
we	O
conduct	O
the	O
AOE	O
.	O

As	O
the	O
AOE	O
subtask	O
requires	O
to	O
detect	O
the	O
opinion	O
terms	O
given	O
aspect	O
terms	O
in	O
advance	O
,	O
the	O
aspect	O
terms	O
need	O
to	O
be	O
fed	O
to	O
our	O
decoder	O
first	O
.	O

We	O
publicly	O
release	O
PhoNLP	O
as	O
an	O
open	O
-	O
source	O
toolkit	O
under	O
the	O
Apache	O
License	O
2.0	O
.	O

Our	O
PhoNLP	O
is	O
available	O
at	O
https://github	O
.	O

It	O
has	O
been	O
boosted	O
by	O
the	O
success	O
of	O
the	O
national	O
project	O
on	O
Vietnamese	O
language	O
and	O
speech	O
processing	O
(	O
VLSP	O
)	O
KC01.01/2006	O
-	O
2010	O
and	O
VLSP	O
workshops	O
that	O
have	O
run	O
shared	O
tasks	O
since	O
2013	O
.	O

2016	O
)	O
,	O
sentiment	O
analysis	O
(	O
Bang	O
and	O
Sornlertlamvanich	O
,	O
2018	O
)	O
,	O
relation	O
extraction	O
(	O
To	O
and	O
Do	O
,	O
2020	O
)	O
,	O
semantic	O
parsing	O
,	O
open	O
information	O
extraction	O
(	O
Truong	O
et	O
al	O
.	O
,	O

Vietnamese	O
POS	O
tagging	O
,	O
NER	O
and	O
dependency	O
parsing	O
.	O

However	O
,	O
VnCoreNLP	O
is	O
now	O
no	O
longer	O
considered	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
because	O
its	O
performance	O
results	O
are	O
significantly	O
outperformed	O
by	O
ones	O
obtained	O
when	O
fine	O
-	O
tuning	O
PhoBERT	O
-	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
monolingual	O
pre	O
-	O
trained	O
language	O
model	O
for	O
Vietnamese	O
.	O

In	O
addition	O
,	O
POS	O
tagging	O
,	O
NER	O
and	O
dependency	O
parsing	O
are	O
related	O
tasks	O
:	O
POS	O
tags	O
are	O
essential	O
input	O
features	O
used	O
for	O
dependency	O
parsing	O
and	O
POS	O
tags	O
are	O
also	O
used	O
as	O
additional	O
features	O
for	O
NER	O
.	O

These	O
contextualized	O
word	O
embeddings	O
are	O
fed	O
into	O
a	O
POS	O
tagging	O
layer	O
that	O
is	O
in	O
fact	O
a	O
linear	O
prediction	O
layer	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2001	O
)	O
to	O
predict	O
NER	O
labels	O
for	O
the	O
input	O
words	O
,	O
while	O
the	O
dependency	O
parsing	O
layer	O
uses	O
a	O
Biaffine	O
classifier	O
(	O
Dozat	O
and	O
Manning	O
,	O
2017	O
)	O
to	O
predict	O
dependency	O
arcs	O
between	O
the	O
words	O
and	O
another	O
Biaffine	O
classifier	O
to	O
label	O
the	O
predicted	O
arcs	O
.	O

Figure	O
1	O
illustrates	O
our	O
PhoNLP	O
architecture	O
that	O
can	O
be	O
viewed	O
as	O
a	O
mixture	O
of	O
a	O
BERT	O
-	O
based	O
encoding	O
layer	O
and	O
three	O
decoding	O
layers	O
of	O
POS	O
tagging	O
,	O
NER	O
and	O
dependency	O
parsing	O
.	O

Given	O
an	O
input	O
sentence	O
consisting	O
of	O
n	O
word	O
tokens	O
w	O
1	O
,	O
w	O
2	O
,	O
...	O
,	O
w	O
n	O
,	O
the	O
encoding	O
layer	O
employs	O
PhoBERT	O
to	O
generate	O
contextualized	O
latent	O
feature	O
embeddings	O
e	O
i	O
each	O
representing	O
the	O
i	O
th	O
word	O
w	O
i	O
:	O
e	O
i	O
=	O
PhoBERT	O
base	O
w	O
1	O
:	O
n	O
,	O
i(1)In	O
particular	O
,	O
the	O
encoding	O
layer	O
employs	O
the	O
PhoBERT	O
base	O
version	O
.	O

Because	O
PhoBERT	O
uses	O
BPE	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
the	O
POS	O
tagging	O
layer	O
is	O
a	O
linear	O
prediction	O
layer	O
that	O
is	O
appended	O
on	O
top	O
of	O
the	O
encoder	O
.	O

The	O
NER	O
layer	O
creates	O
a	O
sequence	O
of	O
vectors	O
v	O
1	O
:	O
n	O
in	O
which	O
each	O
v	O
i	O
is	O
resulted	O
in	O
by	O
concatenating	O
the	O
contextualized	O
word	O
embedding	O
e	O
i	O
and	O
a	O
"	O
soft	O
"	O
POS	O
tag	O
embedding	O
t(1)i	O
:	O
v	O
i	O
=	O
e	O
i	O
•	O
t	O
(	O
1	O
)	O
i(3)where	O
following	O
Hashimoto	O
et	O
al	O
.	O
(	O

The	O
parsing	O
layer	O
also	O
uses	O
another	O
Biaffine	O
classifier	O
to	O
label	O
the	O
predicted	O
arcs	O
,	O
based	O
on	O
input	O
vectors	O
h(L	O
-	O
H	O
)	O
i	O
and	O
h	O
(	O
L	O
-	O
D	O
)	O
j.	O
An	O
objective	O
loss	O
L	O
DEP	O
is	O
computed	O
by	O
summing	O
a	O
cross	O
entropy	O
loss	O
for	O
unlabeled	O
dependency	O
parsing	O
and	O
another	O
cross	O
entropy	O
loss	O
for	O
dependency	O
label	O
prediction	O
during	O
training	O
based	O
on	O
gold	O
arcs	O
and	O
arc	O
labels	O
.	O

Kondratyuk	O
and	O
Straka	O
(	O
2019	O
)	O
L	O
=	O
λ	O
1	O
L	O
POS	O
+	O
λ	O
2	O
L	O
NER	O
+	O
(	O
1−λ	O
1	O
−λ	O
2	O
)	O
L	O
DEP	O
(	O
To	O
conduct	O
experiments	O
,	O
we	O
use	O
the	O
benchmark	O
datasets	O
of	O
the	O
VLSP	O
2013	O
POS	O
tagging	O
dataset	O
,	O
3	O
the	O
VLSP	O
2016	O
NER	O
dataset	O
and	O
the	O
VnDT	O
dependency	O
treebank	O
v1.1	O
Nguyen	O
et	O
al	O
.	O
(	O

2018	O
)	O
.	O

To	O
handle	O
the	O
data	O
leakage	O
issue	O
,	O
we	O
have	O
to	O
re	O
-	O
split	O
the	O
VLSP	O
2013	O
POS	O
tagging	O
dataset	O
to	O
avoid	O
the	O
data	O
leakage	O
issue	O
:	O
The	O
POS	O
tagging	O
validation	O
/	O
test	O
set	O
now	O
only	O
contains	O
sentences	O
that	O
appear	O
in	O
the	O
union	O
of	O
the	O
NER	O
and	O
dependency	O
parsing	O
validation	O
/	O
test	O
sets	O
(	O
i.e.	O
the	O
validation	O
/	O
test	O
sentences	O
for	O
NER	O
and	O
dependency	O
parsing	O
only	O
appear	O
in	O
the	O
POS	O
tagging	O
validation	O
/	O
test	O
set	O
)	O
.	O

Table	O
1	O
details	O
the	O
statistics	O
of	O
the	O
experimental	O
datasets.3	B-Doc-Begin
https://vlsp.org.vn/vlsp2013/eval	O
PhoNLP	B-MethodName
is	O
implemented	O
based	O
on	O
PyTorch	O
(	O
Paszke	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
employing	O
the	O
PhoBERT	B-MethodName
encoder	O
implementation	O
available	O
from	O
the	O
transformers	O
library	O
(	O
Wolf	O
et	O
al	O
.	O
,	O

2020	O
)	O
and	O
the	O
Biaffine	B-MethodName
classifier	O
implementation	O
from	O
Qi	O
et	O
al	O
.	O
(	O

2020	O
)	O
.	O

We	O
set	O
both	O
the	O
label	O
weight	O
matrices	O
W	O
(	O
1	O
)	O
and	O
W	O
(	O
2	O
)	O
to	O
have	O
100	B-HyperparameterValue
rows	B-HyperparameterName
,	O
resulting	O
in	O
100	O
-	O
dimensional	O
soft	O
POS	O
tag	O
embeddings	O
.	O

We	O
use	O
the	O
AdamW	B-HyperparameterValue
optimizer	B-HyperparameterName
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
and	O
a	O
fixed	O
batch	B-HyperparameterName
size	I-HyperparameterName
at	O
32	B-HyperparameterValue
,	O
and	O
train	O
for	O
40	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

The	O
sizes	O
of	O
training	O
sets	O
are	O
different	O
,	O
in	O
which	O
the	O
POS	O
tagging	O
training	O
set	O
is	O
the	O
largest	O
,	O
consisting	O
of	O
23906	B-HyperparameterValue
sentences	B-HyperparameterName
.	O

Thus	O
for	O
each	O
training	O
epoch	O
,	O
we	O
repeatedly	O
sample	O
from	O
the	O
NER	B-DatasetName
and	O
dependency	B-DatasetName
parsing	I-DatasetName
training	O
sets	O
to	O
fill	O
the	O
gaps	O
between	O
the	O
training	O
set	O
sizes	O
.	O

We	O
perform	O
a	O
grid	O
search	O
to	O
select	O
the	O
initial	O
AdamW	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
λ	B-HyperparameterName
1	I-HyperparameterName
and	O
λ	B-HyperparameterName
2	I-HyperparameterName
.	O

We	O
find	O
the	O
optimal	O
initial	O
AdamW	B-HyperparameterValue
learning	B-HyperparameterName
rate	I-HyperparameterName
,	O
λ	B-HyperparameterName
1	I-HyperparameterName
and	O
λ	B-HyperparameterName
2	I-HyperparameterName
at	O
1e-5	B-HyperparameterValue
,	O
0.4	B-HyperparameterValue
and	O
0.2	B-HyperparameterValue
,	O
respectively	O
.	O

Here	O
,	O
we	O
compute	O
the	O
average	B-MetricName
of	I-MetricName
the	I-MetricName
POS	I-MetricName
tagging	I-MetricName
accuracy	I-MetricName
,	O
NER	B-MetricName
F	I-MetricName
1	I-MetricName
-score	O
and	O
dependency	B-MetricName
parsing	I-MetricName
score	O
LAS	O
after	O
each	O
training	O
epoch	O
on	O
the	O
validation	O
sets	O
.	O

We	O
select	O
the	O
model	O
checkpoint	O
that	O
produces	O
the	O
highest	O
average	O
score	O
over	O
the	O
validation	O
sets	O
to	O
apply	O
to	O
the	O
test	O
sets	O
.	O

Table	B-Doc-Begin
2	O
presents	O
results	O
obtained	O
for	O
our	O
PhoNLP	O
and	O
compares	O
them	O
with	O
those	O
of	O
a	O
baseline	O
approach	O
of	O
single	O
-	O
task	O
training	O
.	O

ii	O
)	O
For	O
NER	O
,	O
instead	O
of	O
a	O
linear	O
prediction	O
layer	O
,	O
we	O
append	O
a	O
CRF	O
prediction	O
layer	O
on	O
top	O
of	O
PhoBERT	O
.	O
(	O

2018	O
)	O
,	O
results	O
for	O
POS	O
tagging	O
and	O
dependency	O
parsing	O
are	O
reported	O
w.r.t	O
.	O

the	O
data	O
leakage	O
issue	O
.	O

For	O
example	O
,	O
annotating	O
a	O
corpus	O
with	O
POS	O
tagging	O
,	O
NER	O
and	O
dependency	O
parsing	O
can	O
be	O
performed	O
by	O
using	O
a	O
simple	O
command	O
as	O
in	O
Figure	O
2.Assume	O
that	O
the	O
input	O
file	O
"	O
input.txt	O
"	O
in	O
Figure	O
2	O
contains	O
a	O
sentence	O
"	O
Tôi	O
đang	O
làm_việc	O
tại	O
python3	O
run_phonlp.py	O
--save_dir	O
./pretrained_phonlp	O
--mode	O
annotate	O
--input_file	O
input.txt	O
--output_file	O
output.txt	O
CH	O
O	O
3	O
punct	O
Table	O
3	O
shows	O
the	O
annotated	O
output	O
in	O
plain	O
text	O
form	O
for	O
this	O
sentence	O
.	O

The	O
dataset	O
construction	O
includes	O
work	O
on	O
retrieval	O
techniques	O
and	O
similarity	O
measurements	O
to	O
ensure	O
a	O
unique	O
set	O
of	O
claims	O
.	O

1	O
https://panacea2020.github.io/	O
https://doi.org/10.5281/zenodo.6493847	O
critically	O
analysing	O
claims	O
about	O
health	O
care	O
interventions	O
;	O
PUBHEALTH	O
(	O
Kotonya	O
and	O
Toni	O
,	O
2020	O
)	O
with	O
11,832	O
claims	O
related	O
to	O
health	O
topics	O
;	O
FEVER	O
(	O
Thorne	O
et	O
al	O
.	O
,	O

In	O
the	O
first	O
part	O
of	O
our	O
work	O
,	O
we	O
contribute	O
to	O
the	O
global	O
effort	O
on	O
addressing	O
misinformation	O
in	O
the	O
context	O
of	O
COVID-19	O
by	O
creating	O
a	O
dataset	O
for	O
PANdemic	O
Ai	O
Claim	O
vEracity	O
Assessment	O
,	O
called	O
the	O
PANACEA	O
dataset	O
.	O

To	O
this	O
effect	O
our	O
dataset	O
brings	O
together	O
a	O
heterogeneous	O
set	O
of	O
True	O
and	O
False	O
COVID	O
claims	O
and	O
online	O
sources	O
of	O
information	O
for	O
each	O
claim	O
.	O

The	O
collected	O
claims	O
have	O
been	O
obtained	O
from	O
online	O
fact	O
-	O
checking	O
sources	O
,	O
existing	O
datasets	O
and	O
research	O
challenges	O
.	O

Finally	O
,	O
the	O
homogenisation	O
of	O
datasets	O
and	O
information	O
media	O
has	O
presented	O
an	O
additional	O
challenge	O
,	O
since	O
fact	O
-	O
checkers	O
use	O
different	O
criteria	O
for	O
labelling	O
the	O
claims	O
,	O
requiring	O
a	O
specific	O
review	O
of	O
the	O
different	O
kinds	O
of	O
labels	O
in	O
order	O
to	O
combine	O
them	O
.	O

2019	O
)	O
with	O
713,534	O
articles	O
from	O
194	O
news	O
outlets	O
;	O
FakeHealth	O
(	O
Dai	O
et	O
al	O
.	O
,	O

Other	O
authors	O
considered	O
multiple	O
pieces	O
of	O
evidence	O
together	O
(	O
Thorne	O
et	O
al	O
.	O
,	O

Many	O
of	O
these	O
authors	O
have	O
centred	O
their	O
techniques	O
on	O
the	O
use	O
of	O
NLI	O
(	O
Chen	O
et	O
al	O
.	O
,	O

Our	O
aim	O
is	O
to	O
have	O
the	O
largest	O
compilation	O
of	O
non	O
-	O
overlapping	O
,	O
labelled	O
and	O
verified	O
claims	O
from	O
different	O
media	O
and	O
information	O
domains	O
(	O
Twitter	O
,	O
Facebook	O
,	O
general	O
websites	O
,	O
academia	O
)	O
,	O
and	O
used	O
for	O
different	O
applications	O
(	O
media	O
reporting	O
,	O
veracity	O
evaluation	O
,	O
information	O
retrieval	O
challenges	O
,	O
etc	O
.	O
)	O
.	O

We	O
have	O
included	O
any	O
large	O
dataset	O
or	O
media	O
,	O
to	O
our	O
knowledge	O
,	O
related	O
Published	O
by	O
Poynter	O
,	O
this	O
online	O
source	O
combines	O
factchecking	O
articles	O
from	O
more	O
than	O
100	O
fact	O
-	O
checkers	O
from	O
all	O
over	O
the	O
world	O
,	O
being	O
the	O
largest	O
journalist	O
factchecking	O
collaboration	O
on	O
the	O
topic	O
worldwide	O
.	O

to	O
that	O
objective	O
that	O
includes	O
claims	O
together	O
with	O
their	O
information	O
sources	O
.	O

1995;Crestani	O
et	O
al	O
.	O
,	O

The	O
LARGE	O
dataset	O
excludes	O
claims	O
with	O
a	O
90	O
%	O
probability	O
of	O
being	O
similar	O
,	O
while	O
in	O
the	O
SMALL	O
dataset	O
the	O
probability	O
is	O
increased	O
to	O
99	O
%	O
,	O
as	O
obtained	O
through	O
the	O
MonoT5	O
model	O
.	O

We	O
compared	O
each	O
claim	O
with	O
all	O
the	O
other	O
claims	O
in	O
the	O
dataset	O
and	O
kept	O
the	O
score	O
of	O
the	O
most	O
similar	O
match	O
.	O

Claim	O
2	O
:	O
COVID-19	O
hitting	O
some	O
African	O
American	O
communities	O
harder	O
.	O

To	O
illustrate	O
the	O
difference	O
between	O
the	O
two	O
versions	O
of	O
the	O
dataset	O
,	O
we	O
present	O
some	O
examples	O
of	O
claims	O
in	O
Table	O
2	O
.	O

For	O
Claim	O
1	O
,	O
the	O
semantically	O
similar	O
claim	O
'	O
Loss	O
of	O
smell	O
may	O
suggest	O
milder	O
COVID-19	O
'	O
is	O
identified	O
and	O
excluded	O
from	O
both	O
LARGE	O
and	O
SMALL	O
datasets	O
.	O

But	O
the	O
claim	O
'	O
Loss	O
of	O
smell	O
and	O
taste	O
validated	O
as	O
COVID-19	O
symptoms	O
in	O
patients	O
with	O
high	O
recovery	O
rate	O
'	O
,	O
which	O
includes	O
mentions	O
of	O
another	O
symptom	O
and	O
the	O
recovery	O
rate	O
,	O
is	O
only	O
excluded	O
from	O
the	O
SMALL	O
dataset	O
.	O

Our	O
final	O
dataset	O
statistics	O
are	O
shown	O
in	O
the	O
lower	O
part	O
of	O
Table	O
3	O
,	O
where	O
the	O
original	O
and	O
the	O
two	O
reduced	O
versions	O
are	O
presented	O
.	O

The	O
labels	O
are	O
:	O
False	O
,	O
and	O
True	O
.	O

Given	O
a	O
claim	O
,	O
we	O
first	O
retrieve	O
the	O
most	O
relevant	O
documents	O
from	O
COVID-19	O
related	O
sources	O
and	O
then	O
further	O
retrieve	O
the	O
top	O
N	O
most	O
relevant	O
sentences	O
.	O

These	O
information	O
sources	O
represent	O
a	O
real	O
-	O
world	O
comprehensive	O
database	O
about	O
COVID-19	O
that	O
can	O
be	O
used	O
as	O
a	O
primary	O
source	O
of	O
information	O
on	O
the	O
pandemic	O
.	O

Additionally	O
,	O
we	O
tested	O
the	O
effect	O
of	O
multistage	O
retrieval	O
by	O
re	O
-	O
ranking	O
the	O
initial	O
results	O
using	O
MonoBERT	O
(	O
Nogueira	O
et	O
al	O
.	O
,	O

The	O
similarity	O
is	O
obtained	O
with	O
the	O
pre	O
-	O
trained	O
model	O
MiniLM	O
-	O
L12	O
-	O
v2	O
(	O
Wang	O
et	O
al	O
.	O
,	O

For	O
each	O
of	O
the	O
most	O
similar	O
sentences	O
(	O
pieces	O
of	O
evidence	O
)	O
retrieved	O
for	O
a	O
claim	O
,	O
we	O
apply	O
the	O
pretrained	O
NLI	O
model	O
RoBERTa	O
-	O
large	O
-	O
MNLI	O
8	O
.	O

The	O
model	O
is	O
trained	O
on	O
the	O
Multi	O
-	O
Genre	O
Natural	O
Language	O
Inference	O
(	O
MultiNLI	O
)	O
dataset	O
(	O
Williams	O
et	O
al	O
.	O
,	O

2018	O
)	O
.	O

Each	O
pair	O
(	O
c	O
,	O
e	O
i	O
)	O
is	O
fed	O
into	O
a	O
RoBERTa	O
-	O
large	O
8	O
model	O
,	O
and	O
the	O
last	O
hidden	O
layer	O
output	O
S	O
i	O
is	O
used	O
as	O
its	O
representation	O
.	O

The	O
representation	O
dimensionality	O
is	O
d	O
K	O
=	O
d	O
V	O
=	O
d	O
Q	O
=	O
1024	O
.	O

The	O
attention	O
function	O
is	O
defined	O
as	O
:	O
Att(Q	O
,	O
K	O
,	O
V	O
)	O
=	O
softmax(QK	O
/	O
√	O
d)V	O
(	O
2)While	O
standard	O
attention	O
mechanisms	O
use	O
only	O
the	O
sentence	O
representation	O
information	O
for	O
the	O
Key	O
,	O
Value	O
and	O
Query	O
,	O
here	O
the	O
inference	O
information	O
is	O
used	O
in	O
the	O
Query	O
.	O

This	O
attention	O
mechanism	O
is	O
applied	O
to	O
each	O
of	O
the	O
claim	O
-	O
evidence	O
pairs	O
,	O
and	O
the	O
outputs	O
are	O
concatenated	O
into	O
an	O
output	O
O	O
SAN	O
that	O
is	O
passed	O
through	O
a	O
Multi	O
-	O
Layer	O
Perceptron	O
(	O
MLP	O
)	O
with	O
hidden	O
size	O
d	O
h	O
and	O
a	O
Softmax	O
layer	O
to	O
generate	O
the	O
veracity	O
classification	O
output.y	O
=	O
softmax(MLP	O
ReLU	O
(	O
O	O
SAN	O
)	O
)	O
(	O
3)NLI	O
-	O
graph	O
.	O

C	O
i	O
=	O
RoBERTa(c	O
)	O
;	O
E	O
i	O
=	O
RoBERTa(e	O
i	O
)	O
(	O
4	O
)	O
I	O
i	O
=	O
RoBERTa	O
NLI	O
(	O
c	O
,	O
e	O
i	O
)	O
(	O
5)Next	O
,	O
we	O
build	O
an	O
evidence	O
network	O
in	O
which	O
the	O
central	O
node	O
is	O
the	O
claim	O
and	O
the	O
rest	O
of	O
the	O
nodes	O
are	O
the	O
evidence	O
.	O

The	O
network	O
is	O
implemented	O
with	O
the	O
package	O
PyTorch	O
Geometric	O
(	O
Fey	O
and	O
Lenssen	O
,	O
2019	O
)	O
,	O
using	O
in	O
the	O
first	O
layer	O
the	O
GCNConv	O
operator	O
(	O
Kipf	O
and	O
Welling	O
,	O
2016	O
)	O
with	O
50	O
output	O
channels	O
and	O
self	O
-	O
loops	O
to	O
the	O
nodes	O
,	O
represented	O
by	O
:X	O
=D	O
−1/2ÂD−1/2	O
XW	O
,	O
(	O
6)where	O
X	O
is	O
the	O
matrix	O
of	O
node	O
feature	O
vectors	O
,	O
Â	O
=	O
A	O
+	O
I	O
denotes	O
the	O
adjacency	O
matrix	O
with	O
inserted	O
self	O
-	O
loops	O
,	O
D	O
ii	O
=	O
j=0Â	O
ij	O
its	O
diagonal	O
degree	O
matrix	O
,	O
and	O
W	O
is	O
a	O
trainable	O
weight	O
matrix	O
.	O

We	O
downloaded	O
the	O
websites	O
used	O
as	O
fact	O
-	O
checking	O
sources	O
of	O
false	O
claims	O
and	O
the	O
websites	O
where	O
correct	O
information	O
on	O
true	O
claims	O
is	O
gathered	O
from	O
68	O
different	O
domains	O
.	O

Note	O
that	O
in	O
our	O
subsequent	O
experiments	O
,	O
we	O
have	O
excluded	O
all	O
fact	O
-	O
checking	O
websites	O
to	O
avoid	O
finding	O
directly	O
the	O
claim	O
references	O
.	O

We	O
can	O
see	O
that	O
by	O
using	O
BM25	O
,	O
it	O
is	O
possible	O
in	O
many	O
cases	O
to	O
retrieve	O
the	O
relevant	O
results	O
at	O
the	O
very	O
top	O
of	O
our	O
searches	O
.	O

The	O
retrieval	O
procedure	O
is	O
as	O
in	O
sections	O
4.1	O
and	O
4.2	O
.	O

2019	O
)	O
andKGAT	O
10	O
(	O
Liu	O
et	O
al	O
.	O
,	O

By	O
using	O
the	O
NLI	O
information	O
alone	O
it	O
is	O
possible	O
to	O
obtain	O
reasonable	O
results	O
for	O
the	O
True	O
claims	O
,	O
however	O
,	O
this	O
is	O
not	O
the	O
case	O
for	O
the	O
most	O
relevant	O
False	O
claims	O
.	O

Our	O
method	O
performs	O
on	O
a	O
par	O
with	O
KGAT	O
,	O
while	O
being	O
simpler	O
,	O
and	O
outperforms	O
GEAR.Complementing	O
the	O
results	O
for	O
the	O
SMALL	O
dataset	O
,	O
Table	O
7	O
presents	O
the	O
results	O
for	O
the	O
LARGE	O
dataset	O
.	O

This	O
is	O
challenging	O
as	O
it	O
is	O
easy	O
to	O
retrieve	O
information	O
that	O
omits	O
relevant	O
nuances	O
.	O

E.g.	O
The	O
claim	O
"	O
Barron	O
Trump	O
had	O
COVID-19	O
,	O
Melania	O
Trump	O
says	O
"	O
retrieves	O
sentences	O
such	O
as	O
"	O
Rudy	O
Giuliani	O
has	O
tested	O
positive	O
for	O
COVID-19	O
,	O
Trump	O
says	O
.	O
"	O

E.g.	O
The	O
claim	O
"	O
Vice	O
President	O
of	O
Bharat	O
Biotech	O
got	O
a	O
shot	O
of	O
the	O
indigenous	O
COV	O
-	O
AXIN	O
vaccine	O
"	O
retrieves	O
correct	O
documents	O
on	O
the	O
issue	O
.	O

Despite	O
being	O
similar	O
such	O
retrieved	O
sentences	O
give	O
no	O
information	O
about	O
the	O
claimed	O
situation	O
.	O

contains	O
the	O
essential	O
information	O
to	O
debunk	O
the	O
original	O
claim	O
but	O
is	O
missed	O
by	O
the	O
sentence	O
retrieval	O
engine	O
as	O
it	O
is	O
very	O
different	O
from	O
the	O
claim	O
(	O
See	O
Table	O
A1	O
in	O
Appendix	O
C	O
for	O
other	O
examples).Such	O
cases	O
are	O
more	O
difficult	O
to	O
deal	O
with	O
,	O
as	O
the	O
similarity	O
between	O
claim	O
and	O
evidence	O
is	O
certainly	O
a	O
good	O
indicator	O
of	O
relevance	O
.	O

Nevertheless	O
,	O
these	O
cases	O
are	O
very	O
interesting	O
for	O
future	O
work	O
using	O
more	O
complex	O
approaches	O
.	O

The	O
multilingual	O
dataset	O
(	O
Li	O
et	O
al	O
.	O
,	O

The	O
claims	O
used	O
in	O
the	O
track	O
were	O
obtained	O
and	O
reformulated	O
manually	O
by	O
us	O
as	O
affirmative	O
claims	O
(	O
e.g.	O
,	O
"	O
Can	O
vitamin	O
D	O
cure	O
COVID-19	O
?	O
"	O
was	O
changed	O
to	O
"	O
Vitamin	O
D	O
cures	O
COVID-19	O
"	O
)	O
for	O
consistency	O
with	O
the	O
rest	O
of	O
the	O
data	O
sources	O
and	O
to	O
allow	O
claim	O
veracity	O
assessment	O
.	O

These	O
entities	O
have	O
been	O
detected	O
using	O
a	O
RoBERTa	O
base	O
English	O
model	O
trained	O
on	O
the	O
OntoNotes	O
Release	O
5.0	O
dataset	B-Doc-Begin
(	O
Weischedel	O
et	O
al	O
.	O
,	O

The	O
inputs	O
are	O
padded	O
and	O
truncated	O
to	O
the	O
longest	O
sequence	O
,	O
and	O
a	O
ReLU	B-MetricValue
function	O
is	O
used	O
as	O
the	O
activation	B-MetricName
function	I-MetricName
for	O
the	O
hidden	O
layer	O
.	O

The	O
GCNConv	O
outputs	O
are	O
padded	O
to	O
the	O
longest	O
graph	O
size	O
.	O

The	O
size	B-MetricName
of	I-MetricName
the	I-MetricName
hidden	I-MetricName
layer	I-MetricName
is	O
50	B-MetricValue
,	O
the	O
batch	B-MetricName
size	I-MetricName
is	O
30	B-MetricValue
,	O
and	O
the	O
training	O
is	O
performed	O
for	O
100	B-MetricValue
epochs	B-MetricName
for	O
NLI	O
-	O
SAN	O
and	O
its	O
variants	O
,	O
and	O
200	B-MetricValue
epochs	B-MetricName
for	O
NLI	O
-	O
graph	O
and	O
its	O
variants	O
.	O

The	O
optimizer	B-MetricName
used	O
is	O
AdamW	B-MetricValue
(	O
Loshchilov	O
and	O
Hutter	O
,	O
2019	O
)	O
with	O
β	B-MetricName
1	I-MetricName
=	O
0.9	B-MetricValue
,	O
β	B-MetricName
2	I-MetricName
=	O
0.999	B-MetricValue
,	O
a	O
weight	B-MetricName
decay	I-MetricName
of	O
0.01	B-MetricValue
,	O
and	O
a	O
learning	B-MetricName
rate	I-MetricName
of	O
10	B-MetricValue
−2	I-MetricValue
for	O
NLI	B-TaskName
,	O
10	B-MetricValue
−4	I-MetricValue
for	O
NLI+Sent	B-TaskName
and	O
NLI	B-TaskName
-	I-TaskName
SAN	I-TaskName
,	O
and	O
Claim	O
Description	O
"	O
Sugar	O
causes	O
a	O
cytokine	O
storm	O
in	O
the	O
lungs	O
that	O
promotes	O
COVID-19	O
"	O
Retrieved	O
documents	O
are	O
relating	O
COVID	O
and	O
its	O
cytokine	O
storm	O
effects	O
,	O
but	O
without	O
the	O
specific	O
mention	O
of	O
sugar	O
,	O
which	O
does	O
not	O
cause	O
a	O
cytokine	O
storm	O
.	O
"	O

with	O
a	O
similar	O
structure	O
and	O
mentions	O
but	O
mistaking	O
the	O
family	O
members	O
and	O
missing	O
the	O
key	O
name	O
.	B-Doc-Begin

However	O
,	O
being	O
similar	O
they	O
give	O
no	O
information	O
about	O
the	O
claimed	O
situation	O
.	O

However	O
,	O
sentences	O
such	O
as	O
"	O
The	O
study	O
authors	O
cautioned	O
health	O
care	O
workers	O
against	O
trying	O
to	O
clean	O
masks	O
this	O
way	O
.	B-Doc-Begin

Microwaves	O
melted	O
the	O
masks	O
,	O
making	O
them	O
useless	O
.	O
"	O

a	O
learning	B-MetricName
rates	I-MetricName
of	O
10	B-MetricValue
−4	I-MetricValue
for	O
NLI	B-TaskName
-	I-TaskName
graph	I-TaskName
,	O
10	B-MetricValue
−3	I-MetricValue
for	O
NLI	B-TaskName
-	I-TaskName
graph	I-TaskName
−abl	I-TaskName
,	O
and	O
10	B-MetricValue
−5	I-MetricValue
for	O
NLI+PSent	B-TaskName
,	O
these	O
last	O
three	O
with	O
a	O
step	B-MetricName
size	I-MetricName
of	O
0.1	B-MetricValue
after	O
100	B-MetricValue
epochs	B-MetricName
.	B-Doc-Begin

Table	O
A1	O
presents	O
in	O
more	O
detail	O
the	O
cases	O
previously	O
mentioned	O
,	O
and	O
includes	O
new	O
examples	O
.	O

Here	O
we	O
present	O
detailed	O
information	O
of	O
the	O
data	O
sources	O
introduced	O
in	O
section	O
3.1.It	O
is	O
worth	O
noting	O
that	O
for	O
the	O
construction	O
of	O
our	O
dataset	O
,	O
we	O
have	O
only	O
included	O
sources	O
or	O
datasets	O
that	O
contain	O
explicit	O
veracity	O
labels	O
of	O
specific	O
claims	O
,	O
thus	O
we	O
have	O
not	O
included	O
collections	O
of	O
tweets	O
related	O
to	O
COVID	O
that	O
do	O
not	O
have	O
veracity	O
labels	O
(	O
Chen	O
et	O
al	O
.	O
,	O

Unlike	O
most	O
other	O
datasets	O
,	O
it	O
contains	O
a	O
wide	O
selection	O
of	O
true	O
claims	O
.	O

2015;Vaswani	O
et	O
al	O
.	O
,	O

2017;Aharoni	O
et	O
al	O
.	O
,	O

2019;Zhang	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

An	O
alternative	O
solution	O
is	O
to	O
design	O
language	O
-	O
aware	O
components	O
,	O
such	O
as	O
division	O
of	O
the	O
hidden	O
cells	O
into	O
shared	O
and	O
language	O
-	O
dependent	O
ones	O
(	O
Wang	O
et	O
al	O
.	O
,	O

2018	O
)	O
,	O
adaptation	O
layers	O
Philip	O
et	O
al	O
.	O
,	O

2020	O
)	O
,	O
language	O
-	O
aware	O
layer	O
normalization	O
and	O
linear	O
transformation	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

In	O
this	O
way	O
,	O
multilingual	O
NMT	O
can	O
model	O
language	O
specific	O
and	O
language	O
universal	O
features	O
for	O
each	O
language	O
pair	O
in	O
one	O
single	O
model	O
without	O
interference	O
.	O

LaSS	O
alleviates	O
parameter	O
interference	O
,	O
potentially	O
improving	O
the	O
model	O
capacity	O
and	O
boosting	O
performance	O
.	O
•	O

LaSS	O
shows	O
its	O
strong	O
generalization	O
performance	O
at	O
easy	O
adaptation	O
to	O
new	O
language	O
pairs	O
and	O
zero	O
-	O
shot	O
translation	O
.	O

Besides	O
,	O
LaSS	O
can	O
boost	O
zero	O
-	O
shot	O
translation	O
by	O
up	O
to	O
26.5	O
BLEU	O
.	O

The	O
standard	O
multilingual	O
NMT	O
model	O
uses	O
a	O
shared	O
encoder	O
and	O
a	O
shared	O
decoder	O
for	O
different	O
languages	O
(	O
Johnson	O
et	O
al	O
.	O
,	O

Li	O
et	O
al.(2020	O
)	O
uses	O
a	O
binary	O
conditional	O
latent	O
variable	O
to	O
decide	O
which	O
language	O
each	O
layer	O
belongs	O
to	O
.	O

Evci	O
et	O
al	O
.	O
(	O

2020	O
)	O
iteratively	O
redistribute	O
the	O
sparse	O
network	O
architecture	O
by	O
the	O
gradient	O
.	O

We	O
describe	O
LaSS	O
method	O
in	O
this	O
section	O
.	O

The	O
goal	O
is	O
to	O
learn	O
a	O
single	O
unified	O
model	O
for	O
many	O
translation	O
directions	O
.	O

L	O
=	O
i	O
x	O
,	O
y	O
∼Ds	O
i	O
→t	O
i	O
−	O
log	O
P	O
θ	O
(	O
y	O
|	O
x	O
)	O
(	O
1)where	O
x	O
,	O
y	O
is	O
a	O
sentence	O
pair	O
from	O
the	O
language	O
s	O
i	O
to	O
t	O
i	O
,	O
and	O
θ	O
is	O
the	O
model	O
parameter	O
.	O

2017	O
)	O
.	O

Such	O
sub	O
-	O
network	O
is	O
specific	O
to	O
each	O
language	O
pair	O
.	O

Then	O
the	O
parameters	O
associated	O
withs	O
i	O
→	O
t	O
i	O
is	O
θ	O
s	O
i	O
→t	O
i	O
=	O
{	O
θ	O
j	O
0	O
|	O
M	O
j	O
s	O
i	O
→t	O
i	O
=	O
1	O
}	O
,	O
where	O
j	O
denotes	O
the	O
jth	O
element	O
in	O
θ	O
0	O
.	O

We	O
intend	O
to	O
find	O
such	O
language	O
specific	O
sub	O
-	O
networks	O
.	O

Figure	O
1	O
illustrates	O
the	O
original	O
model	O
and	O
its	O
language	O
specific	O
sub	O
-	O
networks	O
.	O

During	O
the	O
back	O
-	O
propagation	O
step	O
,	O
we	O
only	O
update	O
the	O
parameters	O
in	O
θ	O
0	O
belonging	O
to	O
the	O
sub	O
-	O
network	O
indicated	O
by	O
M	O
s	O
i	O
→t	O
i	O
.	O

For	O
every	O
given	O
input	O
sentence	O
in	O
language	O
s	O
and	O
a	O
target	O
language	O
t	O
,	O
the	O
forward	O
inference	O
step	O
only	O
uses	O
the	O
parameter	O
θ	O
*	O
M	O
s→t	O
to	O
calculate	O
model	O
output	O
.	B-Doc-Begin

The	O
experiments	O
are	O
conducted	O
on	O
IWSLT	B-DatasetName
and	O
WMT	B-DatasetName
benchmarks	O
.	O

For	O
IWSLT	B-DatasetName
,	O
we	O
collect	O
8	O
English	O
-	O
centric	O
language	O
pairs	O
from	O
IWSLT2014	B-DatasetName
,	O
whose	O
size	O
ranges	O
from	O
89k	O
to	O
169k	O
.	O

We	O
apply	O
byte	O
pair	O
encoding	O
(	O
BPE	O
)	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O

For	O
zeroshot	O
,	O
where	O
standard	O
testsets	O
(	O
for	O
example	O
,	O
Fr→Zh	O
)	O
of	O
some	O
language	O
pairs	O
are	O
not	O
available	O
,	O
we	O
use	O
OPUS-100	B-DatasetName
(	O
Zhang	O
et	O
al	O
.	O
,	O

We	O
report	O
tokenized	O
BLEU	B-MetricName
,	O
as	O
well	O
as	O
win	B-MetricName
ratio	I-MetricName
(	O
WR	B-MetricName
)	O
,	O
informing	O
the	O
proportion	O
of	O
language	O
pairs	O
we	O
outperform	O
the	O
baseline	O
.	O

In	O
zero	B-TaskName
-	I-TaskName
shot	I-TaskName
translation	I-TaskName
,	O
we	O
also	O
report	O
translation	B-MetricName
-	I-MetricName
language	I-MetricName
accuracy	I-MetricName
1	O
,	O
which	O
is	O
commonly	O
used	O
to	O
measure	O
the	O
accuracy	B-MetricName
of	O
translating	O
into	O
the	O
right	O
target	O
language	O
.	O

Model	O
Settings	O
Considering	O
the	O
diversity	O
of	O
dataset	O
volume	O
,	O
we	O
perform	O
our	O
experiments	O
with	O
variants	O
of	O
Transformer	O
architecture	O
.	O

For	O
IWSLT	B-DatasetName
,	O
we	O
adopt	O
a	O
smaller	O
Transformer	O
(	O
Transformersmall	O
2	O
Transformer	O
-	O
base	O
and	O
Transformer	O
-	O
big	O
3	O
.	O

The	O
pruning	B-HyperparameterName
rate	I-HyperparameterName
α	I-HyperparameterName
of	O
IWSLT	B-DatasetName
and	O
WMT	B-DatasetName
is	O
0.7	B-HyperparameterValue
and	O
0.3	B-HyperparameterValue
,	O
respectively	O
.	O

For	O
simplicity	O
,	O
we	O
only	O
report	O
the	O
highest	O
BLEU	B-MetricName
from	O
the	O
best	O
pruning	O
rate	O
and	O
we	O
also	O
discuss	O
the	O
impact	O
of	O
different	O
pruning	O
rate	O
on	O
performance	O
in	O
Sec.6	O
.	B-Doc-Begin

In	O
Sec	O
.	O

Firstly	O
,	O
we	O
show	O
that	O
LaSS	O
obtains	O
consistent	O
performance	O
gains	O
on	O
IWSLT	O
and	O
WMT	O
datasets	O
with	O
different	O
Transformer	O
architecture	O
variants	O
.	O

Finally	O
,	O
we	O
observe	O
that	O
LaSS	O
can	O
even	O
improve	O
zero	O
-	O
shot	O
translation	O
,	O
obtaining	O
performance	O
gains	O
by	O
up	O
to	O
26.5	O
BLEU	O
.	O

This	O
phenomenon	O
is	O
intuitive	O
since	O
rich	O
resource	O
dataset	O
suffers	O
more	O
parameter	O
interference	O
than	O
low	O
resource	O
dataset	O
.	O

We	O
also	O
find	O
that	O
the	O
BLEU	O
and	O
WR	O
gains	O
obtained	O
in	O
Transformer	O
-	O
base	O
are	O
larger	O
than	O
that	O
in	O
Transformer	O
-	O
large	O
.	O

A	O
natural	O
question	O
arises	O
that	O
can	O
LaSS	O
adapt	O
to	O
a	O
new	O
language	O
or	O
language	O
pair	O
that	O
it	O
has	O
not	O
seen	O
in	O
training	O
phase	O
?	O
In	O
other	O
words	O
,	O
can	O
LaSS	O
generalize	O
to	O
other	O
language	O
pairs	O
?	O
In	O
this	O
section	O
,	O
we	O
show	O
the	O
generalization	O
of	O
LaSS	O
in	O
two	O
settings	O
.	O

We	O
firstly	O
show	O
that	O
LaSS	O
can	O
easily	O
adapt	O
to	O
new	O
unseen	O
languages	O
to	O
match	O
bilingual	O
models	O
with	O
training	O
for	O
only	O
a	O
few	O
hundred	O
steps	O
while	O
keeping	O
the	O
performance	O
of	O
the	O
existing	O
language	O
pairs	O
hardly	O
dropping	O
.	O

2017	O
that	O
LaSS	O
can	O
also	O
easily	O
adapt	O
to	O
new	O
unseen	O
languages	O
without	O
dramatic	O
drop	O
for	O
other	O
existing	O
languages	O
.	O

We	O
verify	O
the	O
extensibility	O
of	O
LaSS	O
on	O
4	O
language	O
pairs	O
.	O

For	O
both	O
multilingual	O
baseline	O
and	O
our	O
method	O
,	O
we	O
train	O
on	O
only	O
the	O
specific	O
language	O
pair	O
for	O
fixed	O
steps	O
.	O

Another	O
benefit	O
of	O
updating	O
corresponding	O
parameters	O
is	O
its	O
fast	O
adaptation	O
towards	O
specific	O
language	O
pairs	O
.	O

One	O
of	O
the	O
biggest	O
challenges	O
is	O
the	O
offtarget	O
issue	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

In	O
previous	O
experiments	O
,	O
we	O
apply	O
specific	O
masks	O
to	O
their	O
corresponding	O
language	O
pairs	O
.	O

As	O
the	O
training	O
dataset	O
is	O
English	O
-	O
centric	O
,	O
non	O
-	O
Englishcentric	O
masks	O
are	O
not	O
available	O
.	O

The	O
mask	O
similarity	O
is	O
positively	O
correlated	O
to	O
the	O
language	O
family	O
similarity.decoder	O
mask	O
of	O
En→Y.	O
We	O
select	O
6	O
languages	O
and	O
evaluate	O
zero	O
-	O
shot	O
translation	O
in	O
language	O
pairs	O
between	O
each	O
other	O
.	O

For	O
readers	O
not	O
familiar	O
with	O
language	O
family	O
and	O
clustering	O
,	O
Figure	O
4	O
is	O
the	O
hierarchical	O
clustering	O
according	O
to	O
language	O
family	O
.	O

Ideally	O
,	O
similar	O
languages	O
should	O
share	O
more	O
parameters	O
since	O
they	O
share	O
more	O
language	O
characteristics	O
.	O

We	O
define	O
mask	O
similarity	O
as	O
the	O
number	O
of	O
1	O
where	O
two	O
masks	O
share	O
divided	O
by	O
the	O
number	O
of	O
1	O
of	O
the	O
first	O
mask	O
:	O
Sim(M	O
1	O
,	O
M	O
2	O
)	O
=	O
M	O
1	O
∩	O
M	O
2	O
0	O
M	O
1	O
0	O
,	O
(	O
2)where	O
•	O
0	O
represent	O
L	O
0	O
norm	O
.	O

To	O
take	O
a	O
step	O
further	O
,	O
we	O
study	O
how	O
model	O
schedule	O
language	O
specific	O
capacity	O
across	O
layers	O
.	O

As	O
shown	O
in	O
Table	O
5	O
,	O
we	O
observe	O
that	O
replacing	O
the	O
encoder	O
mask	O
with	O
other	O
languages	O
causes	O
only	O
littler	O
performance	O
drop	O
,	O
while	O
replacing	O
the	O
decoder	O
mask	O
causes	O
dramatic	O
performance	O
drop	O
.	O

Further	O
,	O
LaSS	O
can	O
generalize	O
well	O
to	O
new	O
language	O
pairs	O
by	O
training	O
with	O
a	O
few	O
hundred	O
steps	O
,	O
while	O
keeping	O
the	O
performance	O
of	O
existing	O
language	O
pairs	O
.	O

Biao	O
Zhang	O
,	O
Philip	O
Williams	O
,	O
Ivan	O
Titov	O
,	O
and	O
Rico	O
Sennrich	O
.	O

Improving	O
massively	O
multilingual	O
neural	O
machine	O
translation	O
and	O
zero	O
-	O
shot	O
translation	O
.	O

In	O
Proceedings	O
of	O
the	O
58th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
,	O
pages	O
1628	O
-	O
1639	O
,	O
Online	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	B-Doc-Begin

After	O
that	O
we	O
train	O
the	O
LaSS	B-MethodName
model	O
with	O
the	O
obtained	O
masks	O
(	O
Phase	O
3	O
)	O
.	O

We	O
apply	O
label	B-MetricName
smoothing	I-MetricName
of	O
value	O
0.1	B-MetricValue
in	O
all	O
our	O
experiments	O
.	O

We	O
adopt	O
Transformer	O
-	O
small	O
4	O
with	O
dropout	B-MetricName
0.1.Data	B-MetricValue
Following	O
Tan	O
et	O
al	O
.	O
(	O

We	O
apply	O
over	O
-	O
sampling	O
with	O
a	O
temperature	O
of	O
T	B-MetricName
=	O
2.Training	B-MetricValue
For	O
Phase	O
1	O
,	O
we	O
train	O
the	O
baseline	O
with	O
Adam	B-MetricValue
with	O
a	O
learning	B-MetricName
rate	I-MetricName
schedule	I-MetricName
of	O
(	O
5e-4,4k	B-MetricValue
)	O
.	O

The	O
max	B-MetricName
tokens	I-MetricName
per	I-MetricName
batch	I-MetricName
is	O
set	O
to	O
262144	B-MetricValue
.	O

For	O
Phase	O
2	O
,	O
we	O
keep	O
all	O
other	O
settings	O
unchanged	O
except	O
we	O
set	O
the	O
max	B-MetricName
tokens	I-MetricName
to	O
be	O
16384	B-MetricValue
and	O
the	O
dropout	B-MetricName
0.3	B-MetricValue
.	O

We	O
replace	O
fixed	O
positional	O
embedding	O
with	O
learnable	O
one	O
and	O
replace	O
ReLU	B-MetricValue
with	O
GeLU	B-MetricValue
.	O

Since	O
the	O
WMT	B-DatasetName
dataset	O
is	O
highly	O
imbalanced	O
,	O
we	O
apply	O
a	O
temperature	O
-	O
based	O
sampling	O
strategy	O
with	O
T	B-MetricName
=	O
5	B-MetricValue
.	O

Training	O
For	O
Phase	O
1	O
,	O
we	O
train	O
the	O
baseline	O
with	O
Adam	B-MetricValue
with	O
a	O
learning	B-MetricName
rate	I-MetricName
schedule	I-MetricName
of	O
(	O
5e-4,8k	B-MetricValue
)	O
.	O

The	O
max	B-MetricName
tokens	I-MetricName
per	I-MetricName
batch	I-MetricName
is	O
set	O
to	O
524288	B-MetricValue
.	O

For	O
Phase	O
2	O
,	O
the	O
warm	B-MetricName
-	I-MetricName
up	I-MetricName
updates	I-MetricName
are	O
set	O
to	O
1000	B-MetricValue
.	O

Concretely	O
,	O
we	O
finetune	O
on	O
>	O
10k	O
,	O
>	O
100k	O
,	O
>	O
1	O
m	O
,	O
>	O
10	O
m	O
language	O
pairs	O
with	O
1k	B-MetricValue
,	O
2k	B-MetricValue
,	O
4k	B-MetricValue
,	O
8k	B-MetricValue
steps	B-MetricName
and	O
max	B-MetricName
tokens	I-MetricName
per	I-MetricName
batch	I-MetricName
with	O
20480	B-MetricValue
,	O
40960	B-MetricValue
,	O
81920	B-MetricValue
and	O
163840	B-MetricValue
.	O

For	B-Doc-Begin
Phase	O
3	O
,	O
we	O
keep	O
the	O
setting	O
the	O
same	O
as	O
Phase	O
1	O
.	O

At	O
each	O
step	O
,	O
it	O
attends	O
to	O
different	O
parts	O
of	O
the	O
question	O
,	O
computes	O
activated	O
scores	O
for	O
relations	O
,	O
and	O
then	O
transfer	O
the	O
previous	O
entity	O
scores	O
along	O
activated	O
relations	O
in	O
a	O
differentiable	O
way	O
.	O

2019	O
)	O
.	O

Along	O
with	O
the	O
fast	O
development	O
of	O
deep	O
learning	O
,	O
especially	O
the	O
pretraining	O
technology	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2019	O
)	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
have	O
been	O
shown	O
comparative	O
with	O
human	O
per	O
-	O
Figure	O
1	O
:	O
Answering	O
a	O
multi	O
-	O
hop	O
question	O
over	O
the	O
relation	O
graph	O
.	O

The	O
relations	O
are	O
constrained	O
predicates	O
in	O
the	O
label	O
form	O
(	O
i.e.	O
,	O
knowledge	O
graph	O
)	O
while	O
free	O
texts	O
in	O
the	O
text	O
form	O
.	O

Besides	O
,	O
they	O
are	O
mostly	O
proposed	O
for	O
the	O
label	O
form	O
.	O

The	O
second	O
strand	O
is	O
to	O
collect	O
evidences	O
by	O
using	O
graph	O
neural	O
networks	O
(	O
Sun	O
et	O
al	O
.	O
,	O

We	O
formulate	O
these	O
relation	O
scores	O
into	O
an	O
adjacent	O
matrix	O
,	O
where	O
each	O
entry	O
indicates	O
the	O
transfer	O
probability	O
of	O
an	O
entity	O
pair	O
.	O

2016	O
and	O
CompWebQ	O
(	O
Talmor	O
and	O
Berant	O
,	O
2018	O
)	O
.	O

On	O
WebQSP	O
and	O
CompWebQ	O
,	O
we	O
also	O
achieve	O
a	O
significant	O
improvement	O
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
.	O

2019b;Saxena	O
et	O
al	O
.	O
,	O

2018;Saha	O
et	O
al	O
.	O
,	O

2019	O
)	O
.	O

For	O
simple	O
questions	O
,	O
whose	O
answer	O
can	O
be	O
retrieved	O
directly	O
from	O
the	O
text	O
,	O
pretrained	O
models	O
(	O
Devlin	O
et	O
al	O
.	O
,	O

2018;Lan	O
et	O
al	O
.	O
,	O

The	O
latter	O
can	O
be	O
easily	O
extracted	O
from	O
large	O
-	O
scale	O
document	O
corpora	O
according	O
to	O
the	O
co	O
-	O
occurence	O
of	O
entity	O
pairs	O
.	O

To	O
infer	O
the	O
answer	O
of	O
a	O
multi	O
-	O
hop	O
question	O
,	O
Trans	O
-	O
ferNet	O
starts	O
from	O
the	O
topic	O
entity	O
and	O
jumps	O
for	O
T	O
steps	O
.	O

a	O
0	O
is	O
the	O
initial	O
scores	O
,	O
i.e.	O
,	O
only	O
the	O
topic	O
entity	O
e	O
x	O
gets	O
1	O
.	O

q	O
t	O
is	O
the	O
weighted	O
sum	O
of	O
h	O
i	O
.	O

We	O
will	O
have	O
different	O
implementations	O
of	O
g	O
for	O
the	O
label	O
form	O
and	O
the	O
text	O
form	O
,	O
which	O
will	O
be	O
introduced	O
in	O
Sec.3.5	O
.	O

Then	O
we	O
can	O
simulate	O
the	O
"	O
jumping	O
across	O
edges	O
"	O
as	O
the	O
following	O
formulation	O
:	O
a	O
t	O
=	O
a	O
t−1	O
W	O
t	O
.(3	O
)	O
Specifically	O
,	O
we	O
havea	O
t	O
j	O
=	O
n	O
i=1	O
a	O
t−1	O
i	O
×	O
W	O
t	O
i	O
,	O
j	O
.(4)It	O
means	O
that	O
the	O
production	O
of	O
entity	O
e	O
i	O
's	O
previous	O
score	O
and	O
the	O
edge	O
r	O
i	O
,	O
j	O
's	O
current	O
score	O
will	O
be	O
collected	O
into	O
e	O
j	O
's	O
current	O
score	O
.	O

We	O
propose	O
such	O
a	O
truncation	O
function	O
:	O
Trunc(a	O
)	O
=	O
a	O
/	O
z(a	O
)	O
,	O
z(a	O
)	O
=	O
a.detach	O
(	O
)	O
,	O
if	O
a	O
>	O
1	O
,	O
1	O
,	O
if	O
a	O
≤	O
1.(8)After	O
each	O
transfer	O
step	O
,	O
we	O
truncate	O
a	O
t	O
by	O
applying	O
this	O
function	O
to	O
each	O
of	O
its	O
elements	O
.	O

As	O
a	O
result	O
,	O
given	O
the	O
question	O
Where	O
was	O
Harry	O
Potter	O
published	O
,	O
TransferNet	O
will	O
produce	O
the	O
same	O
scores	O
for	O
United	O
Kingdom	O
and	O
1997	O
,	O
and	O
thus	O
use	O
1997	O
to	O
wrongly	O
answer	O
the	O
Where	O
-	O
question	O
.	O

We	O
predict	O
a	O
mask	O
score	O
for	O
each	O
entity	O
using	O
the	O
question	O
embedding	O
:	O
m	O
=	O
Sigmoid(MLP(q)),(9)where	O
m	O
∈	O
[	O
0	O
,	O
1	O
]	O
n	O
,	O
m	O
i	O
denotes	O
the	O
mask	O
score	O
of	O
entity	O
e	O
i	O
,	O
MLP	O
(	O
short	O
for	O
multi	O
-	O
layer	O
perceptron	O
)	O
projects	O
d	O
-	O
dimensional	O
feature	O
to	O
n	O
-	O
dimension	O
.	O

We	O
can	O
get	O
the	O
reverse	O
relations	O
by	O
exchanging	O
the	O
placeholders	O
of	O
subject	O
and	O
object	O
,	O
but	O
for	O
simplicity	O
,	O
we	O
do	O
not	O
show	O
them	O
in	O
the	O
figure	O
.	O

Let	O
r	O
i	O
,	O
j	O
=	O
{	O
r	O
i	O
,	O
j,1	O
,	O
•	O
•	O
•	O
,	O
r	O
i	O
,	O
j	O
,	O
b	O
}	O
and	O
r	O
i	O
,	O
j	O
,	O
k	O
denotes	O
the	O
k	O
-	O
th	O
relation	O
sentence	O
.	O

So	O
in	O
practice	O
,	O
we	O
select	O
a	O
subset	O
of	O
relations	O
at	O
each	O
step	O
.	O

2017	O
)	O
is	O
a	O
largescale	O
dataset	O
of	O
multi	O
-	O
hop	O
question	O
answering	O
over	O
knowledge	O
graph	O
,	O
which	O
extends	O
Wiki	O
-	O
Movies	O
(	O
Miller	O
et	O
al	O
.	O
,	O

WebQSP	O
(	O
Yih	O
et	O
al	O
.	O
,	O

2016	O
)	O
has	O
a	O
smaller	O
scale	O
of	O
questions	O
but	O
larger	O
scale	O
of	O
knowledge	O
graph	O
.	O

We	O
only	O
consider	O
the	O
label	O
form	O
of	O
WebQSP	O
due	O
to	O
its	O
huge	O
scale	O
.	O

On	O
average	O
,	O
there	O
are	O
1948	O
entities	O
in	O
each	O
subgraph	O
and	O
the	O
recall	O
is	O
64	O
%	O
.	O

2017	O
)	O
learns	O
the	O
reasoning	O
path	O
via	O
reinforcement	O
learning	O
.	O

Its	O
intermediate	O
results	O
have	O
a	O
good	O
interpretability	O
.	O

SRN	O
(	O
Qiu	O
et	O
al	O
.	O
,	O

2020	O
)	O
improves	O
VRN	O
by	O
beam	O
search	O
and	O
reward	O
shaping	O
strategy	O
,	O
boosting	O
its	O
speed	O
and	O
performance	O
.	O

2018	O
)	O
extracts	O
a	O
questionspecific	O
subgraph	O
from	O
the	O
entire	O
relation	O
graph	O
with	O
heuristics	O
,	O
and	O
then	O
uses	O
graph	O
neural	O
networks	O
to	O
infer	O
the	O
answer	O
.	O

2020	O
)	O
proposes	O
a	O
scalable	O
implementation	O
of	O
probability	O
transfer	O
over	O
largescale	O
knowledge	O
graph	O
of	O
label	O
form	O
.	O

2016	O
)	O
to	O
help	O
predict	O
the	O
answer	O
.	O

For	O
the	O
text	O
form	O
,	O
we	O
exchanged	O
the	O
placeholder	O
<	O
sub	O
>	O
and	O
<	O
obj	O
>	O
as	O
the	O
reversed	O
relation	O
,	O
e.g.	O
,	O
<	O
sub	O
>	O
co	O
-	O
founded	O
the	O
<	O
obj	O
>	O
is	O
converted	O
to	O
<	O
obj	O
>	O
cofounded	O
the	O
<	O
sub>.For	B-Doc-Begin
the	O
experiments	O
of	O
MetaQA	B-DatasetName
,	O
we	O
set	O
the	O
step	O
number	O
T	B-HyperparameterName
=	O
3	B-HyperparameterValue
.	O

We	O
used	O
bi	O
-	O
directional	O
GRU	O
(	O
Chung	O
et	O
al	O
.	O
,	O

2014	O
)	O
as	O
the	O
question	O
encoder	O
,	O
and	O
set	O
the	O
hidden	B-HyperparameterName
dimension	I-HyperparameterName
as	O
1024	B-HyperparameterValue
.	O

The	O
projecting	O
function	O
f	O
t	O
was	O
a	O
stack	O
of	O
linear	O
layer	O
and	O
Tanh	O
layer	O
.	O

The	O
threshold	O
τ	B-HyperparameterName
was	O
set	O
to	O
0.7	B-HyperparameterValue
and	O
ω	B-HyperparameterName
was	O
set	O
to	O
400	B-HyperparameterValue
.	O

Since	O
the	O
question	O
hop	O
is	O
provided	O
in	O
MetaQA	B-DatasetName
,	O
we	O
used	O
the	O
golden	O
hop	O
number	O
as	O
an	O
auxiliary	O
objective	O
to	O
help	O
learn	O
the	O
hop	O
distribution	O
c.	O
We	O
computed	O
the	O
cross	O
entropy	O
loss	O
and	O
added	O
it	O
into	O
Equation	O
7	O
after	O
multiplying	O
a	O
factor	O
of	O
0.01	O
.	O

The	O
model	O
was	O
optimized	O
using	O
RAdam	B-HyperparameterValue
(	O
Liu	O
et	O
al	O
.	O
